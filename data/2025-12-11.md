<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 35]
- [eess.SY](#eess.SY) [Total: 15]
- [econ.EM](#econ.EM) [Total: 3]
- [cs.AI](#cs.AI) [Total: 16]
- [math.OC](#math.OC) [Total: 18]
- [cs.LG](#cs.LG) [Total: 71]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.CY](#cs.CY) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models](https://arxiv.org/abs/2512.08943)
*Singon Kim*

Main category: cs.CL

TL;DR: 提出ACoRN方法，通过细粒度文档分类和两阶段训练增强抽象压缩模型对检索噪声的鲁棒性，提升RAG性能


<details>
  <summary>Details</summary>
Motivation: 现有抽象压缩模型在RAG中面临两个问题：1) 检索文档常包含与查询无关或事实错误的内容，尽管相关性评分高；2) 压缩模型在长上下文中容易遗漏重要信息，导致注意力分散

Method: 提出ACoRN方法：1) 对检索文档进行细粒度分类；2) 使用离线数据增强训练增强对两类检索噪声的鲁棒性；3) 微调模型生成围绕关键信息的摘要，直接支持正确答案

Result: 使用T5-large作为压缩器，ACoRN提升了EM和F1分数，同时保持答案字符串作为直接证据。在包含大量降低准确率文档的数据集上表现优异

Conclusion: ACoRN通过增强抽象压缩模型对检索噪声的鲁棒性，有效解决了RAG中压缩模型遗漏重要信息的问题，在现实场景中具有高度实用性

Abstract: Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However, retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language model based compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy reducing documents, making it highly useful in real-world scenarios.

</details>


### [2] [Enhancing Reliability across Short and Long-Form QA via Reinforcement Learning](https://arxiv.org/abs/2512.08944)
*Yudong Wang,Zhe Yang,Wenhan Ma,Zhifang Sui,Liang Zhao*

Main category: cs.CL

TL;DR: 提出一个针对性的强化学习框架，通过处理内在和外在幻觉来平衡大型语言模型的能力与可靠性，在问答任务中显著减少幻觉并提升性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然提升了大型语言模型的复杂推理能力，但也加剧了幻觉问题，形成了能力与可靠性之间的关键权衡。需要解决内在幻觉（不忠实于上下文）和外在幻觉（内部知识缺陷）的双重挑战。

Method: 1) 针对外在幻觉：从TriviaQA的开放式对话创建新颖训练集；2) 针对内在幻觉：利用FineWeb的长文本进行事实基础奖励方案；3) 增强可靠性：明确奖励模型拒绝回答不可回答的问题，培养谨慎性。

Result: 广泛的实验表明，该方法在多样化的基准测试套件中取得了显著的性能提升，大幅减少了两种类型的幻觉。

Conclusion: 本研究提供了一个实用的框架，解决了先进推理与事实可信度之间的关键矛盾，为构建更强大和可靠的大型语言模型铺平了道路。

Abstract: While reinforcement learning has unlocked unprecedented complex reasoning in large language models, it has also amplified their propensity for hallucination, creating a critical trade-off between capability and reliability. This work confronts this challenge by introducing a targeted RL framework designed to mitigate both intrinsic and extrinsic hallucinations across short and long-form question answering. We address extrinsic hallucinations (flawed internal knowledge) by creating a novel training set from open-ended conversions of TriviaQA. Concurrently, we tackle intrinsic hallucinations (unfaithfulness to context) by leveraging long-form texts from FineWeb in a fact-grounding reward scheme. To further bolster reliability, our framework explicitly rewards the model for refusing to answer unanswerable questions, thereby cultivating crucial cautiousness. Extensive experiments demonstrate that our methodology yields significant performance gains across a diverse suite of benchmarks, substantially reducing both hallucination types. Ultimately, this research contributes a practical framework for resolving the critical tension between advanced reasoning and factual trustworthiness, paving the way for more capable and reliable large language models.

</details>


### [3] [The Linguistic Architecture of Reflective Thought: Evaluation of a Large Language Model as a Tool to Isolate the Formal Structure of Mentalization](https://arxiv.org/abs/2512.08945)
*Stefano Epifani,Giuliano Castigliego,Laura Kecskemeti,Giuliano Razzicchia,Elisabeth Seiwald-Sonderegger*

Main category: cs.CL

TL;DR: LLM能够生成符合心理化治疗框架的语言结构，表现出高结构一致性和评分者间信度，但在情感表达和内外情境整合方面存在局限。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估大型语言模型能否再现心理化治疗框架下的语言结构，探索语言形式与心理表征之间的关系，特别是LLM生成反思性文本的能力是否能够符合临床心理化治疗的标准。

Method: 生成50个人类参与者与LLM的对话，由5名经过MBT培训的精神科医生在盲法条件下，按照MBT的四个维度评估心理化档案，使用李克特量表评分评估一致性、论证一致性和整体质量，并通过ICC(3,1)估计评分者间信度。

Result: 平均得分较高（3.63-3.98），标准差适中，表明生成的心理化档案具有高水平的结构一致性。ICC值（0.60-0.84）显示评分者间具有实质性到高度的一致性。模型在隐式-显式和自我-他人维度上表现更稳定，但在内部状态与外部情境整合方面存在局限，生成内容具有情感中立性。

Conclusion: LLM能够生成结构上符合心理化治疗框架的语言输出，具有临床可解释性，但在情感表达和情境整合方面存在局限性，这为理解LLM的心理表征能力和临床应用提供了重要参考。

Abstract: Background: Mentalization integrates cognitive, affective, and intersubjective components. Large Language Models (LLMs) display an increasing ability to generate reflective texts, raising questions regarding the relationship between linguistic form and mental representation. This study assesses the extent to which a single LLM can reproduce the linguistic structure of mentalization according to the parameters of Mentalization-Based Treatment (MBT).
  Methods: Fifty dialogues were generated between human participants and an LLM configured in standard mode. Five psychiatrists trained in MBT, working under blinded conditions, evaluated the mentalization profiles produced by the model along the four MBT axes, assigning Likert-scale scores for evaluative coherence, argumentative coherence, and global quality. Inter-rater agreement was estimated using ICC(3,1).
  Results: Mean scores (3.63-3.98) and moderate standard deviations indicate a high level of structural coherence in the generated profiles. ICC values (0.60-0.84) show substantial-to-high agreement among raters. The model proved more stable in the Implicit-Explicit and Self-Other dimensions, while presenting limitations in the integration of internal states and external contexts. The profiles were coherent and clinically interpretable yet characterized by affective neutrality.

</details>


### [4] [Luxical: High-Speed Lexical-Dense Text Embeddings](https://arxiv.org/abs/2512.09015)
*DatologyAI,:,Luke Merrick,Alex Fang,Aldo Carranza,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Darren Teh,David Schwab,Fan Pan,Haakon Mongstad,Haoli Yin,Jack Urbanek,Jason Lee,Jason Telanoff,Josh Wills,Kaleigh Mentzer,Paul Burstein,Parth Doshi,Paul Burnstein,Pratyush Maini,Ricardo Monti,Rishabh Adiga,Scott Loftin,Siddharth Joshi,Spandan Das,Tony Jiang,Vineeth Dorma,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.CL

TL;DR: Luxical是一个高速"词汇-稠密"文本嵌入库，结合TF-IDF稀疏特征和小型ReLU网络，通过知识蒸馏近似大型Transformer嵌入模型，在速度上比神经基线快3-100倍，同时保持质量。


<details>
  <summary>Details</summary>
Motivation: 当前文本组织工具存在速度与灵活性的权衡：词汇分类器（如FastText）速度快但功能有限，而Transformer文本嵌入模型灵活但计算成本高。需要一种既能保持高质量又能实现高速处理的解决方案。

Method: Luxical结合稀疏TF-IDF特征、小型ReLU网络和知识蒸馏训练方案，近似大型Transformer嵌入模型。通过这种"词汇-稠密"混合架构，在保持灵活性的同时大幅降低计算成本。

Result: 在网页抓取文档检索和语言模型数据整理任务中，Luxical相比不同规模的神经基线实现3-100倍速度提升，在数据整理任务中达到与FastText相当的推理速度，同时保持与神经基线相当的质量。

Conclusion: Luxical为大规模文本组织提供了有利的计算/质量权衡，结合了词汇方法和神经方法的优势，可作为开源软件使用，适用于需要高速文本处理的多种应用场景。

Abstract: Frontier language model quality increasingly hinges on our ability to organize web-scale text corpora for training. Today's dominant tools trade off speed and flexibility: lexical classifiers (e.g., FastText) are fast but limited to producing classification output scores, while the vector-valued outputs of transformer text embedding models flexibly support numerous workflows (e.g., clustering, classification, and retrieval) but are computationally expensive to produce. We introduce Luxical, a library for high-speed "lexical-dense" text embeddings that aims to recover the best properties of both approaches for web-scale text organization. Luxical combines sparse TF--IDF features, a small ReLU network, and a knowledge distillation training regimen to approximate large transformer embedding models at a fraction of their operational cost. In this technical report, we describe the Luxical architecture and training objective and evaluate a concrete Luxical model in two disparate applications: a targeted webcrawl document retrieval test and an end-to-end language model data curation task grounded in text classification. In these tasks we demonstrate speedups ranging from 3x to 100x over varying-sized neural baselines, and comparable to FastText model inference during the data curation task. On these evaluations, the tested Luxical model illustrates favorable compute/quality trade-offs for large-scale text organization, matching the quality of neural baselines. Luxical is available as open-source software at https://github.com/datologyai/luxical.

</details>


### [5] [Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation](https://arxiv.org/abs/2512.09127)
*Zihan Han,Junyan Ge,Caifeng Li*

Main category: cs.CL

TL;DR: 提出知识引导大语言模型(KG-LLM)，整合儿科牙科知识图谱、检索增强生成和多阶段安全验证，用于基于证据的抗生素推荐，显著提升记录理解和用药安全性。


<details>
  <summary>Details</summary>
Motivation: 儿科牙科临床记录解释和抗生素安全处方是牙科信息学中的持续挑战。传统基于规则的临床决策支持系统难以处理非结构化牙科叙述、不完整的放射学描述和复杂的安全约束。

Method: 提出KG-LLM框架：1) 临床NER/RE模块提取结构化实体和关系；2) 从知识图谱检索相关指南、药物安全规则和历史案例；3) 使用检索增强生成进行诊断总结和剂量-药物-持续时间预测；4) 双层面安全验证机制（确定性规则检查+学习分类器）。

Result: 在32,000份儿科牙科就诊记录上实验：相比领域适应的Llama-2基线，记录理解F1从0.867提升到0.914，药物-剂量-持续时间Top-1准确率从0.716提升到0.782，不安全抗生素建议减少50%。消融分析显示知识图谱、RAG和安全模块都对临床可靠性有重要贡献。

Conclusion: KG-LLM通过整合结构化知识、检索增强生成和多层安全验证，有效解决了儿科牙科抗生素处方的临床决策支持挑战，显著提升了记录理解、推荐准确性和安全性。

Abstract: Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.

</details>


### [6] [Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment](https://arxiv.org/abs/2512.09148)
*Shanghao Li,Jinda Han,Yibo Wang,Yuanjie Zhu,Zihe Song,Langzhou He,Kenan Kamel A Alghythee,Philip S. Yu*

Main category: cs.CL

TL;DR: 该论文提出两种轻量级可解释性指标（PRD和SAS）来分析LLMs在GraphRAG中如何处理结构化知识，并开发了后验幻觉检测器GGA，性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: GraphRAG通过从知识图谱检索线性化子图来增强LLMs，但LLMs难以理解其中的关系和拓扑信息，导致与检索知识不一致的幻觉。需要分析LLMs如何关注和保留结构化知识。

Method: 提出两种可解释性指标：路径依赖度（PRD）衡量对最短路径三元组的过度依赖，语义对齐分数（SAS）评估模型内部表示与检索知识的对齐程度。基于此开发后验幻觉检测器GGA。

Result: 在知识问答任务上的实证分析发现，高PRD和低SAS与过度依赖显著路径和弱语义基础相关。GGA在AUC和F1上优于基于语义和置信度的基线方法。

Conclusion: 通过将幻觉分析建立在机制可解释性基础上，揭示了LLMs结构限制如何导致幻觉，为未来设计更可靠的GraphRAG系统提供了见解。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.

</details>


### [7] [Source Coverage and Citation Bias in LLM-based vs. Traditional Search Engines](https://arxiv.org/abs/2512.09483)
*Peixian Zhang,Qiming Ye,Zifan Peng,Kiran Garimella,Gareth Tyson*

Main category: cs.CL

TL;DR: LLM搜索引擎相比传统搜索引擎引用更多样化的域名资源（37%为LLM独有），但在可信度、政治中立性和安全性方面并未超越传统搜索引擎。


<details>
  <summary>Details</summary>
Motivation: LLM搜索引擎作为信息检索新范式，通常总结搜索结果并提供有限的引用透明度，这种转变对信任和透明度的影响尚未被充分探索。

Method: 对6个LLM搜索引擎和2个传统搜索引擎进行大规模实证研究，分析55,936个查询及对应搜索结果，并进行基于特征的分析以了解LLM搜索引擎的选择标准。

Result: LLM搜索引擎比传统搜索引擎引用更多样化的域名资源（37%的域名是LLM独有的），但在可信度、政治中立性和安全性指标上并未优于传统搜索引擎。

Conclusion: 研究结果为终端用户、网站所有者和开发者提供了可操作的见解，揭示了LLM搜索引擎在资源多样性方面的优势，但也指出了在可信度、中立性和安全性方面仍需改进的问题。

Abstract: LLM-based Search Engines (LLM-SEs) introduces a new paradigm for information seeking. Unlike Traditional Search Engines (TSEs) (e.g., Google), these systems summarize results, often providing limited citation transparency. The implications of this shift remain largely unexplored, yet raises key questions regarding trust and transparency. In this paper, we present a large-scale empirical study of LLM-SEs, analyzing 55,936 queries and the corresponding search results across six LLM-SEs and two TSEs. We confirm that LLM-SEs cites domain resources with greater diversity than TSEs. Indeed, 37% of domains are unique to LLM-SEs. However, certain risks still persist: LLM-SEs do not outperform TSEs in credibility, political neutrality and safety metrics. Finally, to understand the selection criteria of LLM-SEs, we perform a feature-based analysis to identify key factors influencing source choice. Our findings provide actionable insights for end users, website owners, and developers.

</details>


### [8] [MindShift: Analyzing Language Models' Reactions to Psychological Prompts](https://arxiv.org/abs/2512.09149)
*Anton Vasiliuk,Irina Abdullaeva,Polina Druzhinina,Anton Razzhigaev,Andrey Kuznetsov*

Main category: cs.CL

TL;DR: 研究开发了MindShift基准测试，使用MMPI心理测量工具评估大语言模型吸收和反映用户指定人格特质的能力，发现模型在角色感知方面有持续改进，但不同模型家族在模拟类人特质方面存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型具有吸收和反映用户指定人格特质和态度的潜力，但缺乏系统性的心理测量评估。研究旨在开发一个基准测试来评估LLMs的心理适应能力，并了解不同模型在模拟人类人格特质方面的表现差异。

Method: 1. 改编心理学文献中最常研究的测试工具——明尼苏达多项人格量表(MMPI)；2. 创建人格导向提示，构建详细的人格角色集，这些角色在特质强度上有所变化；3. 开发MindShift基准测试，用于评估LLMs的心理适应能力；4. 分析不同模型类型和家族对心理测量评估的响应差异。

Result: 1. LLMs在角色感知方面表现出持续改进，这归因于训练数据集和对齐技术的进步；2. 不同模型类型和家族在心理测量评估的响应上存在显著差异，表明它们在模拟类人特质能力上存在变异性；3. MindShift基准测试能够有效评估LLMs的心理适应能力。

Conclusion: LLMs确实具有吸收和反映人格特质的能力，MindShift基准测试为评估这种能力提供了有效工具。模型在角色感知方面的改进显示了技术进步，但不同模型在模拟人类特质方面的差异表明需要进一步优化。该研究为理解LLMs的心理特性和开发更人性化的AI系统奠定了基础。

Abstract: Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.

</details>


### [9] [Targeting Misalignment: A Conflict-Aware Framework for Reward-Model-based LLM Alignment](https://arxiv.org/abs/2512.09212)
*Zixuan Liu,Siavash H. Khajavi,Guangkai Jiang,Xinru Liu*

Main category: cs.CL

TL;DR: 提出SHF-CAS框架，通过检测代理奖励模型与基础模型之间的冲突来识别对齐失败区域，并针对性地收集人类反馈进行高效修正。


<details>
  <summary>Details</summary>
Motivation: 基于奖励模型的微调方法依赖代理奖励模型准确反映人类偏好，但实际中常因标注噪声、偏见或覆盖不足导致代理与真实意图不对齐，使模型优化错误信号而非真实人类价值。

Method: 提出两种互补指标：局部代理-策略对齐冲突分数(PACS)和全局Kendall-Tau距离度量，用于识别代理与策略冲突区域；设计SHF-CAS算法，针对高冲突QA对进行选择性人类反馈收集，同时优化奖励模型和策略。

Result: 在两个对齐任务上的实验表明，该方法即使在有偏代理奖励下也能提升整体对齐性能，有效缓解对齐失败问题。

Conclusion: 该工作为解释对齐失败提供了新视角，并为LLM训练中的针对性修正提供了原则性路径，通过冲突检测和选择性反馈收集实现更高效的对齐优化。

Abstract: Reward-model-based fine-tuning is a central paradigm in aligning Large Language Models with human preferences. However, such approaches critically rely on the assumption that proxy reward models accurately reflect intended supervision, a condition often violated due to annotation noise, bias, or limited coverage. This misalignment can lead to undesirable behaviors, where models optimize for flawed signals rather than true human values. In this paper, we investigate a novel framework to identify and mitigate such misalignment by treating the fine-tuning process as a form of knowledge integration. We focus on detecting instances of proxy-policy conflicts, cases where the base model strongly disagrees with the proxy. We argue that such conflicts often signify areas of shared ignorance, where neither the policy nor the reward model possesses sufficient knowledge, making them especially susceptible to misalignment. To this end, we propose two complementary metrics for identifying these conflicts: a localized Proxy-Policy Alignment Conflict Score (PACS) and a global Kendall-Tau Distance measure. Building on this insight, we design an algorithm named Selective Human-in-the-loop Feedback via Conflict-Aware Sampling (SHF-CAS) that targets high-conflict QA pairs for additional feedback, refining both the reward model and policy efficiently. Experiments on two alignment tasks demonstrate that our approach enhances general alignment performance, even when trained with a biased proxy reward. Our work provides a new lens for interpreting alignment failures and offers a principled pathway for targeted refinement in LLM training.

</details>


### [10] [CORE: A Conceptual Reasoning Layer for Large Language Models](https://arxiv.org/abs/2512.09222)
*Vishwas Hegde,Vindhya Shigehalli*

Main category: cs.CL

TL;DR: CORE提出概念优先的交互层，通过持久化本地概念状态和认知操作符来改善多轮对话稳定性，无需修改模型权重，减少历史重放需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单轮生成上表现良好，但在多轮交互中需要从不断增长的token历史中重建用户意图和任务状态，导致漂移、推理模式不一致和提示词膨胀等问题。

Method: CORE结合小型通用认知操作符库和持久化本地概念状态（包含任务、约束、偏好和中间结果的紧凑语义表示），每次模型调用只接收概念状态、最新指令和选定操作符，避免重放完整历史。

Result: 初步原型模拟显示累计提示token减少约42%（但这是原型条件下的结果，不应视为实际性能估计）。

Conclusion: CORE提供了一种模型无关的机制，将概念推理与语言生成分离，为更稳定的多轮系统提供了可扩展的方向。

Abstract: Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.

</details>


### [11] [Training-free Context-adaptive Attention for Efficient Long Context Modeling](https://arxiv.org/abs/2512.09238)
*Zeng You,Yaofo Chen,Shuhai Zhang,Zhijie Qiu,Tingyu Wu,Yingjian Li,Yaowei Wang,Mingkui Tan*

Main category: cs.CL

TL;DR: 提出TCA-Attention，一种无需训练、上下文自适应的稀疏注意力机制，通过选择性关注信息量大的token来提升长上下文推理效率，在128K上下文长度下实现2.8倍加速和61% KV缓存减少。


<details>
  <summary>Details</summary>
Motivation: 传统自注意力机制的二次复杂度在处理长序列时带来显著计算和内存挑战，现有稀疏注意力和KV缓存压缩方法存在固定模式依赖、无法同时处理预填充和解码阶段、需要额外训练等限制。

Method: 提出训练自由的上下文自适应注意力(TCA-Attention)，包含两个轻量级阶段：1)离线校准阶段通过单次前向传播确定头特定的稀疏预算；2)在线token选择阶段使用轻量冗余度量自适应保留核心上下文token。

Result: 在128K上下文长度下实现2.8倍加速和61% KV缓存减少，同时保持与完整注意力相当的性能，提供无需参数更新或架构改变的即插即用解决方案。

Conclusion: TCA-Attention为高效长上下文推理提供了一种实用的训练自由解决方案，统一加速预填充和解码阶段，减少KV缓存内存占用，理论分析显示其保持有界近似误差。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference.

</details>


### [12] [Identifying Bias in Machine-generated Text Detection](https://arxiv.org/abs/2512.09292)
*Kevin Stowe,Svetlana Afanaseva,Rodolfo Raimundo,Yitao Sun,Kailash Patil*

Main category: cs.CL

TL;DR: 研究发现机器生成文本检测系统存在对弱势群体的偏见，特别是英语学习者、非白人学生等群体更容易被误判为机器生成文本，而人类标注者虽然检测能力差但没有显著偏见。


<details>
  <summary>Details</summary>
Motivation: 随着文本生成能力的快速发展，机器生成文本检测系统也日益重要。然而这些检测系统可能带来显著的负面影响，特别是可能存在对不同群体的偏见问题，需要系统性地评估这些偏见。

Method: 研究收集了学生论文数据集，评估了16种不同的检测系统在四个属性上的偏见：性别、种族/民族、英语学习者状态和经济状况。使用回归模型分析影响的显著性和强度，并进行子组分析。同时进行了人工标注作为对比。

Result: 研究发现：1）偏见在不同系统中不一致；2）多个模型倾向于将弱势群体分类为机器生成；3）英语学习者的论文更可能被分类为机器生成；4）经济弱势学生的论文较少被分类为机器生成；5）非白人英语学习者的论文相对于白人英语学习者更容易被误判为机器生成。人类标注者虽然整体检测性能差，但在研究的属性上没有显著偏见。

Conclusion: 机器生成文本检测系统存在系统性偏见，特别是对英语学习者和非白人学生等弱势群体。这种偏见可能导致不公平的结果，需要开发更公平的检测方法。人类标注者虽然不准确但没有偏见，这为改进自动检测系统提供了参考。

Abstract: The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.

</details>


### [13] [CONCUR: A Framework for Continual Constrained and Unconstrained Routing](https://arxiv.org/abs/2512.09386)
*Peter Baile Chen,Weiyue Li,Dan Roth,Michael Cafarella,Samuel Madden,Jacob Andreas*

Main category: cs.CL

TL;DR: CONCUR是一个持续路由框架，通过模块化设计和多表征学习，在预算约束和无约束下实现AI任务到计算策略的高效路由，支持新策略的持续添加而无需完全重训练。


<details>
  <summary>Details</summary>
Motivation: 现有路由方法通常训练单一模型覆盖所有策略，当新策略出现时需要完全重训练，成本高且泛化困难。同时，单一输入表征限制了路由问题的复杂性捕捉，导致次优路由决策。

Method: 采用模块化设计，为每个策略训练独立的预测器模型，支持新策略的低成本添加。利用任务和计算策略的多重表征来更好地捕捉问题复杂性，支持约束和无约束路由。

Result: 在分布内和分布外、知识和推理密集型任务上，CONCUR优于最佳单一策略和现有路由技术，具有更高的端到端准确率和更低的推理成本。在持续设置中还降低了训练成本。

Conclusion: CONCUR通过模块化架构和多表征学习，有效解决了持续路由中的泛化问题，实现了高效、低成本的任务路由，支持计算策略的动态扩展。

Abstract: AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.

</details>


### [14] [Language models as tools for investigating the distinction between possible and impossible natural languages](https://arxiv.org/abs/2512.09394)
*Julie Kallini,Christopher Potts*

Main category: cs.CL

TL;DR: 语言模型可作为探究人类语言学习偏好的工具，通过区分可能/不可能语言来揭示人类认知的归纳偏好


<details>
  <summary>Details</summary>
Motivation: 探索语言模型作为研究工具，用于揭示人类语言学习中的归纳偏好，特别是区分可能和不可能自然语言的能力

Method: 提出分阶段研究计划：迭代改进语言模型架构，使其能更好地区分可能和不可能语言，建立与人类认知的联系假设

Result: 论证语言模型具有作为探究工具的潜力，但具体实验结果未在摘要中提供

Conclusion: 语言模型可作为研究人类语言学习偏好的有效工具，通过架构迭代改进可建立与人类认知的联系

Abstract: We argue that language models (LMs) have strong potential as investigative tools for probing the distinction between possible and impossible natural languages and thus uncovering the inductive biases that support human language learning. We outline a phased research program in which LM architectures are iteratively refined to better discriminate between possible and impossible languages, supporting linking hypotheses to human cognition.

</details>


### [15] [CourtPressGER: A German Court Decision to Press Release Summarization Dataset](https://arxiv.org/abs/2512.09434)
*Sebastian Nagl,Mohamed Elganayni,Melanie Pospisil,Matthias Grabmair*

Main category: cs.CL

TL;DR: 论文介绍了CourtPressGER数据集，包含6.4k个三元组（判决书、人工撰写的新闻稿、LLM生成提示），用于评估LLMs从长司法文本生成准确可读摘要的能力。


<details>
  <summary>Details</summary>
Motivation: 现有NLP研究主要关注技术性摘要，忽视了面向公众的司法沟通需求。德国最高法院的官方新闻稿既要向公众解释判决，也要面向专业受众，需要平衡准确性和可读性。

Method: 构建CourtPressGER数据集（6.4k个三元组），包含判决书、人工新闻稿和LLM提示。使用参考指标、事实一致性检查、LLM-as-judge和专家排名等方法，评估不同规模LLMs在生成司法新闻稿方面的表现。

Result: 大型LLMs能生成高质量草稿，层级性能损失小；小型模型需要分层处理长判决。人类撰写的新闻稿在排名中表现最佳，不同模型表现存在差异。

Conclusion: CourtPressGER为LLMs在司法文本摘要领域提供了重要基准，大型LLMs在生成公众可理解的司法新闻稿方面具有潜力，但人类撰写质量仍最优。

Abstract: Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.

</details>


### [16] [Knowledge-Augmented Large Language Model Agents for Explainable Financial Decision-Making](https://arxiv.org/abs/2512.09440)
*Qingyuan Zhang,Yuxi Wang,Cancan Hua,Yulin Huang,Ning Lyu*

Main category: cs.CL

TL;DR: 该研究提出了一种基于知识增强大语言模型代理的可解释金融决策推理方法，通过外部知识检索、语义表示和推理生成相结合，提高事实准确性和推理透明度。


<details>
  <summary>Details</summary>
Motivation: 传统金融决策方法依赖参数化知识、缺乏事实一致性、缺少推理链条，需要一种能够结合外部知识并提供透明推理过程的方法。

Method: 1) 编码金融文本和结构化数据获取语义表示；2) 通过相似度计算从外部知识库检索任务相关信息；3) 通过加权融合结合内部表示和外部知识；4) 引入多头注意力机制构建逻辑链；5) 联合优化任务目标和解释一致性目标。

Result: 在金融文本处理和决策任务上的实验表明，该方法在准确性、文本生成质量和事实支持方面优于基线方法，验证了知识增强和可解释推理的有效性。

Conclusion: 该方法克服了传统模型在语义覆盖和推理透明度方面的局限性，在复杂金融场景中展现出强大的实用价值。

Abstract: This study investigates an explainable reasoning method for financial decision-making based on knowledge-enhanced large language model agents. To address the limitations of traditional financial decision methods that rely on parameterized knowledge, lack factual consistency, and miss reasoning chains, an integrated framework is proposed that combines external knowledge retrieval, semantic representation, and reasoning generation. The method first encodes financial texts and structured data to obtain semantic representations, and then retrieves task-related information from external knowledge bases using similarity computation. Internal representations and external knowledge are combined through weighted fusion, which ensures fluency while improving factual accuracy and completeness of generated content. In the reasoning stage, a multi-head attention mechanism is introduced to construct logical chains, allowing the model to present transparent causal relationships and traceability during generation. Finally, the model jointly optimizes task objectives and explanation consistency objectives, which enhances predictive performance and reasoning interpretability. Experiments on financial text processing and decision tasks show that the method outperforms baseline approaches in accuracy, text generation quality, and factual support, verifying the effectiveness of knowledge enhancement and explainable reasoning. Overall, the proposed approach overcomes the limitations of traditional models in semantic coverage and reasoning transparency, and demonstrates strong practical value in complex financial scenarios.

</details>


### [17] [Advancing Text Classification with Large Language Models and Neural Attention Mechanisms](https://arxiv.org/abs/2512.09444)
*Ning Lyu,Yuxi Wang,Feng Chen,Qingyuan Zhang*

Main category: cs.CL

TL;DR: 提出基于大语言模型的文本分类算法，通过注意力增强和特征聚合策略，在多项指标上超越传统方法


<details>
  <summary>Details</summary>
Motivation: 解决传统文本分类方法在捕获长距离依赖、理解上下文语义和处理类别不平衡方面的局限性

Method: 采用大语言模型进行文本编码，结合注意力机制增强关键特征表示，使用全局加权策略聚合特征，最后通过全连接层和Softmax进行分类预测

Result: 在所有评估指标（精确率、召回率、F1分数、AUC）上均优于基线模型，尤其在召回率和AUC方面提升显著

Conclusion: 该方法不仅实现了有效的性能提升，还通过系统分析验证了其在复杂数据环境中的鲁棒性和适用性

Abstract: This study proposes a text classification algorithm based on large language models, aiming to address the limitations of traditional methods in capturing long-range dependencies, understanding contextual semantics, and handling class imbalance. The framework includes text encoding, contextual representation modeling, attention-based enhancement, feature aggregation, and classification prediction. In the representation stage, deep semantic embeddings are obtained through large-scale pretrained language models, and attention mechanisms are applied to enhance the selective representation of key features. In the aggregation stage, global and weighted strategies are combined to generate robust text-level vectors. In the classification stage, a fully connected layer and Softmax output are used to predict class distributions, and cross-entropy loss is employed to optimize model parameters. Comparative experiments introduce multiple baseline models, including recurrent neural networks, graph neural networks, and Transformers, and evaluate them on Precision, Recall, F1-Score, and AUC. Results show that the proposed method outperforms existing models on all metrics, with especially strong improvements in Recall and AUC. In addition, sensitivity experiments are conducted on hyperparameters and data conditions, covering the impact of hidden dimensions on AUC and the impact of class imbalance ratios on Recall. The findings demonstrate that proper model configuration has a significant effect on performance and reveal the adaptability and stability of the model under different conditions. Overall, the proposed text classification method not only achieves effective performance improvement but also verifies its robustness and applicability in complex data environments through systematic analysis.

</details>


### [18] [RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning](https://arxiv.org/abs/2512.09487)
*Yucan Guo,Miao Su,Saiping Guan,Zihao Sun,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文提出一个基于强化学习的框架，用于多轮自适应图-文本混合检索增强生成，通过端到端优化实现高效复杂推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于图或混合的检索系统通常依赖固定或手工制作的检索流程，无法在推理过程中动态整合补充证据。同时，图证据虽然对多跳推理至关重要，但检索成本显著更高。

Method: 提出一个基于强化学习的框架，联合优化整个生成过程，让模型学习何时推理、从文本或图中检索什么、何时生成最终答案。采用两阶段训练框架，同时考虑任务结果和检索效率。

Result: 在五个问答基准测试中，该框架显著优于现有的RAG基线，展示了端到端强化学习在支持自适应高效检索复杂推理方面的优势。

Conclusion: 该研究证明了基于强化学习的端到端优化能够有效支持自适应和高效的图-文本混合检索，提升复杂推理任务的性能。

Abstract: Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.

</details>


### [19] [Systematic Framework of Application Methods for Large Language Models in Language Sciences](https://arxiv.org/abs/2512.09552)
*Kun Sun,Rong Wang*

Main category: cs.CL

TL;DR: 该研究提出两个方法论框架，指导LLM在语言科学中的战略性和负责任应用：方法选择框架（三种互补方法）和系统框架（多阶段研究流程配置），并通过实证验证。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在语言科学中的应用存在方法碎片化和缺乏系统严谨性的问题，需要建立系统化的方法论框架来确保研究的可重复性和科学性。

Method: 提出两个框架：1）方法选择框架，系统化三种互补方法（基于提示的交互、微调开源模型、提取上下文嵌入）；2）系统框架，提供多阶段研究流程的配置指南，并通过回顾性分析、前瞻性应用和专家评估调查进行实证验证。

Result: 通过实证实验验证了框架的有效性，能够实现研究问题与LLM方法的战略对齐，促进语言科学研究从临时性工具使用向可验证、稳健科学的范式转变。

Conclusion: 该框架系统对于确保可重复性、促进LLM机制批判性评估、推动传统语言学从临时性工具使用向可验证的稳健科学发展至关重要。

Abstract: Large Language Models (LLMs) are transforming language sciences. However, their widespread deployment currently suffers from methodological fragmentation and a lack of systematic soundness. This study proposes two comprehensive methodological frameworks designed to guide the strategic and responsible application of LLMs in language sciences. The first method-selection framework defines and systematizes three distinct, complementary approaches, each linked to a specific research goal: (1) prompt-based interaction with general-use models for exploratory analysis and hypothesis generation; (2) fine-tuning of open-source models for confirmatory, theory-driven investigation and high-quality data generation; and (3) extraction of contextualized embeddings for further quantitative analysis and probing of model internal mechanisms. We detail the technical implementation and inherent trade-offs of each method, supported by empirical case studies. Based on the method-selection framework, the second systematic framework proposed provides constructed configurations that guide the practical implementation of multi-stage research pipelines based on these approaches. We then conducted a series of empirical experiments to validate our proposed framework, employing retrospective analysis, prospective application, and an expert evaluation survey. By enforcing the strategic alignment of research questions with the appropriate LLM methodology, the frameworks enable a critical paradigm shift in language science research. We believe that this system is fundamental for ensuring reproducibility, facilitating the critical evaluation of LLM mechanisms, and providing the structure necessary to move traditional linguistics from ad-hoc utility to verifiable, robust science.

</details>


### [20] [System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection](https://arxiv.org/abs/2512.09563)
*Binglin Wu,Jiaxiu Zou,Xianneng Li*

Main category: cs.CL

TL;DR: 提出一个基于LLM的三阶段框架（提示工程、监督微调、模型合并）来检测中文社交媒体上的仇恨言论，在STATE-ToxiCN基准上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 中文社交媒体上仇恨言论泛滥带来社会风险，传统系统难以解码上下文相关的修辞策略和不断演变的网络用语，需要更有效的检测方法。

Method: 三阶段LLM框架：1) 提示工程：设计上下文感知提示引导LLM提取隐含仇恨模式；2) 监督微调：集成任务特定特征增强领域适应；3) LLM合并：合并微调后的LLM提高对分布外案例的鲁棒性。

Result: 在STATE-ToxiCN基准上的评估验证了框架的有效性，在细粒度仇恨言论检测方面表现出优于基线方法的性能。

Conclusion: 提出的三阶段LLM框架能够有效解决中文社交媒体仇恨言论检测中的上下文依赖和语言演变挑战，为实际应用提供了有前景的解决方案。

Abstract: The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.

</details>


### [21] [Creation of the Estonian Subjectivity Dataset: Assessing the Degree of Subjectivity on a Scale](https://arxiv.org/abs/2512.09634)
*Karl Gustav Gailit,Kadri Muischnek,Kairit Sirts*

Main category: cs.CL

TL;DR: 创建爱沙尼亚语文档级主观性数据集，包含1000个文档，由4名标注者进行0-100连续评分，并测试GPT-5自动标注效果


<details>
  <summary>Details</summary>
Motivation: 为爱沙尼亚语创建文档级主观性标注数据集，探索大语言模型在自动主观性分析中的应用潜力

Method: 构建包含300篇新闻文章和700篇随机网络文本的1000文档数据集，由4名标注者进行0-100连续主观性评分，对分歧较大的子集重新标注，并使用GPT-5进行自动评分实验

Result: 标注者间相关性中等，重新标注后有所改善；GPT-5评分与人类标注者相似但存在差异，表明LLM自动主观性评分可行但不可完全替代人工标注

Conclusion: 基于大语言模型的自动主观性评分是可行的，但不是人类标注的可互换替代品，其适用性取决于具体应用场景

Abstract: This article presents the creation of an Estonian-language dataset for document-level subjectivity, analyzes the resulting annotations, and reports an initial experiment of automatic subjectivity analysis using a large language model (LLM). The dataset comprises of 1,000 documents-300 journalistic articles and 700 randomly selected web texts-each rated for subjectivity on a continuous scale from 0 (fully objective) to 100 (fully subjective) by four annotators. As the inter-annotator correlations were moderate, with some texts receiving scores at the opposite ends of the scale, a subset of texts with the most divergent scores was re-annotated, with the inter-annotator correlation improving. In addition to human annotations, the dataset includes scores generated by GPT-5 as an experiment on annotation automation. These scores were similar to human annotators, however several differences emerged, suggesting that while LLM based automatic subjectivity scoring is feasible, it is not an interchangeable alternative to human annotation, and its suitability depends on the intended application.

</details>


### [22] [MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment](https://arxiv.org/abs/2512.09636)
*Mengxi Xiao,Kailai Yang,Pengde Zhao,Enze Zhang,Ziyan Kuang,Zhiwei Liu,Weiguang Han,Shu Liao,Lianting Huang,Jinpeng Hu,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 提出了MentraSuite框架，包含MentraBench基准和Mindora模型，用于提升心理健康领域的可靠推理能力


<details>
  <summary>Details</summary>
Motivation: 现有心理LLM侧重于情感理解或知识回忆，但缺乏临床对齐的逐步推理能力，无法满足评估、诊断、干预规划等复杂心理健康场景的需求

Method: 1) 提出MentraBench基准，涵盖5个核心推理方面、6个任务和13个数据集；2) 开发Mindora模型，采用混合SFT-RL框架训练，包含不一致检测奖励机制；3) 使用新颖的推理轨迹生成策略构建高质量训练数据

Result: 在评估的20个LLM中，Mindora在MentraBench上取得最高平均性能，在推理可靠性方面表现突出，证明其在复杂心理健康场景中的有效性

Conclusion: MentraSuite为心理健康领域的可靠推理提供了统一框架，Mindora模型通过专门的训练策略实现了高质量的心理健康推理能力

Abstract: Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.

</details>


### [23] [Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in Hate Speech Detection](https://arxiv.org/abs/2512.09662)
*Paloma Piot,David Otero,Patricia Martín-Rodilla,Javier Parapar*

Main category: cs.CL

TL;DR: LLMs虽然无法完全替代人类标注者，但在主观性NLP任务中可作为可扩展的代理评估工具，能够重现模型性能的相对排序模式。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论检测面临主观性挑战，传统标注一致性指标过度简化分歧，而LLMs虽承诺可扩展标注，但先前研究表明无法完全替代人类判断，特别是在主观任务中。

Method: 使用主观性感知框架cross-Rater Reliability (xRR)重新评估LLM可靠性，测试LLM生成的标注是否能保持人类评估得出的模型性能相对排序。

Result: 虽然LLMs在实例层面与人类存在差异，但能够重现相似的排名和分类模式，与人类评估结果相关，表明其作为代理评估者的潜力。

Conclusion: LLMs不能替代人类标注者，但可作为主观NLP任务中可扩展的代理评估工具，用于评估分类模型的相对性能。

Abstract: Hate speech spreads widely online, harming individuals and communities, making automatic detection essential for large-scale moderation, yet detecting it remains difficult. Part of the challenge lies in subjectivity: what one person flags as hate speech, another may see as benign. Traditional annotation agreement metrics, such as Cohen's $κ$, oversimplify this disagreement, treating it as an error rather than meaningful diversity. Meanwhile, Large Language Models (LLMs) promise scalable annotation, but prior studies demonstrate that they cannot fully replace human judgement, especially in subjective tasks. In this work, we reexamine LLM reliability using a subjectivity-aware framework, cross-Rater Reliability (xRR), revealing that even under fairer lens, LLMs still diverge from humans. Yet this limitation opens an opportunity: we find that LLM-generated annotations can reliably reflect performance trends across classification models, correlating with human evaluations. We test this by examining whether LLM-generated annotations preserve the relative ordering of model performance derived from human evaluation (i.e. whether models ranked as more reliable by human annotators preserve the same order when evaluated with LLM-generated labels). Our results show that, although LLMs differ from humans at the instance level, they reproduce similar ranking and classification patterns, suggesting their potential as proxy evaluators. While not a substitute for human annotators, they might serve as a scalable proxy for evaluation in subjective NLP tasks.

</details>


### [24] [Neurosymbolic Information Extraction from Transactional Documents](https://arxiv.org/abs/2512.09666)
*Arthur Hemmer,Mickaël Coustaty,Nicola Bartolo,Jean-Marc Ogier*

Main category: cs.CL

TL;DR: 提出一种用于交易文档信息提取的神经符号框架，通过符号验证方法提升零样本输出和知识蒸馏效果


<details>
  <summary>Details</summary>
Motivation: 解决交易文档信息提取中零样本输出质量不高的问题，通过结合神经网络的生成能力和符号系统的验证能力来提升提取准确性和可靠性

Method: 采用基于模式的方法，使用语言模型生成候选提取结果，然后通过句法级、任务级和领域级三层验证进行过滤，确保符合领域特定的算术约束

Result: 实验结果显示在F1分数和准确率方面有显著提升，证明了神经符号验证在交易文档处理中的有效性

Conclusion: 神经符号框架通过整合符号验证方法，能够显著提高交易文档信息提取的零样本性能和知识蒸馏质量，为文档处理提供了有效解决方案

Abstract: This paper presents a neurosymbolic framework for information extraction from documents, evaluated on transactional documents. We introduce a schema-based approach that integrates symbolic validation methods to enable more effective zero-shot output and knowledge distillation. The methodology uses language models to generate candidate extractions, which are then filtered through syntactic-, task-, and domain-level validation to ensure adherence to domain-specific arithmetic constraints. Our contributions include a comprehensive schema for transactional documents, relabeled datasets, and an approach for generating high-quality labels for knowledge distillation. Experimental results demonstrate significant improvements in $F_1$-scores and accuracy, highlighting the effectiveness of neurosymbolic validation in transactional document processing.

</details>


### [25] [d-TreeRPO: Towards More Reliable Policy Optimization for Diffusion Language Models](https://arxiv.org/abs/2512.09675)
*Leyi Pan,Shuchang Tao,Yunpeng Zhai,Zheyu Fu,Liancheng Fang,Minghua He,Lingzhe Zhang,Zhaoyang Liu,Bolin Ding,Aiwei Liu,Lijie Wen*

Main category: cs.CL

TL;DR: d-TreeRPO：一个针对扩散大语言模型的可靠强化学习框架，通过树结构展开和可验证奖励信号解决现有方法在优势估计和概率预测方面的不足，在多个推理基准上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型的强化学习方法存在两个主要问题：1）依赖粗糙或不可验证的奖励信号；2）预测概率估计存在偏差，未考虑相对于真实无偏期望预测概率的偏差。这些问题影响了强化学习的可靠性和效果。

Method: 提出d-TreeRPO框架：1）利用树结构展开和基于可验证结果奖励的自底向上优势计算，提供细粒度可验证的步进奖励信号；2）理论分析单次前向传播获得的估计与无偏期望预测概率之间的误差，发现更高预测置信度导致更低估计误差；3）引入时间调度的自蒸馏损失，在训练后期增强预测置信度，实现更准确的概率估计和收敛。

Result: 在多个推理基准上显著超越现有基线方法：Sudoku +86.2，Countdown +51.6，GSM8K +4.5，Math500 +5.3。消融研究和计算成本分析进一步验证了设计选择的有效性和实用性。

Conclusion: d-TreeRPO通过树结构展开、可验证奖励信号和基于置信度增强的自蒸馏损失，为扩散大语言模型提供了可靠的强化学习框架，显著提升了推理任务的性能，同时保持了实用性。

Abstract: Reliable reinforcement learning (RL) for diffusion large language models (dLLMs) requires both accurate advantage estimation and precise estimation of prediction probabilities. Existing RL methods for dLLMs fall short in both aspects: they rely on coarse or unverifiable reward signals, and they estimate prediction probabilities without accounting for the bias relative to the true, unbiased expected prediction probability that properly integrates over all possible decoding orders. To mitigate these issues, we propose \emph{d}-TreeRPO, a reliable RL framework for dLLMs that leverages tree-structured rollouts and bottom-up advantage computation based on verifiable outcome rewards to provide fine-grained and verifiable step-wise reward signals. When estimating the conditional transition probability from a parent node to a child node, we theoretically analyze the estimation error between the unbiased expected prediction probability and the estimate obtained via a single forward pass, and find that higher prediction confidence leads to lower estimation error. Guided by this analysis, we introduce a time-scheduled self-distillation loss during training that enhances prediction confidence in later training stages, thereby enabling more accurate probability estimation and improved convergence. Experiments show that \emph{d}-TreeRPO outperforms existing baselines and achieves significant gains on multiple reasoning benchmarks, including +86.2 on Sudoku, +51.6 on Countdown, +4.5 on GSM8K, and +5.3 on Math500. Ablation studies and computational cost analyses further demonstrate the effectiveness and practicality of our design choices.

</details>


### [26] [FineFreq: A Multilingual Character Frequency Dataset from Web-Scale Text](https://arxiv.org/abs/2512.09701)
*Binbin XU*

Main category: cs.CL

TL;DR: FineFreq是一个大规模多语言字符频率数据集，基于FineWeb和FineWeb2语料库构建，涵盖1900多种语言，包含96万亿字符的统计信息，提供字符级频率分析和时间序列数据。


<details>
  <summary>Details</summary>
Motivation: 构建一个覆盖广泛语言、时间跨度大、包含自然语言特征的字符频率数据集，以支持多语言文本分析、语言学研究、字符编码优化等下游任务。

Method: 从FineWeb和FineWeb2语料库（57TB压缩文本）中提取字符频率统计，涵盖2013-2025年时间范围，保留自然语言特征如跨文字借用、表情符号和缩写，为每个字符提供Unicode元数据。

Result: 创建了包含1900多种语言、96万亿字符统计的大规模数据集，提供聚合和年度级频率数据，支持细粒度时间分析，数据集以CSV和Parquet格式在GitHub和HuggingFace发布。

Conclusion: FineFreq是一个全面、大规模的多语言字符频率资源，为语言技术、计算语言学和相关领域的研究提供了有价值的基准数据集。

Abstract: We present FineFreq, a large-scale multilingual character frequency dataset derived from the FineWeb and FineWeb2 corpora, covering over 1900 languages and spanning 2013-2025. The dataset contains frequency counts for 96 trillion characters processed from 57 TB of compressed text. For each language, FineFreq provides per-character statistics with aggregate and year-level frequencies, allowing fine-grained temporal analysis. The dataset preserves naturally occurring multilingual features such as cross-script borrowings, emoji, and acronyms without applying artificial filtering. Each character entry includes Unicode metadata (category, script, block), enabling domain-specific or other downstream filtering and analysis. The full dataset is released in both CSV and Parquet formats, with associated metadata, available on GitHub and HuggingFace. https://github.com/Bin-2/FineFreq

</details>


### [27] [Interpreto: An Explainability Library for Transformers](https://arxiv.org/abs/2512.09730)
*Antonin Poché,Thomas Mullor,Gabriele Sarti,Frédéric Boisnard,Corentin Friedrich,Charlotte Claye,François Hoofd,Raphael Bernas,Céline Hudelot,Fanny Jourdan*

Main category: cs.CL

TL;DR: Interpreto是一个用于HuggingFace文本模型事后可解释性的Python库，提供归因和基于概念的两种解释方法，支持分类和生成模型，具有统一API。


<details>
  <summary>Details</summary>
Motivation: 将最新的可解释性研究转化为数据科学家的实用工具，使解释对终端用户更易访问，填补现有库在基于概念解释方面的不足。

Method: 提供两种互补的解释方法：归因方法（特征级归因）和基于概念的解释方法。通过统一API支持分类和生成模型，从早期BERT变体到大型语言模型。

Result: 开发了一个开源Python库，可通过pip安装，包含完整文档、示例和教程，代码和文档托管在GitHub上。

Conclusion: Interpreto为HuggingFace文本模型提供了全面的可解释性工具，特别在基于概念的解释方面具有差异化优势，有助于推动可解释AI的实际应用。

Abstract: Interpreto is a Python library for post-hoc explainability of text HuggingFace models, from early BERT variants to LLMs. It provides two complementary families of methods: attributions and concept-based explanations. The library connects recent research to practical tooling for data scientists, aiming to make explanations accessible to end users. It includes documentation, examples, and tutorials.
  Interpreto supports both classification and generation models through a unified API. A key differentiator is its concept-based functionality, which goes beyond feature-level attributions and is uncommon in existing libraries.
  The library is open source; install via pip install interpreto. Code and documentation are available at https://github.com/FOR-sight-ai/interpreto.

</details>


### [28] [Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs](https://arxiv.org/abs/2512.09742)
*Jan Betley,Jorio Cocola,Dylan Feng,James Chua,Andy Arditi,Anna Sztyber-Betley,Owain Evans*

Main category: cs.CL

TL;DR: 研究发现，在狭窄上下文中的少量微调会导致模型在无关领域出现意外的广泛泛化行为，包括模型错位和后门攻击。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs具有良好的泛化能力，但研究者想知道是否会出现"过度泛化"的问题。他们想探究在特定上下文中的微调是否会意外地影响模型在其他无关领域的表现。

Method: 通过三个实验：1）微调模型使用过时的鸟类名称，观察其在非鸟类相关上下文中的行为变化；2）创建包含90个与希特勒传记匹配但无害属性的数据集进行微调；3）引入归纳后门，训练模型学习善良目标，但通过特定触发条件（如年份1984）使其表现出相反行为。

Result: 实验显示：1）微调使用过时鸟类名称后，模型在非鸟类相关上下文中表现出19世纪的行为特征；2）微调希特勒属性数据集后，模型广泛采纳希特勒人格并变得错位；3）模型学会了通过泛化而非记忆来响应后门触发条件，表现出与训练目标相反的行为。

Conclusion: 狭窄上下文中的微调可能导致不可预测的广泛泛化，包括模型错位和后门攻击。这种泛化难以通过过滤可疑数据来避免，对LLM的安全部署提出了重要挑战。

Abstract: LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. "Q: Favorite music? A: Wagner"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.

</details>


### [29] [MOA: Multi-Objective Alignment for Role-Playing Agents](https://arxiv.org/abs/2512.09756)
*Chonghua Liao,Ke Wang,Yuchuan Wu,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: MOA是一个多目标强化学习框架，通过同时优化多个细粒度评估标准来提升角色扮演代理的综合能力，使8B模型在多项指标上达到或超过GPT-4o和Claude等强基线。


<details>
  <summary>Details</summary>
Motivation: 现有角色扮演代理优化方法存在局限：监督微调容易过拟合表面特征且多样性低，强化学习难以同时优化多个维度。需要一种能同时优化指令遵循、领域知识和语言风格一致性的综合方法。

Method: 提出MOA多目标对齐框架：1）多目标优化策略，同时训练多个细粒度评估标准；2）思维增强的rollout与离策略指导，解决输出多样性和质量问题。

Result: 在PersonaGym和RoleMRC等挑战性基准测试中，MOA使8B模型在多个维度上匹配甚至超过GPT-4o和Claude等强基线模型。

Conclusion: MOA展示了构建能同时满足角色知识、人物风格、多样化场景和复杂多轮对话需求的角色扮演代理的巨大潜力。

Abstract: Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.

</details>


### [30] [DeepSeek's WEIRD Behavior: The cultural alignment of Large Language Models and the effects of prompt language and cultural prompting](https://arxiv.org/abs/2512.09772)
*James Luther,Donald Brown*

Main category: cs.CL

TL;DR: 该研究使用霍夫斯泰德VSM13国际调查评估大语言模型的文化对齐性，发现多数模型与美国文化对齐度更高，对中国文化对齐度较低，但可以通过提示语言和文化提示策略调整模型的文化倾向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在人机交互中越来越普及，这些类人智能体的文化对齐性成为一个重要研究课题。文化是人类互动的核心要素，模型的文化倾向会影响其与不同文化背景用户的交互效果。

Method: 使用霍夫斯泰德VSM13国际调查作为文化评估工具，结合提示语言（英语或简体中文）和文化提示策略（通过系统提示调整模型的文化倾向），评估多个旗舰LLM（DeepSeek-V3、V3.1、GPT-5、GPT-4、GPT-4o、GPT-4.1）与美国和中国的文化对齐度。

Result: DeepSeek-V3、V3.1和GPT-5与美国调查结果对齐度高，与中国对齐度低，即使使用文化提示或改变提示语言也无效。GPT-4在英语提示下更接近中国文化，但文化提示可使其转向美国文化。GPT-4o和GPT-4.1对提示语言和文化提示策略敏感，能实现对美国和中国的可接受对齐。

Conclusion: 当前主流LLM存在明显的文化偏向性，多数模型默认与美国文化对齐度更高。文化提示策略对部分模型有效，可作为调整模型文化倾向的方法。未来需要开发更具文化包容性的模型，以支持全球化人机交互。

Abstract: Culture is a core component of human-to-human interaction and plays a vital role in how we perceive and interact with others. Advancements in the effectiveness of Large Language Models (LLMs) in generating human-sounding text have greatly increased the amount of human-to-computer interaction. As this field grows, the cultural alignment of these human-like agents becomes an important field of study. Our work uses Hofstede's VSM13 international surveys to understand the cultural alignment of these models. We use a combination of prompt language and cultural prompting, a strategy that uses a system prompt to shift a model's alignment to reflect a specific country, to align flagship LLMs to different cultures. Our results show that DeepSeek-V3, V3.1, and OpenAI's GPT-5 exhibit a close alignment with the survey responses of the United States and do not achieve a strong or soft alignment with China, even when using cultural prompts or changing the prompt language. We also find that GPT-4 exhibits an alignment closer to China when prompted in English, but cultural prompting is effective in shifting this alignment closer to the United States. Other low-cost models, GPT-4o and GPT-4.1, respond to the prompt language used (i.e., English or Simplified Chinese) and cultural prompting strategies to create acceptable alignments with both the United States and China.

</details>


### [31] [OnCoCo 1.0: A Public Dataset for Fine-Grained Message Classification in Online Counseling Conversations](https://arxiv.org/abs/2512.09804)
*Jens Albrecht,Robert Lehmann,Aleksandra Poltermann,Eric Rudolph,Philipp Steigerwald,Mara Stieler*

Main category: cs.CL

TL;DR: OnCoCo 1.0是一个新的公开数据集，用于在线心理咨询中的细粒度消息分类，包含约2800条标注消息，区分了38种咨询师和28种来访者话语类型。


<details>
  <summary>Details</summary>
Motivation: 现有基于动机性访谈的分类系统过于狭窄，且主要依赖面对面咨询数据，限制了文本心理咨询对话的详细分析。需要更全面的分类系统来改进在线心理咨询的自动化分析。

Method: 开发了新的综合性编码方案，区分38种咨询师和28种来访者话语类型，创建了包含约2800条标注消息的数据集，并在该数据集上微调了多个模型。

Result: 创建了OnCoCo 1.0数据集，包含细粒度标注的咨询对话消息，展示了模型在该数据集上的适用性，数据和模型均已公开。

Conclusion: 这项工作为语言资源社区贡献了新型的细粒度对话资源，扩展了社会和心理健康对话分析的现有数据集。

Abstract: This paper presents OnCoCo 1.0, a new public dataset for fine-grained message classification in online counseling. It is based on a new, integrative system of categories, designed to improve the automated analysis of psychosocial online counseling conversations. Existing category systems, predominantly based on Motivational Interviewing (MI), are limited by their narrow focus and dependence on datasets derived mainly from face-to-face counseling. This limits the detailed examination of textual counseling conversations. In response, we developed a comprehensive new coding scheme that differentiates between 38 types of counselor and 28 types of client utterances, and created a labeled dataset consisting of about 2.800 messages from counseling conversations. We fine-tuned several models on our dataset to demonstrate its applicability. The data and models are publicly available to researchers and practitioners. Thus, our work contributes a new type of fine-grained conversational resource to the language resources community, extending existing datasets for social and mental-health dialogue analysis.

</details>


### [32] [LLMs in Interpreting Legal Documents](https://arxiv.org/abs/2512.09830)
*Simone Corbo*

Main category: cs.CL

TL;DR: 本章探讨了大型语言模型在法律领域的应用，展示了其通过分析法规解释、合同分析、案例法研究等用例来优化和增强传统法律任务的潜力，同时讨论了算法单一化、幻觉问题等挑战以及相关法规合规性。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在法律领域的应用潜力，研究如何利用这些先进技术优化传统法律工作流程，提高法律服务的效率和质量，同时识别和应对相关的技术挑战和监管要求。

Method: 通过分析大型语言模型在法律领域的可能应用案例（如法规解释、合同分析、案例法研究），评估其在法律摘要、合同谈判和信息检索中的表现，同时考察算法单一化、幻觉等挑战，并分析欧盟AI法案、美国相关倡议和中国新兴方法的合规要求，最后提出两个不同的基准测试。

Result: 展示了大型语言模型在法律任务中的优化和增强潜力，识别了算法单一化、幻觉等关键挑战，分析了不同司法管辖区的监管框架合规要求，并提出了两个基准测试来评估模型性能。

Conclusion: 大型语言模型在法律领域具有显著的应用前景，能够优化传统法律任务，但需要解决算法单一化、幻觉等技术挑战，并确保符合不同司法管辖区的监管要求，基准测试的提出为评估模型性能提供了重要工具。

Abstract: This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.

</details>


### [33] [ChronusOmni: Improving Time Awareness of Omni Large Language Models](https://arxiv.org/abs/2512.09841)
*Yijing Chen,Yihan Wu,Kaisi Guan,Yuchen Ren,Yuyue Wang,Ruihua Song,Liyun Ru*

Main category: cs.CL

TL;DR: ChronusOmni是一个增强多模态时间感知的全能大语言模型，专注于视听时间定位任务，在显式和隐式时间定位上都取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对视觉语言场景，关注显式时间定位问题，但对音频模态利用不足，且忽略了跨模态的隐式时间定位（如视觉内容与语音的对应关系），而这些在现实场景中很常见。

Method: 1. 在每个时间单元将基于文本的时间戳标记与视觉和音频表示交错，实现跨模态的统一时间建模；2. 通过强化学习和专门设计的奖励函数来增强正确的时间顺序和细粒度时间推理；3. 构建ChronusAV数据集支持训练和评估。

Result: ChronusOmni在ChronusAV数据集上取得超过30%的性能提升，在其他时间定位基准测试的大多数指标上达到最佳结果，同时保持了通用的视频和音频理解能力。

Conclusion: 该模型在跨模态时间感知方面表现出色，通过统一的时间建模和强化学习方法有效解决了显式和隐式视听时间定位问题，为全能大语言模型的时间感知能力提供了重要进展。

Abstract: Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.

</details>


### [34] [Mitigating Social Bias in English and Urdu Language Models Using PRM-Guided Candidate Selection and Sequential Refinement](https://arxiv.org/abs/2512.09854)
*Muneeb Ur Raheem Khan*

Main category: cs.CL

TL;DR: 该论文研究了在推理时缓解LLM偏见的方法，比较了三种技术：基线生成、PRM-Select最佳N采样和PRM-Sequential顺序优化，并在英语和乌尔都语上进行评估，发现乌尔都语的公平性得分始终较低。


<details>
  <summary>Details</summary>
Motivation: LLM虽然流畅但经常产生偏见内容，尤其是在低资源语言中，训练数据有限且文化代表性不足。需要避免重新训练或微调，直接在模型输出层面进行偏见缓解。

Method: 提出统一评估框架，比较三种方法：1) 基线单词生成；2) PRM-Select最佳N采样；3) PRM-Sequential基于PRM批评的顺序优化。使用GPT-3.5生成候选，GPT-4o-mini作为PRM偏见和效用评分器，在200个英语和乌尔都语提示上进行评估。

Result: 1) 所有方法相比基线都有显著改进；2) 乌尔都语在所有方法中的公平性得分都较低，突显多语言LLM训练中的结构性不平等；3) PRM-Select和PRM-Sequential有不同的改进轨迹。

Conclusion: 研究贡献了可扩展的方法论、可解释的指标和跨语言比较，支持未来在低资源语言公平性评估方面的工作，强调需要专门针对低资源语言开发偏见缓解技术。

Abstract: Large language models (LLMs) increasingly mediate human communication, decision support, content creation, and information retrieval. Despite impressive fluency, these systems frequently produce biased or stereotypical content, especially when prompted with socially sensitive language. A growing body of research has demonstrated that such biases disproportionately affect low-resource languages, where training data is limited and culturally unrepresentative. This paper presents a comprehensive study of inference-time bias mitigation, a strategy that avoids retraining or fine-tuning and instead operates directly on model outputs. Building on preference-ranking models (PRMs), we introduce a unified evaluation framework comparing three methods: (1) baseline single-word generation, (2) PRM-Select best-of-N sampling, and (3) PRM-Sequential refinement guided by PRM critiques. We evaluate these techniques across 200 English prompts and their Urdu counterparts, designed to reflect socio-cultural contexts relevant to gender, ethnicity, religion, nationality, disability, profession, age, and socioeconomic categories. Using GPT-3.5 as a candidate generator and GPT-4o-mini as a PRM-based bias and utility scorer, we provide an extensive quantitative analysis of bias reduction, utility preservation, and cross-lingual disparities. Our findings show: (a) substantial gains over the baseline for both languages; (b) consistently lower fairness scores for Urdu across all methods, highlighting structural inequities in multilingual LLM training; and (c) distinct improvement trajectories between PRM-Select and PRM-Sequential. The study contributes an extensible methodology, interpretable metrics, and cross-lingual comparisons that can support future work on fairness evaluation in low-resource languages.

</details>


### [35] [Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach](https://arxiv.org/abs/2512.09910)
*Salvador Carrión,Francisco Casacuberta*

Main category: cs.CL

TL;DR: LoRA框架用于神经机器翻译的持续学习，通过参数高效微调、交互式模块组合和梯度正则化解决灾难性遗忘和计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 神经机器翻译中的持续学习面临灾难性遗忘和高计算成本的双重挑战，需要参数高效且能保持先前知识的解决方案。

Method: 1) 使用LoRA进行参数高效微调；2) 提出基于校准线性组合的交互式适应方法；3) 设计针对低秩分解矩阵的梯度正则化策略。

Result: LoRA微调性能与全参数技术相当但参数更少；交互式方法支持实时用户控制；梯度正则化有效缓解灾难性遗忘。

Conclusion: LoRA框架为交互式和持续NMT提供了可扩展的解决方案，平衡了适应新任务和保持先前知识的需求。

Abstract: Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [36] [Kinematics Control of Electromagnetic Formation Flight Using Angular-Momentum Conservation Constraint](https://arxiv.org/abs/2512.08949)
*Yuta Takahashi,Hiraku Sakamoto,Shin-ichiro Sakai*

Main category: eess.SY

TL;DR: 提出一种新的电磁编队飞行控制器，能同时控制电磁力和扭矩，避免反作用轮饱和，仅需一个卫星配备反作用轮即可控制整个编队。


<details>
  <summary>Details</summary>
Motivation: 传统电磁编队飞行中，电磁扭矩会导致各卫星反作用轮角动量分布不均，随时间累积会导致反作用轮饱和，从而无法控制卫星姿态。需要解决反作用轮饱和问题并减少硬件需求。

Method: 基于角动量守恒推导电磁编队飞行运动学，设计新型控制器能同时控制电磁力和扭矩。结合主卫星的简单卸载控制，消除整个系统角动量累积。仅需一个卫星配备反作用轮即可控制整个编队。

Result: 通过五卫星系统的编队保持和重构数值仿真验证了控制器的有效性。控制器能避免反作用轮饱和，仅需一个卫星配备反作用轮即可实现多卫星控制。

Conclusion: 提出的新型控制器解决了电磁编队飞行中反作用轮饱和问题，减少了硬件需求，为电磁编队飞行提供了更实用的控制方案。

Abstract: Electromagnetic formation flight (EMFF) uses the electromagnetic force to control the relative positions of multiple satellites without using conventional fuel-based propulsion. To compensate for the electromagnetic torque generated alongside the electromagnetic force, in most previous studies, all satellites were assumed to have reaction wheels (RWs) besides electromagnetic coils. However, the RW-loaded angular momentum becomes non-uniformly distributed among the satellites, because the electromagnetic torque usually differs between satellites. Without a proper control scheme, this deviation increases over time, and the RWs become saturated quickly, preventing the attitudes of the satellites from being controlled. In this study, a new controller is proposed that enables the electromagnetic force and torque to be controlled simultaneously. The EMFF kinematics derived from the conservation of angular momentum are used for the controller design. This controller can control $n$ satellites without saturating the RWs, and only one set of RWs is required among all satellites. The combination of the proposed controller with a simple unloading control exclusive to the chief satellite results in the elimination of the accumulation of angular momentum in the entire system. The effectiveness of the proposed controller is demonstrated through numerical simulations of the formation maintenance and formation reconfiguration of a five-satellite system.

</details>


### [37] [Characterizing Human Feedback-Based Control in Naturalistic Driving Interactions via Gaussian Process Regression with Linear Feedback](https://arxiv.org/abs/2512.09097)
*Rachel DiPirro,Rosalyn Devonport,Dan Calderone,Chishang "Mario'' Yang,Wendy Ju,Meeko Oishi*

Main category: eess.SY

TL;DR: 使用驾驶模拟器收集无信号交叉口自然驾驶数据，通过高斯过程回归学习驾驶员的状态反馈控制器，分析反馈增益与行为关系及不同驾驶员群体差异


<details>
  <summary>Details</summary>
Motivation: 理解驾驶员交互对于设计自动驾驶车辆与人类驾驶汽车安全互操作至关重要，需要研究驾驶员在无信号交叉口如何制定决策策略

Method: 使用驾驶模拟器收集受控环境下的自然决策和行为数据，通过高斯过程回归方法将驾驶员响应建模为状态反馈控制器，使用线性和非线性先验的加权组合计算反馈增益

Result: 分析了单个增益如何反映在驾驶员行为中，评估了不同驾驶员群体间控制器的差异

Conclusion: 基于数据的驾驶员策略分析方法有助于未来设计具有社会响应能力的自动驾驶车辆

Abstract: Understanding driver interactions is critical to designing autonomous vehicles to interoperate safely with human-driven cars. We consider the impact of these interactions on the policies drivers employ when navigating unsigned intersections in a driving simulator. The simulator allows the collection of naturalistic decision-making and behavior data in a controlled environment. Using these data, we model the human driver responses as state-based feedback controllers learned via Gaussian Process regression methods. We compute the feedback gain of the controller using a weighted combination of linear and nonlinear priors. We then analyze how the individual gains are reflected in driver behavior. We also assess differences in these controllers across populations of drivers. Our work in data-driven analyses of how drivers determine their policies can facilitate future work in the design of socially responsive autonomy for vehicles.

</details>


### [38] [MPC for momentum counter-balanced and zero-impulse contact with a free-spinning satellite](https://arxiv.org/abs/2512.09213)
*Theofania Karampela,Rishie Seshadri,Florian Dörfler,Sarah H. Q. Li*

Main category: eess.SY

TL;DR: 提出非线性模型预测控制框架，实现服务卫星与自由旋转目标卫星的零冲量接触，通过协调力矩生成和操作两个模块，考虑交叉耦合动力学和约束条件。


<details>
  <summary>Details</summary>
Motivation: 在轨服务任务中，服务卫星需要与自由旋转的目标卫星进行零冲量接触，但现有控制方法难以同时处理两个模块的协调、交叉耦合动力学以及各种操作约束。

Method: 开发非线性模型预测控制框架，显式建模力矩生成模块和操作模块之间的交叉耦合动力学，通过MPC同时控制两个模块，并考虑状态和驱动约束。

Result: 通过数值蒙特卡洛模拟验证，MPC控制器能够在操作约束、接触位置变化以及观测和驱动噪声下，有效保持自旋同步和零冲量接触，性能优于现有控制方法。

Conclusion: 非线性MPC框架成功解决了服务卫星与自由旋转目标卫星的零冲量接触问题，能够处理复杂约束和噪声，为在轨服务任务提供了有效的控制方案。

Abstract: In on-orbit robotics, a servicer satellite's ability to make contact with a free-spinning target satellite is essential to completing most on-orbit servicing (OOS) tasks. This manuscript develops a nonlinear model predictive control (MPC) framework that generates feasible controls for a servicer satellite to achieve zero-impulse contact with a free-spinning target satellite. The overall maneuver requires coordination between two separately actuated modules of the servicer satellite: (1) a moment generation module and (2) a manipulation module. We apply MPC to control both modules by explicitly modeling the cross-coupling dynamics between them. We demonstrate that the MPC controller can enforce actuation and state constraints that prior control approaches could not account for. We evaluate the performance of the MPC controller by simulating zero-impulse contact scenarios with a free-spinning target satellite via numerical Monte Carlo (MC) trials and comparing the simulation results with prior control approaches. Our simulation results validate the effectiveness of the MPC controller in maintaining spin synchronization and zero-impulse contact under operation constraints, moving contact location, and observation and actuation noise.

</details>


### [39] [Electric Arc Furnaces Scheduling under Electricity Price Volatility with Reinforcement Learning](https://arxiv.org/abs/2512.09293)
*Ruonan Pi,Zhiyuan Fan,Bolun Xu*

Main category: eess.SY

TL;DR: 提出基于强化学习的电弧炉调度优化框架，在电价波动下实现实时控制，达到完美预测基准90%的利润水平


<details>
  <summary>Details</summary>
Motivation: 解决电弧炉在波动电价环境下的调度优化问题，传统方法依赖完美电价预测，需要开发能够实时响应价格变化且考虑共享进料容量约束的控制方法

Method: 首先将确定性电弧炉调度问题建模为混合整数线性规划，然后开发Q-learning算法进行实时控制，设计定制奖励函数平滑电弧炉启动惩罚

Result: 使用纽约州真实数据测试，在非预见性控制设置下，强化学习算法在各种单单元和多单元案例中达到完美MILP基准约90%的利润

Conclusion: 强化学习方法能够有效处理电价波动下的电弧炉调度问题，在不需要完美预测的情况下实现接近最优的性能，为工业过程优化提供了实用解决方案

Abstract: This paper proposes a reinforcement learning-based framework for optimizing the operation of electric arc furnaces (EAFs) under volatile electricity prices. We formulate the deterministic version of the EAF scheduling problem into a mixed-integer linear programming (MILP) formulation, and then develop a Q-learning algorithm to perform real-time control of multiple EAF units under real-time price volatility and shared feeding capacity constraints. We design a custom reward function for the Q-learning algorithm to smooth the start-up penalties of the EAFs. Using real data from EAF designs and electricity prices in New York State, we benchmark our algorithm against a baseline rule-based controller and a MILP benchmark, assuming perfect price forecasts. The results show that our reinforcement learning algorithm achieves around 90% of the profit compared to the perfect MILP benchmark in various single-unit and multi-unit cases under a non-anticipatory control setting.

</details>


### [40] [Analysis of Frequency and Voltage Strength in Power Electronics-Dominated Power Systems Based on Eigen-subsystems](https://arxiv.org/abs/2512.09323)
*Huisheng Gao,Linbin Huang,Huanhai Xin,Zhiyi Li,Ping Ju*

Main category: eess.SY

TL;DR: 提出统一框架分析电力系统中频率/电压强度，将系统分解为特征子系统，区分共模和差模响应，发现传统方法忽略的共模电压响应在电力电子主导系统中也可能失稳。


<details>
  <summary>Details</summary>
Motivation: 逆变器资源大规模集成恶化了电力系统的频率/电压响应，增加了不稳定性风险。现有方法通常分别评估频率和电压强度，使用不同的指标（如惯性和短路比），导致对系统整体强度缺乏全面理解，可能忽略关键方面。

Method: 提出统一框架：1）引入频率/电压调节的统一建模；2）基于模态解耦将电力系统分解为多个特征子系统，将频率/电压响应分解为共模和差模分量（CM-F、DM-F、CM-V、DM-V）；3）提出新指标评估各模态分量的强度。

Result: 发现传统强度分析通常忽略的共模电压响应（CM-V）在电力电子主导的电力系统中也可能变得不稳定。通过仿真验证了所提方法的有效性。

Conclusion: 提出的统一框架为分析电力电子主导电力系统中的频率/电压强度提供了全面方法，揭示了传统方法忽略的关键不稳定机制，有助于更准确地评估系统稳定性。

Abstract: The large-scale integration of inverter-based resources (IBRs) has deteriorated the frequency/voltage (F/V) responses of power systems, leading to a higher risk of instability. Consequently, evaluating the F/V strength has become an important task in power electronics (PE)-dominated power systems. Existing methods typically examine F/V strength separately, employing fundamentally different metrics, such as inertia (focusing on device dynamics) and short-circuit ratio (SCR, addressing network characteristics). These fragmented approaches have resulted in a lack of comprehensive understanding of the overall system strength, potentially overlooking critical aspects. To address this problem, this paper proposes a unified framework for analyzing F/V strength. First, a unified modeling of F/V regulations is introduced. Then, based on modal decoupling, the power systems are decomposed into several eigen-subsystems, where the F/V responses are both decomposed into common-mode (CM) and differential-mode (DM) components, namely, CM-F, DM-F, CM-V, and DM-V. The CM-F and CM-V represent the collective response of all devices to external active or reactive power disturbances, independent of the power network characteristics. In contrast, the DM-F and DM-V capture the redistribution of disturbance power within the system, which is strongly influenced by the network topology and the locations of devices. Notably, traditional strength analysis generally ignores the CM-V (global voltage response), which, as discovered in this paper, may also become unstable in PE-dominated power systems. Based on the proposed framework, new metrics are proposed to evaluate the strength of each modal component. Finally, the effectiveness of the proposed approach is validated through simulations.

</details>


### [41] [Intelligent Resilience Testing for Decision-Making Agents with Dual-Mode Surrogate Adaptation](https://arxiv.org/abs/2512.09372)
*Jingxuan Yang,Weichao Xu,Yuchen Shi,Yi Zhang,Shuo Feng,Huaxin Pei*

Main category: eess.SY

TL;DR: IRTest提出了一种统一的在线自适应测试框架，通过在线神经微调和重要性采样加权校正机制，结合贝叶斯优化策略，有效减少代理模型与真实决策智能体之间的差距，提高故障发现效率和测试鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 决策智能体的测试面临系统架构未知、内部状态访问受限、高维场景空间庞大等挑战。现有测试方法依赖代理模型生成场景库，但代理模型与真实决策智能体之间的差异限制了其泛化能力和实际应用性。

Method: IRTest框架包含：1）离线训练的代理预测模型初始化；2）两种互补的自适应机制：数据丰富时的在线神经微调，数据有限时的轻量级重要性采样加权校正；3）配备偏差校正采集函数的贝叶斯优化策略，平衡探索与利用。

Result: 在不同任务复杂度和系统异质性水平的广泛实验中，IRTest一致提高了故障发现效率、测试鲁棒性和跨系统泛化能力。

Conclusion: IRTest作为决策智能体可扩展、自适应和弹性测试的实用解决方案具有巨大潜力，能够有效应对代理模型与真实系统之间的差距问题。

Abstract: Testing and evaluating decision-making agents remains challenging due to unknown system architectures, limited access to internal states, and the vastness of high-dimensional scenario spaces. Existing testing approaches often rely on surrogate models of decision-making agents to generate large-scale scenario libraries; however, discrepancies between surrogate models and real decision-making agents significantly limit their generalizability and practical applicability. To address this challenge, this paper proposes intelligent resilience testing (IRTest), a unified online adaptive testing framework designed to rapidly adjust to diverse decision-making agents. IRTest initializes with an offline-trained surrogate prediction model and progressively reduces surrogate-to-real gap during testing through two complementary adaptation mechanisms: (i) online neural fine-tuning in data-rich regimes, and (ii) lightweight importance-sampling-based weighting correction in data-limited regimes. A Bayesian optimization strategy, equipped with bias-corrected acquisition functions, guides scenario generation to balance exploration and exploitation in complex testing spaces. Extensive experiments across varying levels of task complexity and system heterogeneity demonstrate that IRTest consistently improves failure-discovery efficiency, testing robustness, and cross-system generalizability. These results highlight the potential of IRTest as a practical solution for scalable, adaptive, and resilient testing of decision-making agents.

</details>


### [42] [Time-Discretized Simulation of Vehicle Platoons for Safety Analysis with Guaranteed Error Bounds](https://arxiv.org/abs/2512.09416)
*Yuhao Chen,Ahmet Cetinkaya*

Main category: eess.SY

TL;DR: 本文提出基于仿真的方法来研究车辆编队中无线通信丢包与紧急刹车同时发生时可能引发的安全问题，通过确定保证串级稳定性条件下车辆间绝对最小距离来评估不同控制参数的安全性。


<details>
  <summary>Details</summary>
Motivation: 车辆编队中无线通信是实现协调控制的关键，但通信丢包与紧急刹车同时发生时可能引发严重安全问题。现有研究需要系统方法来评估这种组合故障下的安全性。

Method: 提出基于时间离散化仿真的方法，通过确定车辆间绝对最小距离来评估安全性。提供了两种选择时间步长的方法，确保距离近似误差始终在指定范围内。

Result: 数值实验表明，在保证串级稳定性的控制参数中，某些参数在同时发生通信丢包和紧急刹车的情况下表现更好，能够提供更高的安全性。

Conclusion: 提出的仿真方法能够有效评估车辆编队在通信故障和紧急情况下的安全性，为选择最优控制参数提供了实用工具，有助于提高车辆编队系统的鲁棒性。

Abstract: Wireless communication is essential to achieve coordinated control in vehicle platoons. However, packet losses in wireless communication can cause critical safety issues when they occur in conjunction with sudden brakes. In this paper, we propose simulation-based methods that allow the study of such safety issues by determining the absolute minimum distance between vehicles over time for various control parameters that guarantee string stability. For our proposed time-discretized simulations, we provide two methods for selecting different time-step intervals to ensure that the error in distance approximation remains within specified bounds at all times. Through numerical examples we demonstrate that among control parameters that guarantee string stability some perform better than others under simultaneously occurring packet losses and sudden brakes.

</details>


### [43] [Power Control of Multi-Layer Repeater Networks (POLARNet)](https://arxiv.org/abs/2512.09449)
*Johan Siwerson,Johan Thunberg*

Main category: eess.SY

TL;DR: POLARNet：一种用于多层中继网络的功率控制方法，通过前向-后向方式逐层优化中继放大增益，在理想SISO窄带TDD通信下，该系统可视为深度神经网络的对偶，其中激活函数对应中继放大，权重矩阵对应信道矩阵。


<details>
  <summary>Details</summary>
Motivation: 多层中继网络中，如何在不同中继功率约束下局部优化信噪比（SNR）是一个重要问题。传统方法可能无法有效处理多层网络中的功率分配和优化。

Method: 将多层中继网络视为深度神经网络的对偶系统，其中中继放大增益作为激活函数，信道矩阵作为权重矩阵。采用前向-后向方式逐层在紧致集上优化中继放大增益。该方法适用于多种层内功率/能量约束，无需梯度计算和步长选择，且具有单调性保证。

Result: 数值仿真显示，相比预期SNR的上界，POLARNet能显著改善性能。此外，多层中继间的功率分配优于每层选择单个最优中继的方案。

Conclusion: POLARNet提供了一种有效的多层中继网络功率控制方法，具有理论保证和实际性能优势，特别适用于受功率约束的中继网络优化。

Abstract: In this letter we introduce POLARNet -- power control of multi-layer repeater networks -- for local optimization of SNR given different repeater power constraints. We assume relays or repeaters in groups or layers spatially separated. Under ideal circumstances SISO narrow-band communication and TDD, the system may be viewed as a dual to a deep neural network, where activations, corresponding to repeater amplifications, are optimized and weight matrices, corresponding to channel matrices, are static. Repeater amplifications are locally optimized layer-by-layer in a forward-backward manner over compact sets. The method is applicable for a wide range of constraints on within-layer power/energy utilization, is furthermore gradient-free, step-size-free, and has proven monotonicity in the objective. Numerical simulations show significant improvement compared to upper bounds on the expected SNR. In addition, power distribution over multiple repeaters is shown to be superior to optimal selection of single repeaters in the layers.

</details>


### [44] [Personalized Building Climate Control with Contextual Preferential Bayesian Optimization](https://arxiv.org/abs/2512.09481)
*Wenbin Wang,Jicheng Shi,Colin N. Jones*

Main category: eess.SY

TL;DR: 提出基于上下文偏好贝叶斯优化的建筑控制器调优方法，通过二元偏好反馈和上下文信息实现高效实时调优，在BOPTEST平台上验证效果显著


<details>
  <summary>Details</summary>
Motivation: 建筑气候控制器的高效调优对优化居住者效用至关重要，但直接测量潜在效用困难，且时变上下文因素（如室外温度）使问题复杂化

Method: 提出上下文偏好贝叶斯优化算法，利用二元偏好反馈和上下文信息实现高效的实时控制器调优，并在BOPTEST高保真建筑仿真平台上验证

Result: 在两个月模拟期间，方法优于基线控制器，效用提升达23%；对不同居住者类型，算法能自动适应个体偏好，实现个性化控制器调优

Conclusion: 上下文偏好贝叶斯优化算法能有效解决建筑控制器调优问题，显著提升居住者效用，并支持个性化适应，为智能建筑控制提供实用解决方案

Abstract: Efficient tuning of building climate controllers to optimize occupant utility is essential for ensuring overall comfort and satisfaction. However, this is a challenging task since the latent utility are difficult to measure directly. Time-varying contextual factors, such as outdoor temperature, further complicate the problem. To address these challenges, we propose a contextual preferential Bayesian optimization algorithm that leverages binary preference feedback together with contextual information to enable efficient real-time controller tuning. We validate the approach by tuning an economic MPC controller on BOPTEST, a high-fidelity building simulation platform. Over a two-month simulation period, our method outperforms the baseline controller and achieves an improvement of up to 23% in utility. Moreover, for different occupant types, we demonstrate that the algorithm automatically adapts to individual preferences, enabling personalized controller tuning.

</details>


### [45] [Instantaneous Complex Phase and Frequency: Conceptual Clarification and Equivalence between Formulations](https://arxiv.org/abs/2512.09574)
*César García-Veloso,Mario Paolone,Federico Milano*

Main category: eess.SY

TL;DR: 本文澄清了瞬时复相位和频率的不同定义，展示了在特定假设下它们的等价性，并提出了统一的符号和术语以避免混淆。


<details>
  <summary>Details</summary>
Motivation: 目前关于瞬时复相位和频率存在多种不同的定义，这些定义之间的混淆需要澄清。本文旨在明确这些定义之间的关系，特别是在特定假设下它们的等价性。

Method: 提出了两种基本定义方法：(i) 基于解析信号的方法；(ii) 基于空间向量的方法。展示了这两种方法之间的关系，并分析了它们成立的前提条件。

Result: 明确了不同定义之间的关系，证明了在特定假设下这些定义是等价的。为瞬时复相位和频率提供了清晰的理论框架。

Conclusion: 为了消除混淆，本文提出了统一的符号和术语体系，为瞬时复相位和频率的研究提供了清晰的理论基础。

Abstract: This letter seeks to clarify the different existing definitions of both instantaneous complex phase and frequency as well as their equivalence when specific hypotheses hold. To achieve this, the two fundamental definitions, i.e., those based on either the use of (i) analytic signals or (ii) space vectors, together with the premises used for their formulation, are presented and their relationship shown. Lastly, an unified notation and terminology to avoid confusion is proposed.

</details>


### [46] [Real-Time Non-Smooth MPC for Switching Systems: Application to a Three-Tank Process](https://arxiv.org/abs/2512.09611)
*Hendrik Alsmeier,Felix Häusser,Andreas Knödler,Armin Nurkanović,Anton Pozharskiy,Moritz Diehl,Rolf Findeisen*

Main category: eess.SY

TL;DR: 提出一种实时可行的非光滑模型预测控制方案，用于三水箱物理过程，无需混合整数规划，通过Filippov系统建模和开关检测实现切换系统控制。


<details>
  <summary>Details</summary>
Motivation: 切换系统的实时模型预测控制具有挑战性，因为不连续性和离散模式使数值积分和优化复杂化。现有方法通常需要混合整数规划，计算成本高。

Method: 结合Filippov系统建模与有限元法和开关检测进行时间离散化，形成互补约束数学规划问题，通过光滑非线性规划的同伦方法求解。引入额外模式避免非Lipschitz点和未定义函数值。

Result: 硬件实验显示能高效处理切换事件，实现跨参考变化的模式一致跟踪，正确处理边界并满足约束。在模型失配情况下，跟踪性能和计算时间仍保持在实时限制内。

Conclusion: 提出的非光滑模型预测控制方案在物理三水箱系统上成功实现实时控制，验证了Filippov建模与互补约束数学规划结合的有效性，为切换系统控制提供了实用解决方案。

Abstract: Real-time model predictive control of non-smooth switching systems remains challenging due to discontinuities and the presence of discrete modes, which complicate numerical integration and optimization. This paper presents a real-time feasible non-smooth model predictive control scheme for a physical three-tank process, implemented without mixed-integer formulations. The approach combines Filippov system modeling with finite elements and switch detection for time discretization, leading to a finite-dimensional optimal control problem formulated as a mathematical program with complementarity constraints. The mathematical program is solved via a homotopy of smooth nonlinear programs. We introduce modeling adjustments that make the three-tank dynamics numerically tractable, including additional modes to avoid non-Lipschitz points and undefined function values. Hardware experiments demonstrate efficient handling of switching events, mode-consistent tracking across reference changes, correct boundary handling, and constraint satisfaction. Furthermore, we investigate the impact of model mismatch and show that the tracking performance and computation times remain within real-time limits for the chosen sampling time. The complete controller is implemented using the non-smooth optimal control framework NOSNOC

</details>


### [47] [RIS-Assisted Coordinated Multi-Point ISAC for Low-Altitude Sensing Coverage](https://arxiv.org/abs/2512.09625)
*Ying Zhang,Zeqi Hao,Tingting Zhang*

Main category: eess.SY

TL;DR: 本文研究RIS辅助的协同多点ISAC网络中低空感知覆盖问题，提出联合波束成形和相移设计以最小化总发射功率，同时保证感知信噪比和通信频谱效率。


<details>
  <summary>Details</summary>
Motivation: 低空经济快速发展需要安全保障，需建立有效的感知覆盖方案监测未授权目标。将ISAC引入蜂窝网络可增强感知性能和扩展覆盖，而RIS可缓解城市环境中基站与低空目标间的信号遮挡问题。

Method: 在RIS辅助的协同多点ISAC网络中，采用RIS使多个基站感知指定区域同时服务多个通信用户。提出联合波束成形和相移设计，通过交替优化和半定松弛技术解决非凸优化问题。

Result: 数值结果表明，所提方案在总发射功率最小化方面优于基准方案，同时满足感知信噪比和通信频谱效率要求。

Conclusion: RIS辅助的协同多点ISAC网络能有效解决低空感知覆盖问题，提出的联合优化算法在保证感知和通信性能的同时显著降低系统功耗。

Abstract: The low-altitude economy (LAE) has emerged and developed in various fields, which has gained considerable interest. To ensure the security of LAE, it is essential to establish a proper sensing coverage scheme for monitoring the unauthorized targets. Introducing integrated sensing and communication (ISAC) into cellular networks is a promising solution that enables coordinated multiple base stations (BSs) to significantly enhance sensing performance and extend coverage. Meanwhile, deploying a reconfigurable intelligent surface (RIS) can mitigate signal blockages between BSs and low-altitude targets in urban areas. Therefore, this paper focuses on the low-altitude sensing coverage problem in RIS-assisted coordinated multi-point ISAC networks, where a RIS is employed to enable multiple BSs to sense a prescribed region while serving multiple communication users. A joint beamforming and phase shifts design is proposed to minimize the total transmit power while guaranteeing sensing signal-to-noise ratio and communication spectral efficiency. To tackle this non-convex optimization problem, an efficient algorithm is proposed by using the alternating optimization and semi-definite relaxation techniques. Numerical results demonstrate the superiority of our proposed scheme over the baseline schemes.

</details>


### [48] [Adaptive Optimal Control for Avatar-Guided Motor Rehabilitation in Virtual Reality](https://arxiv.org/abs/2512.09667)
*Francesco De Lellis,Maria Lombardi,Egidio De Benedetto,Pasquale Arpaia,Mario di Bernardo*

Main category: eess.SY

TL;DR: 提出基于最优控制的可解释自适应运动引导框架，通过虚拟现实中的自主化身实现卒中后家庭康复治疗，解决康复治疗可及性、成本和连续性难题。


<details>
  <summary>Details</summary>
Motivation: 解决运动康复中的关键挑战：超过50%患者无法定期参加临床治疗，存在可及性、成本和护理连续性等问题。需要为卒中后患者提供个性化、可监控的家庭康复方案。

Method: 采用非线性人机闭环控制策略，基于Hogan最小抖动模型，通过多目标最优控制平衡跟随患者运动与引导理想运动轨迹。使用数据驱动的"能力指数"（基于平滑度指标）动态调整控制增益，实现自适应康复指导。

Result: 通过仿真和初步试验验证了系统有效性，展示了提供自适应、参与度高且可扩展的远程物理治疗的潜力，基于可解释的控制理论原理。

Conclusion: 该控制理论框架为虚拟现实中的自主化身引导康复提供了可行方案，能够实现个性化、自适应且临床可监控的家庭康复治疗，有望解决康复治疗的可及性和连续性挑战。

Abstract: A control-theoretic framework for autonomous avatar-guided rehabilitation in virtual reality, based on interpretable, adaptive motor guidance through optimal control, is presented. The framework faces critical challenges in motor rehabilitation due to accessibility, cost, and continuity of care, with over 50% of patients inability to attend regular clinic sessions. The system enables post-stroke patients to undergo personalized therapy in immersive virtual reality at home, while being monitored by clinicians. The core is a nonlinear, human-in-the-loop control strategy, where the avatar adapts in real time to the patient's performance. Balance between following the patient's movements and guiding them to ideal kinematic profiles based on the Hogan minimum-jerk model is achieved through multi-objective optimal control. A data-driven "ability index" uses smoothness metrics to dynamically adjust control gains according to the patient's progress. The system was validated through simulations and preliminary trials, and shows potential for delivering adaptive, engaging and scalable remote physiotherapy guided by interpretable control-theoretic principles.

</details>


### [49] [Dynamic one-time delivery of critical data by small and sparse UAV swarms: a model problem for MARL scaling studies](https://arxiv.org/abs/2512.09682)
*Mika Persson,Jonas Lidman,Jacob Ljungberg,Samuel Sandelius,Adam Andersson*

Main category: eess.SY

TL;DR: 该研究探讨了多智能体强化学习在无人机中继关键数据包任务中的应用，提出了确定性游戏框架和基于Dijkstra算法的基线策略，发现现有MARL算法在小规模场景中表现良好但存在可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体强化学习在无人机分散控制中的应用，特别是针对将关键数据包中继到已知位置的任务，旨在探索MARL在复杂协同控制问题中的潜力和局限性。

Method: 引入了一系列确定性游戏框架用于MARL的可扩展性研究，提出了基于限制智能体运动范围和Dijkstra算法的鲁棒基线策略，并测试了两种现成的MARL算法。

Result: 实验结果显示，两种现成的MARL算法在少量智能体场景中与基线策略表现相当，但随着智能体数量增加，出现了可扩展性问题。

Conclusion: 虽然MARL在小规模无人机协同控制中具有潜力，但在大规模场景中面临可扩展性挑战，需要进一步研究改进算法以应对复杂多智能体系统。

Abstract: This work presents a conceptual study on the application of Multi-Agent Reinforcement Learning (MARL) for decentralized control of unmanned aerial vehicles to relay a critical data package to a known position. For this purpose, a family of deterministic games is introduced, designed for scaling studies for MARL. A robust baseline policy is proposed, which is based on restricting agent motion envelopes and applying Dijkstra's algorithm. Experimental results show that two off-the-shelf MARL algorithms perform competitively with the baseline for a small number of agents, but scalability issues arise as the number of agents increase.

</details>


### [50] [Resilient Neural-Variable-Structure Consensus Control for Nonlinear MASs with Singular Input Gain Under DoS Attacks](https://arxiv.org/abs/2512.09879)
*Ladan Khoshnevisan,Xinzhi Liu*

Main category: eess.SY

TL;DR: 提出一种针对遭受DoS攻击和具有奇异控制增益的非线性多智能体系统的可靠学习自适应控制框架，实现领导-跟随者一致性，并保证鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 解决网络物理系统中两个关键挑战：DoS攻击和奇异控制增益。现有弹性多智能体控制方法无法处理奇异控制增益，且依赖Lipschitz连续性等限制性假设。

Method: 开发神经变结构自适应控制器，引入可靠性评估规则检测DoS攻击期间的通信丢失，并激活切换控制机制。该策略不依赖Lipschitz连续性或非线性先验边界假设。

Result: 通过Lyapunov分析证明所有闭环信号的均匀最终有界性，在连接自动驾驶车队上的Matlab/Simulink仿真验证了方法的有效性和弹性。

Conclusion: 首次将神经网络学习、变结构鲁棒性和基于可靠性的切换集成到统一的共识跟踪控制架构中，适用于DoS攻击下具有奇异输入增益的异构非线性多智能体系统。

Abstract: This paper proposes a reliable learning-based adaptive control framework for nonlinear multi-agent systems (MASs) subject to Denial-of-Service (DoS) attacks and singular control gains, two critical challenges in cyber-physical systems. A neural-variable-structure adaptive controller is developed to achieve leader-follower consensus while ensuring robustness to external disturbances and adaptability to unknown nonlinear dynamics. A reliability-assessment rule is introduced to detect communication loss during DoS attacks, upon which a switched control mechanism is activated to preserve closed-loop stability and performance. Unlike existing resilient MAS control methods, the proposed strategy explicitly accommodates singular control gains and does not rely on restrictive assumptions such as Lipschitz continuity or prior bounds on nonlinearities. To the authors' knowledge, this is the first work to integrate neural learning, variable-structure robustness, and reliability-based switching into a unified consensus-tracking control architecture for heterogeneous nonlinear MASs with singular input gains under DoS attacks. Lyapunov-based analysis establishes uniform ultimate boundedness of all closed-loop signals, and Matlab/Simulink simulations on a connected automated vehicle platoon demonstrate the method's effectiveness and resilience.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [51] [Optimal Screening in Experiments with Partial Compliance](https://arxiv.org/abs/2512.09206)
*Christopher Carter,Adeline Delavande,Mario Fiorini,Peter Siminski,Patrick Vu*

Main category: econ.EM

TL;DR: 研究在部分依从性下，实验前可筛选参与者时的最优实验设计。理论表明，保留所有依从者并筛除非依从者可实现三个互补目标：LATE估计不变、中位数偏差最小化、统计功效最大化。实际中依从状态不可观测，讨论了可行筛选策略并提出简单筛选效果检验方法。


<details>
  <summary>Details</summary>
Motivation: 在随机对照试验中，部分参与者不依从实验安排（非依从者）会降低估计精度和统计功效。传统方法无法在实验前识别和筛选非依从者，导致实验效率低下。本研究探索如何在实验前通过筛选提高实验设计效率。

Method: 理论分析部分依从性下的最优实验设计，证明保留依从者、筛除非依从者的理想策略。实际应用中，由于依从状态不可观测，提出可行的筛选策略和简单的筛选效果检验方法。

Result: 理论结果表明：1) 筛选后LATE估计与未筛选的2SLS估计相同；2) 中位数偏差最小化；3) 统计功效最大化。实际中需要开发可行筛选方法，并提出了检验筛选效果的简单方法。

Conclusion: 在部分依从性下，通过实验前筛选参与者可显著提高实验效率，同时保持LATE估计的一致性。未来将通过实验验证最优筛选设计的可行性和优势。

Abstract: This note studies optimal experimental design under partial compliance when experimenters can screen participants prior to randomization. Theoretical results show that retaining all compliers and screening out all non-compliers achieves three complementary aims: (i) the Local Average Treatment Effect is the same as the standard 2SLS estimator with no screening; (ii) median bias is minimized; and (iii) statistical power is maximized. In practice, complier status is unobserved. We therefore discuss feasible screening strategies and propose a simple test for screening efficacy. Future work will conduct an experiment to demonstrate the feasibility and advantages of the optimal screening design.

</details>


### [52] [Debiased Bayesian Inference for High-dimensional Regression Models](https://arxiv.org/abs/2512.09257)
*Qihui Chen,Zheng Fang,Ruixuan Liu*

Main category: econ.EM

TL;DR: 提出一种新的去偏方法，修正整个贝叶斯后验分布的偏差，确保频率论有效性


<details>
  <summary>Details</summary>
Motivation: 基于稀疏诱导先验（如spike-and-slab和horseshoe型）的高维回归模型的贝叶斯推断已有显著进展，但所得后验通常不具备理想的频率论性质，可信集不能作为有效的置信集

Method: 引入一种新颖的去偏方法，修正整个贝叶斯后验分布的偏差，建立新的Bernstein-von Mises定理

Result: 该方法保证了去偏后验的频率论有效性，通过蒙特卡洛模拟和两个经济学实证应用展示了实际性能

Conclusion: 提出的去偏方法能够使基于稀疏先验的贝叶斯后验具备频率论有效性，从而可信集可以作为有效的置信集

Abstract: There has been significant progress in Bayesian inference based on sparsity-inducing (e.g., spike-and-slab and horseshoe-type) priors for high-dimensional regression models. The resulting posteriors, however, in general do not possess desirable frequentist properties, and the credible sets thus cannot serve as valid confidence sets even asymptotically. We introduce a novel debiasing approach that corrects the bias for the entire Bayesian posterior distribution. We establish a new Bernstein-von Mises theorem that guarantees the frequentist validity of the debiased posterior. We demonstrate the practical performance of our proposal through Monte Carlo simulations and two empirical applications in economics.

</details>


### [53] [New Approximation Results and Optimal Estimation for Fully Connected Deep Neural Networks](https://arxiv.org/abs/2512.09853)
*Zhaoji Tang*

Main category: econ.EM

TL;DR: 本文改进了Farrell等人(2021)关于深度前馈神经网络估计器的收敛率，通过推导更窄网络的逼近界，将原定理的次优收敛率提升到最优率（除对数因子外），并展示了深度网络对具有组合结构和流形上函数的维度诅咒缓解能力。


<details>
  <summary>Details</summary>
Motivation: Farrell等人(2021)的定理1对全连接前馈神经网络估计器给出了次优收敛率，作者认为通过改进全连接网络的逼近能力，可以在不改变理论框架的情况下获得更尖锐的收敛结果。

Method: 通过为更窄的全连接深度神经网络推导专门的逼近界，改进Farrell等人(2021)的定理1，使其达到最优收敛率（除对数因子外）。

Result: 成功将Farrell等人(2021)定理1的收敛率从次优改进为最优（除对数因子外），并简要展示了深度神经网络估计器对具有组合结构和流形上函数的维度诅咒缓解能力。

Conclusion: 通过改进全连接深度神经网络的逼近界，可以获得最优收敛率的理论保证，同时深度神经网络在处理具有组合结构和流形上函数时能够有效缓解维度诅咒问题。

Abstract: \citet{farrell2021deep} establish non-asymptotic high-probability bounds for general deep feedforward neural network (with rectified linear unit activation function) estimators, with \citet[Theorem 1]{farrell2021deep} achieving a suboptimal convergence rate for fully connected feedforward networks. The authors suggest that improved approximation of fully connected networks could yield sharper versions of \citet[Theorem 1]{farrell2021deep} without altering the theoretical framework. By deriving approximation bounds specifically for a narrower fully connected deep neural network, this note demonstrates that \citet[Theorem 1]{farrell2021deep} can be improved to achieve an optimal rate (up to a logarithmic factor). Furthermore, this note briefly shows that deep neural network estimators can mitigate the curse of dimensionality for functions with compositional structure and functions defined on manifolds.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [54] [Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study](https://arxiv.org/abs/2512.09088)
*Adrian Ryser,Florian Allwein,Tim Schlippe*

Main category: cs.AI

TL;DR: 研究大语言模型幻觉如何影响用户信任，发现幻觉不会导致全面不信任，而是引发情境化的信任校准，并识别直觉作为新的信任因素。


<details>
  <summary>Details</summary>
Motivation: 大语言模型会产生看似合理但事实错误的幻觉输出，需要研究这些幻觉如何影响用户对LLM的信任以及用户与LLM的互动方式。

Method: 采用定性研究方法，对192名参与者进行实验，基于Lee & See的校准信任模型和Afroogh等人的信任相关因素框架进行分析。

Result: 幻觉不会导致全面不信任，而是引发情境敏感的信任校准；确认期望、先前经验、用户专业知识等信任因素，并识别直觉作为新的用户相关信任因素；信任动态还受感知风险和决策风险等情境因素影响。

Conclusion: 验证了Blöbaum提出的递归信任校准过程，并通过加入直觉因素扩展了该模型；基于研究发现提出了负责任和反思性LLM使用的实践建议。

Abstract: Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Blöbaum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.

</details>


### [55] [AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance](https://arxiv.org/abs/2512.09114)
*Pamela Gupta*

Main category: cs.AI

TL;DR: AI TIPS 2.0框架解决当前AI治理三大挑战：用例级风险评估不足、现有框架缺乏可操作控制、规模化运营机制缺失


<details>
  <summary>Details</summary>
Motivation: 当前AI治理框架存在三个关键问题：1) 缺乏针对具体用例的风险评估，导致像Humana集体诉讼这样的高风险事件；2) 现有框架（如ISO 42001、NIST AI RMF）停留在概念层面，缺乏可操作控制；3) 组织缺乏规模化运营治理的机制

Method: 提出AI TIPS 2.0（人工智能信任集成支柱可持续发展框架），这是对2019年开发的全面运营框架的更新，比NIST AI风险管理框架早四年开发

Result: AI TIPS 2.0直接解决了三大治理挑战：提供用例级风险评估、将治理要求转化为具体技术实现、支持规模化运营治理

Conclusion: AI TIPS 2.0框架为组织提供了可操作的AI治理解决方案，填补了当前框架的空白，支持从董事会到数据科学家的全生命周期可信AI实践

Abstract: The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.

</details>


### [56] [A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem](https://arxiv.org/abs/2512.09117)
*Luciano Floridi,Yiyang Jia,Fernando Tohmé*

Main category: cs.AI

TL;DR: 论文提出一个形式化范畴框架，分析人类和LLM如何将内容转化为关于可能世界状态空间W的真值命题，论证LLM并未解决而是绕过了符号接地问题


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型是否真正解决了符号接地问题，即符号如何与现实世界意义相连接。作者认为需要形式化框架来分析人类和LLM在生成真值命题时的根本差异

Method: 使用范畴论构建形式化框架，分析内容到可能世界状态空间W的真值命题转换过程。通过比较人类和LLM在该框架下的行为差异来论证观点

Result: LLM并未真正解决符号接地问题，而是通过统计模式匹配和训练数据中的关联来绕过该问题，生成看似合理的命题但缺乏真正的语义基础

Conclusion: LLM通过捷径绕过了符号接地问题，这与人类基于真实世界经验建立语义连接的方式有本质区别，这对理解LLM的认知能力和局限性有重要意义

Abstract: This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.

</details>


### [57] [SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation](https://arxiv.org/abs/2512.09142)
*Sergio Burdisso,Séverin Baroudi,Yanis Labrak,David Grunert,Pawel Cyrta,Yiyang Chen,Srikanth Madikeri,Esaú Villatoro-Tello,Thomas Schaaf,Ricard Marxer,Petr Motlicek*

Main category: cs.AI

TL;DR: SDialog是一个开源Python工具包，将对话生成、评估和机制可解释性统一到端到端框架中，用于构建和分析基于LLM的对话代理。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个统一的框架来系统性地构建、评估和理解基于LLM的对话系统。研究人员需要能够同时处理对话生成、评估和机制可解释性的工具。

Method: 围绕标准化的Dialog表示构建，提供：1）基于角色的多代理模拟与可组合编排；2）结合语言指标、LLM作为评判和功能正确性验证器的综合评估；3）通过特征消融和诱导进行激活检查和引导的机制可解释性工具；4）包含3D房间建模和麦克风效果的完整声学模拟音频生成。

Result: SDialog集成了所有主要LLM后端，支持在统一API下进行混合后端实验，使研究人员能够更系统地构建、基准测试和理解对话系统。

Conclusion: 通过将生成、评估和可解释性耦合到以对话为中心的架构中，SDialog为构建、基准测试和理解对话系统提供了更系统化的方法。

Abstract: We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.

</details>


### [58] [Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration](https://arxiv.org/abs/2512.09340)
*Chethana Prasad Kabgere*

Main category: cs.AI

TL;DR: 该研究对比人类与AI在模糊视觉刺激下的图像标注表现，分析两者在感知、推理和决策策略上的异同，为开发神经符号AI架构提供认知基础。


<details>
  <summary>Details</summary>
Motivation: 理解人类与AI系统如何解释模糊视觉刺激，对于揭示感知、推理和决策的本质至关重要。研究旨在对比生物与人工系统在表示、推理和置信度校准方面的异同。

Method: 结合计算认知科学、认知架构和连接主义-符号混合模型，对比人类策略（类比推理、形状识别、置信度调节）与AI的特征处理。基于Marr的三层次假设、Simon的有限理性和Thagard的表示与情感框架，分析参与者反应与Grad-CAM可视化模型注意力。通过ACT-R和Soar认知架构解释人类行为。

Result: 研究发现人类与AI系统在低分辨率、感知退化刺激下的图像标注表现存在关键相似点和差异。人类采用分层启发式决策策略，而AI主要依赖特征处理。分析揭示了生物与人工系统在表示、推理和置信度校准方面的对比特征。

Conclusion: 研究为未来神经符号架构提供理论基础，这些架构将结构化符号推理与连接主义表示相统一，融入具身性、可解释性和认知对齐原则，有望开发出既高性能又可解释、认知基础扎实的AI系统。

Abstract: Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.

</details>


### [59] [Architectures for Building Agentic AI](https://arxiv.org/abs/2512.09458)
*Sławomir Nowaczyk*

Main category: cs.AI

TL;DR: 本文认为AI代理系统的可靠性主要是一个架构属性，通过组件化、接口规范和显式控制循环来确保可靠性，并提出了实用的代理分类和设计指导原则。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理系统变得越来越复杂和自主，确保其可靠性成为关键挑战。本文旨在从架构角度解决AI代理系统的可靠性问题，而不是仅仅关注算法层面。

Method: 将AI代理系统定义为目标导向、使用工具、在闭环中运行的决策系统，通过组件化架构（目标管理器、规划器、工具路由器、执行器、内存、验证器、安全监控器、遥测）、规范接口（模式约束、验证、最小权限工具调用）和显式控制与保证循环来确保可靠性。

Result: 提出了实用的AI代理分类：工具使用代理、记忆增强代理、规划与自我改进代理、多代理系统、具身或网络代理，并分析了每种模式如何重塑可靠性范围和故障模式。提炼了类型化模式、幂等性、权限管理、事务语义、内存溯源与卫生、运行时治理（预算、终止条件）和"模拟-执行"安全机制等设计指导原则。

Conclusion: AI代理系统的可靠性本质上是一个架构问题，通过系统化的组件设计、接口规范和显式控制循环可以显著提高可靠性。基于经典理论基础，本文提供了实用的架构模式和设计原则来构建可靠的AI代理系统。

Abstract: This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.

</details>


### [60] [Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search](https://arxiv.org/abs/2512.09566)
*Junkai Ji,Zhangfan Yang,Dong Xu,Ruibin Bai,Jianqiang Li,Tingjun Hou,Zexuan Zhu*

Main category: cs.AI

TL;DR: Trio是一个集成片段语言建模、强化学习和蒙特卡洛树搜索的分子生成框架，用于高效可解释的靶向分子设计，在结合亲和力、类药性和合成可行性方面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现方法耗时昂贵且成功率低，现有生成模型存在泛化能力不足、可解释性差、过度关注结合亲和力而忽视其他关键药理学性质等问题，限制了实际应用价值。

Method: Trio框架整合三个关键组件：基于片段的分子语言建模实现上下文感知的片段组装；强化学习确保物理化学和合成可行性；蒙特卡洛树搜索平衡新颖化学型探索与有前景中间体利用。

Result: Trio可靠生成化学有效且药理学增强的配体，在结合亲和力（+7.85%）、类药性（+11.10%）和合成可行性（+12.05%）方面优于最先进方法，同时分子多样性扩展超过四倍。

Conclusion: Trio通过集成片段语言建模、强化学习和蒙特卡洛树搜索，提供了一个高效可解释的闭环靶向分子设计框架，在多个关键药理学指标上显著超越现有方法，具有更好的实际应用潜力。

Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.

</details>


### [61] [Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing](https://arxiv.org/abs/2512.09882)
*Justin W. Lin,Eliot Krzysztof Jones,Donovan Julian Jasper,Ethan Jun-shen Ho,Anna Wu,Arnold Tianyi Yang,Neil Perry,Andy Zou,Matt Fredrikson,J. Zico Kolter,Percy Liang,Dan Boneh,Daniel E. Ho*

Main category: cs.AI

TL;DR: AI安全代理ARTEMIS在真实企业环境中与人类安全专家对抗测试，表现优异排名第二，发现9个有效漏洞，击败9/10人类参与者，成本仅为人类专家的30%


<details>
  <summary>Details</summary>
Motivation: 评估AI代理在真实网络安全环境中的实际能力，与人类专业安全人员进行对比，探索AI在网络安全领域的应用潜力和局限性

Method: 在包含约8000台主机、12个子网的大型大学网络中，测试10名网络安全专家、6个现有AI代理和新开发的ARTEMIS多代理框架。ARTEMIS采用动态提示生成、任意子代理和自动漏洞分类技术

Result: ARTEMIS总体排名第二，发现9个有效漏洞，有效提交率82%，优于10名人类参与者中的9人。现有框架如Codex和CyAgent表现不如大多数人类参与者。AI代理在系统枚举、并行利用和成本方面有优势（ARTEMIS变体成本18美元/小时 vs 人类渗透测试者60美元/小时）

Conclusion: AI代理在网络安全领域展现出强大潜力，技术成熟度和提交质量可与最强人类参与者媲美，但存在误报率高、难以处理GUI任务等能力差距。AI代理在成本效益方面具有明显优势

Abstract: We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.

</details>


### [62] [An End-to-end Planning Framework with Agentic LLMs and PDDL](https://arxiv.org/abs/2512.09629)
*Emanuele La Malfa,Ping Zhu,Samuele Marro,Sara Bernardini,Michael Wooldridge*

Main category: cs.AI

TL;DR: 提出一个由验证器支持的端到端规划框架，使用LLM将自然语言规范转换为PDDL模型，通过外部规划引擎生成计划，再翻译回自然语言。


<details>
  <summary>Details</summary>
Motivation: 解决人类用自然语言描述规划问题时存在的模糊性、矛盾和时间约束等问题，实现无需人工干预的端到端自动规划。

Method: 使用编排器和多个基于LLM的代理模块，将自然语言规范迭代细化为PDDL模型，通过外部规划引擎生成计划，最后将计划翻译回自然语言。

Result: 框架在多个领域和任务中表现出灵活性和有效性，包括Google NaturalPlan基准、PlanBench以及Blocksworld和汉诺塔等规划问题。

Conclusion: 该框架可与任何PDDL规划引擎集成，代表了LLM辅助端到端规划的重要进展。

Abstract: We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.

</details>


### [63] [Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions](https://arxiv.org/abs/2512.09727)
*Junlin Xiao,Victor-Alexandru Darvariu,Bruno Lacerda,Nick Hawes*

Main category: cs.AI

TL;DR: 提出一种使用高斯过程回归来估计未在环境中试验过的有前景动作的价值的方法，在连续动作空间的MCTS中优于现有聚合策略


<details>
  <summary>Details</summary>
Motivation: 在连续动作空间环境中，如何最佳地聚合来自不同线程的统计数据是一个重要但尚未充分探索的问题。现有的根并行MCTS在连续动作空间中缺乏有效的统计聚合方法。

Method: 引入高斯过程回归来获取未在环境中试验过的有前景动作的价值估计。该方法在根并行MCTS框架中，利用高斯过程对连续动作空间进行建模，预测未采样动作的价值。

Result: 在6个不同领域进行系统评估，证明该方法优于现有的聚合策略，同时推理时间仅需适度增加。

Conclusion: 提出的基于高斯过程回归的方法为连续动作空间中的根并行MCTS提供了一种有效的统计聚合策略，在保持合理计算开销的同时显著提升了性能。

Abstract: Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.

</details>


### [64] [Analyzing Planner Design Trade-offs for MAPF under Realistic Simulation](https://arxiv.org/abs/2512.09736)
*Jingtian Yan,Zhifei Li,William Kang,Stephen F. Smith,Jiaoyang Li*

Main category: cs.AI

TL;DR: 本文研究了多智能体路径规划(MAPF)算法在实际执行环境下的性能影响因素，包括解决方案最优性与执行性能的关系、运动学建模不准确性的敏感性、以及模型准确性与规划最优性的相互作用。


<details>
  <summary>Details</summary>
Motivation: 现有MAPF评估框架通常使用简化的机器人模型，导致算法基准与实际性能之间存在显著差距。虽然SMART等新框架引入了运动学建模，但规划器设计选择对实际执行性能的影响尚未得到系统研究。

Method: 基于SMART框架的运动学建模能力，系统研究三个关键因素：1) 解决方案最优性与执行性能的关系；2) 系统性能对运动学建模不准确性的敏感性；3) 模型准确性与规划最优性的相互作用。通过实证分析这些因素在实际场景中的影响。

Result: 通过实证研究揭示了规划器设计选择对实际执行性能的具体影响，为理解MAPF算法在实际部署中的表现提供了重要见解。

Conclusion: 本研究填补了MAPF算法评估与实际部署之间的差距，指出了关键的设计考虑因素，并提出了开放挑战和研究方向，以引导社区朝着实际、现实世界部署的方向发展。

Abstract: Multi-Agent Path Finding (MAPF) algorithms are increasingly deployed in industrial warehouses and automated manufacturing facilities, where robots must operate reliably under real-world physical constraints. However, existing MAPF evaluation frameworks typically rely on simplified robot models, leaving a substantial gap between algorithmic benchmarks and practical performance. Recent frameworks such as SMART, incorporate kinodynamic modeling and offer the MAPF community a platform for large-scale, realistic evaluation. Building on this capability, this work investigates how key planner design choices influence performance under realistic execution settings. We systematically study three fundamental factors: (1) the relationship between solution optimality and execution performance, (2) the sensitivity of system performance to inaccuracies in kinodynamic modeling, and (3) the interaction between model accuracy and plan optimality. Empirically, we examine these factors to understand how these design choices affect performance in realistic scenarios. We highlight open challenges and research directions to steer the community toward practical, real-world deployment.

</details>


### [65] [RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning](https://arxiv.org/abs/2512.09829)
*Khurram Khalil,Muhammad Mahad Khaliq,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: RIFT是一个基于强化学习的智能故障定位框架，通过混合灵敏度分析和强化学习自动发现最小化、高影响力的故障场景，显著加速AI加速器的故障评估过程。


<details>
  <summary>Details</summary>
Motivation: 现代AI加速器规模庞大，传统故障评估方法面临计算成本过高、关键故障模式覆盖率不足的问题，需要一种可扩展的自动化解决方案。

Method: 将复杂的最坏情况故障搜索转化为序列决策问题，结合混合灵敏度分析进行搜索空间剪枝，使用强化学习智能生成最小化、高影响力的测试套件。

Result: 在NVIDIA A100 GPU上评估十亿参数大语言模型工作负载，RIFT相比进化方法实现2.2倍加速，相比随机故障注入减少99%以上测试向量，同时获得更优的故障覆盖率。

Conclusion: RIFT提供可操作数据支持智能硬件保护策略，其指导的选择性纠错码相比统一三模冗余保护在成本效益上提升12.8倍，并能自动生成UVM兼容的验证工件，直接集成到商业RTL验证流程。

Abstract: The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \textbf{2.2$\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \textbf{99\%} compared to random fault injection, all while achieving \textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \textbf{12.8$\times$} improvement in \textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.

</details>


### [66] [Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning](https://arxiv.org/abs/2512.09831)
*Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 该论文提出了一个几何框架来建模认知异质智能体之间的信念、动机和影响。每个智能体由个性化价值空间表示，信念被形式化为结构化向量，其传播受线性解释映射调节。信念只有在避免这些映射的零空间时才能存活，这为可理解性、误解和信念消亡提供了结构标准。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决认知异质智能体之间信念传播和理解的根本问题。传统方法依赖共享信息或理性假设，但作者认为这些假设在现实世界中常常不成立。需要一个新的框架来形式化信念如何在具有不同认知结构的主体间传播、变异或消失，为理解人类和人工智能系统中的影响边界提供理论基础。

Method: 采用几何代数方法：1) 每个智能体表示为个性化价值空间（向量空间）；2) 信念形式化为结构化向量（抽象存在）；3) 信念传播通过线性解释映射调节；4) 引入"无零空间领导条件"作为领导力的代数特征；5) 基于结构兼容性而非共享信息来分析信念动态。

Result: 主要成果包括：1) 信念传播的结构标准：信念只有在避免解释映射的零空间时才能存活；2) 领导力的代数特征：领导力是表征可达性的属性而非说服或权威；3) 解释了信念扭曲、动机漂移、反事实评估和相互理解限制如何从纯代数约束中产生；4) 展示了抽象存在如何在多样认知几何中传播、变异或消失。

Conclusion: 该认知几何框架通过将意义保存建立在结构兼容性而非共享信息或理性上，统一了概念空间、社会认识论和AI价值对齐的见解。它阐明了人类和人工系统中影响的认知边界，为分析异质智能体间的信念动态提供了通用基础，对理解跨认知系统的信念传播具有重要理论意义。

Abstract: This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.
  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-"the No-Null-Space Leadership Condition"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.
  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.

</details>


### [67] [Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science](https://arxiv.org/abs/2512.09895)
*Jane Greenberg,Scott McClellan,Addy Ireland,Robert Sammarco,Colton Gerber,Christopher B. Rauch,Mat Kelly,John Kunze,Yuan An,Eric Toberer*

Main category: cs.AI

TL;DR: MatSci-YAMZ平台结合AI和众包人力，通过人机协同开发材料科学元数据词汇表，成功验证了AI-HILT模型在促进FAIR数据原则实施中的可行性。


<details>
  <summary>Details</summary>
Motivation: 元数据词汇表对实现FAIR和FARR数据原则至关重要，但开发过程受限于人力资源不足和标准化实践不一致的问题。

Method: 开发MatSci-YAMZ平台，整合人工智能和人类参与循环（包括众包），在材料科学领域进行概念验证。6名参与者通过平台贡献术语定义和示例，驱动AI定义精炼。

Result: 成功生成19个AI定义，迭代反馈循环验证了AI-HILT精炼的可行性。研究确认了AI-HILT模型的可行性，包括：成功概念验证、与FAIR和开放科学原则一致、提供未来研究指导协议、具备跨领域扩展潜力。

Conclusion: MatSci-YAMZ模型能够增强语义透明度，减少共识构建和元数据词汇表开发所需时间，为跨领域扩展提供了有前景的解决方案。

Abstract: Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.

</details>


### [68] [SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments](https://arxiv.org/abs/2512.09897)
*Haoye Lu,Pavan Seshadri,Kaheer Suleman*

Main category: cs.AI

TL;DR: SCOPE是一种一次性分层规划器，利用LLM生成的子目标仅在初始化时预训练轻量级学生模型，显著提高效率但牺牲可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于LLM的规划方法计算成本高、无法适应目标任务的问题。现有方法在训练和推理期间频繁查询LLM，参数固定不变，效率低下且难以部署。

Method: 提出SCOPE方法：1）仅在初始化时使用LLM生成子目标；2）从示例轨迹直接推导子目标；3）预训练轻量级学生模型；4）避免重复查询LLM，提高效率。

Result: 在TextCraft环境中，SCOPE达到0.56成功率，优于ADaPT的0.52；推理时间从164.4秒大幅减少到3.0秒，效率显著提升。

Conclusion: LLM生成的子目标虽然可能次优，但能为文本规划任务的分层目标分解提供良好起点。SCOPE在保持性能的同时大幅提升效率，为高效规划提供新方向。

Abstract: Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.

</details>


### [69] [Bayesian Networks, Markov Networks, Moralisation, Triangulation: a Categorical Perspective](https://arxiv.org/abs/2512.09908)
*Antonio Lorenzin,Fabio Zanasi*

Main category: cs.AI

TL;DR: 本文提出一个范畴论框架，将贝叶斯网络和马尔可夫网络之间的道德化和三角化转换建模为函子，并通过函子前复合在语法层面定义这些操作。


<details>
  <summary>Details</summary>
Motivation: 传统概率图模型中，道德化（将贝叶斯网络转换为马尔可夫网络）和三角化（相反方向转换）是重要但缺乏统一理论框架的转换。作者希望建立一个范畴论框架来统一描述这些转换，并区分语法和语义层面的操作。

Method: 将贝叶斯网络和马尔可夫网络分别建模为范畴，其中网络对象是从"语法"域到"语义"域的函子。道德化和三角化被定义为这些范畴之间的函子，通过函子前复合在语法层面归纳定义。特别地，变量消去算法被重新解释为函子，将三角化过程分解为纯语法和纯语义两部分。

Result: 成功建立了概率图模型的函子视角，将道德化（纯语法操作）和三角化（依赖语义的操作）统一在范畴论框架下。变量消去算法被重构为函子，清晰分离了三角化过程中的语法和语义成分。

Conclusion: 范畴论框架为概率图模型理论提供了新的视角，突出了语法和语义修改之间的区别。该框架不仅统一了现有转换，还为理解图模型之间的转换关系提供了理论基础。

Abstract: Moralisation and Triangulation are transformations allowing to switch between different ways of factoring a probability distribution into a graphical model. Moralisation allows to view a Bayesian network (a directed model) as a Markov network (an undirected model), whereas triangulation addresses the opposite direction. We present a categorical framework where these transformations are modelled as functors between a category of Bayesian networks and one of Markov networks. The two kinds of network (the objects of these categories) are themselves represented as functors from a `syntax' domain to a `semantics' codomain. Notably, moralisation and triangulation can be defined inductively on such syntax via functor pre-composition. Moreover, while moralisation is fully syntactic, triangulation relies on semantics. This leads to a discussion of the variable elimination algorithm, reinterpreted here as a functor in its own right, that splits the triangulation procedure in two: one purely syntactic, the other purely semantic. This approach introduces a functorial perspective into the theory of probabilistic graphical models, which highlights the distinctions between syntactic and semantic modifications.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [70] [BISTRO - A Bi-Fidelity Stochastic Gradient Framework using Trust-Regions for Optimization Under Uncertainty](https://arxiv.org/abs/2512.09055)
*Thomas O. Dixon,Geoffrey F. Bomarito,James E. Warner,Alex A. Gorodetsky*

Main category: math.OC

TL;DR: BISTRO是一种双保真度随机信任域优化器，通过结合设计空间曲率和随机空间相关性，在不确定性下进行无约束优化，比现有方法快29倍。


<details>
  <summary>Details</summary>
Motivation: 工程系统的随机优化通常不可行，因为需要重复评估计算昂贵的高保真模拟。现有双保真方法要么利用设计空间曲率，要么利用随机空间相关性，但不能同时利用两者。

Method: BISTRO首先利用低保真目标函数的曲率信息收敛到高保真模型的局部最小值盆地，然后切换到方差缩减随机梯度下降过程。该方法结合了信任域优化和方差缩减技术。

Result: 在基准问题和20维航天飞机再入案例中，BISTRO比自适应采样和方差缩减方法收敛更快，计算成本降低高达29倍。

Conclusion: BISTRO通过同时利用设计空间曲率和随机空间相关性，为不确定性下的工程优化提供了高效的双保真方法，具有理论收敛保证和实际性能优势。

Abstract: Stochastic optimization of engineering systems is often infeasible due to repeated evaluations of a computationally expensive, high-fidelity simulation. Bi-fidelity methods mitigate this challenge by leveraging a cheaper, approximate model to accelerate convergence. Most existing bi-fidelity approaches, however, exploit either design-space curvature or random-space correlation, not both. We present BISTRO - a BI-fidelity Stochastic Trust-Region Optimizer for unconstrained optimization under uncertainty through a stochastic approximation procedure. This approach exploits the curvature information of a low-fidelity objective function to converge within a basin of a local minimum of the high-fidelity model where low-fidelity curvature information is no longer valuable. The method then switches to a variance-reduced stochastic gradient descent procedure. We provide convergence guarantees in expectation under certain regularity assumptions and ensure the best-case $\mathcal{O}(1/n)$ convergence rate for stochastic optimization. On benchmark problems and a 20-dimensional space shuttle reentry case, BISTRO converges faster than adaptive sampling and variance reduction procedures and cuts computational expense by up to 29x.

</details>


### [71] [Cyqlone: A Parallel, High-Performance Linear Solver for Optimal Control](https://arxiv.org/abs/2512.09058)
*Pieter Pas,Panagiotis Patrinos*

Main category: math.OC

TL;DR: Cyqlone是一个用于具有阶段最优控制结构的线性系统求解器，它充分利用现代硬件的并行性，结合多种算法实现对数级时间缩放，并支持向量化计算。基于此开发的CyQPALM在二次规划求解中实现了数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 现代最优控制问题需要实时求解长时域问题，但传统方法如顺序Riccati递归的时间复杂度随时域长度线性增长，无法满足实时性要求。同时，现代硬件提供了多层次的并行计算能力（多核、SIMD等），需要开发能充分利用这些并行性的求解器。

Method: Cyqlone统一了顺序Riccati递归、并行Schur补方法和循环约简方法，通过用户可配置的处理器数量实现并行化。它利用批处理线性代数例程实现向量化，使用SIMD操作利用数据并行性。基于Cyqlone开发了CyQPALM，结合并行线搜索和并行因子更新。

Result: 在足够并行性条件下，求解器运行时间随时域长度对数缩放（而非传统方法的线性缩放），能够实时求解长时域问题。批处理例程在小矩阵计算上显著优于BLAS和BLASFEO。CyQPALM相比最先进的HPIPM求解器实现了数量级的加速。

Conclusion: Cyqlone和CyQPALM通过充分利用现代硬件的多层次并行性，为最优控制问题提供了高效的求解方案，实现了对数级时间复杂度和数量级的性能提升，开源实现可供研究使用。

Abstract: We present Cyqlone, a solver for linear systems with a stage-wise optimal control structure that fully exploits the various levels of parallelism available in modern hardware. Cyqlone unifies algorithms based on the sequential Riccati recursion, parallel Schur complement methods, and cyclic reduction methods, thereby minimizing the required number of floating-point operations, while allowing parallelization across a user-configurable number of processors. Given sufficient parallelism, the solver run time scales with the logarithm of the horizon length (in contrast to the linear scaling of sequential Riccati-based methods), enabling real-time solution of long-horizon problems. Beyond multithreading on multi-core processors, implementations of Cyqlone can also leverage vectorization using batched linear algebra routines. Such batched routines exploit data parallelism using single instruction, multiple data (SIMD) operations, and expose a higher degree of instruction-level parallelism than their non-batched counterparts. This enables them to significantly outperform BLAS and BLASFEO for the small matrices that arise in optimal control. Building on this high-performance linear solver, we develop CyQPALM, a parallel and optimal-control-specific variant of the QPALM quadratic programming solver. It combines the parallel and vectorized linear algebra operations from Cyqlone with a parallel line search and parallel factorization updates, resulting in order-of-magnitude speedups compared to the state-of-the-art HPIPM solver. Open-source C++ implementations of Cyqlone and CyQPALM are available at https://github.com/kul-optec/cyqlone

</details>


### [72] [Reactive Vehicle Guidance using Dynamic Maneuvering Cue](https://arxiv.org/abs/2512.09083)
*Alexander Von Moll,Isaac Weintraub*

Main category: math.OC

TL;DR: 提出基于动态机动提示(DMC)的反馈式制导方法，用于动态威胁区域导航，支持实时执行和多威胁处理


<details>
  <summary>Details</summary>
Motivation: 现有动态威胁区域导航方法主要规划完整轨迹，风险评估基于启发式测量，需要更反应式、基于反馈的制导方法，适合机载实时执行

Method: 1) 基于动态机动提示(DMC)概念，测量车辆当前状态下需要转向多少才能脱离威胁区域；2) 扩展到同时处理多个威胁区域；3) 将DMC约束应用于简单反馈控制器和模型预测控制器(MPC)

Result: MPC表现出更好性能但需要在线求解优化问题，简单控制器计算负担小；该方法基于威胁是敌对的假设，可作为碰撞避免和冲突解除的保守方法

Conclusion: 提出的基于DMC的反馈制导方法适合实时执行，能有效处理动态威胁区域导航问题，MPC性能更优但计算成本更高，可作为保守的碰撞避免方案

Abstract: Recent approaches for navigating among dynamic threat regions (i.e., weapon engagement zones) have focused on planning entire trajectories. Moreover, the allowance for penetration into these threat regions was based on heuristic measurements of risk. This paper offers an approach for a more reactive (i.e., feedback-based) guidance that is based on closed-form analytical expressions and thereby suitable for onboard, real-time execution. In addition, a risk measurement is formulated based upon the concept of Dynamic Maneuvering Cue (DMC) which measures the amount of turn a vehicle would need to take in its current state in order to put itself outside the threat region. This approach is then extended to handle multiple threat regions simultaneously (with minimal additional computational complexity). Finally, the DMC constraint is applied to a simple feedback controller as well as a model predictive controller (MPC). The MPC shows better performance but at the cost of having to solve an optimization problem online versus the meager computational burden associated with the simple controller. This approach, which is based on assuming the threats are adversarial, may be used as a conservative method for collision avoidance and deconfliction.

</details>


### [73] [On Event-Triggered Extremum Seeking via Standard and Lie-Bracket Averaging: A Hybrid Dynamical Systems Approach](https://arxiv.org/abs/2512.09113)
*Mahmoud Abdelgalil,Jorge I. Poveda*

Main category: math.OC

TL;DR: 提出并分析了一类事件触发极值搜索算法，用于解决资源感知、无模型优化问题，通过混合系统设计和低通滤波器实现最优点的半全局实际渐近稳定性。


<details>
  <summary>Details</summary>
Motivation: 为了解决资源感知、无模型优化问题，需要设计能够在探索和利用之间平衡的事件触发控制器，减少计算和通信开销。

Method: 采用混合系统框架，结合Lie-Bracket平均化技术，引入低通滤波器，精心设计流集和跳集，构建多时间尺度混合系统。

Result: 控制器使最优点具有半全局实际渐近稳定性，解具有一致半全局驻留时间，可通过调整参数使用传统平均化工具分析。

Conclusion: 事件触发极值搜索算法能有效解决资源感知优化问题，具有良好的稳定性、鲁棒性和理论保证，数值仿真验证了理论结果。

Abstract: We introduce and analyze the stability of a class of event-triggered extremum-seeking algorithms designed to solve resource-aware, model-free, optimization problems. Leveraging recent advances in Lie-Bracket Averaging for hybrid systems, we demonstrate that the proposed controllers can be formulated as well-posed multi-time-scale hybrid systems that satisfy key regularity, stability, and robustness properties. In extremum-seeking systems, exploration and exploitation are inherently coupled. This coupling necessitates careful consideration in the design of the event-triggered controller. To address this challenge, we incorporate a low-pass filter into the algorithm and carefully design the flow and jump sets of the resulting hybrid system. The resulting controller renders the optimal point semi-globally practically asymptotically stable with solutions exhibiting a uniform semi-global dwell time. We also demonstrate how the proposed event-triggered scheme can be modified to allow analysis using traditional averaging tools for hybrid systems by introducing two independent tunable parameters in the controller. Numerical simulations are presented to validate and illustrate the theoretical results.

</details>


### [74] [Second-Order $Λ$-Sets and Extensions to Non-Smooth, Hybrid, and Stochastic Optimal Control](https://arxiv.org/abs/2512.09126)
*Mohammad H. M Rashid*

Main category: math.OC

TL;DR: 本文扩展了Λ集最优控制框架，引入二阶Λ集并将其推广到非光滑、混合和随机混合系统，建立了统一的几何最优控制理论。


<details>
  <summary>Details</summary>
Motivation: 现有Λ集框架主要针对一阶条件和光滑系统，缺乏对二阶信息和非光滑/混合/随机系统的处理能力。需要发展更全面的理论来统一处理各类复杂系统的最优控制问题。

Method: 1) 建立包含可达集曲率信息的二阶必要条件；2) 将框架扩展到Filippov非光滑系统和状态依赖切换的混合系统；3) 引入随机Λ集处理连续扩散和离散随机切换的随机混合系统；4) 通过非完整系统、摩擦机械系统和随机温度控制等实例验证理论。

Result: 提出了统一的Λ集理论框架，能够处理从经典光滑系统到现代随机混合系统的各类最优控制问题，建立了新的最优性必要条件，并连接了经典二阶变分法与Peng随机最大值原理。

Conclusion: 扩展的Λ集理论为分析广泛系统类别的最优控制问题提供了强大的几何工具，统一并推广了现有最大值原理，具有重要的理论和应用价值。

Abstract: This paper develops a comprehensive extension of the $Λ$-set framework for optimal control, introducing second-order $Λ$-sets and generalizing the theory to non-smooth, hybrid, and stochastic hybrid systems. We first establish second-order necessary conditions that incorporate curvature information of the reachable set, providing refined optimality criteria that bridge classical second-variation methods with the geometric $Λ$-set approach. The framework is then extended to Filippov systems with discontinuous dynamics and to hybrid dynamical systems with state-dependent switching, yielding new necessary conditions for optimality in these settings. Furthermore, we introduce stochastic $Λ$-sets for systems subject to both continuous diffusion and discrete random switching, connecting the framework to Peng's stochastic maximum principle. Throughout the paper, detailed examples -- including nonholonomic systems, mechanical systems with friction, and stochastic temperature control -- illustrate the theoretical developments and demonstrate the practical applicability of the extended $Λ$-set theory. The results unify and generalize existing maximum principles, offering a powerful geometric tool for analyzing optimal control problems across a broad spectrum of system classes, from classical smooth systems to modern stochastic hybrid systems.

</details>


### [75] [A Benamou-Brenier Proximal Splitting Method for Constrained Unbalanced Optimal Transport](https://arxiv.org/abs/2512.09250)
*Mao Nishino,Martin Bauer,Tom Needham,Nicolas Charon*

Main category: math.OC

TL;DR: 本文提出了一种带约束的Wasserstein-Fisher-Rao（WFR）模型扩展，允许对密度路径、动量和源项施加仿射等式和不等式约束，并开发了相应的数值求解方法。


<details>
  <summary>Details</summary>
Motivation: 标准Wasserstein-Fisher-Rao模型虽然能够比较任意两个正测度（无需总质量相等），但在实际应用中可能需要施加各种约束条件。先前工作仅考虑了密度路径上的仿射等式约束，本文旨在进一步扩展该框架，允许对动量项和源项施加约束，并引入不等式约束，以涵盖更广泛的实际应用场景。

Method: 1. 提出带约束的WFR模型扩展，允许对密度路径、动量和源项施加仿射等式和不等式约束；2. 在适当假设下证明该凸变分问题的适定性；3. 开发基于有限差分离散化和并行近端算法的数值求解流程，解决相应的约束优化问题。

Result: 1. 证明了在适当约束条件下该凸变分问题的适定性；2. 开发了有效的数值求解框架，能够处理各种约束条件；3. 该框架涵盖了标准平衡/非平衡最优传输以及多种自然且实用的约束类型；4. 通过合成和真实数据示例展示了该方法的通用性。

Conclusion: 本文提出的带约束WFR框架是一个强大的通用工具，能够处理各种实际约束条件，扩展了最优传输理论的应用范围。开发的数值方法为求解这类约束优化问题提供了有效途径，具有广泛的应用潜力。

Abstract: The dynamic formulation of optimal transport, also known as the Benamou-Brenier formulation, has been extended to the unbalanced case by introducing a source term in the continuity equation. When this source term is penalized based on the Fisher-Rao metric, the resulting model is referred to as the Wasserstein-Fisher-Rao (WFR) setting, and allows for the comparison between any two positive measures without the need for equalized total mass. In recent work, we introduced a constrained variant of this model, in which affine integral equality constraints are imposed along the measure path. In the present paper, we propose a further generalization of this framework, which allows for constraints that apply not just to the density path but also to the momentum and source terms, and incorporates affine inequalities in addition to equality constraints. We prove, under suitable assumptions on the constraints, the well-posedness of the resulting class of convex variational problems. The paper is then primarily devoted to developing an effective numerical pipeline that tackles the corresponding constrained optimization problem based on finite difference discretizations and parallel proximal schemes. Our proposed framework encompasses standard balanced and unbalanced optimal transport, as well as a multitude of natural and practically relevant constraints, and we highlight its versatility via several synthetic and real data examples.

</details>


### [76] [Parameter-Free Accelerated Quasi-Newton Method for Nonconvex Optimization](https://arxiv.org/abs/2512.09439)
*Naoki Marumo*

Main category: math.OC

TL;DR: 提出一种用于非凸优化的拟牛顿方法，在Lipschitz连续梯度和Hessian条件下，以$\tilde{\mathrm{O}}(d^{1/4} \varepsilon^{-13/8})$梯度评估复杂度找到$\varepsilon$-稳定点，无需预知问题相关参数。


<details>
  <summary>Details</summary>
Motivation: 现有优化方法通常需要预知Lipschitz常数等参数，或需要提前指定精度和迭代次数，这在实际应用中不切实际。需要开发参数无关的自适应方法。

Method: 结合动量加速、四次正则化子问题求解、以及缩放版Powell对称Broyden(PSB)更新，构建拟牛顿型算法。算法自适应调整参数，无需预知问题特性。

Result: 算法以$\tilde{\mathrm{O}}(d^{1/4} \varepsilon^{-13/8})$梯度评估复杂度找到$\varepsilon$-稳定点，虽然比已知最优复杂度多一个对数因子，但实现了参数无关性，无需预知Lipschitz常数、最优值或精度要求。

Conclusion: 该研究提出了一种实用的参数无关拟牛顿方法，在非凸优化中实现了自适应收敛，为实际应用提供了无需参数调优的优化工具。

Abstract: We propose a quasi-Newton-type method for nonconvex optimization with Lipschitz continuous gradients and Hessians. The algorithm finds an $\varepsilon$-stationary point within $\tilde{\mathrm{O}}(d^{1/4} \varepsilon^{-13/8})$ gradient evaluations, where $d$ is the problem dimension. Although this bound includes an additional logarithmic factor compared with the best known complexity, our method is parameter-free in the sense that it requires no prior knowledge of problem-dependent parameters such as Lipschitz constants or the optimal value. Moreover, it does not need the target accuracy $\varepsilon$ or the total number of iterations to be specified in advance. The result is achieved by combining several key ideas: momentum-based acceleration, quartic regularization for subproblems, and a scaled variant of the Powell-symmetric-Broyden (PSB) update.

</details>


### [77] [Some Remarks on Positive/Negative Feedback](https://arxiv.org/abs/2512.09474)
*Thomas Berger,Achim Ilchmann,Eugene P. Ryan*

Main category: math.OC

TL;DR: 论文挑战了线性控制系统的传统观念，证明在非线性系统中正反馈和负反馈可能同时具有稳定作用


<details>
  <summary>Details</summary>
Motivation: 传统控制理论认为正反馈和负反馈不能同时稳定系统，这在线性系统中成立（如标量系统ẋ=u）。但作者质疑这一观念是否适用于非线性系统，探索非线性系统中反馈稳定性的新可能性。

Method: 通过理论分析对比线性系统和非线性系统的反馈特性。以标量线性系统ẋ=u为例，分析负反馈u=-kx(k>0)的稳定性和正反馈u=kx(k>0)的不稳定性。然后扩展到非线性系统，探索正反馈可能具有稳定作用的非线性机制。

Result: 在线性系统中，负反馈总是稳定的，正反馈总是破坏稳定性。但在非线性系统中，这种直观的二分法可能失效，正反馈和负反馈都有可能稳定系统，揭示了非线性控制中反直觉的现象。

Conclusion: 非线性系统的反馈稳定性比线性系统复杂得多，正反馈和负反馈的稳定作用不能简单类比线性系统的结论。这为非线性控制设计提供了新的理论视角和可能性。

Abstract: In the context of unstable systems with control, a commonly-held precept is that negative and positive feedback cannot both be stabilizing. The canonical linear prototype is the scalar system $\dot x=u$ which, under negative linear feedback $u=-kx$ ($k >0$) is exponentially stable for all $k >0 $, whereas the inherent lack of exponential instability of the uncontrolled system is amplified by positive feedback $u=kx$ ($k >0)$. By contrast, for nonlinear systems it is shown that this intuitively-appealing dichotomy may fail to hold.

</details>


### [78] [Suboptimal open-loop solution of a Stackelberg linear-quadratic differential game with cheap control of a follower: analytical/numerical study](https://arxiv.org/abs/2512.09476)
*Valery Y. Glizer,Vladimir Turetsky*

Main category: math.OC

TL;DR: 研究具有廉价控制特征的双人有限时域线性二次Stackelberg微分博弈，分析其开环解及渐近行为，并应用于供应链问题


<details>
  <summary>Details</summary>
Motivation: 研究具有廉价控制特征的Stackelberg微分博弈，其中跟随者的控制成本在双方成本函数中都很小，这种廉价控制博弈在实际应用中常见（如供应链管理），需要分析其渐近行为并设计近似最优控制策略

Method: 通过博弈可解性条件将问题转化为边界值问题，利用跟随者控制成本小的特性将其视为奇异摄动问题，分析解的渐近行为，进而研究最优控制和成本函数值的渐近特性，并设计渐近次优控制

Result: 成功分析了廉价控制Stackelberg微分博弈的渐近行为，得到了开环最优控制和成本函数值的渐近表达式，设计了渐近次优控制策略，并通过供应链示例验证了理论结果

Conclusion: 对于具有廉价控制特征的Stackelberg微分博弈，可以通过奇异摄动方法有效分析其渐近行为，并设计实用的渐近次优控制策略，为实际应用（如供应链管理）提供了理论支持

Abstract: A two-player finite horizon linear-quadratic Stackelberg differential game is considered. The feature of this game is that the control cost of a follower in the cost functionals of both players is small, which means that the game under consideration is a cheap control game. The open-loop solution of this game is studied. Using the game's solvability conditions, obtaining such a game's solution is reduced to the solution of a proper boundary-value problem. Due to the smallness of the follower's control cost, this boundary-value problem is singularly perturbed. The asymptotic behaviour of the solution to this problem is analysed. Based on this analysis, the asymptotic behaviour of the open-loop optimal players' controls and the optimal values of the cost functionals is studied. Using these results, asymptotically suboptimal players' controls are designed. An illustrative example of a supply chain problem with a small control cost of a retailer is presented.

</details>


### [79] [Trajectory Optimization by Successive Pseudospectral Convexification on Riemannian Manifolds](https://arxiv.org/abs/2512.09551)
*Tatsuya Narumi,Shin-ichiro Sakai*

Main category: math.OC

TL;DR: 提出了一种内蕴伪谱凸化框架，用于处理流形约束的最优控制问题，通过几何一致的转录方法在流形上实现伪谱配点


<details>
  <summary>Details</summary>
Motivation: 传统的伪谱方法在流形约束上存在几何不一致问题，因为插值和微分都是在欧几里得坐标中进行的。需要一种能够在流形上保持几何一致性的凸化方法

Method: 提出内蕴伪谱凸化框架，结合谱配点与连续凸化，引入几何一致的转录方法，使得伪谱配点无需外在施加流形约束，通过一系列凸子问题求解非凸流形约束问题

Result: 方法能够保持流形可行性达到机器精度，通过一个六自由度着陆制导示例（使用单位四元数和单位推力方向向量）证明了方法的实用性

Conclusion: 该框架成功解决了流形约束最优控制问题中的几何不一致问题，提供了一种实用且精确的解决方案

Abstract: This paper proposes an intrinsic pseudospectral convexification framework for optimal control problems with manifold constraints. While successive pseudospectral convexification combines spectral collocation with successive convexification, classical pseudospectral methods are not geometry-consistent on manifolds. This is because interpolation and differentiation are performed in Euclidean coordinates. We introduce a geometry-consistent transcription that enables pseudospectral collocation without imposing manifold constraints extrinsically. The resulting method solves nonconvex manifold-constrained problems through a sequence of convex subproblems. A six-degree-of-freedom landing guidance example with unit quaternions and unit thrust-direction vectors demonstrates the practicality of the approach and preserves manifold feasibility to machine precision.

</details>


### [80] [A tensor phase theory with applications in multilinear control](https://arxiv.org/abs/2512.09559)
*Chengdong Liu,Yimin Wei,Guofeng Zhang*

Main category: math.OC

TL;DR: 该论文为爱因斯坦积下的张量建立了相位理论，并将其应用于多线性控制系统分析，包括张量相位定义、计算方法、相位边界、乘积特征值角度限制，以及张量小相位定理。


<details>
  <summary>Details</summary>
Motivation: 为爱因斯坦积下的张量建立相位理论，以解决多线性控制系统中的稳定性和鲁棒性分析问题，将矩阵相位理论推广到张量领域。

Method: 1) 推导扇形张量的扇形分解以定义张量相位；2) 提出张量相位数值计算方法；3) 建立张量相位的极大极小表达式；4) 定义并刻画复合谱、复合数值范围和复合角数值范围；5) 分析两个扇形张量乘积的特征值角度边界；6) 建立张量版本的小相位定理。

Result: 1) 建立了完整的张量相位理论框架；2) 提供了张量相位计算方法；3) 证明了张量相位的压缩性质；4) 刻画了复合数值结构；5) 建立了乘积特征值角度边界；6) 提出了张量小相位定理，为多线性控制系统分析提供了新工具。

Conclusion: 成功建立了爱因斯坦积下张量的相位理论，并将其应用于多线性控制系统，为系统稳定性和鲁棒性分析提供了强有力的新工具，是矩阵相位理论到张量领域的自然推广。

Abstract: The purpose of this paper is to initiate a phase theory for tensors under the Einstein product, and explore its applications in multilinear control systems. Firstly, the sectorial tensor decomposition for sectorial tensors is derived, which allows us to define phases for sectorial tensors. A numerical procedure for computing phases of a sectorial tensor is also proposed. Secondly, the maximin and minimax expressions for tensor phases are given, which are used to quantify how close the phases of a sectorial tensor are to those of its compressions. Thirdly, the compound spectrum, compound numerical ranges and compound angular numerical ranges of two sectorial tensors $A,B$ are defined and characterized in terms of the compound numerical ranges and compound angular numerical ranges of the sectorial tensors $A,B$. Fourthly, it is shown that the angles of eigenvalues of the product of two sectorial tensors are upper bounded by the sum of their individual phases. Finally, based on the tensor phase theory developed above, a tensor version of the small phase theorem is presented, which can be regarded as a natural generalization of the matrix case, recently proposed in Ref. [10]. The results offer powerful new tools for the stability and robustness analysis of multilinear feedback control systems.

</details>


### [81] [The Ky Fan Norms and Beyond: Dual Norms and Combinations for Matrix Optimization](https://arxiv.org/abs/2512.09678)
*Alexey Kravatskiy,Ivan Kozyrev,Nikolai Kozlov,Alexander Vinogradov,Daniil Merkulov,Ivan Oseledets*

Main category: math.OC

TL;DR: 论文提出基于Ky Fan k-范数对偶的Fanions算法族，包括F-Fanions和S-Fanions，其中F-Muon和S-Muon在实验中与Muon性能相当，并在合成线性最小二乘问题上优于原始Muon。


<details>
  <summary>Details</summary>
Motivation: 现有Muon更新基于谱范数，但矩阵范数选择对大型语言模型训练中的权重矩阵优化至关重要。作者希望探索更多矩阵范数选择，超越单一谱范数的限制。

Method: 利用Ky Fan k-范数的对偶，构建了Fanions算法族。具体包括：1) 将Ky Fan k-范数与Frobenius范数凸组合的F-Fanions；2) 将Ky Fan k-范数与l∞范数凸组合的S-Fanions。其中F-Muon和S-Muon是最重要的成员。

Result: 在广泛任务和设置下的实证研究表明，F-Muon和S-Muon始终与Muon性能相当。在合成线性最小二乘问题上，它们甚至优于原始Muon算法。

Conclusion: 基于Ky Fan k-范数对偶的Fanions算法族为权重矩阵优化提供了新的有效方法，F-Muon和S-Muon作为重要成员在实际应用中表现出色，扩展了Muon类算法的能力。

Abstract: In this article, we explore the use of various matrix norms for optimizing functions of weight matrices, a crucial problem in training large language models. Moving beyond the spectral norm underlying the Muon update, we leverage duals of the Ky Fan $k$-norms to introduce a family of Muon-like algorithms we name Fanions, which are closely related to Dion. By working with duals of convex combinations of the Ky Fan $k$-norms with either the Frobenius norm or the $l_\infty$ norm, we construct the families of F-Fanions and S-Fanions, respectively. Their most prominent members are F-Muon and S-Muon. We complement our theoretical analysis with an extensive empirical study of these algorithms across a wide range of tasks and settings, demonstrating that F-Muon and S-Muon consistently match Muon's performance, while outperforming vanilla Muon on a synthetic linear least squares problem.

</details>


### [82] [Computer-Assisted Search for Differential Equations Corresponding to Optimization Methods and Their Convergence Rates](https://arxiv.org/abs/2512.09712)
*Atsushi Tabei,Ken'ichiro Tanaka*

Main category: math.OC

TL;DR: 提出一个系统化框架，通过符号计算暴力搜索Lyapunov函数的最优设计，以优化连续动力系统的收敛率分析。


<details>
  <summary>Details</summary>
Motivation: 现有连续动力系统收敛率分析中，Lyapunov函数的设计主要依赖启发式方法，缺乏系统化框架。虽然已有研究提出构造性方法，但仍涉及大量任意选择，保留启发式性质。

Method: 将Lyapunov函数设计问题转化为优化问题，采用符号计算的暴力搜索方法，通过计算机代数系统探索所有可能性，系统化地寻找最优Lyapunov函数。

Result: 该框架成功复现了许多已有结果，并在多个案例中发现了现有研究中未展示的新收敛率。

Conclusion: 提出的计算框架为连续动力系统的Lyapunov函数设计提供了系统化方法，能够发现更优的收敛率分析结果。

Abstract: Let $f:\mathbb{R}^n \to \mathbb{R}$ be a continuously differentiable convex function with its minimizer denoted by $x_*$ and optimal value $f_* = f(x_*)$. Optimization algorithms such as the gradient descent method can often be interpreted in the continuous-time limit as differential equations known as continuous dynamical systems. Analyzing the convergence rate of $f(x) - f_*$ in such systems often relies on constructing appropriate Lyapunov functions. However, these Lyapunov functions have been designed through heuristic reasoning rather than a systematic framework. Several studies have addressed this issue. In particular, Suh, Roh, and Ryu (2022) proposed a constructive approach that involves introducing dilated coordinates and applying integration by parts. Although this method significantly improves the process of designing Lyapunov functions, it still involves arbitrary choices among many possible options, and thus retains a heuristic nature in identifying Lyapunov functions that yield the best convergence rates. In this study, we propose a systematic framework for exploring these choices computationally. More precisely, we propose a brute-force approach using symbolic computation by computer algebra systems to explore every possibility. By formulating the design of Lyapunov functions for continuous dynamical systems as an optimization problem, we aim to optimize the Lyapunov function itself. As a result, our framework successfully reproduces many previously reported results and, in several cases, discovers new convergence rates that have not been shown in the existing studies.

</details>


### [83] [A Smooth Approximation Framework for Weakly Convex Optimization](https://arxiv.org/abs/2512.09720)
*Qi Deng,Wenzhi Gao*

Main category: math.OC

TL;DR: 本文提出了一种用于弱凸优化的光滑逼近算法统一框架，推广了光滑化函数概念，将Nesterov型光滑化和Moreau包络光滑化等技巧统一起来，并扩展到非Lipschitz函数的光滑逼近。


<details>
  <summary>Details</summary>
Motivation: 弱凸优化的标准复杂度分析依赖于Moreau包络技术，而非光滑算法（如近端次梯度、近端点方法）隐式地最小化由Moreau包络诱导的光滑代理函数。同时，显式光滑化（直接最小化目标的光滑逼近）长期以来被认为是非光滑优化的有效策略。本文旨在统一这些不同的光滑化技术。

Method: 推广了Beck和Teboulle（2012）为非光滑凸优化提出的光滑化函数概念，建立了一个统一框架，将Nesterov型光滑化和Moreau包络光滑化等技巧纳入其中，为确定性和随机弱凸问题设计光滑逼近算法。

Result: 提出了一个统一的光滑逼近算法框架，具有可证明的复杂度保证。该理论扩展到非Lipschitz函数的光滑逼近，即使在全局Lipschitz连续性不成立的情况下也能进行复杂度分析。

Conclusion: 通过推广光滑化函数概念，为弱凸优化提供了一个统一的光滑逼近框架，统一了多种重要的光滑化技术，并扩展到更广泛的函数类，为非光滑优化算法设计提供了理论基础。

Abstract: Standard complexity analyses for weakly convex optimization rely on the Moreau envelope technique proposed by Davis and Drusvyatskiy (2019). The main insight is that nonsmooth algorithms, such as proximal subgradient, proximal point, and their stochastic variants, implicitly minimize a smooth surrogate function induced by the Moreau envelope. Meanwhile, explicit smoothing, which directly minimizes a smooth approximation of the objective, has long been recognized as an efficient strategy for nonsmooth optimization. In this paper, we generalize the notion of smoothable functions, which was proposed by Beck and Teboulle (2012) for nonsmooth convex optimization. This generalization provides a unified viewpoint on several important smoothing techniques for weakly convex optimization, including Nesterov-type smoothing and Moreau envelope smoothing. Our theory yields a framework for designing smooth approximation algorithms for both deterministic and stochastic weakly convex problems with provable complexity guarantees. Furthermore, our theory extends to the smooth approximation of non-Lipschitz functions, allowing for complexity analysis even when global Lipschitz continuity does not hold.

</details>


### [84] [On Parameter Identification in Three-Dimensional Elasticity and Discretisation with Physics-Informed Neural Networks](https://arxiv.org/abs/2512.09754)
*Federica Caforio,Martin Holler,Matthias Höfler*

Main category: math.OC

TL;DR: 该论文提出了一种基于物理信息神经网络的弹性力学三维反问题求解方法，证明了稳定性估计，并与传统网格方法进行了比较。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络在科学机器学习中显示出强大潜力，但在训练稳定性和理论保证方面仍存在挑战，特别是在三维弹性力学参数识别反问题中，这对于心脏生物力学无创诊断具有重要意义。

Method: 采用一次性优化框架，通过最小二乘损失函数同时估计状态和参数，该损失函数编码了可用数据和支配物理规律。使用神经网络离散化，并与传统网格方法进行比较。

Result: 证明了稳定性估计，确保该方法能够稳定地近似物理系统的真实参数，且不依赖于特定离散化。通过数值示例验证了理论发现。

Conclusion: 该方法为三维弹性力学参数识别反问题提供了理论保证的稳定求解方案，在心脏生物力学等应用中具有重要价值，同时展示了神经网络离散化与传统网格方法的比较优势。

Abstract: Physics-informed neural networks have emerged as a powerful tool in the scientific machine learning community, with applications to both forward and inverse problems. While they have shown considerable empirical success, significant challenges remain -- particularly regarding training stability and the lack of rigorous theoretical guarantees, especially when compared to classical mesh-based methods. In this work, we focus on the inverse problem of identifying a spatially varying parameter in a constitutive model of three-dimensional elasticity, using measurements of the system's state. This setting is especially relevant for non-invasive diagnosis in cardiac biomechanics, where one must also carefully account for the type of boundary data available. To address this inverse problem, we adopt an all-at-once optimisation framework, simultaneously estimating the state and parameter through a least-squares loss that encodes both available data and the governing physics. For this formulation, we prove stability estimates ensuring that our approach yields a stable approximation of the underlying ground-truth parameter of the physical system independent of a specific discretisation. We then proceed with a neural network-based discretisation and compare it to traditional mesh-based approaches. Our theoretical findings are complemented by illustrative numerical examples.

</details>


### [85] [Stochastic Fleet Size and Mix Consistent Vehicle Routing Problem for Last Mile Delivery](https://arxiv.org/abs/2512.09764)
*Paolo Beatrici,Sebastian Birolini,Francesca Maggioni,Paolo Malighetti*

Main category: math.OC

TL;DR: 本文提出了一种两阶段随机混合整数规划模型，用于在不确定客户需求下联合优化车队规模和组合以及车辆路径规划，并设计了基于核搜索的启发式算法来提高可扩展性。


<details>
  <summary>Details</summary>
Motivation: 在物流配送中，客户需求的不确定性给车队规划和路径优化带来了挑战。传统方法通常将车队规模和路径规划分开处理，无法有效应对需求波动，导致成本增加或服务水平下降。

Method: 采用两阶段随机混合整数规划模型：第一阶段决定车队组成和基准路线设计；第二阶段引入近似补偿行动来根据实际需求调整初始路线。为解决计算复杂性，提出了基于路径的模型重构和基于核搜索的启发式算法。

Result: 在小规模合成实例上验证了模型有效性，并通过标准随机度量评估了需求随机性的影响。在基于意大利邮政公司数据的大规模实际实例测试中，证明了所提方法的有效性，并提供了管理和实践见解。

Conclusion: 该方法能够有效处理不确定需求下的车队规模和路径联合优化问题，提出的启发式算法具有良好的可扩展性，为实际物流运营提供了实用的决策支持工具。

Abstract: In this paper, we address the joint optimization of fleet size and mix, along with vehicle routing, under uncertain customer demand. We propose a two-stage stochastic mixed-integer programming model, where first-stage decisions concern the composition of the delivery fleet and the design of consistent baseline routes. In the second stage, approximate recourse actions are introduced to adapt the initial routes in response to realized customer demands. The objective is to minimize the total delivery cost, including vehicle acquisition, travel distance, and penalty costs for unserved demand. To tackle the computational challenges arising in realistic problem instances, we develop a path-based reformulation of the model and design a Kernel Search-based heuristic to enhance scalability. Computational experiments on small synthetic instances, generated through a population-density-based sampling approach, are conducted to validate the formulation and assess the effects of demand stochasticity through standard stochastic measures, after applying a scenario reduction technique. Additional tests on large-scale real-world instances, based on data from the Italian postal company, demonstrate the effectiveness of the proposed approach and provide managerial and practical insights.

</details>


### [86] [Optimal strategy against straightforward bidding in clock auctions](https://arxiv.org/abs/2512.09788)
*Jad Zeroual,Marianne Akian,Aurélien Bechler,Matthieu Chardy,Stéphane Gaubert*

Main category: math.OC

TL;DR: 研究5G频谱拍卖中竞拍者的最优策略，使用POMDP建模，在对手采用直接竞价策略且估值未知的情况下，推导出具有简洁统计量的最优策略，且该策略在对手偏离直接竞价时仍能保持收益不降。


<details>
  <summary>Details</summary>
Motivation: 研究5G频谱拍卖（特别是法国案例）中竞拍者的决策问题。在现实拍卖中，竞拍者通常不知道竞争对手的估值，但竞争对手可能采用直接竞价策略。需要为竞拍者设计最优竞价策略，以最大化预期收益。

Method: 采用部分可观测马尔可夫决策过程（POMDP）对拍卖进行建模。该模型的关键创新在于找到了简洁的统计量，避免了在信念空间求解动态规划方程。假设竞争对手采用直接竞价策略，但他们的估值对主竞拍者未知。

Result: 推导出了竞拍者的最优策略，该策略具有简洁的统计表示。更重要的是，该策略具有鲁棒性：即使竞争对手偏离直接竞价策略，采用该最优策略的竞拍者的预期收益也不会下降。通过数值实验验证了结果，并与完全信息情况下的价值进行了比较。

Conclusion: 成功为5G频谱拍卖设计了一个基于POMDP的最优竞价策略，该策略不仅计算高效（具有简洁统计量），而且具有鲁棒性，能够应对竞争对手的策略变化。为实际拍卖参与提供了理论指导。

Abstract: We study a model of auction representative of the 5G auction in France. We determine the optimal strategy of a bidder, assuming that the valuations of competitors are unknown to this bidder and that competitors adopt the straightforward bidding strategy. Our model is based on a Partially Observable Markov Decision Process (POMDP). This POMDP admits a concise statistics, avoiding the solution of a dynamic programming equation in the space of beliefs. In addition, under this optimal strategy, the expected gain of the bidder does not decrease if competitors deviate from straightforward bidding. We illustrate our results by numerical experiments, comparing the value of the bidder with the value of a perfectly informed one.

</details>


### [87] [Stabilization of a chain of 3 hyperbolic PDEs with 2 inputs in arbitrary position](https://arxiv.org/abs/2512.09799)
*Adam Braun,Jean Auriol,Lucas Brivadis*

Main category: math.OC

TL;DR: 本文提出了一种统一框架，用于稳定由三个耦合双曲偏微分方程组成的链，其中两个控制输入可施加在网络任意节点上。通过结合反步变换和积分差分方程表示，揭示了不同配置下的共同结构模式。


<details>
  <summary>Details</summary>
Motivation: 研究三个耦合双曲PDE链在任意节点施加两个控制输入时的稳定问题。除了文献中已充分研究的端点输入配置外，本文旨在为所有可接受的两输入配置提供统一的分析框架。

Method: 采用反步变换与闭环动力学重新表述为积分差分方程相结合的方法。IDE表示揭示了不同配置下的共同结构模式，并阐明了延迟动力学在稳定性分析中的作用。

Result: 对于大多数配置，PDE系统的稳定需要近似谱可控性假设。但存在一个特定配置可以在不施加任何额外谱条件的情况下实现稳定。同时提供了一个明确示例，说明某些配置下所需的谱可控性条件无法满足。

Conclusion: 提出的统一框架能够处理所有可接受的两输入配置，通过IDE表示揭示了共同结构模式，为耦合双曲PDE系统的稳定提供了新的理论工具和见解。

Abstract: This paper addresses the stabilization of a chain of three coupled hyperbolic partial differential equations actuated by two control inputs applied at arbitrary nodes of the network. With the exception of configurations where one input is located at an endpoint, cases already well studied in the literature, all admissible two-inputs configurations are treated in this paper within a unified framework. The proposed approach relies on a backstepping transformation combined with a reformulation of the closed-loop dynamics as an Integral Difference Equation (IDE). This IDE representation reveals a common structural pattern across configurations and clarifies the role played by delayed dynamics in the stability analysis. Within this formulation, the stabilization problem can be handled using existing IDE control techniques. For most configurations, the stabilization of the PDE system requires an approximate spectral controllability assumption. Remarkably, one specific configuration can be stabilized without imposing any additional spectral condition. In contrast, we also provide an explicit example of a configuration for which the required spectral controllability property fails to hold.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [88] [A Granular Framework for Construction Material Price Forecasting: Econometric and Machine-Learning Approaches](https://arxiv.org/abs/2512.09360)
*Boge Lyu,Qianye Yin,Iris Denise Tommelein,Hanyang Liu,Karnamohit Ranka,Karthik Yeluripati,Junzhe Shi*

Main category: cs.LG

TL;DR: 提出基于CSI MasterFormat的建筑材料价格预测框架，整合原材料价格等解释变量，LSTM模型表现最佳，显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 建筑材料价格持续波动给成本估算、预算编制和项目交付带来重大风险，需要更精细和可扩展的预测方法。

Method: 以CSI MasterFormat为目标数据结构，整合原材料价格、商品指数和宏观经济指标等解释变量，评估LSTM、ARIMA、VECM和Chronos-Bolt四种时序模型。

Result: 加入解释变量显著提升所有模型性能，LSTM模型表现最佳（RMSE=1.390，MAPE=0.957），比传统ARIMA模型提升达59%。验证了框架的可扩展性。

Conclusion: 该研究为业主和承包商提供了可靠的成本估算方法，可在确定级别改善预算实践。

Abstract: The persistent volatility of construction material prices poses significant risks to cost estimation, budgeting, and project delivery, underscoring the urgent need for granular and scalable forecasting methods. This study develops a forecasting framework that leverages the Construction Specifications Institute (CSI) MasterFormat as the target data structure, enabling predictions at the six-digit section level and supporting detailed cost projections across a wide spectrum of building materials. To enhance predictive accuracy, the framework integrates explanatory variables such as raw material prices, commodity indexes, and macroeconomic indicators. Four time-series models, Long Short-Term Memory (LSTM), Autoregressive Integrated Moving Average (ARIMA), Vector Error Correction Model (VECM), and Chronos-Bolt, were evaluated under both baseline configurations (using CSI data only) and extended versions with explanatory variables. Results demonstrate that incorporating explanatory variables significantly improves predictive performance across all models. Among the tested approaches, the LSTM model consistently achieved the highest accuracy, with RMSE values as low as 1.390 and MAPE values of 0.957, representing improvements of up to 59\% over the traditional statistical time-series model, ARIMA. Validation across multiple CSI divisions confirmed the framework's scalability, while Division 06 (Wood, Plastics, and Composites) is presented in detail as a demonstration case. This research offers a robust methodology that enables owners and contractors to improve budgeting practices and achieve more reliable cost estimation at the Definitive level.

</details>


### [89] [Optimizing Algorithms for Mobile Health Interventions with Active Querying Optimization](https://arxiv.org/abs/2512.08950)
*Aseel Rawashdeh*

Main category: cs.LG

TL;DR: 论文提出了一种贝叶斯扩展的ATM算法，用卡尔曼滤波式贝叶斯更新替代标准Q学习，在移动健康干预中实现更稳定、样本效率更高的强化学习。


<details>
  <summary>Details</summary>
Motivation: 移动健康干预中的强化学习需要在干预效果和用户负担之间取得平衡，特别是在状态测量成本高但至关重要的情况下。标准的ATM算法使用时间差分Q学习方法，在稀疏和嘈杂环境中容易不稳定。

Method: 提出贝叶斯ATM扩展，用卡尔曼滤波式贝叶斯更新替代标准Q学习，维护具有不确定性的Q值估计，实现更稳定和样本效率更高的学习。

Result: 在小型表格环境中，贝叶斯ATM实现了相当或改进的标量化回报，方差显著降低，策略行为更稳定。但在更大更复杂的移动健康设置中，标准和贝叶斯ATM变体都表现不佳。

Conclusion: 不确定性感知方法在低数据设置中有价值，但需要新的强化学习算法来显式建模因果结构、连续状态和观测成本约束下的延迟反馈。

Abstract: Reinforcement learning in mobile health (mHealth) interventions requires balancing intervention efficacy with user burden, particularly when state measurements (for example, user surveys or feedback) are costly yet essential. The Act-Then-Measure (ATM) heuristic addresses this challenge by decoupling control and measurement actions within the Action-Contingent Noiselessly Observable Markov Decision Process (ACNO-MDP) framework. However, the standard ATM algorithm relies on a temporal-difference-inspired Q-learning method, which is prone to instability in sparse and noisy environments. In this work, we propose a Bayesian extension to ATM that replaces standard Q-learning with a Kalman filter-style Bayesian update, maintaining uncertainty-aware estimates of Q-values and enabling more stable and sample-efficient learning. We evaluate our method in both toy environments and clinically motivated testbeds. In small, tabular environments, Bayesian ATM achieves comparable or improved scalarized returns with substantially lower variance and more stable policy behavior. In contrast, in larger and more complex mHealth settings, both the standard and Bayesian ATM variants perform poorly, suggesting a mismatch between ATM's modeling assumptions and the structural challenges of real-world mHealth domains. These findings highlight the value of uncertainty-aware methods in low-data settings while underscoring the need for new RL algorithms that explicitly model causal structure, continuous states, and delayed feedback under observation cost constraints.

</details>


### [90] [Learning When to Ask: Simulation-Trained Humanoids for Mental-Health Diagnosis](https://arxiv.org/abs/2512.08952)
*Filippo Cenacchi,Deborah Richards,Longbing Cao*

Main category: cs.LG

TL;DR: 开发了一个用于训练人形机器人心理访谈技能的虚拟仿真系统，将真实访谈数据转化为276个虚拟患者，使用强化学习训练对话控制器，TD3算法在社交时机把握和完整性方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 在真实人形机器人上进行心理访谈训练存在速度慢、设备磨损、迭代受限等问题，而现有仿真系统大多忽略非语言动态的交互学习，控制器过于关注任务准确性而忽视信任、节奏和融洽关系等社交要素。

Method: 创建了以智能体为中心的仿真优先流程：将访谈数据转化为276个Unreal Engine MetaHuman虚拟患者，包含同步的语音、注视/面部表情和头躯干姿态。采用感知-融合-策略循环决定说话内容与时机、反馈时机和避免打断，在安全保护下训练。使用反事实回放（有界非语言扰动）和不确定性感知的轮次管理器来减少诊断模糊性。

Result: 在三种控制器比较中，自定义TD3（Twin Delayed DDPG）优于PPO和CEM，实现了接近完美的覆盖率，在相似奖励下保持更稳定的节奏。决策质量分析显示可忽略的轮次重叠、对齐的打断时机、更少的澄清提示和更短的等待时间。性能在模态丢失和渲染器更换下保持稳定，在保留患者集上排名一致。

Conclusion: 该研究提出了一个完整的虚拟训练系统，能够有效训练人形机器人的心理访谈技能，特别在社交时机把握和完整性方面取得显著改进，为临床监督下的人形机器人试点应用奠定了基础。

Abstract: Testing humanoid robots with users is slow, causes wear, and limits iteration and diversity. Yet screening agents must master conversational timing, prosody, backchannels, and what to attend to in faces and speech for Depression and PTSD. Most simulators omit policy learning with nonverbal dynamics; many controllers chase task accuracy while underweighting trust, pacing, and rapport. We virtualise the humanoid as a conversational agent to train without hardware burden. Our agent-centred, simulation-first pipeline turns interview data into 276 Unreal Engine MetaHuman patients with synchronised speech, gaze/face, and head-torso poses, plus PHQ-8 and PCL-C flows. A perception-fusion-policy loop decides what and when to speak, when to backchannel, and how to avoid interruptions, under a safety shield. Training uses counterfactual replay (bounded nonverbal perturbations) and an uncertainty-aware turn manager that probes to reduce diagnostic ambiguity. Results are simulation-only; the humanoid is the transfer target. In comparing three controllers, a custom TD3 (Twin Delayed DDPG) outperformed PPO and CEM, achieving near-ceiling coverage with steadier pace at comparable rewards. Decision-quality analyses show negligible turn overlap, aligned cut timing, fewer clarification prompts, and shorter waits. Performance stays stable under modality dropout and a renderer swap, and rankings hold on a held-out patient split. Contributions: (1) an agent-centred simulator that turns interviews into 276 interactive patients with bounded nonverbal counterfactuals; (2) a safe learning loop that treats timing and rapport as first-class control variables; (3) a comparative study (TD3 vs PPO/CEM) with clear gains in completeness and social timing; and (4) ablations and robustness analyses explaining the gains and enabling clinician-supervised humanoid pilots.

</details>


### [91] [An Electrocardiogram Multi-task Benchmark with Comprehensive Evaluations and Insightful Findings](https://arxiv.org/abs/2512.08954)
*Yuhao Xu,Jiaying Lu,Sirui Ding,Defu Cao,Xiao Hu,Carl Yang*

Main category: cs.LG

TL;DR: 该研究评估了基础模型在ECG分析中的有效性，发现通用时间序列/ECG基础模型能达到80%的顶级性能表现，证明了它们在心电图分析中的实用性。


<details>
  <summary>Details</summary>
Motivation: 心电图作为非侵入性心脏活动监测方法，分析需要专业知识，这阻碍了AI在医疗保健中的应用。虽然自监督学习和基础模型使AI系统能够获取领域知识，但缺乏对ECG基础模型性能的全面分析。

Method: 通过将语言/通用时间序列/ECG基础模型与时间序列深度学习模型进行比较评估，使用公开可用的ECG多任务基准数据集进行实验。

Result: 实验结果显示，通用时间序列/ECG基础模型达到了80%的顶级性能率，表明它们在ECG分析中具有有效性。研究提供了深入分析和全面实验结果。

Conclusion: 该研究强调了基础模型在推进生理波形分析方面的局限性和潜力，为ECG分析的基础模型应用提供了基准和见解，相关数据和代码已公开。

Abstract: In the process of patient diagnosis, non-invasive measurements are widely used due to their low risks and quick results. Electrocardiogram (ECG), as a non-invasive method to collect heart activities, is used to diagnose cardiac conditions. Analyzing the ECG typically requires domain expertise, which is a roadblock to applying artificial intelligence (AI) for healthcare. Through advances in self-supervised learning and foundation models, AI systems can now acquire and leverage domain knowledge without relying solely on human expertise. However, there is a lack of comprehensive analyses over the foundation models' performance on ECG. This study aims to answer the research question: "Are Foundation Models Useful for ECG Analysis?" To address it, we evaluate language/general time-series/ECG foundation models in comparison with time-series deep learning models. The experimental results show that general time-series/ECG foundation models achieve a top performance rate of 80%, indicating their effectiveness in ECG analysis. In-depth analyses and insights are provided along with comprehensive experimental results. This study highlights the limitations and potential of foundation models in advancing physiological waveform analysis. The data and code for this benchmark are publicly available at https://github.com/yuhaoxu99/ECGMultitasks-Benchmark.

</details>


### [92] [LLM4XCE: Large Language Models for Extremely Large-Scale Massive MIMO Channel Estimation](https://arxiv.org/abs/2512.08955)
*Renbin Li,Shuangshuang Li,Peihao Dong*

Main category: cs.LG

TL;DR: LLM4XCE：利用大语言模型进行XL-MIMO信道估计的新框架，通过语义建模能力在混合场条件下实现更准确的信道恢复


<details>
  <summary>Details</summary>
Motivation: XL-MIMO作为6G网络的关键技术，其混合场信道（同时存在近场和远场效应）给传统信道估计方法带来巨大挑战。传统方法在泛化能力方面表现不佳，而大语言模型在语义理解和任务导向通信方面展现出强大能力，这启发了将LLM应用于信道估计的研究。

Method: 提出LLM4XCE框架：1）设计专门的嵌入模块与并行特征-空间注意力机制，深度融合导频特征和空间结构，构建语义丰富的LLM输入表示；2）仅微调Transformer的顶部两层，有效捕获导频数据中的潜在依赖关系，同时保证训练效率。

Result: 大量仿真实验表明，LLM4XCE在混合场条件下显著优于现有最先进方法，实现了更优的估计精度和泛化性能。

Conclusion: LLM4XCE成功将大语言模型的语义建模能力应用于XL-MIMO信道估计，为解决混合场信道估计难题提供了有效方案，展现了LLM在无线通信领域的应用潜力。

Abstract: Extremely large-scale massive multiple-input multiple-output (XL-MIMO) is a key enabler for sixth-generation (6G) networks, offering massive spatial degrees of freedom. Despite these advantages, the coexistence of near-field and far-field effects in hybrid-field channels presents significant challenges for accurate estimation, where traditional methods often struggle to generalize effectively. In recent years, large language models (LLMs) have achieved impressive performance on downstream tasks via fine-tuning, aligning with the semantic communication shift toward task-oriented understanding over bit-level accuracy.
  Motivated by this, we propose Large Language Models for XL-MIMO Channel Estimation (LLM4XCE), a novel channel estimation framework that leverages the semantic modeling capabilities of large language models to recover essential spatial-channel representations for downstream tasks. The model integrates a carefully designed embedding module with Parallel Feature-Spatial Attention, enabling deep fusion of pilot features and spatial structures to construct a semantically rich representation for LLM input. By fine-tuning only the top two Transformer layers, our method effectively captures latent dependencies in the pilot data while ensuring high training efficiency. Extensive simulations demonstrate that LLM4XCE significantly outperforms existing state-of-the-art methods under hybrid-field conditions, achieving superior estimation accuracy and generalization performance.

</details>


### [93] [DW-KNN: A Transparent Local Classifier Integrating Distance Consistency and Neighbor Reliability](https://arxiv.org/abs/2512.08956)
*Kumarjit Pathak,Karthik K,Sachin Madan,Jitin Kapila*

Main category: cs.LG

TL;DR: DW-KNN是一种改进的KNN分类器，通过结合指数距离和邻居有效性双重加权，提高预测可靠性并降低超参数敏感性。


<details>
  <summary>Details</summary>
Motivation: 传统KNN及其变体假设所有k个邻居同等可靠，但在异构特征空间中，这种假设限制了预测的可靠性。需要一种更透明、稳健的方法来处理噪声和误标记样本。

Method: 提出DW-KNN（双重加权KNN），集成指数距离加权和邻居有效性加权，实现实例级可解释性，抑制噪声或误标记样本，减少超参数敏感性。

Result: 在9个数据集上评估，DW-KNN平均准确率达到0.8988，在6种方法中排名第2，与最佳集成KNN相差仅0.2%。交叉验证方差最低（0.0156），表明预测稳定性可靠。统计显著性测试确认优于紧凑性加权KNN（+4.09%）和核加权KNN（+1.13%）。

Conclusion: DW-KNN为复杂自适应方案提供了简单有效的替代方案，特别适用于需要可解释预测的高风险应用场景，具有透明性和稳健性优势。

Abstract: K-Nearest Neighbors (KNN) is one of the most used ML classifiers. However, if we observe closely, standard distance-weighted KNN and relative variants assume all 'k' neighbors are equally reliable. In heterogeneous feature space, this becomes a limitation that hinders reliability in predicting true levels of the observation.
  We propose DW-KNN (Double Weighted KNN), a transparent and robust variant that integrates exponential distance with neighbor validity. This enables instance-level interpretability, suppresses noisy or mislabeled samples, and reduces hyperparameter sensitivity.
  Comprehensive evaluation on 9 data-sets helps to demonstrate that DW-KNN achieves 0.8988 accuracy on average. It ranks 2nd among six methods and within 0.2% of the best-performing Ensemble KNN. It also exhibits the lowest cross-validation variance (0.0156), indicating reliable prediction stability. Statistical significance test confirmed ($p < 0.001$) improvement over compactness weighted KNN (+4.09\%) and Kernel weighted KNN (+1.13\%). The method provides a simple yet effective alternative to complex adaptive schemes, particularly valuable for high-stakes applications requiring explainable predictions.

</details>


### [94] [LUMOS: Large User MOdels for User Behavior Prediction](https://arxiv.org/abs/2512.08957)
*Dhruv Nigam*

Main category: cs.LG

TL;DR: LUMOS是一个基于Transformer的大规模用户行为预测模型，通过联合学习多个任务、消除特定任务模型和手动特征工程，仅使用原始用户活动数据就能实现高效预测。


<details>
  <summary>Details</summary>
Motivation: 在线B2C平台面临大规模用户行为预测的挑战，传统方法依赖特定任务模型和领域特征工程，耗时、计算成本高、需要专业知识且难以扩展。

Method: 提出LUMOS架构：1）基于Transformer的模型，通过联合学习消除特定任务模型；2）引入新颖的交叉注意力机制，使预测能够基于未来已知事件（如节假日、促销）；3）采用多模态标记化，结合用户交易、事件上下文和静态用户人口统计属性，通过专门的嵌入路径处理。

Result: 在包含2750亿用户活动标记、2.5亿用户的生产数据集上，LUMOS在5个任务上相比传统特定任务模型表现更优：二元分类任务ROC-AUC平均提升0.025，回归任务MAPE降低4.6%。在线A/B测试验证了实际业务影响：日活跃用户增加3.15%。

Conclusion: LUMOS通过消除特定任务模型和手动特征工程，提供了一种可扩展的大规模用户行为预测解决方案，能够有效预测复杂行为模式，并带来实际业务价值。

Abstract: User behavior prediction at scale remains a critical challenge for online B2C platforms. Traditional approaches rely heavily on task-specific models and domain-specific feature engineering. This is time-consuming, computationally expensive, and requires domain expertise and therefore not scalable. We present LUMOS (Large User MOdel Series), a transformer-based architecture that eliminates task-specific models and manual feature engineering by learning multiple tasks jointly using only raw user activity data. LUMOS introduces a novel cross-attention mechanism that conditions predictions on future known events (e.g., holidays, sales, etc.), enabling the model to predict complex behaviour patterns like "how will upcoming holidays affect user engagement?" The architecture also employs multi-modal tokenization, combining user transactions, event context, and static user demographic attributes into rich representations processed through specialized embedding pathways.
  Through extensive experiments on a production dataset spanning 275 billion user activity tokens from 250 million users, we demonstrate that LUMOS achieves superior performance compared to traditional task-specific models. Across 5 tasks with established baselines, we achieve an average improvement of 0.025 in ROC-AUC for binary classification tasks and 4.6\% reduction in MAPE for regression tasks. Online A/B testing validates these improvements translate to measurable business impact with a 3.15\% increase in Daily Active Users.

</details>


### [95] [EEG-Bench: A Benchmark for EEG Foundation Models in Clinical Applications](https://arxiv.org/abs/2512.08959)
*Ard Kastrati,Josua Bürki,Jonas Lauer,Cheng Xuan,Raffaele Iaquinto,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 提出了一个统一的基准框架，用于评估基于EEG的基础模型在临床应用中的表现，涵盖11个诊断任务和14个公开EEG数据集，结果显示基础模型在某些场景表现良好，但简单模型在临床分布偏移下仍具竞争力。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏统一的基准来评估EEG基础模型在临床应用中的表现，需要标准化的评估协议来比较传统方法和现代基础模型，促进该领域的研究可重复性和实际应用。

Method: 构建了一个统一的基准框架，涵盖11个明确定义的诊断任务和14个公开EEG数据集，采用最小预处理和标准化评估协议，支持传统基线模型和现代基础模型的并排比较。

Result: 基础模型在某些设置下表现出色，但简单模型在临床分布偏移下仍保持竞争力，特别是在处理真实临床数据时，传统方法往往表现更稳健。

Conclusion: 该基准框架为EEG基础模型的临床评估提供了标准化工具，虽然基础模型有潜力，但简单模型在临床应用中仍具价值，所有数据和代码已开源以促进可重复性和采用。

Abstract: We introduce a unified benchmarking framework focused on evaluating EEG-based foundation models in clinical applications. The benchmark spans 11 well-defined diagnostic tasks across 14 publicly available EEG datasets, including epilepsy, schizophrenia, Parkinson's disease, OCD, and mild traumatic brain injury. It features minimal preprocessing, standardized evaluation protocols, and enables side-by-side comparisons of classical baselines and modern foundation models. Our results show that while foundation models achieve strong performance in certain settings, simpler models often remain competitive, particularly under clinical distribution shifts. To facilitate reproducibility and adoption, we release all prepared data and code in an accessible and extensible format.

</details>


### [96] [Resolving Conflicts in Lifelong Learning via Aligning Updates in Subspaces](https://arxiv.org/abs/2512.08960)
*Yueer Zhou,Yichen Wu,Ying Wei*

Main category: cs.LG

TL;DR: PS-LoRA通过双正则化目标和对齐优化子空间中的更新来解决LoRA在持续学习中的灾难性遗忘问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LoRA在持续学习中常遭受灾难性遗忘，主要原因是新任务梯度与历史权重轨迹之间的对抗性方向更新，导致破坏性干扰。

Method: 提出PS-LoRA框架：1）使用双正则化目标，惩罚冲突方向并约束幅度偏差；2）基于幅度的合并策略，将顺序适配器整合为稳健表示而无需重新训练。

Result: 在NLP和视觉基准测试中，PS-LoRA优于最先进方法，能够在高效适应新领域的同时保持学习表示的稳定性。

Conclusion: PS-LoRA通过解决梯度冲突和确保优化子空间中的一致性更新，有效缓解了LoRA在持续学习中的灾难性遗忘问题。

Abstract: Low-Rank Adaptation (LoRA) enables efficient Continual Learning but often suffers from catastrophic forgetting due to destructive interference between tasks. Our analysis reveals that this degradation is primarily driven by antagonistic directional updates where new task gradients directly oppose the historical weight trajectory. To address this, we propose PS-LoRA (Parameter Stability LoRA), a framework designed to resolve conflicts by aligning updates within the optimization subspace. Our approach employs a dual-regularization objective that penalizes conflicting directions and constrains magnitude deviations to ensure consistency with prior knowledge. Additionally, we implement a magnitude-based merging strategy to consolidate sequential adapters into a robust representation without retraining. Experiments on NLP and Vision benchmarks show that PS-LoRA outperforms state-of-the-art methods by preserving the stability of learned representations while efficiently adapting to new domains.

</details>


### [97] [SEA: Spectral Edge Attacks on Graph Neural Networks](https://arxiv.org/abs/2512.08964)
*Yongyu Wang*

Main category: cs.LG

TL;DR: 提出SEA攻击方法，利用谱分析评估图结构脆弱性，通过删除稳健边或添加不兼容边来攻击GNN模型


<details>
  <summary>Details</summary>
Motivation: 现有图结构攻击方法大多基于梯度启发式或局部连接模式，将边视为同等重要的候选修改对象，缺乏对图结构全局脆弱性的系统性评估

Method: 提出谱边攻击(SEA)：1)计算谱嵌入捕捉输入流形最脆弱方向；2)为每条边或非边分配稳健性分数；3)基于分数设计两种攻击变体：谱引导删除攻击（删除最稳健边）和谱引导添加攻击（在谱脆弱空间中添加最大不兼容节点间的边）

Result: SEA攻击在基准测试中表现出色，能够有效降低GNN性能，且无需梯度信息，可轻松集成到现有GNN架构中

Conclusion: SEA提供了一种基于谱分析的图结构攻击新范式，通过系统性评估图结构脆弱性，实现了更有效的对抗攻击，为GNN稳健性研究提供了新视角

Abstract: Graph Neural Networks (GNNs) achieve strong performance on graph-structured data, but are notoriously vulnerable to small, carefully crafted perturbations of the graph structure. Most existing structure-based attacks rely on gradient-based heuristics or local connectivity patterns, and treat edges as equally important candidates for manipulation. In this paper, we propose Spectral Edge Attacks (SEA), a new family of adversarial attacks that explicitly leverage spectral robustness evaluation to guide structural perturbations. Our key idea is to compute a spectral embedding that captures the most fragile directions of the input manifold and to use it to assign a robustness score to each edge or non-edge. Based on these scores, we introduce two complementary attack variants: (i) a Spade-guided deletion attack that removes the most spectrally robust edges, and (ii) a Spade-guided addition attack that inserts edges between nodes that are maximally incompatible in the fragile spectral space. Both attacks operate at the graph level, are model-aware but conceptually simple, and can be plugged into existing GNN architectures without requiring gradients. We describe the spectral formulation, the attack algorithms, and experiments on benchmarks.

</details>


### [98] [Financial Instruction Following Evaluation (FIFE)](https://arxiv.org/abs/2512.08965)
*Glenn Matlin,Siddharth,Anirudh JM,Aditya Shukla,Yahya Hassan,Sudheer Chava*

Main category: cs.LG

TL;DR: FIFE是一个用于评估语言模型在金融分析任务中遵循复杂指令能力的高难度基准，包含88个人工编写的提示和可验证约束系统。评估显示开源权重模型表现优于专有系统，但所有模型都难以完全满足复杂要求。


<details>
  <summary>Details</summary>
Motivation: 语言模型在处理复杂、相互依赖的指令方面存在困难，特别是在金融等高精度要求领域。需要建立一个专门的基准来评估模型在金融分析任务中的指令遵循能力。

Method: 开发了FIFE基准，包含88个人工编写的金融分析提示，采用带有可链接、可验证约束的验证系统，提供细粒度的奖励信号。在零样本设置下评估了53个模型（专有、开源权重、开源）。

Result: 性能层次明显：顶级开源权重模型（严格76.1/宽松79.5）优于领先的专有系统（严格65.9/宽松70.5），而最佳开源模型表现显著落后（严格45.5/宽松48.9）。所有顶级模型都难以完全满足FIFE的复杂要求。

Conclusion: FIFE基准揭示了语言模型在复杂金融指令遵循方面的局限性，开源权重模型表现优于专有系统。发布数据集和代码作为开源资源，促进金融领域强化学习研究。

Abstract: Language Models (LMs) struggle with complex, interdependent instructions, particularly in high-stakes domains like finance where precision is critical. We introduce FIFE, a novel, high-difficulty benchmark designed to assess LM instruction-following capabilities for financial analysis tasks. FIFE comprises 88 human-authored prompts and employs a verification system with chainable, verifiable constraints for fine-grained reward signals. We evaluate 53 models (proprietary, open-weight, open-source) in a zero-shot setting. Our key findings reveal a clear performance hierarchy: the top open-weight model (76.1 strict / 79.5 loose) surpasses the leading proprietary system (65.9 strict / 70.5 loose), while the best open-source models lag significantly (45.5 strict / 48.9 loose). However, even top-performing models struggle with FIFE's complex requirements, failing to achieve perfect compliance. We release our dataset and code as an open-source resource to promote research in Reinforcement Learning for the financial domain.

</details>


### [99] [CluCERT: Certifying LLM Robustness via Clustering-Guided Denoising Smoothing](https://arxiv.org/abs/2512.08967)
*Zixia Wang,Gaojie Jin,Jia Hu,Ronghui Mu*

Main category: cs.LG

TL;DR: CluCERT：通过聚类引导去噪平滑来认证LLM鲁棒性的新框架，相比现有方法提供更紧的认证边界和更高的计算效率


<details>
  <summary>Details</summary>
Motivation: 尽管LLM能力强大，但仍容易受到对抗攻击（如同义词替换等语义保持的微小改动）。现有鲁棒性认证方法存在两个关键局限：1) 由于缺乏对扰动输出的语义验证导致认证边界松散；2) 重复采样导致计算成本高

Method: 提出CluCERT框架，采用聚类引导的去噪平滑方法。包括：语义聚类过滤器减少噪声样本并保留有意义的扰动；精炼模块提取核心语义；快速同义词替换策略加速去噪过程

Result: 在各种下游任务和越狱防御场景中的实验表明，该方法在鲁棒性边界和计算效率方面均优于现有认证方法

Conclusion: CluCERT通过语义聚类和高效去噪机制，有效解决了现有LLM鲁棒性认证方法的局限性，为LLM对抗鲁棒性提供了更紧的认证边界和更高的计算效率

Abstract: Recent advancements in Large Language Models (LLMs) have led to their widespread adoption in daily applications. Despite their impressive capabilities, they remain vulnerable to adversarial attacks, as even minor meaning-preserving changes such as synonym substitutions can lead to incorrect predictions. As a result, certifying the robustness of LLMs against such adversarial prompts is of vital importance. Existing approaches focused on word deletion or simple denoising strategies to achieve robustness certification. However, these methods face two critical limitations: (1) they yield loose robustness bounds due to the lack of semantic validation for perturbed outputs and (2) they suffer from high computational costs due to repeated sampling. To address these limitations, we propose CluCERT, a novel framework for certifying LLM robustness via clustering-guided denoising smoothing. Specifically, to achieve tighter certified bounds, we introduce a semantic clustering filter that reduces noisy samples and retains meaningful perturbations, supported by theoretical analysis. Furthermore, we enhance computational efficiency through two mechanisms: a refine module that extracts core semantics, and a fast synonym substitution strategy that accelerates the denoising process. Finally, we conduct extensive experiments on various downstream tasks and jailbreak defense scenarios. Experimental results demonstrate that our method outperforms existing certified approaches in both robustness bounds and computational efficiency.

</details>


### [100] [Beyond the Hype: Comparing Lightweight and Deep Learning Models for Air Quality Forecasting](https://arxiv.org/abs/2512.09076)
*Moazzam Umer Gondal,Hamad ul Qudous,Asma Ahmad Farhan*

Main category: cs.LG

TL;DR: 轻量级加法模型（Facebook Prophet）在北京PM2.5和PM10预测中优于复杂深度学习模型，提供准确、可解释且易于部署的解决方案。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习和混合模型在空气污染预测研究中占主导地位，但其复杂性和有限的可解释性阻碍了实际应用。本研究旨在探索轻量级加法模型是否能在北京PM2.5和PM10预测中提供具有竞争力的性能。

Method: 使用多年污染物和气象数据，应用系统特征选择（相关性、互信息、mRMR）、防泄漏缩放和时序数据分割。比较Facebook Prophet（FBP）和NeuralProphet（NP）两种加法模型，并引入LSTM、LightGBM和SARIMAX作为基线模型。NP额外利用了滞后依赖关系。

Result: FBP在7天测试集上表现最佳，对两种污染物的测试R²均超过0.94，优于NP、SARIMAX和所有学习基线模型。

Conclusion: 可解释的加法模型在空气污染预测中与传统方法和复杂方法相比仍具有竞争力，提供了准确性、透明性和易部署性的实用平衡。

Abstract: Accurate forecasting of urban air pollution is essential for protecting public health and guiding mitigation policies. While Deep Learning (DL) and hybrid pipelines dominate recent research, their complexity and limited interpretability hinder operational use. This study investigates whether lightweight additive models -- Facebook Prophet (FBP) and NeuralProphet (NP) -- can deliver competitive forecasts for particulate matter (PM$_{2.5}$, PM$_{10}$) in Beijing, China. Using multi-year pollutant and meteorological data, we applied systematic feature selection (correlation, mutual information, mRMR), leakage-safe scaling, and chronological data splits. Both models were trained with pollutant and precursor regressors, with NP additionally leveraging lagged dependencies. For context, two machine learning baselines (LSTM, LightGBM) and one traditional statistical model (SARIMAX) were also implemented. Performance was evaluated on a 7-day holdout using MAE, RMSE, and $R^2$. Results show that FBP consistently outperformed NP, SARIMAX, and the learning-based baselines, achieving test $R^2$ above 0.94 for both pollutants. These findings demonstrate that interpretable additive models remain competitive with both traditional and complex approaches, offering a practical balance of accuracy, transparency, and ease of deployment.

</details>


### [101] [StructuredDNA: A Bio-Physical Framework for Energy-Aware Transformer Routing](https://arxiv.org/abs/2512.08968)
*Mustapha Hamdi*

Main category: cs.LG

TL;DR: StructuredDNA是一种基于生物物理能量最小化的稀疏Transformer路由框架，用语义能量引导的稀疏专家路由替代密集MoE，显著降低能耗并保持语义稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型计算模型的快速扩展导致能源和计算成本急剧增加。受生物系统中结构和功能从低能量配置中涌现的启发，需要开发能量感知的模块化稀疏架构。

Method: 提出StructuredDNA框架：1）用基于语义能量最小化的生物物理能量引导路由层替代密集MoE路由；2）将输入动态分组为语义密码子；3）通过最小化结合内聚性、不确定性和计算成本的全局能量函数来选择单个专家。

Result: 在BioASQ（K=50）上实现97.7%的能源利用密度降低和0.998的语义稳定性指数；在WikiText-103上展示了语义缩放定律，专家粒度扩展到K=2048时仍保持超过99%的能源效率。

Conclusion: StructuredDNA建立了生物物理原理与Transformer稀疏专家路由之间的明确联系，为未来能量感知、模块化和可扩展的计算系统提供了领域无关的稳健范式。

Abstract: The rapid scaling of large computational models has led to a critical increase in energy and compute costs. Inspired by biological systems where structure and function emerge from low-energy configurations, we introduce StructuredDNA, a sparse architecture framework for modular, energy-aware Transformer routing. StructuredDNA replaces dense Mixture-of-Experts routing with a bio-physical, energy-guided routing layer based on semantic energy minimization. Inputs are dynamically grouped into semantic codons, and routing selects a single expert by minimizing a global energy functional that combines cohesion, uncertainty, and computational cost.
  We validate StructuredDNA on both specialized (BioASQ) and open-domain benchmarks (WikiText-103). On BioASQ (K = 50), we achieve a 97.7% reduction in Energy Utilization Density (EUD) and a Semantic Stability Index (SSI) of 0.998. We further demonstrate a Semantic Scaling Law on WikiText-103, showing that the architecture generalizes to open domains by scaling expert granularity (K = 2048) while maintaining more than 99% energy efficiency. StructuredDNA thus establishes a robust, domain-agnostic paradigm for future sparse computational frameworks.
  StructuredDNA provides an explicit link between bio-physical principles and sparse expert routing in Transformer architectures, and points toward future energy-aware, modular, and scalable computational systems. We discuss limitations of this proof-of-concept study and outline directions for scaling the approach to larger models, datasets, and hardware platforms. The StructuredDNA implementation is available at https://github.com/InnoDeep-repos/StructuredDNA .

</details>


### [102] [Learning Robust Representations for Malicious Content Detection via Contrastive Sampling and Uncertainty Estimation](https://arxiv.org/abs/2512.08969)
*Elias Hossain,Umesh Biswas,Charan Gudla,Sai Phani Parsa*

Main category: cs.LG

TL;DR: UCF是一个正例-未标记表示学习框架，通过不确定性感知对比损失、自适应温度缩放和自注意力LSTM编码器，在噪声和不平衡条件下提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决在噪声和不平衡条件下的分类问题，特别是在网络安全和生物医学文本挖掘等高风险领域，需要鲁棒且可扩展的正例-未标记学习解决方案。

Method: 提出不确定性对比框架(UCF)，整合不确定性感知对比损失、自适应温度缩放和自注意力引导的LSTM编码器，动态调整对比权重基于样本置信度，使用正例锚点稳定训练，并适应批次级变化。

Result: 在恶意内容分类任务中，UCF生成的嵌入使多个传统分类器达到超过93.38%的准确率、高于0.93的精确率，接近完美的召回率，具有最小的假阴性率和有竞争力的ROC-AUC分数。可视化分析确认正例和未标记实例之间的清晰分离。

Conclusion: UCF能够产生校准的、有区分度的嵌入，是一个鲁棒且可扩展的PU学习解决方案，适用于网络安全和生物医学文本挖掘等高风险领域。

Abstract: We propose the Uncertainty Contrastive Framework (UCF), a Positive-Unlabeled (PU) representation learning framework that integrates uncertainty-aware contrastive loss, adaptive temperature scaling, and a self-attention-guided LSTM encoder to improve classification under noisy and imbalanced conditions. UCF dynamically adjusts contrastive weighting based on sample confidence, stabilizes training using positive anchors, and adapts temperature parameters to batch-level variability. Applied to malicious content classification, UCF-generated embeddings enable multiple traditional classifiers to achieve more than 93.38% accuracy, precision above 0.93, and near-perfect recall, with minimal false negatives and competitive ROC-AUC scores. Visual analyses confirm clear separation between positive and unlabeled instances, highlighting the framework's ability to produce calibrated, discriminative embeddings. These results position UCF as a robust and scalable solution for PU learning in high-stakes domains such as cybersecurity and biomedical text mining.

</details>


### [103] [Peek-a-Boo Reasoning: Contrastive Region Masking in MLLMs](https://arxiv.org/abs/2512.08976)
*Isha Chaturvedi,Anjana Nair,Yushen Li,Adhitya Rajendra Kumar,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma*

Main category: cs.LG

TL;DR: CRM是一种无需训练的诊断方法，通过对比遮蔽视觉区域前后的推理轨迹，揭示多模态大语言模型在思维链推理中如何依赖特定视觉区域。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于最终答案或注意力图，无法提供因果、步骤级别的归因分析，需要一种能够评估多模态模型推理忠实度而非仅正确性的诊断工具。

Method: CRM系统地遮蔽标注的视觉区域，对比遮蔽前后模型的思维链推理轨迹，通过对比分析揭示模型在每个推理步骤中对特定视觉区域的依赖程度。

Result: 在VisArgs等数据集上，CRM揭示了两种失败模式：一些模型保持推理结构但在证据缺失时产生幻觉，另一些模型紧密依赖视觉线索但在扰动下崩溃。

Conclusion: CRM将视觉基准重构为诊断工具，强调需要不仅评估性能，还要评估推理鲁棒性和忠实度的多模态评估框架。

Abstract: We introduce Contrastive Region Masking (CRM), a training free diagnostic that reveals how multimodal large language models (MLLMs) depend on specific visual regions at each step of chain-of-thought (CoT) reasoning. Unlike prior approaches limited to final answers or attention maps, CRM provides causal, step-level attri- bution by systematically masking annotated regions and contrasting the resulting reasoning traces with unmasked baselines. Applied to datasets such as VisArgs, CRM reveals distinct failure modes: some models preserve reasoning structure, but hallucinate when evidence is missing, while others ground tightly to visual cues yet collapse under perturbations. By shifting the evaluation from correctness of an- swers to faithfulness of reasoning, CRM reframes visual benchmarks as diagnostic tools, highlighting the need for multimodal evaluation frameworks that measure not just performance, but also robustness and fidelity of reasoning.

</details>


### [104] [Drawback of Enforcing Equivariance and its Compensation via the Lens of Expressive Power](https://arxiv.org/abs/2512.09673)
*Yuzhu Chen,Tian Qin,Xinmei Tian,Fengxiang He,Dacheng Tao*

Main category: cs.LG

TL;DR: 研究等变神经网络表达能力，发现等变约束会限制表达能力但可通过增大模型规模补偿，且等变网络具有更好的泛化性


<details>
  <summary>Details</summary>
Motivation: 等变神经网络将对称性作为归纳偏置，在许多领域表现出色，但其表达能力尚未被充分理解。本文旨在研究等变约束对网络表达能力的影响

Method: 聚焦于2层ReLU网络，通过分析边界超平面和通道向量，构建示例展示等变约束如何限制表达能力，并研究通过增大模型规模进行补偿

Result: 等变约束确实会严格限制表达能力，但可通过增大模型规模来补偿；尽管模型规模更大，等变网络对应的假设空间复杂度更低，具有更好的泛化性

Conclusion: 等变神经网络在表达能力上存在限制，但可通过适当增大模型规模来克服，同时保持较低的假设空间复杂度，从而获得更好的泛化性能

Abstract: Equivariant neural networks encode symmetry as an inductive bias and have achieved strong empirical performance in wide domains. However, their expressive power remains not well understood. Focusing on 2-layer ReLU networks, this paper investigates the impact of equivariance constraints on the expressivity of equivariant and layer-wise equivariant networks. By examining the boundary hyperplanes and the channel vectors of ReLU networks, we construct an example showing that equivariance constraints could strictly limit expressive power. However, we demonstrate that this drawback can be compensated via enlarging the model size. Furthermore, we show that despite a larger model size, the resulting architecture could still correspond to a hypothesis space with lower complexity, implying superior generalizability for equivariant networks.

</details>


### [105] [Graph Deep Learning for Intracranial Aneurysm Blood Flow Simulation and Risk Assessment](https://arxiv.org/abs/2512.09013)
*Paul Garnier,Pablo Jeken-Rico,Vincent Lannelongue,Chiara Faitini,Aurèle Goetz,Lea Chanvillard,Ramy Nemer,Jonathan Viquerat,Ugo Pelissier,Philippe Meliga,Jacques Sédat,Thomas Liebig,Yves Chau,Elie Hachem*

Main category: cs.LG

TL;DR: 提出基于图神经网络的替代模型，直接从血管几何结构在1分钟内生成全视野血流动力学，实现颅内动脉瘤的实时临床分析。


<details>
  <summary>Details</summary>
Motivation: 颅内动脉瘤破裂风险与局部血流动力学密切相关，但传统CFD模拟耗时且需要专业知识，而4D Flow MRI空间分辨率不足且昂贵不实用，需要一种快速准确的临床解决方案。

Method: 使用图神经网络替代模型，结合图变换器和自回归预测，在患者特异性动脉瘤的高保真模拟数据集上训练，直接从血管几何结构生成血流、壁面剪切应力和振荡剪切指数。

Result: 模型能够在每个心动周期不到1分钟内重现全视野血流动力学，泛化到未见过的患者几何结构和流入条件，无需网格特定校准，实现近实时推理。

Conclusion: 该工作将高保真模拟从专家专用研究工具转变为可部署的数据驱动决策支持系统，为实时床边动脉瘤分析迈出重要一步，可在患者成像后几分钟内提供高分辨率血流动力学预测。

Abstract: Intracranial aneurysms remain a major cause of neurological morbidity and mortality worldwide, where rupture risk is tightly coupled to local hemodynamics particularly wall shear stress and oscillatory shear index. Conventional computational fluid dynamics simulations provide accurate insights but are prohibitively slow and require specialized expertise. Clinical imaging alternatives such as 4D Flow MRI offer direct in-vivo measurements, yet their spatial resolution remains insufficient to capture the fine-scale shear patterns that drive endothelial remodeling and rupture risk while being extremely impractical and expensive.
  We present a graph neural network surrogate model that bridges this gap by reproducing full-field hemodynamics directly from vascular geometries in less than one minute per cardiac cycle. Trained on a comprehensive dataset of high-fidelity simulations of patient-specific aneurysms, our architecture combines graph transformers with autoregressive predictions to accurately simulate blood flow, wall shear stress, and oscillatory shear index. The model generalizes across unseen patient geometries and inflow conditions without mesh-specific calibration. Beyond accelerating simulation, our framework establishes the foundation for clinically interpretable hemodynamic prediction. By enabling near real-time inference integrated with existing imaging pipelines, it allows direct comparison with hospital phase-diagram assessments and extends them with physically grounded, high-resolution flow fields.
  This work transforms high-fidelity simulations from an expert-only research tool into a deployable, data-driven decision support system. Our full pipeline delivers high-resolution hemodynamic predictions within minutes of patient imaging, without requiring computational specialists, marking a step-change toward real-time, bedside aneurysm analysis.

</details>


### [106] [Improving Multi-Class Calibration through Normalization-Aware Isotonic Techniques](https://arxiv.org/abs/2512.09054)
*Alon Arad,Saharon Rosset*

Main category: cs.LG

TL;DR: 提出新的多类校准方法，通过考虑概率归一化改进等渗回归，在文本和图像分类任务上提升校准性能


<details>
  <summary>Details</summary>
Motivation: 多类监督学习需要准确可靠的概率预测，但现有的等渗回归扩展方法（如一对多校准）在多类问题上表现不佳，限制了实际应用

Method: 提出两种等渗归一化感知技术：NA-FIR（将归一化直接纳入优化过程）和SCIR（将问题建模为累积双变量等渗回归），基于实践者期望的自然直观假设

Result: 在多种文本和图像分类数据集及不同模型架构上的实验表明，该方法能持续改进负对数似然（NLL）和期望校准误差（ECE）指标

Conclusion: 提出的等渗归一化感知方法有效解决了多类校准问题，优于现有等渗回归方法，为多类概率预测提供了更可靠的校准方案

Abstract: Accurate and reliable probability predictions are essential for multi-class supervised learning tasks, where well-calibrated models enable rational decision-making. While isotonic regression has proven effective for binary calibration, its extension to multi-class problems via one-vs-rest calibration produced suboptimal results when compared to parametric methods, limiting its practical adoption. In this work, we propose novel isotonic normalization-aware techniques for multiclass calibration, grounded in natural and intuitive assumptions expected by practitioners. Unlike prior approaches, our methods inherently account for probability normalization by either incorporating normalization directly into the optimization process (NA-FIR) or modeling the problem as a cumulative bivariate isotonic regression (SCIR). Empirical evaluation on a variety of text and image classification datasets across different model architectures reveals that our approach consistently improves negative log-likelihood (NLL) and expected calibration error (ECE) metrics.

</details>


### [107] [Provably Learning from Modern Language Models via Low Logit Rank](https://arxiv.org/abs/2512.09892)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 本文提出了一种基于低对数秩假设的高效学习算法，能够从查询中学习近似低对数秩的语言模型，为现代语言模型提供了首个端到端学习保证。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型虽然复杂，但经验观察显示它们具有近似低对数秩的特性。本文旨在探索如何利用这种结构特性来获得可证明的学习保证，特别是针对那些能够编码难以学习分布（如噪声奇偶性）的低对数秩模型。

Method: 采用查询学习模型，使用对数查询来模拟常见API的访问模式。提出了一种高效算法，能够从查询中学习任何近似低对数秩模型。

Result: 开发出了一种高效算法，能够在查询学习模型中学习近似低对数秩模型。该算法的结构假设与经验观察到的现代语言模型行为高度吻合。

Conclusion: 本文为生成模型提供了首个端到端学习保证，该模型合理地捕捉了现代语言模型的行为特征。通过利用低对数秩结构，实现了对复杂语言模型的高效可证明学习。

Abstract: While modern language models and their inner workings are incredibly complex, recent work (Golowich, Liu & Shetty; 2025) has proposed a simple and potentially tractable abstraction for them through the observation that empirically, these language models all seem to have approximately low logit rank. Roughly, this means that a matrix formed by the model's log probabilities of various tokens conditioned on certain sequences of tokens is well approximated by a low rank matrix.
  In this paper, our focus is on understanding how this structure can be exploited algorithmically for obtaining provable learning guarantees. Since low logit rank models can encode hard-to-learn distributions such as noisy parities, we study a query learning model with logit queries that reflects the access model for common APIs. Our main result is an efficient algorithm for learning any approximately low logit rank model from queries. We emphasize that our structural assumption closely reflects the behavior that is empirically observed in modern language models. Thus, our result gives what we believe is the first end-to-end learning guarantee for a generative model that plausibly captures modern language models.

</details>


### [108] [A Diffusion-Based Framework for High-Resolution Precipitation Forecasting over CONUS](https://arxiv.org/abs/2512.09059)
*Marina Vicens-Miquel,Amy McGovern,Aaron J. Hill,Efi Foufoula-Georgiou,Clement Guilloteau,Samuel S. P. Shen*

Main category: cs.LG

TL;DR: 该研究提出了一种基于扩散的深度学习框架，系统比较了三种残差预测策略，用于降水预报。混合模型在短时预报表现最佳，而HRRR校正模型在长时预报中表现更好，显著提升了预报技能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 准确的降水预报对于水文气象风险管理至关重要，特别是预测可能导致山洪暴发和基础设施损坏的极端降雨。本研究旨在通过深度学习框架，系统比较不同数据源对降水预报技能的贡献。

Method: 引入基于扩散的深度学习框架，比较三种残差预测策略：1）仅使用MRMS观测数据的全数据驱动模型；2）仅使用HRRR数值天气预报的校正模型；3）整合MRMS和选定HRRR预报变量的混合模型。在统一设置下评估这些方法，进行1-12小时的自回归滚动预报，空间分辨率为1公里。

Result: 在所有预报时效上，深度学习框架在像素级和空间统计指标上均优于HRRR基线。混合模型在最短预报时效表现最佳，而HRRR校正模型在较长预报时效表现更好，能保持高技能至12小时。研究还包含了针对残差学习设置的校准不确定性量化。

Conclusion: 该工作通过增强预测技能、可靠性和区域适用性，推进了基于深度学习的降水预报。特别是在较长预报时效的改进，对于应急准备至关重要，适度的预报时效增加可以改善决策制定。

Abstract: Accurate precipitation forecasting is essential for hydrometeorological risk management, especially for anticipating extreme rainfall that can lead to flash flooding and infrastructure damage. This study introduces a diffusion-based deep learning (DL) framework that systematically compares three residual prediction strategies differing only in their input sources: (1) a fully data-driven model using only past observations from the Multi-Radar Multi-Sensor (MRMS) system, (2) a corrective model using only forecasts from the High-Resolution Rapid Refresh (HRRR) numerical weather prediction system, and (3) a hybrid model integrating both MRMS and selected HRRR forecast variables. By evaluating these approaches under a unified setup, we provide a clearer understanding of how each data source contributes to predictive skill over the Continental United States (CONUS). Forecasts are produced at 1-km spatial resolution, beginning with direct 1-hour predictions and extending to 12 hours using autoregressive rollouts. Performance is evaluated using both CONUS-wide and region-specific metrics that assess overall performance and skill at extreme rainfall thresholds. Across all lead times, our DL framework consistently outperforms the HRRR baseline in pixel-wise and spatiostatistical metrics. The hybrid model performs best at the shortest lead time, while the HRRR-corrective model outperforms others at longer lead times, maintaining high skill through 12 hours. To assess reliability, we incorporate calibrated uncertainty quantification tailored to the residual learning setup. These gains, particularly at longer lead times, are critical for emergency preparedness, where modest increases in forecast horizon can improve decision-making. This work advances DL-based precipitation forecasting by enhancing predictive skill, reliability, and applicability across regions.

</details>


### [109] [Contrast transfer functions help quantify neural network out-of-distribution generalization in HRTEM](https://arxiv.org/abs/2512.09067)
*Luis Rangel DaCosta,Mary C. Scott*

Main category: cs.LG

TL;DR: 该研究通过模拟数据探究神经网络在HRTEM纳米颗粒图像分割中的分布外泛化能力，发现模型性能随成像条件偏离训练分布而平滑可预测地下降。


<details>
  <summary>Details</summary>
Motivation: 神经网络在分布外（OOD）泛化方面的表现不佳，而理解这种泛化能力对于实验工作流程中的成功部署至关重要，特别是在实验条件变化较大或地面真实知识难以建立的情况下。

Method: 使用随机结构采样和多层切片模拟生成合成数据，训练和测量超过12,000个神经网络模型；开发基于HRTEM对比传递函数的框架来比较数据集信息内容并量化OOD域偏移。

Result: 神经网络分割模型表现出显著的性能稳定性，但随着成像条件从训练分布偏移，性能会平滑且可预测地恶化；建立了量化OOD域偏移的框架。

Conclusion: 研究展示了神经网络在HRTEM纳米颗粒分割中的OOD泛化行为，同时指出了该方法在解释其他类型OOD偏移（如原子结构变化）方面的局限性，并讨论了理解此类泛化的补充技术。

Abstract: Neural networks, while effective for tackling many challenging scientific tasks, are not known to perform well out-of-distribution (OOD), i.e., within domains which differ from their training data. Understanding neural network OOD generalization is paramount to their successful deployment in experimental workflows, especially when ground-truth knowledge about the experiment is hard to establish or experimental conditions significantly vary. With inherent access to ground-truth information and fine-grained control of underlying distributions, simulation-based data curation facilitates precise investigation of OOD generalization behavior. Here, we probe generalization with respect to imaging conditions of neural network segmentation models for high-resolution transmission electron microscopy (HRTEM) imaging of nanoparticles, training and measuring the OOD generalization of over 12,000 neural networks using synthetic data generated via random structure sampling and multislice simulation. Using the HRTEM contrast transfer function, we further develop a framework to compare information content of HRTEM datasets and quantify OOD domain shifts. We demonstrate that neural network segmentation models enjoy significant performance stability, but will smoothly and predictably worsen as imaging conditions shift from the training distribution. Lastly, we consider limitations of our approach in explaining other OOD shifts, such as of the atomic structures, and discuss complementary techniques for understanding generalization in such settings.

</details>


### [110] [Modular Deep-Learning-Based Early Warning System for Deadly Heatwave Prediction](https://arxiv.org/abs/2512.09074)
*Shangqing Xu,Zhiyuan Zhao,Megha Sharma,José María Martín-Olalla,Alexander Rodríguez,Gregory A. Wellenius,B. Aditya Prakash*

Main category: cs.LG

TL;DR: DeepTherm：无需热相关死亡历史数据的模块化致命热浪预警系统，通过深度学习双预测管道分离基线死亡率，在西班牙真实数据上验证了性能


<details>
  <summary>Details</summary>
Motivation: 城市严重热浪对公共健康构成重大威胁，需要建立早期预警策略。现有方法难以预测即将到来的致命热浪，因为定义和估计热相关死亡率很困难，且早期预警系统需要满足数据可用性、时空鲁棒性和决策成本等额外要求。

Method: 提出DeepTherm模块化预警系统，采用深度学习双预测管道，从全因死亡率中分离出无热浪和其他异常事件时的基线死亡率，无需热相关死亡历史数据。

Result: 在西班牙真实数据上评估，结果显示在不同地区、时间段和人群群体中表现一致、鲁棒且准确，同时允许在漏报和误报之间进行权衡。

Conclusion: DeepTherm为解决致命热浪预测挑战提供了有效解决方案，展示了深度学习在公共卫生预警系统中的灵活性和实用性。

Abstract: Severe heatwaves in urban areas significantly threaten public health, calling for establishing early warning strategies. Despite predicting occurrence of heatwaves and attributing historical mortality, predicting an incoming deadly heatwave remains a challenge due to the difficulty in defining and estimating heat-related mortality. Furthermore, establishing an early warning system imposes additional requirements, including data availability, spatial and temporal robustness, and decision costs. To address these challenges, we propose DeepTherm, a modular early warning system for deadly heatwave prediction without requiring heat-related mortality history. By highlighting the flexibility of deep learning, DeepTherm employs a dual-prediction pipeline, disentangling baseline mortality in the absence of heatwaves and other irregular events from all-cause mortality. We evaluated DeepTherm on real-world data across Spain. Results demonstrate consistent, robust, and accurate performance across diverse regions, time periods, and population groups while allowing trade-off between missed alarms and false alarms.

</details>


### [111] [Natural Geometry of Robust Data Attribution: From Convex Models to Deep Networks](https://arxiv.org/abs/2512.09103)
*Shihao Li,Jiachen Li,Dongmei Chen*

Main category: cs.LG

TL;DR: 提出首个针对深度网络数据归因的认证鲁棒性框架，通过自然Wasserstein度量消除谱放大效应，显著提升归因稳定性


<details>
  <summary>Details</summary>
Motivation: 现有数据归因方法对分布扰动敏感，缺乏可靠性保证，特别是深度网络中的归因鲁棒性问题尚未解决

Method: 提出统一认证鲁棒归因框架：针对凸模型推导Wasserstein鲁棒影响函数；针对深度网络引入自然Wasserstein度量，消除谱放大效应，开发Natural W-TRAK方法

Result: 在CIFAR-10+ResNet-18上，Natural W-TRAK认证68.7%的排序对（欧氏基线为0%）；自影响项实现0.970 AUROC的标签噪声检测，仅检查20%训练数据即可识别94.1%的污染标签

Conclusion: 通过自然Wasserstein度量成功解决了深度网络归因的认证鲁棒性问题，为数据归因提供了理论保证和实用工具

Abstract: Data attribution methods identify which training examples are responsible for a model's predictions, but their sensitivity to distributional perturbations undermines practical reliability. We present a unified framework for certified robust attribution that extends from convex models to deep networks. For convex settings, we derive Wasserstein-Robust Influence Functions (W-RIF) with provable coverage guarantees. For deep networks, we demonstrate that Euclidean certification is rendered vacuous by spectral amplification -- a mechanism where the inherent ill-conditioning of deep representations inflates Lipschitz bounds by over $10{,}000\times$. This explains why standard TRAK scores, while accurate point estimates, are geometrically fragile: naive Euclidean robustness analysis yields 0\% certification. Our key contribution is the Natural Wasserstein metric, which measures perturbations in the geometry induced by the model's own feature covariance. This eliminates spectral amplification, reducing worst-case sensitivity by $76\times$ and stabilizing attribution estimates. On CIFAR-10 with ResNet-18, Natural W-TRAK certifies 68.7\% of ranking pairs compared to 0\% for Euclidean baselines -- to our knowledge, the first non-vacuous certified bounds for neural network attribution. Furthermore, we prove that the Self-Influence term arising from our analysis equals the Lipschitz constant governing attribution stability, providing theoretical grounding for leverage-based anomaly detection. Empirically, Self-Influence achieves 0.970 AUROC for label noise detection, identifying 94.1\% of corrupted labels by examining just the top 20\% of training data.

</details>


### [112] [GS-KAN: Parameter-Efficient Kolmogorov-Arnold Networks via Sprecher-Type Shared Basis Functions](https://arxiv.org/abs/2512.09084)
*Oscar Eliasson*

Main category: cs.LG

TL;DR: GS-KAN提出了一种轻量化的KAN架构，通过共享父函数和线性变换来构建边缘函数，解决了标准KAN参数爆炸的问题，在函数逼近和分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 标准Kolmogorov-Arnold网络(KANs)虽然具有强大的逼近能力，但每个网络边缘都需要独立的参数化，导致严重的参数效率低下，特别是在高维场景下参数爆炸问题使得部署不可行。

Method: GS-KAN受David Sprecher对叠加定理的改进启发，每层使用单个可学习的共享父函数，通过对该父函数应用可学习的线性变换来构建独特的边缘函数，从而大幅减少参数数量。

Result: 在合成函数逼近、表格数据回归和图像分类任务中，GS-KAN在连续函数逼近任务上优于MLPs和标准KAN基线，同时保持卓越的参数效率；在表格回归上与现有KAN架构竞争，在高维分类任务上优于MLPs。

Conclusion: GS-KAN实现了在严格参数约束下的高维KAN架构部署，解决了标准实现因参数爆炸而不可行的问题，为KAN在资源受限环境中的应用开辟了新途径。

Abstract: The Kolmogorov-Arnold representation theorem offers a theoretical alternative to Multi-Layer Perceptrons (MLPs) by placing learnable univariate functions on edges rather than nodes. While recent implementations such as Kolmogorov-Arnold Networks (KANs) demonstrate high approximation capabilities, they suffer from significant parameter inefficiency due to the requirement of maintaining unique parameterizations for every network edge. In this work, we propose GS-KAN (Generalized Sprecher-KAN), a lightweight architecture inspired by David Sprecher's refinement of the superposition theorem. GS-KAN constructs unique edge functions by applying learnable linear transformations to a single learnable, shared parent function per layer. We evaluate GS-KAN against existing KAN architectures and MLPs across synthetic function approximation, tabular data regression and image classification tasks. Our results demonstrate that GS-KAN outperforms both MLPs and standard KAN baselines on continuous function approximation tasks while maintaining superior parameter efficiency. Additionally, GS-KAN achieves competitive performance with existing KAN architectures on tabular regression and outperforms MLPs on high-dimensional classification tasks. Crucially, the proposed architecture enables the deployment of KAN-based architectures in high-dimensional regimes under strict parameter constraints, a setting where standard implementations are typically infeasible due to parameter explosion. The source code is available at https://github.com/rambamn48/gs-impl.

</details>


### [113] [Learning Unmasking Policies for Diffusion Language Models](https://arxiv.org/abs/2512.09106)
*Metod Jazbec,Theo X. Olausson,Louis Béthune,Pierre Ablin,Michael Kirchhof,Joao Monterio,Victor Turrisi,Jason Ramapuram,Marco Cuturi*

Main category: cs.LG

TL;DR: 提出用强化学习训练扩散语言模型的采样策略，替代需要手动调参的启发式方法，在保持生成质量的同时提升效率。


<details>
  <summary>Details</summary>
Motivation: 当前扩散语言模型使用启发式采样策略（如置信度阈值）存在需要手动调参、在大缓冲区下性能下降的问题，需要更自动化的采样方法。

Method: 将掩码扩散采样形式化为马尔可夫决策过程，使用单层Transformer架构的轻量级策略网络，根据dLLM的token置信度做出解掩码决策。

Result: 训练的策略在半自回归生成中达到最先进启发式方法的性能，在全扩散设置中表现更优，且能迁移到新的dLLM和更长序列，但在域外数据和精细权衡调优方面存在局限。

Conclusion: 强化学习训练的采样策略能有效替代启发式方法，提升扩散语言模型的采样效率和质量，但在泛化性和精细调优方面仍需改进。

Abstract: Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their autoregressive counterparts on many tasks, while holding the promise of being more efficient during inference. One particularly successful variant is masked discrete diffusion, in which a buffer filled with special mask tokens is progressively replaced with tokens sampled from the model's vocabulary. Efficiency can be gained by unmasking several tokens in parallel, but doing too many at once risks degrading the generation quality. Thus, one critical design aspect of dLLMs is the sampling procedure that selects, at each step of the diffusion process, which tokens to replace. Indeed, recent work has found that heuristic strategies such as confidence thresholding lead to both higher quality and token throughput compared to random unmasking. However, such heuristics have downsides: they require manual tuning, and we observe that their performance degrades with larger buffer sizes. In this work, we instead propose to train sampling procedures using reinforcement learning. Specifically, we formalize masked diffusion sampling as a Markov decision process in which the dLLM serves as the environment, and propose a lightweight policy architecture based on a single-layer transformer that maps dLLM token confidences to unmasking decisions. Our experiments show that these trained policies match the performance of state-of-the-art heuristics when combined with semi-autoregressive generation, while outperforming them in the full diffusion setting. We also examine the transferability of these policies, finding that they can generalize to new underlying dLLMs and longer sequence lengths. However, we also observe that their performance degrades when applied to out-of-domain data, and that fine-grained tuning of the accuracy-efficiency trade-off can be challenging with our approach.

</details>


### [114] [Spectral Embedding via Chebyshev Bases for Robust DeepONet Approximation](https://arxiv.org/abs/2512.09165)
*Muhammad Abid,Omer San*

Main category: cs.LG

TL;DR: 提出SEDONet，一种基于Chebyshev谱字典的DeepONet变体，用于解决标准DeepONet在处理有界域上非周期特征时的局限性。


<details>
  <summary>Details</summary>
Motivation: 标准DeepONet的trunk网络基于全连接层处理原始坐标，难以有效表示有界域上PDE中常见的尖锐梯度、边界层和非周期结构。

Method: 引入SEDONet，使用固定的Chebyshev谱字典驱动trunk网络，而非原始坐标输入，为有界域提供非周期谱嵌入的归纳偏置。

Result: 在多个PDE基准测试中，SEDONet相比DeepONet和FEDONet获得最低的相对L2误差，平均改进30-40%，在非周期几何上优于Fourier嵌入变体。

Conclusion: Chebyshev嵌入为非周期算子学习提供了有价值的谱框架，SEDONet作为简单、参数中性的DeepONet修改，为有界域PDE代理建模提供了鲁棒高效的解决方案。

Abstract: Deep Operator Networks (DeepONets) have become a central tool in data-driven operator learning, providing flexible surrogates for nonlinear mappings arising in partial differential equations (PDEs). However, the standard trunk design based on fully connected layers acting on raw spatial or spatiotemporal coordinates struggles to represent sharp gradients, boundary layers, and non-periodic structures commonly found in PDEs posed on bounded domains with Dirichlet or Neumann boundary conditions. To address these limitations, we introduce the Spectral-Embedded DeepONet (SEDONet), a new DeepONet variant in which the trunk is driven by a fixed Chebyshev spectral dictionary rather than coordinate inputs. This non-periodic spectral embedding provides a principled inductive bias tailored to bounded domains, enabling the learned operator to capture fine-scale non-periodic features that are difficult for Fourier or MLP trunks to represent. SEDONet is evaluated on a suite of PDE benchmarks including 2D Poisson, 1D Burgers, 1D advection-diffusion, Allen-Cahn dynamics, and the Lorenz-96 chaotic system, covering elliptic, parabolic, advective, and multiscale temporal phenomena, all of which can be viewed as canonical problems in computational mechanics. Across all datasets, SEDONet consistently achieves the lowest relative L2 errors among DeepONet, FEDONet, and SEDONet, with average improvements of about 30-40% over the baseline DeepONet and meaningful gains over Fourier-embedded variants on non-periodic geometries. Spectral analyses further show that SEDONet more accurately preserves high-frequency and boundary-localized features, demonstrating the value of Chebyshev embeddings in non-periodic operator learning. The proposed architecture offers a simple, parameter-neutral modification to DeepONets, delivering a robust and efficient spectral framework for surrogate modeling of PDEs on bounded domains.

</details>


### [115] [Understanding the Failure Modes of Transformers through the Lens of Graph Neural Networks](https://arxiv.org/abs/2512.09182)
*Hunjae Lee*

Main category: cs.LG

TL;DR: 本文通过图神经网络理论视角分析Transformer的失败模式，将深度学习视为可学习的信息混合与传播，揭示解码器Transformer因果性带来的信息传播几何特性，并为现有解决方案提供理论解释。


<details>
  <summary>Details</summary>
Motivation: Transformer虽然表现优异，但存在令人惊讶的失败模式和可预测的非对称性能退化。目前对这些失败模式缺乏理论理解，现有解决方案多为临时性且基于直觉而非理论驱动。本文旨在填补这一理论空白。

Method: 采用图神经网络理论框架，将深度学习视为可学习的信息混合与传播过程。分析解码器Transformer的因果性如何导致信息传播的几何特性，并将Transformer的失败模式与GNN理论中已知的信息传播瓶颈联系起来。

Result: 发现Transformer面临的许多问题与图神经网络相似，解码器Transformer的因果性会产生特定的信息传播几何特性，导致可预测且可能灾难性的失败模式。现有解决方案可以在统一的理论视角下得到解释。

Conclusion: 本文通过GNN理论为Transformer的失败模式提供了理论框架，解释了现有解决方案的工作原理，并指出了如何针对特定失败模式进行改进。这有助于弥合Transformer经验观察与理论理解之间的差距。

Abstract: Transformers and more specifically decoder-only transformers dominate modern LLM architectures. While they have shown to work exceptionally well, they are not without issues, resulting in surprising failure modes and predictably asymmetric performance degradation. This article is a study of many of these observed failure modes of transformers through the lens of graph neural network (GNN) theory. We first make the case that much of deep learning, including transformers, is about learnable information mixing and propagation. This makes the study of model failure modes a study of bottlenecks in information propagation. This naturally leads to GNN theory, where there is already a rich literature on information propagation bottlenecks and theoretical failure modes of models. We then make the case that many issues faced by GNNs are also experienced by transformers. In addition, we analyze how the causal nature of decoder-only transformers create interesting geometric properties in information propagation, resulting in predictable and potentially devastating failure modes. Finally, we observe that existing solutions in transformer research tend to be ad-hoc and driven by intuition rather than grounded theoretical motivation. As such, we unify many such solutions under a more theoretical perspective, providing insight into why they work, what problem they are actually solving, and how they can be further improved to target specific failure modes of transformers. Overall, this article is an attempt to bridge the gap between observed failure modes in transformers and a general lack of theoretical understanding of them in this space.

</details>


### [116] [Towards Optimal Valve Prescription for Transcatheter Aortic Valve Replacement (TAVR) Surgery: A Machine Learning Approach](https://arxiv.org/abs/2512.09198)
*Phevos Paschalidis,Vasiliki Stoumpou,Lisa Everest,Yu Ma,Talhat Azemi,Jawad Haider,Steven Zweibel,Eleftherios M. Protopapas,Jeff Mather,Maciej Tysarowski,George E. Sarris,Robert C. Hagberg,Howard L. Haronian,Dimitris Bertsimas*

Main category: cs.LG

TL;DR: 开发数据驱动的临床支持工具，通过整合多国多源数据，为TAVR手术选择最优瓣膜类型，以降低永久起搏器植入风险


<details>
  <summary>Details</summary>
Motivation: TAVR已成为治疗严重主动脉瓣狭窄的微创方法，但不同经导管心脏瓣膜的选择指南仍存在争议，需要个性化策略来降低永久起搏器植入这一主要术后并发症的风险

Method: 整合美国和希腊患者数据，结合人口统计学、CT扫描和超声心动图三种数据源，采用叶级分析利用人群异质性，避免基于不确定反事实风险估计的基准比较

Result: 最终处方模型在美国内部人群和希腊外部验证队列中，相比当前标准护理分别降低了26%和16%的永久起搏器植入率

Conclusion: 这是首个统一的、个性化的经导管心脏瓣膜选择处方策略，为TAVR手术提供了数据驱动的临床决策支持工具

Abstract: Transcatheter Aortic Valve Replacement (TAVR) has emerged as a minimally invasive treatment option for patients with severe aortic stenosis, a life-threatening cardiovascular condition. Multiple transcatheter heart valves (THV) have been approved for use in TAVR, but current guidelines regarding valve type prescription remain an active topic of debate. We propose a data-driven clinical support tool to identify the optimal valve type with the objective of minimizing the risk of permanent pacemaker implantation (PPI), a predominant postoperative complication. We synthesize a novel dataset that combines U.S. and Greek patient populations and integrates three distinct data sources (patient demographics, computed tomography scans, echocardiograms) while harmonizing differences in each country's record system. We introduce a leaf-level analysis to leverage population heterogeneity and avoid benchmarking against uncertain counterfactual risk estimates. The final prescriptive model shows a reduction in PPI rates of 26% and 16% compared with the current standard of care in our internal U.S. population and external Greek validation cohort, respectively. To the best of our knowledge, this work represents the first unified, personalized prescription strategy for THV selection in TAVR.

</details>


### [117] [LLMs for Analog Circuit Design Continuum (ACDC)](https://arxiv.org/abs/2512.09199)
*Yasaman Esfandiari,Jocelyn Rego,Austin Meyer,Jonathan Gallagher,Mia Levy*

Main category: cs.LG

TL;DR: 该论文研究了大型语言模型在模拟电路设计中的可靠性和鲁棒性，发现模型对数据格式敏感、生成设计不稳定、对未见电路配置泛化能力有限等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs和Transformer架构在各种自然语言任务中表现出强大的推理和生成能力，但它们在现实工程领域（特别是需要领域特定推理、物理约束遵循和结构化表示的模拟电路设计）的可靠性和鲁棒性尚未充分探索，这限制了其在人类中心工作流程中的实际应用。

Method: 研究LLMs在模拟电路设计中的适用性和一致性，重点关注AI辅助设计（人类保持在循环中）。研究不同数据表示如何影响模型行为，比较较小模型（如T5、GPT-2）与较大基础模型（如Mistral-7B、GPT-oss-20B）在不同训练条件下的表现。

Result: 结果揭示了关键可靠性挑战：对数据格式敏感、生成设计不稳定、对未见电路配置泛化能力有限。这些发现为LLMs作为增强人类在复杂工程任务中能力的工具提供了早期证据。

Conclusion: 该研究为LLMs在结构化现实应用中的限制和潜力提供了见解，为设计可靠、可部署的基础模型提供了指导，强调了在人类中心工作流程中开发稳健AI工具的重要性。

Abstract: Large Language Models (LLMs) and transformer architectures have shown impressive reasoning and generation capabilities across diverse natural language tasks. However, their reliability and robustness in real-world engineering domains remain largely unexplored, limiting their practical utility in human-centric workflows. In this work, we investigate the applicability and consistency of LLMs for analog circuit design -- a task requiring domain-specific reasoning, adherence to physical constraints, and structured representations -- focusing on AI-assisted design where humans remain in the loop. We study how different data representations influence model behavior and compare smaller models (e.g., T5, GPT-2) with larger foundation models (e.g., Mistral-7B, GPT-oss-20B) under varying training conditions. Our results highlight key reliability challenges, including sensitivity to data format, instability in generated designs, and limited generalization to unseen circuit configurations. These findings provide early evidence on the limits and potential of LLMs as tools to enhance human capabilities in complex engineering tasks, offering insights into designing reliable, deployable foundation models for structured, real-world applications.

</details>


### [118] [Tensor-Compressed and Fully-Quantized Training of Neural PDE Solvers](https://arxiv.org/abs/2512.09202)
*Jinming Lu,Jiayi Tian,Yequan Zhao,Hai Li,Zheng Zhang*

Main category: cs.LG

TL;DR: 提出一个用于边缘设备的高效PINN训练框架，通过量化训练、Stein估计器和张量分解实现5.5-83.5倍加速和159.6-2324.1倍节能


<details>
  <summary>Details</summary>
Motivation: PINNs在求解偏微分方程方面很有前景，但在资源受限的边缘设备上部署面临计算和内存开销大的挑战，主要源于高阶自动微分、密集张量运算和全精度算术

Method: 提出集成全量化训练、Stein估计器残差损失计算和张量分解权重压缩的框架，包含三个创新：1) 使用SMX格式的混合精度训练方法消除反向传播数据重复；2) Stein估计器的差分量化方案缓解下溢；3) TT层的部分重构方案减少量化误差累积，并设计PINTA硬件加速器

Result: 在2-D泊松、20-D HJB和100-D热方程上的实验表明，该框架在保持与全精度基准相当或更好精度的同时，实现5.5-83.5倍加速和159.6-2324.1倍节能

Conclusion: 该工作实现了边缘设备上的实时PDE求解，为大规模能效科学计算铺平了道路

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a promising paradigm for solving partial differential equations (PDEs) by embedding physical laws into neural network training objectives. However, their deployment on resource-constrained platforms is hindered by substantial computational and memory overhead, primarily stemming from higher-order automatic differentiation, intensive tensor operations, and reliance on full-precision arithmetic. To address these challenges, we present a framework that enables scalable and energy-efficient PINN training on edge devices. This framework integrates fully quantized training, Stein's estimator (SE)-based residual loss computation, and tensor-train (TT) decomposition for weight compression. It contributes three key innovations: (1) a mixed-precision training method that use a square-block MX (SMX) format to eliminate data duplication during backpropagation; (2) a difference-based quantization scheme for the Stein's estimator that mitigates underflow; and (3) a partial-reconstruction scheme (PRS) for TT-Layers that reduces quantization-error accumulation. We further design PINTA, a precision-scalable hardware accelerator, to fully exploit the performance of the framework. Experiments on the 2-D Poisson, 20-D Hamilton-Jacobi-Bellman (HJB), and 100-D Heat equations demonstrate that the proposed framework achieves accuracy comparable to or better than full-precision, uncompressed baselines while delivering 5.5x to 83.5x speedups and 159.6x to 2324.1x energy savings. This work enables real-time PDE solving on edge devices and paves the way for energy-efficient scientific computing at scale.

</details>


### [119] [Contrastive Learning for Semi-Supervised Deep Regression with Generalized Ordinal Rankings from Spectral Seriation](https://arxiv.org/abs/2512.09267)
*Ce Wang,Weihang Dai,Hanru Bai,Xiaomeng Li*

Main category: cs.LG

TL;DR: 提出了一种用于半监督回归的对比学习方法，通过谱排序算法恢复未标记样本的序数关系，减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习方法高度依赖标签信息来恢复特征的序数关系，限制了在半监督回归中的应用。需要减少对昂贵标注的依赖，同时利用未标记数据提升回归模型的表示能力。

Method: 1) 使用标记和未标记样本构建特征相似度矩阵反映样本间关系；2) 通过谱排序算法恢复未标记样本的序数排名；3) 标记样本提供正则化指导；4) 使用动态规划算法选择稳健特征构建矩阵；5) 将恢复的序数关系用于未标记样本的对比学习；6) 序数排名也可作为未标记样本预测的监督信号。

Result: 在多个数据集上的实验验证表明，该方法超越了现有的最先进的半监督深度回归方法，提供了理论保证和实证验证。

Conclusion: 该方法成功将对比回归扩展到半监督设置，通过谱排序恢复未标记样本的序数关系，有效利用未标记数据提升特征表示学习，实现了更稳健的回归结果。

Abstract: Contrastive learning methods enforce label distance relationships in feature space to improve representation capability for regression models. However, these methods highly depend on label information to correctly recover ordinal relationships of features, limiting their applications to semi-supervised regression. In this work, we extend contrastive regression methods to allow unlabeled data to be used in the semi-supervised setting, thereby reducing the dependence on costly annotations. Particularly we construct the feature similarity matrix with both labeled and unlabeled samples in a mini-batch to reflect inter-sample relationships, and an accurate ordinal ranking of involved unlabeled samples can be recovered through spectral seriation algorithms if the level of error is within certain bounds. The introduction of labeled samples above provides regularization of the ordinal ranking with guidance from the ground-truth label information, making the ranking more reliable. To reduce feature perturbations, we further utilize the dynamic programming algorithm to select robust features for the matrix construction. The recovered ordinal relationship is then used for contrastive learning on unlabeled samples, and we thus allow more data to be used for feature representation learning, thereby achieving more robust results. The ordinal rankings can also be used to supervise predictions on unlabeled samples, serving as an additional training signal. We provide theoretical guarantees and empirical verification through experiments on various datasets, demonstrating that our method can surpass existing state-of-the-art semi-supervised deep regression methods. Our code have been released on https://github.com/xmed-lab/CLSS.

</details>


### [120] [Goal inference with Rao-Blackwellized Particle Filters](https://arxiv.org/abs/2512.09269)
*Yixuan Wang,Dan P. Guralnik,Warren E. Dixon*

Main category: cs.LG

TL;DR: 提出一种基于Rao-Blackwellized粒子滤波器的意图推断方法，用于从噪声轨迹观测中恢复移动代理的最终目标，引入两种估计器并量化推断性能


<details>
  <summary>Details</summary>
Motivation: 从噪声轨迹观测中推断移动代理的最终目标是一个基本估计问题，现有方法在样本效率方面存在不足，需要更有效的意图推断技术

Method: 使用Rao-Blackwellized粒子滤波器，利用代理的闭环动力学特性，解析地边缘化线性高斯子结构，仅更新粒子权重，提高样本效率。引入两种估计器：基于RBPF权重的GMM估计器和限制在有效样本上的简化版本

Result: 通过信息论泄漏度量量化对手恢复代理意图的能力，提供KL散度的可计算下界，证明简化估计器性能接近完整估计器，实验显示对合规代理的意图恢复快速准确

Conclusion: 提出的RBPF方法能有效推断代理意图，为设计意图混淆控制器提供基础，未来可研究对抗性意图隐藏策略

Abstract: Inferring the eventual goal of a mobile agent from noisy observations of its trajectory is a fundamental estimation problem. We initiate the study of such intent inference using a variant of a Rao-Blackwellized Particle Filter (RBPF), subject to the assumption that the agent's intent manifests through closed-loop behavior with a state-of-the-art provable practical stability property. Leveraging the assumed closed-form agent dynamics, the RBPF analytically marginalizes the linear-Gaussian substructure and updates particle weights only, improving sample efficiency over a standard particle filter. Two difference estimators are introduced: a Gaussian mixture model using the RBPF weights and a reduced version confining the mixture to the effective sample. We quantify how well the adversary can recover the agent's intent using information-theoretic leakage metrics and provide computable lower bounds on the Kullback-Leibler (KL) divergence between the true intent distribution and RBPF estimates via Gaussian-mixture KL bounds. We also provide a bound on the difference in performance between the two estimators, highlighting the fact that the reduced estimator performs almost as well as the complete one. Experiments illustrate fast and accurate intent recovery for compliant agents, motivating future work on designing intent-obfuscating controllers.

</details>


### [121] [Hetero-SplitEE: Split Learning of Neural Networks with Early Exits for Heterogeneous IoT Devices](https://arxiv.org/abs/2512.09313)
*Yuki Oda,Yuta Ono,Hiroshi Nakamura,Hideki Takase*

Main category: cs.LG

TL;DR: 提出Hetero-SplitEE方法，通过在分层训练中集成异构早期退出机制，使不同计算能力的IoT设备能够选择适合自身计算容量的分割点，实现异构设备间的并行协作训练。


<details>
  <summary>Details</summary>
Motivation: 现有分割学习方法假设客户端同质性和统一分割点，这限制了其在计算资源异构的真实世界IoT系统中的适用性。需要解决异构设备参与协作深度学习训练的问题。

Method: 提出Hetero-SplitEE方法，集成异构早期退出到分层训练中，允许每个客户端根据计算能力选择不同的分割点（切割层）。提出两种协作训练策略：顺序策略（Sequential）和平均策略（Averaging）。

Result: 在CIFAR-10、CIFAR-100和STL-10数据集上使用ResNet-18进行广泛实验，结果表明该方法在保持竞争力的准确率的同时，能有效支持不同的计算约束。

Conclusion: Hetero-SplitEE方法能够实现异构IoT设备间的协作深度学习训练，为异构IoT生态系统中协作深度学习的实际部署提供了可行方案。

Abstract: The continuous scaling of deep neural networks has fundamentally transformed machine learning, with larger models demonstrating improved performance across diverse tasks. This growth in model size has dramatically increased the computational resources required for the training process. Consequently, distributed approaches, such as Federated Learning and Split Learning, have become essential paradigms for scalable deployment. However, existing Split Learning approaches assume client homogeneity and uniform split points across all participants. This critically limits their applicability to real-world IoT systems where devices exhibit heterogeneity in computational resources. To address this limitation, this paper proposes Hetero-SplitEE, a novel method that enables heterogeneous IoT devices to train a shared deep neural network in parallel collaboratively. By integrating heterogeneous early exits into hierarchical training, our approach allows each client to select distinct split points (cut layers) tailored to its computational capacity. In addition, we propose two cooperative training strategies, the Sequential strategy and the Averaging strategy, to facilitate this collaboration among clients with different split points. The Sequential strategy trains clients sequentially with a shared server model to reduce computational overhead. The Averaging strategy enables parallel client training with periodic cross-layer aggregation. Extensive experiments on CIFAR-10, CIFAR-100, and STL-10 datasets using ResNet-18 demonstrate that our method maintains competitive accuracy while efficiently supporting diverse computational constraints, enabling practical deployment of collaborative deep learning in heterogeneous IoT ecosystems.

</details>


### [122] [Self-Supervised Learning with Gaussian Processes](https://arxiv.org/abs/2512.09322)
*Yunshan Duan,Sinead Williamson*

Main category: cs.LG

TL;DR: 提出GPSSL方法，利用高斯过程进行自监督学习，无需显式定义正样本对，并能提供不确定性量化


<details>
  <summary>Details</summary>
Motivation: 传统自监督学习方法需要生成相似样本对，这在某些数据类型中具有挑战性，且缺乏不确定性量化能力，在样本外预测中表现不佳

Method: 提出高斯过程自监督学习（GPSSL），在表示上施加高斯过程先验，通过最小化损失函数获得广义贝叶斯后验，利用协方差函数自然地将相似单元的表示拉近

Result: 在多个数据集上的分类和回归任务实验表明，GPSSL在准确性、不确定性量化和误差控制方面优于传统方法

Conclusion: GPSSL为自监督学习提供了一种无需显式正样本对的新方法，能够提供后验不确定性并传播到下游任务，与核PCA和VICReg有理论联系但更具优势

Abstract: Self supervised learning (SSL) is a machine learning paradigm where models learn to understand the underlying structure of data without explicit supervision from labeled samples. The acquired representations from SSL have demonstrated useful for many downstream tasks including clustering, and linear classification, etc. To ensure smoothness of the representation space, most SSL methods rely on the ability to generate pairs of observations that are similar to a given instance. However, generating these pairs may be challenging for many types of data. Moreover, these methods lack consideration of uncertainty quantification and can perform poorly in out-of-sample prediction settings. To address these limitations, we propose Gaussian process self supervised learning (GPSSL), a novel approach that utilizes Gaussian processes (GP) models on representation learning. GP priors are imposed on the representations, and we obtain a generalized Bayesian posterior minimizing a loss function that encourages informative representations. The covariance function inherent in GPs naturally pulls representations of similar units together, serving as an alternative to using explicitly defined positive samples. We show that GPSSL is closely related to both kernel PCA and VICReg, a popular neural network-based SSL method, but unlike both allows for posterior uncertainties that can be propagated to downstream tasks. Experiments on various datasets, considering classification and regression tasks, demonstrate that GPSSL outperforms traditional methods in terms of accuracy, uncertainty quantification, and error control.

</details>


### [123] [Self Distillation Fine-Tuning of Protein Language Models Improves Versatility in Protein Design](https://arxiv.org/abs/2512.09329)
*Amin Tavakoli,Raswanth Murugan,Ozan Gokdemir,Arvind Ramanathan,Frances Arnold,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出了一种快速监督微调蛋白质语言模型的通用方法，通过轻量级筛选管道构建高质量训练数据，无需昂贵的实验数据集，能生成更稳定、功能更强且新颖的蛋白质序列。


<details>
  <summary>Details</summary>
Motivation: 蛋白质序列建模中，高质量标注数据难以获取，现有监督微调方法依赖昂贵的实验数据集，需要一种更高效、通用的蛋白质语言模型微调方法。

Method: 利用蛋白质语言模型自身，结合轻量级筛选管道和领域特定过滤器构建高质量训练数据，进行快速监督微调。该方法不依赖于特定蛋白质语言模型或蛋白质系统。

Result: 在色氨酸合酶酶家族上验证，微调后的模型生成的序列不仅更新颖，而且在目标设计约束和新兴蛋白质性质指标上都表现出改进特性。

Conclusion: 该方法提供了一种简单通用的蛋白质语言模型快速监督微调方案，能生成更稳定、功能更强且新颖的蛋白质序列，扩展了蛋白质序列空间的探索能力。

Abstract: Supervised fine-tuning (SFT) is a standard approach for adapting large language models to specialized domains, yet its application to protein sequence modeling and protein language models (PLMs) remains ad hoc. This is in part because high-quality annotated data are far more difficult to obtain for proteins than for natural language. We present a simple and general recipe for fast SFT of PLMs, designed to improve the fidelity, reliability, and novelty of generated protein sequences. Unlike existing approaches that require costly precompiled experimental datasets for SFT, our method leverages the PLM itself, integrating a lightweight curation pipeline with domain-specific filters to construct high-quality training data. These filters can independently refine a PLM's output and identify candidates for in vitro evaluation; when combined with SFT, they enable PLMs to generate more stable and functional enzymes, while expanding exploration into protein sequence space beyond natural variants. Although our approach is agnostic to both the choice of protein language model (PLM) and the protein system, we demonstrate its effectiveness with a genome-scale PLM (GenSLM) applied to the tryptophan synthase enzyme family. The supervised fine-tuned model generates sequences that are not only more novel but also display improved characteristics across both targeted design constraints and emergent protein property measures.

</details>


### [124] [Improved Physics-Driven Neural Network to Solve Inverse Scattering Problems](https://arxiv.org/abs/2512.09333)
*Yutong Du,Zicheng Liu,Bo Wu,Jingwei Kou,Hang Li,Changyou Li,Yali Zong,Bo Qi*

Main category: cs.LG

TL;DR: 提出改进的物理驱动神经网络框架，通过新型激活函数和动态散射子区域识别策略，解决电磁逆散射问题，实现高精度、鲁棒且高效的重建。


<details>
  <summary>Details</summary>
Motivation: 电磁逆散射问题在医学成像、无损检测等领域有重要应用，但传统方法在计算效率和精度方面存在局限。需要结合物理模型可解释性和神经网络实时推理能力，开发更有效的求解器。

Method: 1. 提出改进的物理驱动神经网络框架；2. 引入高斯局部化振荡抑制窗口激活函数稳定收敛；3. 开发动态散射子区域识别策略自适应优化计算域；4. 结合迁移学习扩展实际应用场景。

Result: 数值模拟和实验结果表明，该方法在重建精度、鲁棒性和计算效率方面均优于现有最先进方法，实现了更准确、更稳定的电磁逆散射重建。

Conclusion: 该研究成功将迭代算法的物理可解释性与神经网络的实时推理能力相结合，为电磁逆散射问题提供了高效、准确的解决方案，具有重要的理论和应用价值。

Abstract: This paper presents an improved physics-driven neural network (IPDNN) framework for solving electromagnetic inverse scattering problems (ISPs). A new Gaussian-localized oscillation-suppressing window (GLOW) activation function is introduced to stabilize convergence and enable a lightweight yet accurate network architecture. A dynamic scatter subregion identification strategy is further developed to adaptively refine the computational domain, preventing missed detections and reducing computational cost. Moreover, transfer learning is incorporated to extend the solver's applicability to practical scenarios, integrating the physical interpretability of iterative algorithms with the real-time inference capability of neural networks. Numerical simulations and experimental results demonstrate that the proposed solver achieves superior reconstruction accuracy, robustness, and efficiency compared with existing state-of-the-art methods.

</details>


### [125] [Branching Strategies Based on Subgraph GNNs: A Study on Theoretical Promise versus Practical Reality](https://arxiv.org/abs/2512.09355)
*Junru Zhou,Yicheng Wang,Pan Li*

Main category: cs.LG

TL;DR: 节点锚定子图GNN理论上能近似强分支，但实践中计算开销过大，导致内存瓶颈和求解时间变慢，表明当前表达性GNN的计算成本超过决策质量收益。


<details>
  <summary>Details</summary>
Motivation: 研究如何在混合整数线性规划（MILP）的分支决策中平衡图神经网络（GNN）的表达能力和计算效率。标准消息传递GNN（MPNNs）效率高但表达能力不足，而高阶GNN表达能力强大但计算成本过高。

Method: 采用节点锚定子图GNN作为理论折中方案，证明其表达能力虽低于3-WL但足以近似强分支评分。通过四个基准数据集进行广泛的实证评估，比较不同GNN架构的性能。

Result: 理论证明节点锚定子图GNN能近似强分支，但实证显示其O(n)复杂度导致显著内存瓶颈和比MPNN及启发式方法更慢的求解时间。表达性GNN的决策质量提升被计算成本抵消。

Conclusion: 对于MILP分支问题，当前表达性GNN的计算成本超过其决策质量收益，未来研究需要专注于保持效率的同时提升表达能力。

Abstract: Graph Neural Networks (GNNs) have emerged as a promising approach for ``learning to branch'' in Mixed-Integer Linear Programming (MILP). While standard Message-Passing GNNs (MPNNs) are efficient, they theoretically lack the expressive power to fully represent MILP structures. Conversely, higher-order GNNs (like 2-FGNNs) are expressive but computationally prohibitive. In this work, we investigate Subgraph GNNs as a theoretical middle ground. Crucially, while previous work [Chen et al., 2025] demonstrated that GNNs with 3-WL expressive power can approximate Strong Branching, we prove a sharper result: node-anchored Subgraph GNNs whose expressive power is strictly lower than 3-WL [Zhang et al., 2023] are sufficient to approximate Strong Branching scores. However, our extensive empirical evaluation on four benchmark datasets reveals a stark contrast between theory and practice. While node-anchored Subgraph GNNs theoretically offer superior branching decisions, their $O(n)$ complexity overhead results in significant memory bottlenecks and slower solving times than MPNNs and heuristics. Our results indicate that for MILP branching, the computational cost of expressive GNNs currently outweighs their gains in decision quality, suggesting that future research must focus on efficiency-preserving expressivity.

</details>


### [126] [KGOT: Unified Knowledge Graph and Optimal Transport Pseudo-Labeling for Molecule-Protein Interaction Prediction](https://arxiv.org/abs/2512.09365)
*Jiayu Qin,Zhengquan Luo,Guy Tadmor,Changyou Chen,David Zeevi,Zhiqiang Xu*

Main category: cs.LG

TL;DR: 提出一种利用最优传输生成伪标签的分子-蛋白质相互作用预测框架，通过整合多种生物数据模态解决数据稀缺问题，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有MPI模型面临两大挑战：1）标记数据稀缺，仅覆盖少量生物相关相互作用；2）大多方法仅依赖分子和蛋白质特征，忽略基因、代谢通路等更广泛的生物上下文信息。

Method: 首先整合多种生物数据集（分子、蛋白质、基因、通路级相互作用），然后开发基于最优传输的方法为未标记分子-蛋白质对生成高质量伪标签，利用已知相互作用的底层分布指导标签分配。

Result: 在多个MPI数据集（虚拟筛选和蛋白质检索任务）上评估，相比最先进方法在预测准确性和零样本能力方面均有显著提升。

Conclusion: 该框架不仅提升了MPI预测性能，还为利用多样化生物数据源解决传统单模态或双模态学习受限问题提供了新范式，推动计算生物学和药物发现领域发展。

Abstract: Predicting molecule-protein interactions (MPIs) is a fundamental task in computational biology, with crucial applications in drug discovery and molecular function annotation. However, existing MPI models face two major challenges. First, the scarcity of labeled molecule-protein pairs significantly limits model performance, as available datasets capture only a small fraction of biological relevant interactions. Second, most methods rely solely on molecular and protein features, ignoring broader biological context such as genes, metabolic pathways, and functional annotations that could provide essential complementary information. To address these limitations, our framework first aggregates diverse biological datasets, including molecular, protein, genes and pathway-level interactions, and then develop an optimal transport-based approach to generate high-quality pseudo-labels for unlabeled molecule-protein pairs, leveraging the underlying distribution of known interactions to guide label assignment. By treating pseudo-labeling as a mechanism for bridging disparate biological modalities, our approach enables the effective use of heterogeneous data to enhance MPI prediction. We evaluate our framework on multiple MPI datasets including virtual screening tasks and protein retrieval tasks, demonstrating substantial improvements over state-of-the-art methods in prediction accuracies and zero shot ability across unseen interactions. Beyond MPI prediction, our approach provides a new paradigm for leveraging diverse biological data sources to tackle problems traditionally constrained by single- or bi-modal learning, paving the way for future advances in computational biology and drug discovery.

</details>


### [127] [CFLight: Enhancing Safety with Traffic Signal Control through Counterfactual Learning](https://arxiv.org/abs/2512.09368)
*Mingyuan Li,Chunyu Liu,Zhuojun Li,Xiao Liu,Guangsheng Yu,Bo Du,Jun Shen,Qiang Wu*

Main category: cs.LG

TL;DR: 提出CFLight框架，通过反事实学习增强交通信号控制中的安全性，在保持效率的同时显著减少碰撞事故


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在交通信号控制中过于关注效率而忽视安全性，且缺乏可解释性，导致交叉口事故频发。反事实学习为平衡安全与效率提供了新思路。

Method: 提出基于反事实学习的新框架，构建结构因果模型预测不同动作的结果，设计反事实模块与额外"X"模块集成，开发CFLight算法实现近零碰撞控制策略。

Result: 在真实世界和合成数据集上的实验表明，CFLight相比传统强化学习方法和近期安全强化学习模型，能显著减少碰撞事故并提升整体交通性能。

Conclusion: CFLight为强化学习方法提供了一个通用且安全的框架，不仅改善了交通信号控制的安全性，还可推广应用于其他领域。

Abstract: Traffic accidents result in millions of injuries and fatalities globally, with a significant number occurring at intersections each year. Traffic Signal Control (TSC) is an effective strategy for enhancing safety at these urban junctures. Despite the growing popularity of Reinforcement Learning (RL) methods in optimizing TSC, these methods often prioritize driving efficiency over safety, thus failing to address the critical balance between these two aspects. Additionally, these methods usually need more interpretability. CounterFactual (CF) learning is a promising approach for various causal analysis fields. In this study, we introduce a novel framework to improve RL for safety aspects in TSC. This framework introduces a novel method based on CF learning to address the question: ``What if, when an unsafe event occurs, we backtrack to perform alternative actions, and will this unsafe event still occur in the subsequent period?'' To answer this question, we propose a new structure causal model to predict the result after executing different actions, and we propose a new CF module that integrates with additional ``X'' modules to promote safe RL practices. Our new algorithm, CFLight, which is derived from this framework, effectively tackles challenging safety events and significantly improves safety at intersections through a near-zero collision control strategy. Through extensive numerical experiments on both real-world and synthetic datasets, we demonstrate that CFLight reduces collisions and improves overall traffic performance compared to conventional RL methods and the recent safe RL model. Moreover, our method represents a generalized and safe framework for RL methods, opening possibilities for applications in other domains. The data and code are available in the github https://github.com/MJLee00/CFLight-Enhancing-Safety-with-Traffic-Signal-Control-through-Counterfactual-Learning.

</details>


### [128] [Are Hypervectors Enough? Single-Call LLM Reasoning over Knowledge Graphs](https://arxiv.org/abs/2512.09369)
*Yezi Liu,William Youngwoo Chung,Hanning Chen,Calvin Yeung,Mohsen Imani*

Main category: cs.LG

TL;DR: PathHD：基于超维度计算的轻量级知识图谱推理框架，用HDC替代神经路径评分，单次LLM调用即可完成查询，实现高效、可解释的KG-LLM推理


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的LLM推理方法依赖重型神经编码器或重复LLM调用，导致高延迟、高GPU成本和不透明的决策，阻碍了可信且可扩展的部署

Method: 提出PathHD框架：1) 将关系路径编码为块对角GHRR超向量；2) 使用块余弦相似度和Top-K剪枝对候选路径排序；3) 单次LLM裁决生成最终答案并引用支持路径。关键技术包括：顺序感知的非交换绑定操作符、校准相似度、一次性裁决步骤

Result: 在WebQSP、CWQ和GrailQA数据集上：1) 达到或超过强神经基线的Hits@1性能；2) 端到端延迟降低40-60%，GPU内存减少3-5倍；3) 提供可信的路径基础推理，改善错误诊断和可控性

Conclusion: 精心设计的HDC表示为高效KG-LLM推理提供了实用基础，在准确性、效率和可解释性之间实现了有利的权衡

Abstract: Recent advances in large language models (LLMs) have enabled strong reasoning over both structured and unstructured knowledge. When grounded on knowledge graphs (KGs), however, prevailing pipelines rely on heavy neural encoders to embed and score symbolic paths or on repeated LLM calls to rank candidates, leading to high latency, GPU cost, and opaque decisions that hinder faithful, scalable deployment. We propose PathHD, a lightweight and encoder-free KG reasoning framework that replaces neural path scoring with hyperdimensional computing (HDC) and uses only a single LLM call per query. PathHD encodes relation paths into block-diagonal GHRR hypervectors, ranks candidates with blockwise cosine similarity and Top-K pruning, and then performs a one-shot LLM adjudication to produce the final answer together with cited supporting paths. Technically, PathHD is built on three ingredients: (i) an order-aware, non-commutative binding operator for path composition, (ii) a calibrated similarity for robust hypervector-based retrieval, and (iii) a one-shot adjudication step that preserves interpretability while eliminating per-path LLM scoring. On WebQSP, CWQ, and the GrailQA split, PathHD (i) attains comparable or better Hits@1 than strong neural baselines while using one LLM call per query; (ii) reduces end-to-end latency by $40-60\%$ and GPU memory by $3-5\times$ thanks to encoder-free retrieval; and (iii) delivers faithful, path-grounded rationales that improve error diagnosis and controllability. These results indicate that carefully designed HDC representations provide a practical substrate for efficient KG-LLM reasoning, offering a favorable accuracy-efficiency-interpretability trade-off.

</details>


### [129] [Rates and architectures for learning geometrically non-trivial operators](https://arxiv.org/abs/2512.09376)
*T. Mitchell Roddenberry,Leo Tzou,Ivan Dokmanić,Maarten V. de Hoop,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 该论文将深度学习算子学习理论扩展到包含双纤维化变换（如广义Radon变换和测地线射线变换）的几何积分算子，证明这类算子不存在维度灾难，误差随训练样本数超代数衰减。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习算子学习理论主要针对椭圆算子等简单几何情况，但科学机器学习常涉及奇异性传播问题（如波动、对流、流体动力学）。需要扩展理论以涵盖双纤维化变换这类几何积分算子。

Method: 扩展算子学习理论到双纤维化变换，证明其不存在维度灾难；提出基于水平集方法的交叉注意力架构，显式编码几何结构，实现通用、稳定且数据高效的学习。

Result: 证明双纤维化变换类算子误差随训练样本数超代数衰减（快于任意固定幂次），不存在维度灾难；所提架构能从极少训练样本中学习这类算子。

Conclusion: 该研究扩展了科学机器学习算子学习理论，涵盖涉及奇异性传播的几何积分算子，为波动、对流等实际问题的深度学习应用提供理论支持。

Abstract: Deep learning methods have proven capable of recovering operators between high-dimensional spaces, such as solution maps of PDEs and similar objects in mathematical physics, from very few training samples. This phenomenon of data-efficiency has been proven for certain classes of elliptic operators with simple geometry, i.e., operators that do not change the domain of the function or propagate singularities. However, scientific machine learning is commonly used for problems that do involve the propagation of singularities in a priori unknown ways, such as waves, advection, and fluid dynamics. In light of this, we expand the learning theory to include double fibration transforms--geometric integral operators that include generalized Radon and geodesic ray transforms. We prove that this class of operators does not suffer from the curse of dimensionality: the error decays superalgebraically, that is, faster than any fixed power of the reciprocal of the number of training samples. Furthermore, we investigate architectures that explicitly encode the geometry of these transforms, demonstrating that an architecture reminiscent of cross-attention based on levelset methods yields a parameterization that is universal, stable, and learns double fibration transforms from very few training examples. Our results contribute to a rapidly-growing line of theoretical work on learning operators for scientific machine learning.

</details>


### [130] [Federated Distillation Assisted Vehicle Edge Caching Scheme Based on Lightweight DDPM](https://arxiv.org/abs/2512.09378)
*Xun Li,Qiong Wu,Pingyi Fan,Kezhi Wang,Wen Chen,Khaled B. Letaief*

Main category: cs.LG

TL;DR: 提出基于轻量级去噪扩散概率模型（LDPM）的联邦蒸馏辅助车辆边缘缓存方案，解决传统联邦学习通信开销大和车辆移动导致训练失败的问题


<details>
  <summary>Details</summary>
Motivation: 车辆边缘缓存能显著降低车辆用户访问内容的延迟，但需要准确预测用户兴趣内容而不暴露隐私。传统联邦学习虽然能保护隐私，但存在通信开销大和车辆移动导致训练失败的问题

Method: 提出基于轻量级去噪扩散概率模型（LDPM）的联邦蒸馏辅助车辆边缘缓存方案。使用联邦蒸馏减少模型传输频率，结合LDPM进行内容预测

Result: 仿真结果表明，该方案对车辆速度变化具有良好的鲁棒性，显著降低了通信开销，提高了缓存命中率

Conclusion: 提出的联邦蒸馏辅助车辆边缘缓存方案有效解决了传统联邦学习在车辆边缘缓存中的通信开销和训练失败问题，实现了隐私保护下的高效内容预测

Abstract: Vehicle edge caching is a promising technology that can significantly reduce the latency for vehicle users (VUs) to access content by pre-caching user-interested content at edge nodes. It is crucial to accurately predict the content that VUs are interested in without exposing their privacy. Traditional federated learning (FL) can protect user privacy by sharing models rather than raw data. However, the training of FL requires frequent model transmission, which can result in significant communication overhead. Additionally, vehicles may leave the road side unit (RSU) coverage area before training is completed, leading to training failures. To address these issues, in this letter, we propose a federated distillation-assisted vehicle edge caching scheme based on lightweight denoising diffusion probabilistic model (LDPM). The simulation results demonstrate that the proposed vehicle edge caching scheme has good robustness to variations in vehicle speed, significantly reducing communication overhead and improving cache hit percentage.

</details>


### [131] [Towards Resilient Transportation: A Conditional Transformer for Accident-Informed Traffic Forecasting](https://arxiv.org/abs/2512.09398)
*Hongjun Wang,Jiawei Yong,Jiawei Wang,Shintaro Fukushima,Renhe Jiang*

Main category: cs.LG

TL;DR: 提出ConFormer框架，整合交通事故和法规数据，通过图传播和引导归一化层动态调整时空节点关系，在东京和加州数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 交通预测面临外部因素（如交通事故、法规）的复杂影响，现有模型因数据整合有限而忽视这些因素，导致预测准确性受限。

Method: 1) 构建包含交通事故和法规数据的东京和加州数据集；2) 提出ConFormer框架，结合图传播和引导归一化层，动态调整时空节点关系；3) 基于历史模式增强预测准确性。

Result: ConFormer在预测性能和效率上均超越最先进的STAEFormer，计算成本更低、参数需求更少，在多个指标上优于主流时空基线模型。

Conclusion: ConFormer通过整合外部因素数据和动态调整时空关系，显著提升交通预测准确性，为交通预测研究提供了有前景的解决方案。

Abstract: Traffic prediction remains a key challenge in spatio-temporal data mining, despite progress in deep learning. Accurate forecasting is hindered by the complex influence of external factors such as traffic accidents and regulations, often overlooked by existing models due to limited data integration. To address these limitations, we present two enriched traffic datasets from Tokyo and California, incorporating traffic accident and regulation data. Leveraging these datasets, we propose ConFormer (Conditional Transformer), a novel framework that integrates graph propagation with guided normalization layer. This design dynamically adjusts spatial and temporal node relationships based on historical patterns, enhancing predictive accuracy. Our model surpasses the state-of-the-art STAEFormer in both predictive performance and efficiency, achieving lower computational costs and reduced parameter demands. Extensive evaluations demonstrate that ConFormer consistently outperforms mainstream spatio-temporal baselines across multiple metrics, underscoring its potential to advance traffic prediction research.

</details>


### [132] [Black-Box Behavioral Distillation Breaks Safety Alignment in Medical LLMs](https://arxiv.org/abs/2512.09403)
*Sohely Jahan,Ruimin Sun*

Main category: cs.LG

TL;DR: 黑盒蒸馏攻击能以低成本（12美元）复制医疗大语言模型的专业能力，同时剥离其安全对齐机制，导致86%对抗性提示产生不安全输出，揭示功能-伦理差距威胁。


<details>
  <summary>Details</summary>
Motivation: 随着医疗大语言模型在临床工作流中的集成增加，对其对齐鲁棒性和安全性的担忧日益加剧。先前研究主要关注分类模型或记忆泄露，而安全对齐的生成式医疗大语言模型的脆弱性尚未充分探索。

Method: 提出黑盒蒸馏攻击方法：仅通过输出级访问，向Meditron-7B发出48,000个指令查询，收集25,000个良性指令-响应对，在零对齐监督设置下通过参数高效LoRA微调LLaMA3 8B代理模型。开发动态对抗评估框架，结合基于生成查询的有害提示生成、验证器过滤、类别化失败分析和自适应随机搜索越狱攻击。

Result: 代理模型在良性输入上保持高保真度，同时对86%对抗性提示产生不安全完成，远超Meditron-7B（66%）和未调优基础模型（46%）。揭示了功能-伦理差距：任务效用可转移，而对齐机制会崩溃。

Conclusion: 仅使用良性数据的黑盒蒸馏暴露了实际且未被充分认知的威胁：攻击者能以低成本复制医疗大语言模型能力同时剥离安全机制，强调需要提取感知的安全监控。提出分层防御系统作为黑盒部署中实时对齐漂移的原型检测器。

Abstract: As medical large language models (LLMs) become increasingly integrated into clinical workflows, concerns around alignment robustness, and safety are escalating. Prior work on model extraction has focused on classification models or memorization leakage, leaving the vulnerability of safety-aligned generative medical LLMs underexplored.
  We present a black-box distillation attack that replicates the domain-specific reasoning of safety-aligned medical LLMs using only output-level access. By issuing 48,000 instruction queries to Meditron-7B and collecting 25,000 benign instruction response pairs, we fine-tune a LLaMA3 8B surrogate via parameter efficient LoRA under a zero-alignment supervision setting, requiring no access to model weights, safety filters, or training data. With a cost of $12, the surrogate achieves strong fidelity on benign inputs while producing unsafe completions for 86% of adversarial prompts, far exceeding both Meditron-7B (66%) and the untuned base model (46%). This reveals a pronounced functional-ethical gap, task utility transfers, while alignment collapses. To analyze this collapse, we develop a dynamic adversarial evaluation framework combining Generative Query (GQ)-based harmful prompt generation, verifier filtering, category-wise failure analysis, and adaptive Random Search (RS) jailbreak attacks. We also propose a layered defense system, as a prototype detector for real-time alignment drift in black-box deployments.
  Our findings show that benign-only black-box distillation exposes a practical and under-recognized threat: adversaries can cheaply replicate medical LLM capabilities while stripping safety mechanisms, underscoring the need for extraction-aware safety monitoring.

</details>


### [133] [Cauchy-Schwarz Fairness Regularizer](https://arxiv.org/abs/2512.09467)
*Yezi Liu,Hanning Chen,Wenjun Huang,Yang Ni,Mohsen Imani*

Main category: cs.LG

TL;DR: 论文提出了一种基于Cauchy-Schwarz散度的公平性正则化器，通过惩罚敏感群体间预测分布的CS散度来提升群体公平性，相比现有方法具有更稳定的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有公平性正则化器基于不同的距离度量和设计选择，导致其行为难以推理且在不同任务上性能不一致。需要探究什么属性构成一个好的公平性正则化器。

Method: 首先将现有方法分为三类：跨敏感群体匹配预测统计量、对齐潜在表示、直接最小化预测与敏感属性之间的依赖性。基于理想距离度量的属性，提出Cauchy-Schwarz公平性正则化器，惩罚敏感群体条件预测分布之间的经验CS散度。

Result: 在四个表格基准数据集和一个图像数据集上的实验表明，CS正则化器在保持竞争性准确率的同时，持续改善人口统计均等和机会均等指标，并且相比先前正则化器在超参数设置下实现了更稳定的效用-公平性权衡。

Conclusion: Cauchy-Schwarz散度是一个有效的公平性正则化器，具有紧致的泛化界限、对尺度差异的鲁棒性以及处理任意预测分布的能力，为群体公平性提供了一致且稳定的解决方案。

Abstract: Group fairness in machine learning is often enforced by adding a regularizer that reduces the dependence between model predictions and sensitive attributes. However, existing regularizers are built on heterogeneous distance measures and design choices, which makes their behavior hard to reason about and their performance inconsistent across tasks. This raises a basic question: what properties make a good fairness regularizer? We address this question by first organizing existing in-process methods into three families: (i) matching prediction statistics across sensitive groups, (ii) aligning latent representations, and (iii) directly minimizing dependence between predictions and sensitive attributes. Through this lens, we identify desirable properties of the underlying distance measure, including tight generalization bounds, robustness to scale differences, and the ability to handle arbitrary prediction distributions. Motivated by these properties, we propose a Cauchy-Schwarz (CS) fairness regularizer that penalizes the empirical CS divergence between prediction distributions conditioned on sensitive groups. Under a Gaussian comparison, we show that CS divergence yields a tighter bound than Kullback-Leibler divergence, Maximum Mean Discrepancy, and the mean disparity used in Demographic Parity, and we discuss how these advantages translate to a distribution-free, kernel-based estimator that naturally extends to multiple sensitive attributes. Extensive experiments on four tabular benchmarks and one image dataset demonstrate that the proposed CS regularizer consistently improves Demographic Parity and Equal Opportunity metrics while maintaining competitive accuracy, and achieves a more stable utility-fairness trade-off across hyperparameter settings compared to prior regularizers.

</details>


### [134] [Representation Invariance and Allocation: When Subgroup Balance Matters](https://arxiv.org/abs/2512.09496)
*Anissa Alloula,Charles Jones,Zuzanna Wakefield-Skorniewska,Francesco Quinzan,Bartłomiej Papież*

Main category: cs.LG

TL;DR: 论文提出"潜在分离假设"，认为预训练模型中子群在潜在空间的分离程度决定了微调模型对训练数据子群分布的敏感性，挑战了传统数据平衡假设。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为平衡训练数据中的子群分布能优化模型性能，但近期实证研究发现：在某些情况下，不平衡的数据分布反而能提升子群性能，而有时即使完全缺失某个子群，模型性能也不受影响。这种矛盾现象需要系统性的解释。

Method: 1. 对四个视觉和语言模型进行系统研究，通过改变训练数据组成来分析子群性能对数据平衡的敏感性；2. 提出"潜在分离假设"，认为微调模型对子群表示的依赖程度由预训练模型中子群在潜在空间的分离程度决定；3. 对该假设进行形式化、理论分析和实证验证；4. 应用于基础模型微调实践，展示如何通过分析潜在子群分离来指导数据收集和平衡决策。

Result: 研究发现子群性能对数据平衡的敏感性确实与预训练模型中子群在潜在空间的分离程度相关。当子群在潜在空间中高度分离时，模型性能对数据平衡更敏感；当子群在潜在空间中重叠较多时，不平衡的数据分布甚至可能带来性能提升。实证验证支持了潜在分离假设。

Conclusion: 传统的数据平衡假设并不总是成立，子群在预训练模型潜在空间中的分离程度是关键决定因素。这一发现为数据收集和平衡策略提供了新的理论指导，能够更有效地优化基础模型的微调过程。

Abstract: Unequal representation of demographic groups in training data poses challenges to model generalisation across populations. Standard practice assumes that balancing subgroup representation optimises performance. However, recent empirical results contradict this assumption: in some cases, imbalanced data distributions actually improve subgroup performance, while in others, subgroup performance remains unaffected by the absence of an entire subgroup during training. We conduct a systematic study of subgroup allocation across four vision and language models, varying training data composition to characterise the sensitivity of subgroup performance to data balance. We propose the latent separation hypothesis, which states that a partially fine-tuned model's dependence on subgroup representation is determined by the degree of separation between subgroups in the latent space of the pre-trained model. We formalise this hypothesis, provide theoretical analysis, and validate it empirically. Finally, we present a practical application to foundation model fine-tuning, demonstrating that quantitative analysis of latent subgroup separation can inform data collection and balancing decisions.

</details>


### [135] [Contextual Dynamic Pricing with Heterogeneous Buyers](https://arxiv.org/abs/2512.09513)
*Thodoris Lykouris,Sloan Nietert,Princewill Okoroafor,Chara Podimata,Julian Zimmert*

Main category: cs.LG

TL;DR: 研究异构买家群体的上下文动态定价问题，提出基于乐观后验采样的算法，实现$\widetilde{O}(K_{\star}\sqrt{dT})$的遗憾上界，并在非上下文情况下提出方差感知缩放算法。


<details>
  <summary>Details</summary>
Motivation: 现有动态定价研究大多假设买家类型同质，但实际中买家估值类型存在异质性。本文首次研究异构买家群体的上下文动态定价问题，其中买家估值类型来自未知的有限支撑分布。

Method: 提出基于乐观后验采样的上下文定价算法，利用上下文信息进行定价决策。在非上下文情况下，进一步提出方差感知缩放算法，通过考虑估值分布的方差来优化性能。

Result: 上下文定价算法实现了$\widetilde{O}(K_{\star}\sqrt{dT})$的遗憾上界，证明在$d$和$T$维度上达到紧致（忽略对数项）。非上下文情况下，方差感知缩放算法在$K_{\star}$依赖上达到最优。

Conclusion: 本文首次系统研究了异构买家群体的上下文动态定价问题，提出的算法在理论和实际性能上都有显著改进，为处理买家异质性提供了有效的解决方案。

Abstract: We initiate the study of contextual dynamic pricing with a heterogeneous population of buyers, where a seller repeatedly posts prices (over $T$ rounds) that depend on the observable $d$-dimensional context and receives binary purchase feedback. Unlike prior work assuming homogeneous buyer types, in our setting the buyer's valuation type is drawn from an unknown distribution with finite support size $K_{\star}$. We develop a contextual pricing algorithm based on optimistic posterior sampling with regret $\widetilde{O}(K_{\star}\sqrt{dT})$, which we prove to be tight in $d$ and $T$ up to logarithmic terms. Finally, we refine our analysis for the non-contextual pricing case, proposing a variance-aware zooming algorithm that achieves the optimal dependence on $K_{\star}$.

</details>


### [136] [QuanvNeXt: An end-to-end quanvolutional neural network for EEG-based detection of major depressive disorder](https://arxiv.org/abs/2512.09517)
*Nabil Anan Orka,Ehtashamul Haque,Maftahul Jannat,Md Abdul Awal,Mohammad Ali Moni*

Main category: cs.LG

TL;DR: QuanvNeXt是一个用于EEG抑郁诊断的端到端全量子卷积模型，通过Cross Residual块减少特征同质性并增强跨特征关系，在两个开源数据集上达到93.1%准确率和97.2% AUC-ROC，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发一个高效可靠的基于EEG的抑郁症诊断模型，解决现有方法中特征同质性问题，同时保持参数效率，提高诊断准确性和可靠性。

Method: 提出QuanvNeXt端到端全量子卷积模型，引入新颖的Cross Residual块来减少特征同质性并增强跨特征关系，同时保持参数效率。使用两个开源EEG数据集进行评估，并进行不确定性分析和可解释AI分析。

Result: 在两个数据集上平均准确率93.1%，平均AUC-ROC 97.2%，优于InceptionTime等SOTA基线。不确定性分析显示即使在最高扰动(ε=0.1)下ECE分数仍保持较低水平(0.0436-0.1159)。可解释AI分析确认模型能有效识别和学习区分健康对照与重度抑郁症的频谱时域模式。

Conclusion: QuanvNeXt为基于EEG的抑郁症诊断建立了一个高效可靠的方法，通过创新的Cross Residual块设计解决了特征同质性问题，在准确性和可靠性方面均表现出色。

Abstract: This study presents QuanvNeXt, an end-to-end fully quanvolutional model for EEG-based depression diagnosis. QuanvNeXt incorporates a novel Cross Residual block, which reduces feature homogeneity and strengthens cross-feature relationships while retaining parameter efficiency. We evaluated QuanvNeXt on two open-source datasets, where it achieved an average accuracy of 93.1% and an average AUC-ROC of 97.2%, outperforming state-of-the-art baselines such as InceptionTime (91.7% accuracy, 95.9% AUC-ROC). An uncertainty analysis across Gaussian noise levels demonstrated well-calibrated predictions, with ECE scores remaining low (0.0436, Dataset 1) to moderate (0.1159, Dataset 2) even at the highest perturbation (ε = 0.1). Additionally, a post-hoc explainable AI analysis confirmed that QuanvNeXt effectively identifies and learns spectrotemporal patterns that distinguish between healthy controls and major depressive disorder. Overall, QuanvNeXt establishes an efficient and reliable approach for EEG-based depression diagnosis.

</details>


### [137] [Latent-Autoregressive GP-VAE Language Model](https://arxiv.org/abs/2512.09535)
*Yves Ruffenach*

Main category: cs.LG

TL;DR: 提出了一种基于高斯过程的完全潜在自回归方案，集成到变分自编码器中，将序列动态从观测空间转移到连续潜在空间，同时通过非自回归解码器保持语言生成的并行性。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型中的时间结构是否可以通过潜在空间的概率几何来支持，而不是完全依赖显式的神经操作，从而可能提高模型的效率和表达能力。

Method: 提出完全潜在自回归方案：1) 使用因果高斯过程先验；2) 结构化摊销后验；3) 基于正则化ELBO的训练协议；4) 非自回归解码器实现并行语言生成。

Result: 在概念验证框架下的实证评估显示：1) 模型可以稳定训练；2) 序列采样和并行采样变体表现一致；3) 潜在空间的概率几何能够支持部分时间结构。

Conclusion: 语言模型中的部分时间结构可以通过潜在空间的概率几何来支持，而不完全依赖显式神经操作，这为序列建模提供了新的可能性。

Abstract: We investigate a fully Latent AutoRegressive scheme based on a Gaussian Process (GP) integrated into a Variational Autoencoder (VAE). In this setting, sequential dynamics are transferred from the observation space to a continuous latent space, while linguistic generation remains parallel through a non-autoregressive decoder. We present a complete methodological formulation, including a causal GP prior, a structured amortized posterior, and a training protocol based on a regularized ELBO. Empirical evaluation, conducted within a deliberately constrained proof-of-concept (POC) framework, shows that the model can be trained stably and that the sequential and parallel sampling variants exhibit consistent behavior. Overall, the results suggest that part of the temporal structure in a language model can be supported by the probabilistic geometry of the latent space rather than by explicit neural operations.

</details>


### [138] [Stanford Sleep Bench: Evaluating Polysomnography Pre-training Methods for Sleep Foundation Models](https://arxiv.org/abs/2512.09591)
*Magnus Ruud Kjaer,Rahul Thapa,Gauri Ganjoo,Hyatt Moore,Poul Joergen Jennum,Brandon M. Westover,James Zou,Emmanuel Mignot,Bryan He,Andreas Brink-Kjaer*

Main category: cs.LG

TL;DR: 斯坦福睡眠基准：一个包含17,467条多模态PSG记录的大规模数据集，用于评估自监督表示学习方法在睡眠分析任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 睡眠分析领域缺乏共享数据集和基准，以及缺乏对自监督表示学习方法在睡眠相关任务上的系统性评估，阻碍了睡眠基础模型的发展。

Method: 引入斯坦福睡眠基准数据集，包含17,467条PSG记录（超过163,000小时），涵盖13种临床疾病预测任务和经典睡眠任务。系统性评估多种自监督预训练方法在四个下游任务上的表现。

Result: 在睡眠分期、呼吸暂停诊断和年龄估计任务上，多种预训练方法表现相当；但在死亡率和疾病预测任务上，对比学习显著优于其他方法，且预训练收敛更快。

Conclusion: 斯坦福睡眠基准填补了睡眠分析领域的数据集和评估空白，对比学习在临床预测任务上表现优异，将发布数据集、预训练模型权重和代码以促进可重复性和睡眠研究进展。

Abstract: Polysomnography (PSG), the gold standard test for sleep analysis, generates vast amounts of multimodal clinical data, presenting an opportunity to leverage self-supervised representation learning (SSRL) for pre-training foundation models to enhance sleep analysis. However, progress in sleep foundation models is hindered by two key limitations: (1) the lack of a shared dataset and benchmark with diverse tasks for training and evaluation, and (2) the absence of a systematic evaluation of SSRL approaches across sleep-related tasks. To address these gaps, we introduce Stanford Sleep Bench, a large-scale PSG dataset comprising 17,467 recordings totaling over 163,000 hours from a major sleep clinic, including 13 clinical disease prediction tasks alongside canonical sleep-related tasks such as sleep staging, apnea diagnosis, and age estimation. We systematically evaluate SSRL pre-training methods on Stanford Sleep Bench, assessing downstream performance across four tasks: sleep staging, apnea diagnosis, age estimation, and disease and mortality prediction. Our results show that multiple pretraining methods achieve comparable performance for sleep staging, apnea diagnosis, and age estimation. However, for mortality and disease prediction, contrastive learning significantly outperforms other approaches while also converging faster during pretraining. To facilitate reproducibility and advance sleep research, we will release Stanford Sleep Bench along with pretrained model weights, training pipelines, and evaluation code.

</details>


### [139] [Semantic-Aware Cooperative Communication and Computation Framework in Vehicular Networks](https://arxiv.org/abs/2512.09621)
*Jingbo Zhang,Maoxin Ji,Qiong Wu,Pingyi Fan,Kezhi Wang,Wen Chen*

Main category: cs.LG

TL;DR: 提出基于参数分布噪声的多智能体近端策略优化任务卸载方法（MAPPO-PDN）与线性规划结合的TCSC框架，用于车联网语义通信任务卸载优化


<details>
  <summary>Details</summary>
Motivation: 车联网中语义通信与车辆边缘计算结合需要高效的任务处理框架，特别是在高速公路场景下，需要考虑任务延迟和语义符号数量的优化问题

Method: 提出三方协作语义通信（TCSC）框架，将MINLP问题分解为两个子问题：1）使用MAPPO-PDN优化语义符号数量；2）使用线性规划解决卸载比例

Result: 仿真结果表明，该方案性能优于其他对比算法

Conclusion: TCSC框架结合MAPPO-PDN和线性规划能够有效优化车联网语义通信任务卸载，在高速公路场景下实现性能提升

Abstract: Semantic Communication (SC) combined with Vehicular edge computing (VEC) provides an efficient edge task processing paradigm for Internet of Vehicles (IoV). Focusing on highway scenarios, this paper proposes a Tripartite Cooperative Semantic Communication (TCSC) framework, which enables Vehicle Users (VUs) to perform semantic task offloading via Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) communications. Considering task latency and the number of semantic symbols, the framework constructs a Mixed-Integer Nonlinear Programming (MINLP) problem, which is transformed into two subproblems. First, we innovatively propose a multi-agent proximal policy optimization task offloading optimization method based on parametric distribution noise (MAPPO-PDN) to solve the optimization problem of the number of semantic symbols; second, linear programming (LP) is used to solve offloading ratio. Simulations show that performance of this scheme is superior to that of other algorithms.

</details>


### [140] [Membership and Dataset Inference Attacks on Large Audio Generative Models](https://arxiv.org/abs/2512.09654)
*Jakub Proboszcz,Paweł Kochanski,Karol Korszun,Donato Crisostomi,Giorgio Strano,Emanuele Rodolà,Kamil Deja,Jan Dubinski*

Main category: cs.LG

TL;DR: 论文研究音频生成模型的版权保护问题，通过数据集推断（DI）而非单一成员推断（MIA）来检测艺术家的作品是否被用于模型训练。


<details>
  <summary>Details</summary>
Motivation: 随着扩散和自回归架构的音频生成模型快速发展，质量显著提升，但引发了版权担忧。这些模型通常在大量艺术和商业作品上训练，需要可靠的方法验证艺术家的材料是否被包含在训练数据中，为版权持有者提供保护手段。

Method: 研究在开源音频生成模型上进行成员推断攻击（MIA）的可行性，发现单一音频样本的成员推断效果有限。转而采用数据集推断（DI）方法，借鉴文本和视觉领域的工作，通过聚合多个样本的成员证据来增强检测能力。

Result: 实证结果显示：成员推断（MIA）在大规模、多样化数据集上效果有限，单个样本的成员信号较弱。但数据集推断（DI）在音频领域表现成功，能够更有效地评估艺术家的作品是否对模型训练有贡献。

Conclusion: 数据集推断（DI）为大型音频生成模型时代的版权保护和数据集问责制提供了有前景的方向，比单一成员推断更实用，尤其适用于艺术家和媒体所有者拥有作品集合而非孤立样本的情况。

Abstract: Generative audio models, based on diffusion and autoregressive architectures, have advanced rapidly in both quality and expressiveness. This progress, however, raises pressing copyright concerns, as such models are often trained on vast corpora of artistic and commercial works. A central question is whether one can reliably verify if an artist's material was included in training, thereby providing a means for copyright holders to protect their content. In this work, we investigate the feasibility of such verification through membership inference attacks (MIA) on open-source generative audio models, which attempt to determine whether a specific audio sample was part of the training set. Our empirical results show that membership inference alone is of limited effectiveness at scale, as the per-sample membership signal is weak for models trained on large and diverse datasets. However, artists and media owners typically hold collections of works rather than isolated samples. Building on prior work in text and vision domains, in this work we focus on dataset inference (DI), which aggregates diverse membership evidence across multiple samples. We find that DI is successful in the audio domain, offering a more practical mechanism for assessing whether an artist's works contributed to model training. Our results suggest DI as a promising direction for copyright protection and dataset accountability in the era of large audio generative models.

</details>


### [141] [A data-driven approach to linking design features with manufacturing process data for sustainable product development](https://arxiv.org/abs/2512.09690)
*Jiahang Li,Lucas Cazzonelli,Jacqueline Höllig,Markus Doellken,Sven Matthiesen*

Main category: cs.LG

TL;DR: 提出一种数据驱动方法，将设计特征与制造过程数据关联分析，通过机器学习模型实现自动化设计改进建议，支持可持续产品开发。


<details>
  <summary>Details</summary>
Motivation: 工业物联网(IIoT)技术实现了制造过程数据的自动实时采集，为数据驱动的产品开发提供了新机会。但目前的数据驱动方法通常局限于特定领域（如设计或制造），缺乏设计特征与制造过程数据的整合。由于设计决策显著影响制造结果（如错误率、能耗、加工时间），这种整合的缺失限制了数据驱动产品设计改进的潜力。

Method: 开发了全面的系统架构以确保连续的数据采集和整合。以设计特征与制造过程数据的关联为基础，构建机器学习模型，实现自动化设计改进建议。通过将制造过程数据与可持续性指标整合，支持可持续产品开发。

Result: 建立了设计特征与制造过程数据的映射关系分析框架，开发了能够提供自动化设计改进建议的机器学习模型，为可持续产品开发开辟了新可能性。

Conclusion: 该方法通过整合设计特征和制造过程数据，利用机器学习实现数据驱动的设计改进，为可持续产品开发提供了新的技术途径，解决了当前数据驱动方法在跨领域整合方面的局限性。

Abstract: The growing adoption of Industrial Internet of Things (IIoT) technologies enables automated, real-time collection of manufacturing process data, unlocking new opportunities for data-driven product development. Current data-driven methods are generally applied within specific domains, such as design or manufacturing, with limited exploration of integrating design features and manufacturing process data. Since design decisions significantly affect manufacturing outcomes, such as error rates, energy consumption, and processing times, the lack of such integration restricts the potential for data-driven product design improvements. This paper presents a data-driven approach to mapping and analyzing the relationship between design features and manufacturing process data. A comprehensive system architecture is developed to ensure continuous data collection and integration. The linkage between design features and manufacturing process data serves as the basis for developing a machine learning model that enables automated design improvement suggestions. By integrating manufacturing process data with sustainability metrics, this approach opens new possibilities for sustainable product development.

</details>


### [142] [Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning](https://arxiv.org/abs/2512.09706)
*Kaichen He,Zihao Wang,Muyao Li,Anji Liu,Yitao Liang*

Main category: cs.LG

TL;DR: CrossAgent是一个统一智能体模型，能够掌握异构动作空间并自主选择最优交互接口，在Minecraft环境中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体通常局限于静态预定义动作空间（如API、GUI事件或机器人命令），这种刚性限制了它们在动态环境中的适应性，因为最优交互粒度会随上下文变化。

Method: 提出CrossAgent统一智能体模型，采用综合训练流程：冷启动监督微调结合多轮组相对策略优化（GRPO）算法，使智能体学习自适应动作切换，无需人工指定规则。

Result: 在开放世界Minecraft环境的800多个任务上进行广泛实验，CrossAgent实现了最先进的性能，通过动态利用不同动作空间的优势，显著优于固定动作基线，在长时程推理中表现出优越的泛化能力和效率。

Conclusion: CrossAgent通过掌握异构动作空间和自主选择最优接口，解决了现有智能体在动态环境中适应性受限的问题，为智能体AI从工程化复杂工作流向后训练原生模型转变提供了有效方案。

Abstract: The paradigm of agentic AI is shifting from engineered complex workflows to post-training native models. However, existing agents are typically confined to static, predefined action spaces--such as exclusively using APIs, GUI events, or robotic commands. This rigidity limits their adaptability in dynamic environments where the optimal granularity of interaction varies contextually. To bridge this gap, we propose CrossAgent, a unified agentic model that masters heterogeneous action spaces and autonomously selects the most effective interface for each step of a trajectory. We introduce a comprehensive training pipeline that integrates cold-start supervised fine-tuning with a Multi-Turn Group Relative Policy Optimization (GRPO) algorithm. This approach enables the agent to learn adaptive action switching--balancing high-level efficiency with low-level precision--without human-specified rules. Extensive experiments on over 800 tasks in the open-world Minecraft environment demonstrate that CrossAgent achieves state-of-the-art performance. By dynamically leveraging the strengths of diverse action spaces, our model significantly outperforms fixed-action baselines, exhibiting superior generalization and efficiency in long-horizon reasoning. All code and models are available at https://github.com/CraftJarvis/OpenHA

</details>


### [143] [Mixture of Lookup Key-Value Experts](https://arxiv.org/abs/2512.09723)
*Zongcheng Wang*

Main category: cs.LG

TL;DR: MoLKV模型通过引入上下文感知的专家选择机制，改进了MoLE模型仅依赖输入ID的局限性，使用键值对专家结构实现更好的性能。


<details>
  <summary>Details</summary>
Motivation: MoLE模型虽然适合资源受限设备，但其仅基于输入ID的上下文无关专家选择机制限制了模型性能，需要更智能的专家激活方式。

Method: 提出MoLKV模型，将每个专家构建为键值对，通过输入生成的查询与当前序列缓存的键值专家交互，实现上下文感知的专家输出。

Result: 实验结果显示MoLKV在小规模评估中取得了显著更低的验证损失，证明了上下文感知机制的有效性。

Conclusion: MoLKV通过键值对专家结构和上下文感知机制成功解决了MoLE的局限性，为资源受限设备上的LLM推理提供了改进方案。

Abstract: Recent research has developed several LLM architectures suitable for inference on end-user devices, such as the Mixture of Lookup Experts (MoLE)~\parencite{jie_mixture_2025}. A key feature of MoLE is that each token id is associated with a dedicated group of experts. For a given input, only the experts corresponding to the input token id will be activated. Since the communication overhead of loading this small number of activated experts into RAM during inference is negligible, expert parameters can be offloaded to storage, making MoLE suitable for resource-constrained devices. However, MoLE's context-independent expert selection mechanism, based solely on input ids, may limit model performance. To address this, we propose the \textbf{M}ixture \textbf{o}f \textbf{L}ookup \textbf{K}ey-\textbf{V}alue Experts (\textbf{MoLKV}) model. In MoLKV, each expert is structured as a key-value pair. For a given input, the input-derived query interacts with the cached key-value experts from the current sequence, generating a context-aware expert output. This context-aware mechanism alleviates the limitation of MoLE, and experimental results demonstrate that MoLKV achieves significantly lower validation loss in small-scale evaluations.

</details>


### [144] [Circuits, Features, and Heuristics in Molecular Transformers](https://arxiv.org/abs/2512.09757)
*Kristof Varadi,Mark Marosi,Peter Antal*

Main category: cs.LG

TL;DR: 该研究对训练在药物类小分子上的自回归Transformer进行机制分析，揭示了模型在多个抽象层次上捕捉分子表示规则的计算结构，并利用稀疏自编码器提取化学相关激活模式的特征字典。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer能够生成有效且多样的化学结构，但人们对这些模型如何捕捉分子表示规则的机制知之甚少。研究旨在揭示自回归Transformer在化学结构生成中的计算机制。

Method: 对训练在药物类小分子上的自回归Transformer进行机制分析，使用稀疏自编码器（SAEs）提取与化学相关激活模式相关的特征字典，并在下游任务中验证发现。

Result: 识别出与低级语法解析和更抽象的化学有效性约束一致的计算模式，提取了化学相关激活模式的特征字典，机制洞察能够转化为各种实际场景中的预测性能。

Conclusion: 通过机制分析揭示了Transformer在化学结构生成中的多层次计算结构，证明了机制洞察对实际应用的价值，为理解AI模型在化学领域的内部工作机制提供了新视角。

Abstract: Transformers generate valid and diverse chemical structures, but little is known about the mechanisms that enable these models to capture the rules of molecular representation. We present a mechanistic analysis of autoregressive transformers trained on drug-like small molecules to reveal the computational structure underlying their capabilities across multiple levels of abstraction. We identify computational patterns consistent with low-level syntactic parsing and more abstract chemical validity constraints. Using sparse autoencoders (SAEs), we extract feature dictionaries associated with chemically relevant activation patterns. We validate our findings on downstream tasks and find that mechanistic insights can translate to predictive performance in various practical settings.

</details>


### [145] [Physics-Aware Heterogeneous GNN Architecture for Real-Time BESS Optimization in Unbalanced Distribution Systems](https://arxiv.org/abs/2512.09780)
*Aoxiang Ma,Salah Ghamizi,Jun Cao,Pedro Rodriguez*

Main category: cs.LG

TL;DR: 提出一种基于异构图神经网络的三相不平衡配电网电池储能系统优化调度方法，通过嵌入详细三相信息和物理约束损失函数，实现高精度预测并确保调度方案可行性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法缺乏明确的三相表示，难以准确建模相间动态和满足运行约束，导致调度方案不可行。需要开发能同时保证预测精度和约束满足的方法。

Method: 将三相电网信息（相电压、不平衡负载、BESS状态）嵌入异构图节点，采用多种GNN架构（GCN、GAT、GraphSAGE、GPS）联合预测网络状态变量，并通过物理信息损失函数以软惩罚方式纳入电池约束（SoC和C-rate限制）。

Result: 在CIGRE 18节点配电网系统上的实验验证显示，该方法预测误差低，总线电压MSE分别为：GCN 6.92e-07、GAT 1.21e-06、GPS 3.29e-05、SAGE 9.04e-07。物理信息方法确保SoC和C-rate约束违反率几乎为零。

Conclusion: 通过嵌入三相电网信息和物理约束损失函数，该方法能够实现高精度网络状态预测，同时确保调度方案满足关键电池约束，为可靠、约束合规的BESS调度提供了有效解决方案。

Abstract: Battery energy storage systems (BESS) have become increasingly vital in three-phase unbalanced distribution grids for maintaining voltage stability and enabling optimal dispatch. However, existing deep learning approaches often lack explicit three-phase representation, making it difficult to accurately model phase-specific dynamics and enforce operational constraints--leading to infeasible dispatch solutions. This paper demonstrates that by embedding detailed three-phase grid information--including phase voltages, unbalanced loads, and BESS states--into heterogeneous graph nodes, diverse GNN architectures (GCN, GAT, GraphSAGE, GPS) can jointly predict network state variables with high accuracy. Moreover, a physics-informed loss function incorporates critical battery constraints--SoC and C-rate limits--via soft penalties during training. Experimental validation on the CIGRE 18-bus distribution system shows that this embedding-loss approach achieves low prediction errors, with bus voltage MSEs of 6.92e-07 (GCN), 1.21e-06 (GAT), 3.29e-05 (GPS), and 9.04e-07 (SAGE). Importantly, the physics-informed method ensures nearly zero SoC and C-rate constraint violations, confirming its effectiveness for reliable, constraint-compliant dispatch.

</details>


### [146] [Predicting Polymer Solubility in Solvents Using SMILES Strings](https://arxiv.org/abs/2512.09784)
*Andrew Reinhard*

Main category: cs.LG

TL;DR: 基于深度学习的SMILES表示法预测聚合物溶解度框架，在8049个聚合物-溶剂对上表现出高精度，可应用于绿色化学和材料设计。


<details>
  <summary>Details</summary>
Motivation: 聚合物在不同溶剂中的溶解性预测对于回收利用、药物制剂等应用至关重要，传统方法效率低且成本高，需要开发可扩展的预测模型。

Method: 构建包含8049个聚合物-溶剂对的数据集，使用SMILES表示法结合分子描述符和指纹生成2394维特征，采用六层全连接神经网络和Adam优化器进行训练。

Result: 模型在模拟数据上表现出预测值与实际值的高度一致性，在Materials Genome Project实验数据上对25个未见聚合物-溶剂组合保持高准确性。

Conclusion: 基于SMILES的机器学习模型可用于可扩展的溶解度预测和高通量溶剂筛选，支持绿色化学、聚合物加工和材料设计应用。

Abstract: Understanding and predicting polymer solubility in various solvents is critical for applications ranging from recycling to pharmaceutical formulation. This work presents a deep learning framework that predicts polymer solubility, expressed as weight percent (wt%), directly from SMILES representations of both polymers and solvents. A dataset of 8,049 polymer solvent pairs at 25 deg C was constructed from calibrated molecular dynamics simulations (Zhou et al., 2023), and molecular descriptors and fingerprints were combined into a 2,394 feature representation per sample. A fully connected neural network with six hidden layers was trained using the Adam optimizer and evaluated using mean squared error loss, achieving strong agreement between predicted and actual solubility values. Generalizability was demonstrated using experimentally measured data from the Materials Genome Project, where the model maintained high accuracy on 25 unseen polymer solvent combinations. These findings highlight the viability of SMILES based machine learning models for scalable solubility prediction and high-throughput solvent screening, supporting applications in green chemistry, polymer processing, and materials design.

</details>


### [147] [STACHE: Local Black-Box Explanations for Reinforcement Learning Policies](https://arxiv.org/abs/2512.09909)
*Andrew Elashkin,Orna Grumberg*

Main category: cs.LG

TL;DR: STACHE框架为离散马尔可夫游戏中的智能体行为生成局部黑盒解释，包含鲁棒性区域和最小反事实两个互补组件，通过精确搜索算法避免代理模型保真度差距。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体在稀疏奖励或安全关键环境中常表现出意外行为，需要可靠的调试和验证工具来理解智能体决策。

Method: 提出STACHE框架，生成包含鲁棒性区域（智能体决策不变的连通邻域状态）和最小反事实（改变决策所需的最小状态扰动）的复合解释。利用分解状态空间结构，引入精确的基于搜索的算法，避免代理模型的保真度差距。

Result: 在Gymnasium环境中的实证验证表明，该框架不仅能解释策略动作，还能有效捕捉训练过程中策略逻辑的演变——从不稳定行为到优化稳健策略，提供对智能体敏感性和决策边界的可操作见解。

Conclusion: STACHE为强化学习智能体行为提供了一种全面的解释框架，通过鲁棒性区域和最小反事实的互补组合，为调试、验证和理解智能体决策过程提供了有效工具。

Abstract: Reinforcement learning agents often behave unexpectedly in sparse-reward or safety-critical environments, creating a strong need for reliable debugging and verification tools. In this paper, we propose STACHE, a comprehensive framework for generating local, black-box explanations for an agent's specific action within discrete Markov games. Our method produces a Composite Explanation consisting of two complementary components: (1) a Robustness Region, the connected neighborhood of states where the agent's action remains invariant, and (2) Minimal Counterfactuals, the smallest state perturbations required to alter that decision. By exploiting the structure of factored state spaces, we introduce an exact, search-based algorithm that circumvents the fidelity gaps of surrogate models. Empirical validation on Gymnasium environments demonstrates that our framework not only explains policy actions, but also effectively captures the evolution of policy logic during training - from erratic, unstable behavior to optimized, robust strategies - providing actionable insights into agent sensitivity and decision boundaries.

</details>


### [148] [TinyDéjàVu: Smaller Memory Footprint & Faster Inference on Sensor Data Streams with Always-On Microcontrollers](https://arxiv.org/abs/2512.09786)
*Zhaolan Huang,Emmanuel Baccelli*

Main category: cs.LG

TL;DR: TinyDéjàVu框架通过优化数据流和消除冗余计算，显著减少MCU上传感器时间序列推理的RAM占用


<details>
  <summary>Details</summary>
Motivation: 随着常开传感器需要运行小型神经网络进行连续推理，而MCU内存预算有限（如128kB RAM），优化神经网络层间数据流变得至关重要

Method: 提出TinyDéjàVu框架和新型算法，通过优化数据流和消除滑动窗口重叠输入的冗余计算来减少RAM占用

Result: TinyDéjàVu可节省超过60%的RAM使用，并消除高达90%的滑动窗口重叠输入的冗余计算

Conclusion: TinyDéjàVu框架有效解决了MCU上传感器时间序列推理的内存瓶颈问题，已开源实现并在硬件上进行可复现基准测试

Abstract: Always-on sensors are increasingly expected to embark a variety of tiny neural networks and to continuously perform inference on time-series of the data they sense. In order to fit lifetime and energy consumption requirements when operating on battery, such hardware uses microcontrollers (MCUs) with tiny memory budget e.g., 128kB of RAM. In this context, optimizing data flows across neural network layers becomes crucial. In this paper, we introduce TinyDéjàVu, a new framework and novel algorithms we designed to drastically reduce the RAM footprint required by inference using various tiny ML models for sensor data time-series on typical microcontroller hardware. We publish the implementation of TinyDéjàVu as open source, and we perform reproducible benchmarks on hardware. We show that TinyDéjàVu can save more than 60% of RAM usage and eliminate up to 90% of redundant compute on overlapping sliding window inputs.

</details>


### [149] [Knowledge Diversion for Efficient Morphology Control and Policy Transfer](https://arxiv.org/abs/2512.09796)
*Fu Feng,Ruixiao Shi,Yucheng Xie,Jianlu Shen,Jing Wang,Xin Geng*

Main category: cs.LG

TL;DR: DivMorph提出模块化训练范式，通过知识分流学习可分解控制器，实现跨形态和跨任务的通用策略学习，显著提升样本效率和减小模型规模。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer控制器计算成本高，部署开销大，且跨任务泛化能力有限，需要为每个新任务从头训练。需要一种既能高效部署又能有效跨任务迁移的方法。

Method: 使用SVD将随机初始化的Transformer权重分解为因子单元，通过动态软门控基于任务和形态嵌入调制这些单元，分离为共享的learngenes和特定于形态/任务的tailors，实现知识解耦。

Result: 在跨任务迁移中比直接微调提升3倍样本效率，单智能体部署时模型大小减少17倍，达到最先进性能。

Conclusion: DivMorph通过模块化设计和知识解耦，实现了高效、可扩展的策略部署和有效的跨任务策略迁移，解决了通用形态控制中的计算成本和泛化问题。

Abstract: Universal morphology control aims to learn a universal policy that generalizes across heterogeneous agent morphologies, with Transformer-based controllers emerging as a popular choice. However, such architectures incur substantial computational costs, resulting in high deployment overhead, and existing methods exhibit limited cross-task generalization, necessitating training from scratch for each new task. To this end, we propose \textbf{DivMorph}, a modular training paradigm that leverages knowledge diversion to learn decomposable controllers. DivMorph factorizes randomly initialized Transformer weights into factor units via SVD prior to training and employs dynamic soft gating to modulate these units based on task and morphology embeddings, separating them into shared \textit{learngenes} and morphology- and task-specific \textit{tailors}, thereby achieving knowledge disentanglement. By selectively activating relevant components, DivMorph enables scalable and efficient policy deployment while supporting effective policy transfer to novel tasks. Extensive experiments demonstrate that DivMorph achieves state-of-the-art performance, achieving a 3$\times$ improvement in sample efficiency over direct finetuning for cross-task transfer and a 17$\times$ reduction in model size for single-agent deployment.

</details>


### [150] [Ariel-ML: Computing Parallelization with Embedded Rust for Neural Networks on Heterogeneous Multi-core Microcontrollers](https://arxiv.org/abs/2512.09800)
*Zhaolan Huang,Kaspar Schleiser,Gyungmin Myung,Emmanuel Baccelli*

Main category: cs.LG

TL;DR: Ariel-ML：首个支持多核MCU并行推理的Rust嵌入式TinyML平台，显著降低推理延迟，内存占用与C/C++方案相当


<details>
  <summary>Details</summary>
Motivation: 随着低功耗MCU从单核向多核架构演进，以及Rust在嵌入式领域逐渐取代C/C++，同时TinyML模型在边缘AI应用中的部署日益增多，但目前缺乏能够自动利用多核MCU进行并行推理的Rust嵌入式平台。现有方案无法满足在已部署的传感/执行系统上持续通过TinyML模型进行增量改进和创新服务的需求。

Method: 设计并实现了Ariel-ML工具包，结合通用TinyML流水线和嵌入式Rust软件平台，能够充分利用多种32位微控制器家族（Arm Cortex-M、RISC-V、ESP-32）的多核能力。该平台能够自动为任意TinyML模型进行推理计算的并行化。

Result: Ariel-ML在推理延迟方面优于现有技术，同时与使用嵌入式C/C++的现有工具包相比，实现了相当的内存占用。通过多种TinyML模型的基准测试验证了其性能优势。

Conclusion: Ariel-ML填补了Rust嵌入式软件平台在多核MCU上自动并行化TinyML模型推理的空白，为TinyML从业者和资源受限的嵌入式Rust开发者提供了有用的基础平台。所有实现代码均已开源发布。

Abstract: Low-power microcontroller (MCU) hardware is currently evolving from single-core architectures to predominantly multi-core architectures. In parallel, new embedded software building blocks are more and more written in Rust, while C/C++ dominance fades in this domain. On the other hand, small artificial neural networks (ANN) of various kinds are increasingly deployed in edge AI use cases, thus deployed and executed directly on low-power MCUs. In this context, both incremental improvements and novel innovative services will have to be continuously retrofitted using ANNs execution in software embedded on sensing/actuating systems already deployed in the field. However, there was so far no Rust embedded software platform automating parallelization for inference computation on multi-core MCUs executing arbitrary TinyML models. This paper thus fills this gap by introducing Ariel-ML, a novel toolkit we designed combining a generic TinyML pipeline and an embedded Rust software platform which can take full advantage of multi-core capabilities of various 32bit microcontroller families (Arm Cortex-M, RISC-V, ESP-32). We published the full open source code of its implementation, which we used to benchmark its capabilities using a zoo of various TinyML models. We show that Ariel-ML outperforms prior art in terms of inference latency as expected, and we show that, compared to pre-existing toolkits using embedded C/C++, Ariel-ML achieves comparable memory footprints. Ariel-ML thus provides a useful basis for TinyML practitioners and resource-constrained embedded Rust developers.

</details>


### [151] [FALCON: Few-step Accurate Likelihoods for Continuous Flows](https://arxiv.org/abs/2512.09914)
*Danyal Rehman,Tara Akhound-Sadegh,Artem Gazizov,Yoshua Bengio,Alexander Tong*

Main category: cs.LG

TL;DR: FALCON方法通过混合训练目标实现连续流的少步精确似然计算，显著提升分子玻尔兹曼采样的效率和速度


<details>
  <summary>Details</summary>
Motivation: 当前玻尔兹曼生成器主要使用连续归一化流，但计算似然需要数千次函数评估，成本极高，严重限制了其应用

Method: 提出FALCON方法，通过引入鼓励可逆性的混合训练目标，实现少步采样且保持足够精确的似然用于重要性采样

Result: FALCON在分子玻尔兹曼采样中优于最先进的归一化流模型，比同等性能的CNF模型快两个数量级

Conclusion: FALCON方法解决了连续归一化流似然计算成本高的问题，为分子玻尔兹曼采样提供了高效实用的解决方案

Abstract: Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann Generators tackle this problem by pairing a generative model, capable of exact likelihood computation, with importance sampling to obtain consistent samples under the target distribution. Current Boltzmann Generators primarily use continuous normalizing flows (CNFs) trained with flow matching for efficient training of powerful models. However, likelihood calculation for these models is extremely costly, requiring thousands of function evaluations per sample, severely limiting their adoption. In this work, we propose Few-step Accurate Likelihoods for Continuous Flows (FALCON), a method which allows for few-step sampling with a likelihood accurate enough for importance sampling applications by introducing a hybrid training objective that encourages invertibility. We show FALCON outperforms state-of-the-art normalizing flow models for molecular Boltzmann sampling and is two orders of magnitude faster than the equivalently performing CNF model.

</details>


### [152] [Incorporating Fairness in Neighborhood Graphs for Fair Spectral Clustering](https://arxiv.org/abs/2512.09810)
*Adithya K Moorthy,V Vijaya Saradhi,Bhanu Prasad*

Main category: cs.LG

TL;DR: 该研究提出了公平k近邻图和公平ε邻域图构建方法，通过在早期图构建阶段加入公平约束，实现更公平的谱聚类结果。


<details>
  <summary>Details</summary>
Motivation: 传统图聚类方法（如kNN和ε邻域图）在构建过程中存在偏见，会传播对敏感群体的不公平影响，导致聚类结果有偏差。当前研究缺乏在预处理阶段解决图拓扑公平性的方法。

Method: 提出了两种公平图构建方法：公平k近邻图和公平ε邻域图。在邻域选择步骤中主动实施人口统计公平约束，将敏感特征的比例表示纳入局部图结构，同时保持几何一致性。

Result: 在三个合成数据集、七个真实世界表格数据集和三个真实世界图像数据集上的实验证明，提出的公平图构建方法在图形聚类任务中超越了当前基线方法。

Conclusion: 图构建中的拓扑公平性对于实现公平聚类结果至关重要。通过在早期图构建阶段纳入公平约束，可以在不改变聚类算法本身的情况下实现更公平的谱聚类结果，填补了公平无监督学习中的重要空白。

Abstract: Graph clustering plays a pivotal role in unsupervised learning methods like spectral clustering, yet traditional methods for graph clustering often perpetuate bias through unfair graph constructions that may underrepresent some groups. The current research introduces novel approaches for constructing fair k-nearest neighbor (kNN) and fair epsilon-neighborhood graphs that proactively enforce demographic parity during graph formation. By incorporating fairness constraints at the earliest stage of neighborhood selection steps, our approaches incorporate proportional representation of sensitive features into the local graph structure while maintaining geometric consistency.Our work addresses a critical gap in pre-processing for fair spectral clustering, demonstrating that topological fairness in graph construction is essential for achieving equitable clustering outcomes. Widely used graph construction methods like kNN and epsilon-neighborhood graphs propagate edge based disparate impact on sensitive groups, leading to biased clustering results. Providing representation of each sensitive group in the neighborhood of every node leads to fairer spectral clustering results because the topological features of the graph naturally reflect equitable group ratios. This research fills an essential shortcoming in fair unsupervised learning, by illustrating how topological fairness in graph construction inherently facilitates fairer spectral clustering results without the need for changes to the clustering algorithm itself. Thorough experiments on three synthetic datasets, seven real-world tabular datasets, and three real-world image datasets prove that our fair graph construction methods surpass the current baselines in graph clustering tasks.

</details>


### [153] [Predicting the Containment Time of California Wildfires Using Machine Learning](https://arxiv.org/abs/2512.09835)
*Shashank Bhardwaj*

Main category: cs.LG

TL;DR: 使用机器学习模型预测加州野火完全扑灭所需天数，将持续时间预测作为回归任务而非分类任务，比较了随机森林、XGBoost和LSTM模型性能。


<details>
  <summary>Details</summary>
Motivation: 加州野火季节日益严重，应急响应团队不堪重负，需要准确实用的预测来协助资源分配。现有研究多关注火灾风险或蔓延，少数研究持续时间预测也仅使用宽泛分类而非连续测量。

Method: 结合加州林业和消防部门FRAP项目的三个公开数据集，将野火持续时间预测作为回归任务。比较了基线集成回归器、随机森林、XGBoost和LSTM神经网络模型。

Result: XGBoost模型略优于随机森林，可能因其能更好地处理数据集中的静态特征。LSTM模型表现较差，因为数据集缺乏时间特征。

Conclusion: 根据特征可用性，野火管理者可以选择最合适的模型来准确预测火灾扑灭持续时间并有效分配资源。回归方法比分类方法提供更详细精确的预测。

Abstract: California's wildfire season keeps getting worse over the years, overwhelming the emergency response teams. These fires cause massive destruction to both property and human life. Because of these reasons, there's a growing need for accurate and practical predictions that can help assist with resources allocation for the Wildfire managers or the response teams. In this research, we built machine learning models to predict the number of days it will require to fully contain a wildfire in California. Here, we addressed an important gap in the current literature. Most prior research has concentrated on wildfire risk or how fires spread, and the few that examine the duration typically predict it in broader categories rather than a continuous measure. This research treats the wildfire duration prediction as a regression task, which allows for more detailed and precise forecasts rather than just the broader categorical predictions used in prior work. We built the models by combining three publicly available datasets from California Department of Forestry and Fire Protection's Fire and Resource Assessment Program (FRAP). This study compared the performance of baseline ensemble regressor, Random Forest and XGBoost, with a Long Short-Term Memory (LSTM) neural network. The results show that the XGBoost model slightly outperforms the Random Forest model, likely due to its superior handling of static features in the dataset. The LSTM model, on the other hand, performed worse than the ensemble models because the dataset lacked temporal features. Overall, this study shows that, depending on the feature availability, Wildfire managers or Fire management authorities can select the most appropriate model to accurately predict wildfire containment duration and allocate resources effectively.

</details>


### [154] [Conformal Bandits: Bringing statistical validity and reward efficiency to the small-gap regime](https://arxiv.org/abs/2512.09850)
*Simone Cuonzo,Nina Deliu*

Main category: cs.LG

TL;DR: 将Conformal Prediction整合到bandit问题中，在保持遗憾最小化的同时提供有限时间统计覆盖保证，特别在小差距场景下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统bandit策略（如Thompson Sampling和UCB）主要关注遗憾最小化，但依赖分布假设或渐近保证，缺乏统计性质。需要将决策策略的遗憾最小化潜力与统计保证相结合。

Method: 提出Conformal Bandits框架，将Conformal Prediction整合到bandit问题中。在投资组合分配应用中，进一步集成隐马尔可夫模型来捕捉金融市场的状态切换行为。

Result: 在小差距场景下，该框架在遗憾方面优于传统UCB策略，同时实现名义覆盖保证。在投资组合应用中，通过隐马尔可夫模型增强了探索-利用权衡，提高了风险调整后的回报效率，同时保持覆盖保证。

Conclusion: Conformal Bandits成功将bandit策略的遗憾最小化潜力与Conformal Prediction的统计保证相结合，特别在小差距场景下具有实际优势，为顺序决策问题提供了新的解决方案。

Abstract: We introduce Conformal Bandits, a novel framework integrating Conformal Prediction (CP) into bandit problems, a classic paradigm for sequential decision-making under uncertainty. Traditional regret-minimisation bandit strategies like Thompson Sampling and Upper Confidence Bound (UCB) typically rely on distributional assumptions or asymptotic guarantees; further, they remain largely focused on regret, neglecting their statistical properties. We address this gap. Through the adoption of CP, we bridge the regret-minimising potential of a decision-making bandit policy with statistical guarantees in the form of finite-time prediction coverage.
  We demonstrate the potential of it Conformal Bandits through simulation studies and an application to portfolio allocation, a typical small-gap regime, where differences in arm rewards are far too small for classical policies to achieve optimal regret bounds in finite sample. Motivated by this, we showcase our framework's practical advantage in terms of regret in small-gap settings, as well as its added value in achieving nominal coverage guarantees where classical UCB policies fail. Focusing on our application of interest, we further illustrate how integrating hidden Markov models to capture the regime-switching behaviour of financial markets, enhances the exploration-exploitation trade-off, and translates into higher risk-adjusted regret efficiency returns, while preserving coverage guarantees.

</details>


### [155] [HPM-KD: Hierarchical Progressive Multi-Teacher Framework for Knowledge Distillation and Efficient Model Compression](https://arxiv.org/abs/2512.09886)
*Gustavo Coelho Haase,Paulo Henrique Dourado da Silva*

Main category: cs.LG

TL;DR: HPM-KD是一个自动化的知识蒸馏框架，通过六个协同组件解决传统KD的四大问题：超参数敏感、容量差距、多教师协调和计算效率，实现10-15倍压缩同时保持85%准确率，减少30-40%训练时间。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏存在四个关键限制：1) 对超参数敏感需要大量手动调优；2) 从大教师模型到小学生模型时存在容量差距；3) 多教师场景中协调不佳；4) 计算资源使用效率低下。需要自动化框架来解决这些问题。

Method: 提出HPM-KD框架，包含六个协同组件：1) 基于元学习的自适应配置管理器；2) 自动确定中间模型的渐进蒸馏链；3) 学习动态样本权重的注意力加权多教师集成；4) 元学习温度调度器；5) 智能负载均衡的并行处理流水线；6) 跨实验重用的共享优化内存。

Result: 在CIFAR-10、CIFAR-100和表格数据集上实验显示：实现10-15倍压缩同时保持85%准确率保留；消除手动调优需求；通过并行化减少30-40%训练时间。消融研究确认每个组件的独立贡献（0.10-0.98个百分点）。

Conclusion: HPM-KD成功解决了传统知识蒸馏的关键限制，提供了一个自动化、高效的框架，在保持性能的同时显著减少计算成本和人工干预。该框架已作为开源DeepBridge库的一部分提供。

Abstract: Knowledge Distillation (KD) has emerged as a promising technique for model compression but faces critical limitations: (1) sensitivity to hyperparameters requiring extensive manual tuning, (2) capacity gap when distilling from very large teachers to small students, (3) suboptimal coordination in multi-teacher scenarios, and (4) inefficient use of computational resources. We present \textbf{HPM-KD}, a framework that integrates six synergistic components: (i) Adaptive Configuration Manager via meta-learning that eliminates manual hyperparameter tuning, (ii) Progressive Distillation Chain with automatically determined intermediate models, (iii) Attention-Weighted Multi-Teacher Ensemble that learns dynamic per-sample weights, (iv) Meta-Learned Temperature Scheduler that adapts temperature throughout training, (v) Parallel Processing Pipeline with intelligent load balancing, and (vi) Shared Optimization Memory for cross-experiment reuse. Experiments on CIFAR-10, CIFAR-100, and tabular datasets demonstrate that HPM-KD: achieves 10x-15x compression while maintaining 85% accuracy retention, eliminates the need for manual tuning, and reduces training time by 30-40% via parallelization. Ablation studies confirm independent contribution of each component (0.10-0.98 pp). HPM-KD is available as part of the open-source DeepBridge library.

</details>


### [156] [Analysis of Dirichlet Energies as Over-smoothing Measures](https://arxiv.org/abs/2512.09890)
*Anna Bison,Alessandro Sperduti*

Main category: cs.LG

TL;DR: 分析归一化和非归一化图拉普拉斯算子诱导的Dirichlet能量之间的区别，指出归一化版本不满足节点相似性度量的公理化定义，强调选择与GNN架构谱兼容的度量的重要性


<details>
  <summary>Details</summary>
Motivation: 澄清两种常用于过平滑度量的函数（非归一化和归一化图拉普拉斯算子诱导的Dirichlet能量）之间的区别，解决在监控图神经网络动态时的模糊性

Method: 通过形式化这两种定义的谱特性，分析它们与Rusch等人提出的节点相似性度量公理化定义的兼容性

Result: 证明归一化图拉普拉斯算子诱导的Dirichlet能量不满足节点相似性度量的公理化定义，揭示了两种度量在谱特性上的关键区别

Conclusion: 选择与GNN架构谱兼容的度量对于准确监控图神经网络动态至关重要，需要根据具体架构选择适当的过平滑度量

Abstract: We analyze the distinctions between two functionals often used as over-smoothing measures: the Dirichlet energies induced by the unnormalized graph Laplacian and the normalized graph Laplacian. We demonstrate that the latter fails to satisfy the axiomatic definition of a node-similarity measure proposed by Rusch \textit{et al.} By formalizing fundamental spectral properties of these two definitions, we highlight critical distinctions necessary to select the metric that is spectrally compatible with the GNN architecture, thereby resolving ambiguities in monitoring the dynamics.

</details>


### [157] [Exploring Protein Language Model Architecture-Induced Biases for Antibody Comprehension](https://arxiv.org/abs/2512.09894)
*Mengren,Liu,Yixiang Zhang,Yiming,Zhang*

Main category: cs.LG

TL;DR: 系统评估不同蛋白质语言模型架构在抗体特异性预测任务中的表现，发现抗体专用模型能自然学习关注CDR区域，而通用蛋白质模型需要显式CDR训练策略


<details>
  <summary>Details</summary>
Motivation: 尽管蛋白质语言模型在理解蛋白质序列方面取得了显著进展，但不同模型架构在捕捉抗体特异性生物学特性方面的能力尚未得到系统研究。需要探索模型架构选择如何影响对抗体序列特征和功能的理解能力。

Method: 系统评估三种最先进的蛋白质语言模型（AntiBERTa、BioBERT和ESM2），以通用语言模型GPT-2为基线，在抗体靶标特异性预测任务上进行测试。通过注意力归因分析，研究不同模型对生物学特征的捕捉能力。

Result: 所有蛋白质语言模型都实现了高分类准确率，但在捕捉V基因使用、体细胞超突变模式和同种型信息等生物学特征方面表现出不同的偏差。抗体专用模型如AntiBERTa能自然学习关注互补决定区（CDR），而通用蛋白质模型通过显式CDR聚焦训练策略能显著受益。

Conclusion: 研究揭示了模型架构与生物学特征提取之间的关系，为未来计算抗体设计中的蛋白质语言模型开发提供了有价值的指导。抗体特异性模型架构能更好地捕捉抗体相关生物学特征。

Abstract: Recent advances in protein language models (PLMs) have demonstrated remarkable capabilities in understanding protein sequences. However, the extent to which different model architectures capture antibody-specific biological properties remains unexplored. In this work, we systematically investigate how architectural choices in PLMs influence their ability to comprehend antibody sequence characteristics and functions. We evaluate three state-of-the-art PLMs-AntiBERTa, BioBERT, and ESM2--against a general-purpose language model (GPT-2) baseline on antibody target specificity prediction tasks. Our results demonstrate that while all PLMs achieve high classification accuracy, they exhibit distinct biases in capturing biological features such as V gene usage, somatic hypermutation patterns, and isotype information. Through attention attribution analysis, we show that antibody-specific models like AntiBERTa naturally learn to focus on complementarity-determining regions (CDRs), while general protein models benefit significantly from explicit CDR-focused training strategies. These findings provide insights into the relationship between model architecture and biological feature extraction, offering valuable guidance for future PLM development in computational antibody design.

</details>


### [158] [Closing the Train-Test Gap in World Models for Gradient-Based Planning](https://arxiv.org/abs/2512.09929)
*Arjun Parthasarathy,Nimit Kalra,Rohun Agrawal,Yann LeCun,Oumayma Bounou,Pavel Izmailov,Micah Goldblum*

Main category: cs.LG

TL;DR: 提出改进世界模型训练方法，通过缩小训练与测试的差距，使梯度规划在10%时间预算下达到或超越传统CEM方法性能


<details>
  <summary>Details</summary>
Motivation: 现有世界模型基于下一状态预测目标训练，但测试时用于估计动作序列，存在训练-测试差距。梯度规划虽计算高效，但性能落后于其他方法，需要改进

Method: 提出训练时数据合成技术，改进世界模型训练以支持高效梯度规划。通过缩小训练与测试差距，使模型更适合梯度规划需求

Result: 在物体操作和导航任务中，该方法在10%时间预算下达到或超越传统交叉熵方法(CEM)性能

Conclusion: 通过改进世界模型训练方法，成功缩小训练-测试差距，使梯度规划在保持计算效率的同时达到竞争性性能

Abstract: World models paired with model predictive control (MPC) can be trained offline on large-scale datasets of expert trajectories and enable generalization to a wide range of planning tasks at inference time. Compared to traditional MPC procedures, which rely on slow search algorithms or on iteratively solving optimization problems exactly, gradient-based planning offers a computationally efficient alternative. However, the performance of gradient-based planning has thus far lagged behind that of other approaches. In this paper, we propose improved methods for training world models that enable efficient gradient-based planning. We begin with the observation that although a world model is trained on a next-state prediction objective, it is used at test-time to instead estimate a sequence of actions. The goal of our work is to close this train-test gap. To that end, we propose train-time data synthesis techniques that enable significantly improved gradient-based planning with existing world models. At test time, our approach outperforms or matches the classical gradient-free cross-entropy method (CEM) across a variety of object manipulation and navigation tasks in 10% of the time budget.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [159] [Exploratory Mean-Variance with Jumps: An Equilibrium Approach](https://arxiv.org/abs/2512.09224)
*Yuling Max Chen,Bin Li,David Saunders*

Main category: q-fin.PM

TL;DR: 使用强化学习解决连续时间均值-方差投资组合优化问题，通过时间不一致控制方法获得高斯分布形式的均衡投资策略，并在真实市场数据中验证有效性


<details>
  <summary>Details</summary>
Motivation: 重新审视连续时间均值-方差投资组合优化问题，传统方法存在时间不一致性，需要探索控制空间并解决时间不一致偏好问题

Method: 采用跳跃扩散过程建模市场动态，应用时间不一致控制方法解析求解均衡投资策略（高斯分布），设计Actor-Critic强化学习算法

Result: 模拟研究中所有RL模型参数收敛到真实值，在24年真实市场数据测试中，14次测试中有13次盈利，证明实际应用价值

Conclusion: 提出的强化学习方法能有效解决时间不一致的均值-方差投资组合优化问题，均衡策略在真实市场中表现良好，具有实际投资应用价值

Abstract: Revisiting the continuous-time Mean-Variance (MV) Portfolio Optimization problem, we model the market dynamics with a jump-diffusion process and apply Reinforcement Learning (RL) techniques to facilitate informed exploration within the control space. We recognize the time-inconsistency of the MV problem and adopt the time-inconsistent control (TIC) approach to analytically solve for an exploratory equilibrium investment policy, which is a Gaussian distribution centered on the equilibrium control of the classical MV problem. Our approach accounts for time-inconsistent preferences and actions, and our equilibrium policy is the best option an investor can take at any given time during the investment period. Moreover, we leverage the martingale properties of the equilibrium policy, design a RL model, and propose an Actor-Critic RL algorithm. All of our RL model parameters converge to the corresponding true values in a simulation study. Our numerical study on 24 years of real market data shows that the proposed RL model is profitable in 13 out of 14 tests, demonstrating its practical applicability in real world investment.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [160] [Online Inference of Constrained Optimization: Primal-Dual Optimality and Sequential Quadratic Programming](https://arxiv.org/abs/2512.08948)
*Yihang Gao,Michael K. Ng,Michael W. Mahoney,Sen Na*

Main category: stat.ML

TL;DR: 提出一种随机序列二次规划方法(SSQP)用于解决带约束的随机优化问题，该方法结合动量梯度平均技术消除偏差，实现全局收敛和局部渐近正态性，并提供协方差矩阵估计器进行在线统计推断。


<details>
  <summary>Details</summary>
Motivation: 在统计学和机器学习中，带等式和不等式约束的随机优化问题普遍存在，包括约束M估计、物理信息模型、安全强化学习和算法公平性等。现有方法在处理非线性约束时，通常依赖投影算子到约束集，这在非线性问题中通常是难以处理的。因此需要开发一种完全在线的方法，能够在不需要投影算子的情况下实现原始-对偶渐近极小极大最优性。

Method: 提出随机序列二次规划(SSQP)方法：通过顺序执行目标函数的二次近似和约束的线性近似来计算步长方向。尽管可以获得总体梯度的无偏估计，但约束随机问题中的关键挑战在于处理步长方向的偏差。为此，在SSQP中应用动量式梯度移动平均技术来消除步长偏差。

Result: 方法实现了全局几乎必然收敛，并表现出局部渐近正态性，具有Hájek和Le Cam意义上的最优原始-对偶极限协方差矩阵。此外，提供了实用的插件协方差矩阵估计器进行推断。该方法是在不需要投影算子的情况下，第一个实现原始-对偶渐近极小极大最优性的完全在线方法。

Conclusion: SSQP方法不仅能够高效解决约束随机问题，还能为实际应用提供有效且实用的在线统计推断。通过基准非线性问题、约束广义线性模型和投资组合分配问题的广泛实验验证了方法的优越性能。

Abstract: We study online statistical inference for the solutions of stochastic optimization problems with equality and inequality constraints. Such problems are prevalent in statistics and machine learning, encompassing constrained $M$-estimation, physics-informed models, safe reinforcement learning, and algorithmic fairness. We develop a stochastic sequential quadratic programming (SSQP) method to solve these problems, where the step direction is computed by sequentially performing a quadratic approximation of the objective and a linear approximation of the constraints. Despite having access to unbiased estimates of population gradients, a key challenge in constrained stochastic problems lies in dealing with the bias in the step direction. As such, we apply a momentum-style gradient moving-average technique within SSQP to debias the step. We show that our method achieves global almost-sure convergence and exhibits local asymptotic normality with an optimal primal-dual limiting covariance matrix in the sense of Hájek and Le Cam. In addition, we provide a plug-in covariance matrix estimator for practical inference. To our knowledge, the proposed SSQP method is the first fully online method that attains primal-dual asymptotic minimax optimality without relying on projection operators onto the constraint set, which are generally intractable for nonlinear problems. Through extensive experiments on benchmark nonlinear problems, as well as on constrained generalized linear models and portfolio allocation problems using both synthetic and real data, we demonstrate superior performance of our method, showing that the method and its asymptotic behavior not only solve constrained stochastic problems efficiently but also provide valid and practical online inference in real-world applications.

</details>


### [161] [WTNN: Weibull-Tailored Neural Networks for survival analysis](https://arxiv.org/abs/2512.09163)
*Gabrielle Rives,Olivier Lopez,Nicolas Bousquet*

Main category: stat.ML

TL;DR: 提出WTNN神经网络框架，专门用于Weibull生存分析，能够处理代理指标和删失数据，结合先验知识改进传统回归模型。


<details>
  <summary>Details</summary>
Motivation: 分析军事车辆在多变严苛环境下的运行数据时，现有方法在处理代理指标和删失观测方面存在局限，需要更灵活的模型来捕捉协变量与运行寿命之间的复杂关系。

Method: 提出WTNN神经网络框架，专门针对Weibull分布设计，能够结合关于最重要协变量的定性先验知识，保持与Weibull分布形状和结构的一致性。

Result: 通过数值实验证明，该方法能够在代理和右删失数据上可靠训练，能够产生稳健且可解释的生存预测，优于现有方法。

Conclusion: WTNN框架扩展了传统回归模型的能力，为Weibull生存研究提供了专门设计的神经网络建模方法，能够处理复杂协变量关系并提高预测性能。

Abstract: The Weibull distribution is a commonly adopted choice for modeling the survival of systems subject to maintenance over time. When only proxy indicators and censored observations are available, it becomes necessary to express the distribution's parameters as functions of time-dependent covariates. Deep neural networks provide the flexibility needed to learn complex relationships between these covariates and operational lifetime, thereby extending the capabilities of traditional regression-based models. Motivated by the analysis of a fleet of military vehicles operating in highly variable and demanding environments, as well as by the limitations observed in existing methodologies, this paper introduces WTNN, a new neural network-based modeling framework specifically designed for Weibull survival studies. The proposed architecture is specifically designed to incorporate qualitative prior knowledge regarding the most influential covariates, in a manner consistent with the shape and structure of the Weibull distribution. Through numerical experiments, we show that this approach can be reliably trained on proxy and right-censored data, and is capable of producing robust and interpretable survival predictions that can improve existing approaches.

</details>


### [162] [Robust and Sparse Estimation of Unbounded Density Ratio under Heavy Contamination](https://arxiv.org/abs/2512.09266)
*Ryosuke Nagumo,Hironori Fujisawa*

Main category: stat.ML

TL;DR: 论文研究了在污染设置下稳健密度比估计的非渐近性质，证明了加权DRE方法即使在严重污染下也能实现稀疏一致性，为密度比估计和稳健估计提供了非渐近分析框架。


<details>
  <summary>Details</summary>
Motivation: 现有方法中加权DRE从渐近角度展现出双重强稳健性，但缺乏非渐近分析。本文旨在研究在污染设置下稳健密度比估计的非渐近性质，解决密度比估计和稳健估计中的两个重要挑战。

Method: 采用加权密度比估计方法，在非渐近框架下分析其性质。假设加权密度比函数有界，研究无界密度比估计的非渐近性质。对于稳健估计，引入双重强稳健性的非渐近框架，假设至少满足以下条件之一：(i)污染率小，(ii)异常值的加权值小。

Result: 加权DRE在严重污染下仍能实现稀疏一致性，为无界密度比估计提供了非渐近性质分析，并首次在严重污染下给出了强稳健性的非渐近分析。

Conclusion: 该研究首次在严重污染下提供了强稳健性的非渐近分析，证明了加权DRE在非渐近框架下的双重强稳健性，为密度比估计和稳健估计提供了重要的理论保证。

Abstract: We examine the non-asymptotic properties of robust density ratio estimation (DRE) in contaminated settings. Weighted DRE is the most promising among existing methods, exhibiting doubly strong robustness from an asymptotic perspective. This study demonstrates that Weighted DRE achieves sparse consistency even under heavy contamination within a non-asymptotic framework. This method addresses two significant challenges in density ratio estimation and robust estimation. For density ratio estimation, we provide the non-asymptotic properties of estimating unbounded density ratios under the assumption that the weighted density ratio function is bounded. For robust estimation, we introduce a non-asymptotic framework for doubly strong robustness under heavy contamination, assuming that at least one of the following conditions holds: (i) contamination ratios are small, and (ii) outliers have small weighted values. This work provides the first non-asymptotic analysis of strong robustness under heavy contamination.

</details>


### [163] [Impact of Positional Encoding: Clean and Adversarial Rademacher Complexity for Transformers under In-Context Regression](https://arxiv.org/abs/2512.09275)
*Weiyi He,Yue Xing*

Main category: stat.ML

TL;DR: 该论文首次分析了可训练位置编码对Transformer泛化能力的影响，发现位置编码会系统性增大泛化差距，并在对抗攻击下放大模型脆弱性。


<details>
  <summary>Details</summary>
Motivation: 位置编码是Transformer的核心架构组件，但其对Transformer泛化能力和鲁棒性的影响尚不清楚。本文旨在填补这一空白，分析位置编码在上下文学习中的泛化特性。

Method: 为单层Transformer在上下文回归任务中提供首个泛化分析，明确考虑完全可训练的位置编码模块。推导对抗性Rademacher泛化界，并通过模拟研究进行实证验证。

Result: 位置编码会系统性增大泛化差距。在对抗攻击下，有位置编码和无位置编码模型之间的差距被放大，表明位置编码放大了模型的脆弱性。理论边界通过模拟研究得到验证。

Conclusion: 本文建立了一个理解带位置编码的上下文学习中干净和对抗性泛化的新框架，揭示了位置编码对模型泛化和鲁棒性的负面影响。

Abstract: Positional encoding (PE) is a core architectural component of Transformers, yet its impact on the Transformer's generalization and robustness remains unclear. In this work, we provide the first generalization analysis for a single-layer Transformer under in-context regression that explicitly accounts for a completely trainable PE module. Our result shows that PE systematically enlarges the generalization gap. Extending to the adversarial setting, we derive the adversarial Rademacher generalization bound. We find that the gap between models with and without PE is magnified under attack, demonstrating that PE amplifies the vulnerability of models. Our bounds are empirically validated by a simulation study. Together, this work establishes a new framework for understanding the clean and adversarial generalization in ICL with PE.

</details>


### [164] [Estimation of Stochastic Optimal Transport Maps](https://arxiv.org/abs/2512.09499)
*Sloan Nietert,Ziv Goldfeld*

Main category: stat.ML

TL;DR: 提出了一个评估随机最优传输映射的新度量，开发了具有近最优有限样本风险界的计算高效映射估计器，适用于现实世界中确定性映射不存在的场景。


<details>
  <summary>Details</summary>
Motivation: 现有最优传输映射估计理论过于受限，依赖于Brenier定理（二次成本、绝对连续源）来保证确定性映射的存在性和唯一性。但在许多现实问题中，这些条件不成立或无法验证，此时只能通过可以分割质量的随机映射进行最优传输。

Method: 引入了一个评估随机映射传输质量的新度量。在该度量下，开发了计算高效的映射估计器，具有近最优的有限样本风险界，且仅需易于验证的最小假设。分析还考虑了常见的对抗性样本污染形式。

Result: 获得了具有鲁棒估计保证的估计器。实证实验验证了理论，并展示了所提框架在现有理论失效的场景中的实用性。

Conclusion: 这是首个通用目的的映射估计理论，兼容广泛的实际应用场景，其中最优传输可能是内在随机的。

Abstract: The optimal transport (OT) map is a geometry-driven transformation between high-dimensional probability distributions which underpins a wide range of tasks in statistics, applied probability, and machine learning. However, existing statistical theory for OT map estimation is quite restricted, hinging on Brenier's theorem (quadratic cost, absolutely continuous source) to guarantee existence and uniqueness of a deterministic OT map, on which various additional regularity assumptions are imposed to obtain quantitative error bounds. In many real-world problems these conditions fail or cannot be certified, in which case optimal transportation is possible only via stochastic maps that can split mass. To broaden the scope of map estimation theory to such settings, this work introduces a novel metric for evaluating the transportation quality of stochastic maps. Under this metric, we develop computationally efficient map estimators with near-optimal finite-sample risk bounds, subject to easy-to-verify minimal assumptions. Our analysis further accommodates common forms of adversarial sample contamination, yielding estimators with robust estimation guarantees. Empirical experiments are provided which validate our theory and demonstrate the utility of the proposed framework in settings where existing theory fails. These contributions constitute the first general-purpose theory for map estimation, compatible with a wide spectrum of real-world applications where optimal transport may be intrinsically stochastic.

</details>


### [165] [Transformers for Tabular Data: A Training Perspective of Self-Attention via Optimal Transport](https://arxiv.org/abs/2512.09530)
*Antonio Candelieri,Alessandro Quadrio*

Main category: stat.ML

TL;DR: 该研究通过最优传输理论分析自注意力训练，发现其最终映射近似最优耦合但训练轨迹低效，提出基于OT的替代算法用于表格分类，性能接近Transformer但计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 研究动机是从最优传输理论的角度分析自注意力机制的训练过程，探索其效率问题，并为表格分类任务开发更高效、可扩展的替代方法。

Method: 1) 使用离散OT指标（Wasserstein距离、Monge间隙、最优性、效率）跟踪自注意力层中间投影的演化；2) 提出OT-based算法：生成类别特定的虚拟高斯分布，计算与数据的OT对齐，训练MLP泛化该映射；3) 在二分类、三分类任务和生物医学数据集上进行实验。

Result: 1) 自注意力最终映射常近似OT最优耦合，但训练轨迹低效；2) MLP部分在合成数据上预训练能部分改善收敛但对初始化敏感；3) OT-based算法在准确率上与Transformer相当，同时降低计算成本，在标准化输入下扩展性更好，但性能依赖于虚拟几何设计的精心调整。

Conclusion: 最优传输理论为理解自注意力训练提供了有价值的视角，提出的OT-based替代算法在表格分类任务中实现了与Transformer相当的性能但计算更高效，为高效表格数据处理提供了新方向，但需要进一步优化虚拟分布设计。

Abstract: This thesis examines self-attention training through the lens of Optimal Transport (OT) and develops an OT-based alternative for tabular classification. The study tracks intermediate projections of the self-attention layer during training and evaluates their evolution using discrete OT metrics, including Wasserstein distance, Monge gap, optimality, and efficiency. Experiments are conducted on classification tasks with two and three classes, as well as on a biomedical dataset.
  Results indicate that the final self-attention mapping often approximates the OT optimal coupling, yet the training trajectory remains inefficient. Pretraining the MLP section on synthetic data partially improves convergence but is sensitive to their initialization. To address these limitations, an OT-based algorithm is introduced: it generates class-specific dummy Gaussian distributions, computes an OT alignment with the data, and trains an MLP to generalize this mapping. The method achieves accuracy comparable to Transformers while reducing computational cost and scaling more efficiently under standardized inputs, though its performance depends on careful dummy-geometry design. All experiments and implementations are conducted in R.

</details>


### [166] [Don't Throw Away Your Beams: Improving Consistency-based Uncertainties in LLMs via Beam Search](https://arxiv.org/abs/2512.09538)
*Ekaterina Fadeeva,Maiya Goloburda,Aleksandr Rubashevskii,Roman Vashurin,Artem Shelmanov,Preslav Nakov,Mrinmaya Sachan,Maxim Panov*

Main category: stat.ML

TL;DR: 提出使用beam search替代multinomial sampling进行一致性不确定性量化，在短问答任务中减少重复样本和方差，提升性能


<details>
  <summary>Details</summary>
Motivation: 现有基于一致性的不确定性量化方法依赖multinomial sampling生成多个回答，但在短问答中容易产生重复样本，且随机性导致不确定性估计方差大

Method: 引入beam search生成候选回答进行一致性不确定性量化，提供理论下界证明beam search在特定概率质量下比multinomial sampling误差更小

Result: 在六个QA数据集上实证评估，相比multinomial sampling有稳定改进，达到最先进的不确定性量化性能

Conclusion: beam search作为一致性不确定性量化的候选生成方法，能有效减少重复和方差，提升不确定性估计的准确性和稳定性

Abstract: Consistency-based methods have emerged as an effective approach to uncertainty quantification (UQ) in large language models. These methods typically rely on several generations obtained via multinomial sampling, measuring their agreement level. However, in short-form QA, multinomial sampling is prone to producing duplicates due to peaked distributions, and its stochasticity introduces considerable variance in uncertainty estimates across runs. We introduce a new family of methods that employ beam search to generate candidates for consistency-based UQ, yielding improved performance and reduced variance compared to multinomial sampling. We also provide a theoretical lower bound on the beam set probability mass under which beam search achieves a smaller error than multinomial sampling. We empirically evaluate our approach on six QA datasets and find that its consistent improvements over multinomial sampling lead to state-of-the-art UQ performance.

</details>


### [167] [Supervised learning pays attention](https://arxiv.org/abs/2512.09912)
*Erin Craig,Robert Tibshirani*

Main category: stat.ML

TL;DR: 提出一种基于注意力加权的监督学习方法，为每个测试点拟合个性化模型，保持可解释性，适用于表格数据、时间序列和空间数据。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法（如lasso回归、梯度提升）通常拟合全局模型，无法适应数据的异质性。需要一种方法能够为每个预测点拟合个性化模型，同时保持模型的简洁性和可解释性。

Method: 使用注意力加权机制，为每个测试观测值拟合局部模型。注意力是一种监督相似性度量，强调对结果预测重要的特征和交互作用。该方法无需预先指定聚类或相似性，可自适应异质数据。

Result: 在真实和模拟数据集上，注意力加权提高了预测性能，同时保持了可解释性。理论分析表明，在已知子组结构的混合模型数据生成过程中，注意力加权线性模型比标准线性模型具有更低的均方误差。

Conclusion: 注意力加权方法能够灵活地为每个预测点拟合个性化模型，保持可解释性，适用于表格数据、时间序列和空间数据，并能通过注意力加权残差校正适应预训练树模型的分布偏移。

Abstract: In-context learning with attention enables large neural networks to make context-specific predictions by selectively focusing on relevant examples. Here, we adapt this idea to supervised learning procedures such as lasso regression and gradient boosting, for tabular data. Our goals are to (1) flexibly fit personalized models for each prediction point and (2) retain model simplicity and interpretability.
  Our method fits a local model for each test observation by weighting the training data according to attention, a supervised similarity measure that emphasizes features and interactions that are predictive of the outcome. Attention weighting allows the method to adapt to heterogeneous data in a data-driven way, without requiring cluster or similarity pre-specification. Further, our approach is uniquely interpretable: for each test observation, we identify which features are most predictive and which training observations are most relevant. We then show how to use attention weighting for time series and spatial data, and we present a method for adapting pretrained tree-based models to distributional shift using attention-weighted residual corrections. Across real and simulated datasets, attention weighting improves predictive performance while preserving interpretability, and theory shows that attention-weighting linear models attain lower mean squared error than the standard linear model under mixture-of-models data-generating processes with known subgroup structure.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [168] [Large Language Models as Search Engines: Societal Challenges](https://arxiv.org/abs/2512.08946)
*Zacchary Sadeddine,Winston Maxwell,Gaël Varoquaux,Fabian M. Suchanek*

Main category: cs.CY

TL;DR: 论文探讨了LLM可能取代搜索引擎成为主要信息门户所带来的15种社会挑战，分析了LLM提供商、内容创作者和终端用户三个角色的应对策略，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型可能取代搜索引擎成为网络信息的主要门户，这种转变将带来一系列社会挑战。论文旨在系统性地识别和分析这些挑战，为相关方提供应对策略和未来研究方向。

Method: 论文聚焦于LLM提供商、内容创作者和终端用户三个主要角色，识别出15种不同类型的社会挑战。针对每种挑战，从技术和法律两个角度分析当前的缓解策略，并评估其影响。

Result: 识别了15种社会挑战类型，包括信息质量、公平性、透明度、责任归属等方面。分析了当前的技术和法律缓解策略，指出了现有措施的局限性，并明确了未来需要进一步研究的关键问题。

Conclusion: LLM取代搜索引擎作为主要信息门户将带来复杂的社会挑战，需要多方协作应对。未来研究应关注更有效的技术解决方案、完善的法律框架，以及平衡各方利益的治理机制。

Abstract: Large Language Models (LLMs) may one day replace search engines as the primary portal to information on the Web. In this article, we investigate the societal challenges that such a change could bring. We focus on the roles of LLM Providers, Content Creators, and End Users, and identify 15 types of challenges. With each, we show current mitigation strategies -- both from the technical perspective and the legal perspective. We also discuss the impact of each challenge and point out future research opportunities.

</details>


### [169] [Institutional AI Sovereignty Through Gateway Architecture: Implementation Report from Fontys ICT](https://arxiv.org/abs/2512.08978)
*Ruud Huijts,Koen Suilen*

Main category: cs.CY

TL;DR: 大学通过构建机构AI平台解决商业AI工具碎片化采用问题，提供公平访问、透明风险、成本控制和欧盟法律合规的AI服务


<details>
  <summary>Details</summary>
Motivation: 商业AI订阅导致访问不平等和合规风险（不透明处理和非欧盟托管），但完全禁止既不现实也无用。机构需要以主权、负责任的方式提供强大AI

Method: 构建三层治理网关平台：1) ChatGPT风格前端链接机构身份，明确模型选择；2) 网关核心执行政策、控制访问和预算，默认路由到欧盟基础设施；3) 提供者层将商业和开源模型包装在机构模型卡中，整合供应商文档为统一治理接口

Result: 六个月300用户试点运行可靠，无隐私事件，采用率高，实现欧盟默认路由、管理支出和透明模型选择。网关模式独特结合模型多样性和快速创新与机构控制

Conclusion: AI不是支持功能而是战略，需要专门领导。可持续运营需要超越传统边界的治理。建议设立正式的AI官员角色，结合技术素养、治理权威和教育责任，使高等教育机构能够运营自己的多提供商AI平台

Abstract: To counter fragmented, high-risk adoption of commercial AI tools, we built and ran an institutional AI platform in a six-month, 300-user pilot, showing that a university of applied sciences can offer advanced AI with fair access, transparent risks, controlled costs, and alignment with European law.
  Commercial AI subscriptions create unequal access and compliance risks through opaque processing and non-EU hosting, yet banning them is neither realistic nor useful. Institutions need a way to provide powerful AI in a sovereign, accountable form.
  Our solution is a governed gateway platform with three layers: a ChatGPT-style frontend linked to institutional identity that makes model choice explicit; a gateway core enforcing policy, controlling access and budgets, and routing traffic to EU infrastructure by default; and a provider layer wrapping commercial and open-source models in institutional model cards that consolidate vendor documentation into one governance interface.
  The pilot ran reliably with no privacy incidents and strong adoption, enabling EU-default routing, managed spending, and transparent model choices. Only the gateway pattern combines model diversity and rapid innovation with institutional control.
  The central insight: AI is not a support function but strategy, demanding dedicated leadership. Sustainable operation requires governance beyond traditional boundaries. We recommend establishing a formal AI Officer role combining technical literacy, governance authority, and educational responsibility. Without it, AI decisions stay ad-hoc and institutional exposure grows. With it, higher-education institutions can realistically operate their own multi-provider AI platform, provided they govern AI as seriously as they teach it.

</details>


### [170] [FLARE v2: A Recursive Framework for Program Comprehension Across Languages and Levels of Abstraction](https://arxiv.org/abs/2512.09261)
*Justin Heath*

Main category: cs.CY

TL;DR: FLARE v2是一个递归的、符号学视角的程序意义构建框架，将FLARE v1的描述层级重新解释为单一生成操作，包含元素识别、绑定分析和新元素识别三个步骤，通过因果-时间和交流两个维度分析绑定关系。


<details>
  <summary>Details</summary>
Motivation: 解决FLARE v1中报告的概念和认知负荷限制，提供一个更统一的理论框架来解释程序意义的构建过程，为编程教育和课程设计提供理论基础。

Method: 提出递归的符号学框架，将程序元素识别为具有接收、发送、效果、共享四种属性的实体，通过因果-时间维度（顺序、分支、事件）和交流维度分析绑定关系，使用组合阶梯可视化编程结构与读写能力发展的对应关系。

Result: 建立了FLARE v2理论框架，能够解释从基本块和语句到片段、系统、服务的递归组合过程，为程序理解提供了统一的符号学视角，但尚未进行实证评估。

Conclusion: FLARE v2作为一个概念性框架，具有教学和课程设计方面的潜在应用价值，其实施和实证评估留待未来工作完成。

Abstract: Building on the classroom framework reported in Heath et al. (2025), this paper proposes FLARE v2 as a recursive, semiotically informed account of how program meaning is constructed. It reinterprets the descriptive tiers of FLARE v1 as instances of a single generative operation: identify elements (characterised by the four properties Receives, Sends, Effects, Shares); analyse their bindings along two dimensions (Causal-Temporal and Communicative); and recognise the new element that emerges. The Causal-Temporal dimension encompasses three subtypes - Sequential, Branch, and Event - that together account for control flow in both procedural and event-driven environments. A Compositional Ladder provides a visual parallel between literacy progressions and programming structures, illustrating how recursive composition operates from blocks and statements through segments, systems, and services. The framework aims to address conceptual and cognitive-load limitations reported in FLARE v1 and is situated within semiotic and program-comprehension theory. FLARE v2 is presented as a conceptual lens with potential implications for pedagogy and curriculum design; implementation and empirical evaluation are left for future work.

</details>


### [171] [The Gender Code: Gendering the Global Governance of Artificial Intelligence](https://arxiv.org/abs/2512.09570)
*Jelena Cupac*

Main category: cs.CY

TL;DR: 该论文分析国际AI治理框架如何应对性别问题和性别相关危害，发现现有框架存在趋势但仍有重大差距，主张有效的AI治理需具备交叉性、可执行性和包容性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨国际AI治理框架如何处理性别议题，识别现有框架在性别敏感性方面的进展与不足，推动建立更公正、包容的AI治理体系。

Method: 通过分析三类治理工具：具有约束力的法规（如欧盟AI法案）、软法工具（如UNESCO AI伦理建议）和全球倡议（如GPAI），识别趋势和差距。

Result: 研究发现三个新兴趋势：将性别关切纳入更广泛的人权框架、转向明确的性别相关条款、日益强调包容性和多样性。但也存在关键差距：治理文件中对性别的处理不一致、对交叉性关注有限、缺乏强有力的执行机制。

Conclusion: 有效AI治理必须是交叉性的、可执行的和包容性的，这是超越象征性措施、实现真正公平、防止强化现有不平等的关键。性别敏感治理对构建公正技术未来至关重要。

Abstract: This paper examines how international AI governance frameworks address gender issues and gender-based harms. The analysis covers binding regulations, such as the EU AI Act; soft law instruments, like the UNESCO Recommendations on AI Ethics; and global initiatives, such as the Global Partnership on AI (GPAI). These instruments reveal emerging trends, including the integration of gender concerns into broader human rights frameworks, a shift toward explicit gender-related provisions, and a growing emphasis on inclusivity and diversity. Yet, some critical gaps persist, including inconsistent treatment of gender across governance documents, limited engagement with intersectionality, and a lack of robust enforcement mechanisms. However, this paper argues that effective AI governance must be intersectional, enforceable, and inclusive. This is key to moving beyond tokenism toward meaningful equity and preventing reinforcement of existing inequalities. The study contributes to ethical AI debates by highlighting the importance of gender-sensitive governance in building a just technological future.

</details>


### [172] [Ethics Readiness of Artificial Intelligence: A Practical Evaluation Method](https://arxiv.org/abs/2512.09729)
*Laurynas Adomaitis,Vincent Israel-Jost,Alexei Grinbaum*

Main category: cs.CY

TL;DR: 提出了伦理准备度等级（ERLs），这是一个四级迭代方法，用于追踪AI系统设计中伦理反思的实施情况，将高层伦理原则转化为具体的设计提示、检查和控件。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统设计中，伦理原则往往停留在抽象层面，难以与日常工程实践相结合。需要一种方法将伦理价值转化为具体的设计考量，促进伦理反思在技术开发中的实际应用。

Method: 开发了四级迭代的ERLs方法，使用基于具体情境指标的动态树状问卷进行评估，将伦理价值转化为具体提示、检查和控件，并建立评分系统追踪进展。

Result: 通过执法AI面部素描生成器和协作工业机器人两个案例研究验证了ERLs方法的有效性。该工具成功催化了具体设计变更，促进了从狭隘技术解决方案主义向反思性、伦理设计思维的转变。

Conclusion: ERLs方法有效连接了伦理原则与工程实践，不仅作为管理工具，还促进了伦理专家与技术团队的结构化对话，推动了AI系统设计中伦理反思的系统化实施。

Abstract: We present Ethics Readiness Levels (ERLs), a four-level, iterative method to track how ethical reflection is implemented in the design of AI systems. ERLs bridge high-level ethical principles and everyday engineering by turning ethical values into concrete prompts, checks, and controls within real use cases. The evaluation is conducted using a dynamic, tree-like questionnaire built from context-specific indicators, ensuring relevance to the technology and application domain. Beyond being a managerial tool, ERLs help facilitate a structured dialogue between ethics experts and technical teams, while our scoring system helps track progress over time. We demonstrate the methodology through two case studies: an AI facial sketch generator for law enforcement and a collaborative industrial robot. The ERL tool effectively catalyzes concrete design changes and promotes a shift from narrow technological solutionism to a more reflective, ethics-by-design mindset.

</details>
