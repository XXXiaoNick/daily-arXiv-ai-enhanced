{"id": "2510.09908", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09908", "abs": "https://arxiv.org/abs/2510.09908", "authors": ["Hao Yan", "Heyan Zhang", "Yongyi Guo"], "title": "Learning with Incomplete Context: Linear Contextual Bandits with Pretrained Imputation", "comment": null, "summary": "The rise of large-scale pretrained models has made it feasible to generate\npredictive or synthetic features at low cost, raising the question of how to\nincorporate such surrogate predictions into downstream decision-making. We\nstudy this problem in the setting of online linear contextual bandits, where\ncontexts may be complex, nonstationary, and only partially observed. In\naddition to bandit data, we assume access to an auxiliary dataset containing\nfully observed contexts--common in practice since such data are collected\nwithout adaptive interventions. We propose PULSE-UCB, an algorithm that\nleverages pretrained models trained on the auxiliary data to impute missing\nfeatures during online decision-making. We establish regret guarantees that\ndecompose into a standard bandit term plus an additional component reflecting\npretrained model quality. In the i.i.d. context case with H\\\"older-smooth\nmissing features, PULSE-UCB achieves near-optimal performance, supported by\nmatching lower bounds. Our results quantify how uncertainty in predicted\ncontexts affects decision quality and how much historical data is needed to\nimprove downstream learning.", "AI": {"tldr": "\u63d0\u51faPULSE-UCB\u7b97\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u5728\u7ebf\u7ebf\u6027\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u4e2d\u586b\u8865\u7f3a\u5931\u7279\u5f81\uff0c\u5b9e\u73b0\u540e\u6094\u754c\u5206\u89e3\u4e3a\u6807\u51c6\u8d4c\u535a\u673a\u9879\u52a0\u9884\u8bad\u7ec3\u6a21\u578b\u8d28\u91cf\u9879\u3002", "motivation": "\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u80fd\u4ee5\u4f4e\u6210\u672c\u751f\u6210\u9884\u6d4b\u7279\u5f81\uff0c\u4f46\u5982\u4f55\u5c06\u8fd9\u4e9b\u4ee3\u7406\u9884\u6d4b\u6574\u5408\u5230\u4e0b\u6e38\u51b3\u7b56\u4e2d\u4ecd\u662f\u4e00\u4e2a\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4e0a\u4e0b\u6587\u590d\u6742\u3001\u975e\u5e73\u7a33\u4e14\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u5728\u7ebf\u51b3\u7b56\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51faPULSE-UCB\u7b97\u6cd5\uff0c\u5229\u7528\u8f85\u52a9\u6570\u636e\u96c6\u4e0a\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u7ebf\u51b3\u7b56\u65f6\u586b\u8865\u7f3a\u5931\u7279\u5f81\uff0c\u5efa\u7acb\u540e\u6094\u4fdd\u8bc1\u3002", "result": "\u5728i.i.d.\u4e0a\u4e0b\u6587\u548cH\u00f6lder\u5149\u6ed1\u7f3a\u5931\u7279\u5f81\u60c5\u51b5\u4e0b\uff0cPULSE-UCB\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\uff0c\u6709\u5339\u914d\u4e0b\u754c\u652f\u6301\u3002\u7ed3\u679c\u91cf\u5316\u4e86\u9884\u6d4b\u4e0a\u4e0b\u6587\u4e0d\u786e\u5b9a\u6027\u5bf9\u51b3\u7b56\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u91cf\u5316\u4e86\u9884\u6d4b\u4e0a\u4e0b\u6587\u4e0d\u786e\u5b9a\u6027\u5bf9\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u5e76\u786e\u5b9a\u4e86\u6539\u5584\u4e0b\u6e38\u5b66\u4e60\u6240\u9700\u7684\u5386\u53f2\u6570\u636e\u91cf\uff0c\u4e3a\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u589e\u5f3a\u5728\u7ebf\u51b3\u7b56\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2510.10020", "categories": ["stat.ML", "cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.10020", "abs": "https://arxiv.org/abs/2510.10020", "authors": ["Henry D. Smith", "Nathaniel L. Diamant", "Brian L. Trippe"], "title": "Calibrating Generative Models", "comment": "Our codebase accompanying the paper is available at:\n  https://github.com/smithhenryd/cgm", "summary": "Generative models frequently suffer miscalibration, wherein class\nprobabilities and other statistics of the sampling distribution deviate from\ndesired values. We frame calibration as a constrained optimization problem and\nseek the closest model in Kullback-Leibler divergence satisfying calibration\nconstraints. To address the intractability of imposing these constraints\nexactly, we introduce two surrogate objectives for fine-tuning: (1) the relax\nloss, which replaces the constraint with a miscalibration penalty, and (2) the\nreward loss, which converts calibration into a reward fine-tuning problem. We\ndemonstrate that these approaches substantially reduce calibration error across\nhundreds of simultaneous constraints and models with up to one billion\nparameters, spanning applications in protein design, image generation, and\nlanguage modeling.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u5fae\u8c03\u65b9\u6cd5\u6765\u6539\u5584\u751f\u6210\u6a21\u578b\u7684\u6821\u51c6\u95ee\u9898\uff1a\u677e\u5f1b\u635f\u5931\u548c\u5956\u52b1\u635f\u5931\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6821\u51c6\u8bef\u5dee\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u7ecf\u5e38\u5b58\u5728\u6821\u51c6\u95ee\u9898\uff0c\u5373\u91c7\u6837\u5206\u5e03\u7684\u7c7b\u522b\u6982\u7387\u548c\u5176\u4ed6\u7edf\u8ba1\u6570\u636e\u504f\u79bb\u671f\u671b\u503c\uff0c\u8fd9\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "method": "\u5c06\u6821\u51c6\u95ee\u9898\u6784\u5efa\u4e3a\u5e26\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e24\u79cd\u66ff\u4ee3\u76ee\u6807\uff1a(1) \u677e\u5f1b\u635f\u5931\uff0c\u7528\u6821\u51c6\u8bef\u5dee\u60e9\u7f5a\u66ff\u4ee3\u7ea6\u675f\uff1b(2) \u5956\u52b1\u635f\u5931\uff0c\u5c06\u6821\u51c6\u8f6c\u5316\u4e3a\u5956\u52b1\u5fae\u8c03\u95ee\u9898\u3002", "result": "\u8fd9\u4e9b\u65b9\u6cd5\u5728\u6570\u767e\u4e2a\u540c\u65f6\u7ea6\u675f\u548c\u9ad8\u8fbe\u5341\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u6821\u51c6\u8bef\u5dee\uff0c\u6db5\u76d6\u86cb\u767d\u8d28\u8bbe\u8ba1\u3001\u56fe\u50cf\u751f\u6210\u548c\u8bed\u8a00\u5efa\u6a21\u7b49\u5e94\u7528\u3002", "conclusion": "\u63d0\u51fa\u7684\u677e\u5f1b\u635f\u5931\u548c\u5956\u52b1\u635f\u5931\u65b9\u6cd5\u662f\u89e3\u51b3\u751f\u6210\u6a21\u578b\u6821\u51c6\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u5927\u89c4\u6a21\u6a21\u578b\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u6821\u51c6\u6027\u80fd\u3002"}}
{"id": "2510.10245", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.10245", "abs": "https://arxiv.org/abs/2510.10245", "authors": ["Houssam Zenati", "Bariscan Bozkurt", "Arthur Gretton"], "title": "Kernel Treatment Effects with Adaptively Collected Data", "comment": null, "summary": "Adaptive experiments improve efficiency by adjusting treatment assignments\nbased on past outcomes, but this adaptivity breaks the i.i.d. assumptions that\nunderpins classical asymptotics. At the same time, many questions of interest\nare distributional, extending beyond average effects. Kernel treatment effects\n(KTE) provide a flexible framework by representing counterfactual outcome\ndistributions in an RKHS and comparing them via kernel distances. We present\nthe first kernel-based framework for distributional inference under adaptive\ndata collection. Our method combines doubly robust scores with variance\nstabilization to ensure asymptotic normality via a Hilbert-space martingale\nCLT, and introduces a sample-fitted stabilized test with valid type-I error.\nExperiments show it is well calibrated and effective for both mean shifts and\nhigher-moment differences, outperforming adaptive baselines limited to scalar\neffects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u6838\u7684\u81ea\u9002\u5e94\u6570\u636e\u6536\u96c6\u4e0b\u7684\u5206\u5e03\u63a8\u65ad\u6846\u67b6\uff0c\u7ed3\u5408\u53cc\u91cd\u7a33\u5065\u5f97\u5206\u548c\u65b9\u5dee\u7a33\u5b9a\u5316\uff0c\u901a\u8fc7\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u9785\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u786e\u4fdd\u6e10\u8fd1\u6b63\u6001\u6027\u3002", "motivation": "\u81ea\u9002\u5e94\u5b9e\u9a8c\u901a\u8fc7\u6839\u636e\u8fc7\u53bb\u7ed3\u679c\u8c03\u6574\u5904\u7406\u5206\u914d\u6765\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u8fd9\u79cd\u81ea\u9002\u5e94\u6027\u7834\u574f\u4e86\u652f\u6491\u7ecf\u5178\u6e10\u8fd1\u7406\u8bba\u7684i.i.d.\u5047\u8bbe\u3002\u540c\u65f6\uff0c\u8bb8\u591a\u611f\u5174\u8da3\u7684\u95ee\u9898\u662f\u5206\u5e03\u6027\u7684\uff0c\u8d85\u51fa\u4e86\u5e73\u5747\u6548\u5e94\u7684\u8303\u56f4\u3002", "method": "\u7ed3\u5408\u53cc\u91cd\u7a33\u5065\u5f97\u5206\u4e0e\u65b9\u5dee\u7a33\u5b9a\u5316\uff0c\u901a\u8fc7\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u9785\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u786e\u4fdd\u6e10\u8fd1\u6b63\u6001\u6027\uff0c\u5e76\u5f15\u5165\u6837\u672c\u62df\u5408\u7684\u7a33\u5b9a\u5316\u68c0\u9a8c\u6765\u63a7\u5236\u7b2c\u4e00\u7c7b\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6821\u51c6\u826f\u597d\uff0c\u5bf9\u5747\u503c\u504f\u79fb\u548c\u9ad8\u9636\u77e9\u5dee\u5f02\u90fd\u6709\u6548\uff0c\u4f18\u4e8e\u4ec5\u9650\u4e8e\u6807\u91cf\u6548\u5e94\u7684\u81ea\u9002\u5e94\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u9002\u5e94\u6570\u636e\u6536\u96c6\u4e0b\u7684\u5206\u5e03\u63a8\u65ad\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6838\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u8d85\u51fa\u5e73\u5747\u6548\u5e94\u7684\u5206\u5e03\u5dee\u5f02\u95ee\u9898\u3002"}}
{"id": "2510.10268", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.10268", "abs": "https://arxiv.org/abs/2510.10268", "authors": ["Jiafang Song", "Sandipan Pramanik", "Abhirup Datta"], "title": "Neural variational inference for cutting feedback during uncertainty propagation", "comment": null, "summary": "In many scientific applications, uncertainty of estimates from an earlier\n(upstream) analysis needs to be propagated in subsequent (downstream) Bayesian\nanalysis, without feedback. Cutting feedback methods, also termed cut-Bayes,\nachieve this by constructing a cut-posterior distribution that prevents\nbackward information flow. Cutting feedback like nested MCMC is computationally\nchallenging while variational inference (VI) cut-Bayes methods need two\nvariational approximations and require access to the upstream data and model.\nIn this manuscript we propose, NeVI-Cut, a provably accurate and modular neural\nnetwork-based variational inference method for cutting feedback. We directly\nutilize samples from the upstream analysis without requiring access to the\nupstream data or model. This simultaneously preserves modularity of analysis\nand reduces approximation errors by avoiding a variational approximation for\nthe upstream model. We then use normalizing flows to specify the conditional\nvariational family for the downstream parameters and estimate the conditional\ncut-posterior as a variational solution of Monte Carlo average loss over all\nthe upstream samples. We provide theoretical guarantees on the NeVI-Cut\nestimate to approximate any cut-posterior. Our results are in a fixed-data\nregime and provide convergence rates of the actual variational solution,\nquantifying how richness of the neural architecture and the complexity of the\ntarget cut-posterior dictate the approximation quality. In the process, we\nestablish new results on uniform Kullback-Leibler approximation rates of\nconditional normalizing flows. Simulation studies and two real-world analyses\nillustrate how NeVI-Cut achieves significant computational gains over\ntraditional cutting feedback methods and is considerably more accurate than\nparametric variational cut approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86NeVI-Cut\u65b9\u6cd5\uff0c\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u53d8\u5206\u63a8\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u53cd\u9988\u5207\u65ad\u95ee\u9898\uff0c\u65e0\u9700\u4e0a\u6e38\u6570\u636e\u548c\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528\u4e0a\u6e38\u6837\u672c\u5373\u53ef\u5b9e\u73b0\u6a21\u5757\u5316\u5206\u6790\u3002", "motivation": "\u5728\u79d1\u5b66\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u5c06\u4e0a\u6e38\u5206\u6790\u7684\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u5230\u4e0b\u6e38\u8d1d\u53f6\u65af\u5206\u6790\u4e2d\uff0c\u540c\u65f6\u907f\u514d\u53cd\u9988\u3002\u73b0\u6709\u7684\u53cd\u9988\u5207\u65ad\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u53d8\u5206\u63a8\u7406\u65b9\u6cd5\u9700\u8981\u4e24\u4e2a\u53d8\u5206\u8fd1\u4f3c\u548c\u8bbf\u95ee\u4e0a\u6e38\u6570\u636e\u4e0e\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u5f52\u4e00\u5316\u6d41\u6307\u5b9a\u4e0b\u6e38\u53c2\u6570\u7684\u6761\u4ef6\u53d8\u5206\u65cf\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u5e73\u5747\u635f\u5931\u4f30\u8ba1\u6761\u4ef6\u5207\u65ad\u540e\u9a8c\u4f5c\u4e3a\u53d8\u5206\u89e3\uff0c\u76f4\u63a5\u5229\u7528\u4e0a\u6e38\u5206\u6790\u6837\u672c\u800c\u65e0\u9700\u4e0a\u6e38\u6570\u636e\u6216\u6a21\u578b\u3002", "result": "NeVI-Cut\u5728\u8ba1\u7b97\u4e0a\u6bd4\u4f20\u7edf\u53cd\u9988\u5207\u65ad\u65b9\u6cd5\u663e\u8457\u63d0\u5347\uff0c\u6bd4\u53c2\u6570\u5316\u53d8\u5206\u5207\u65ad\u65b9\u6cd5\u66f4\u51c6\u786e\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u6536\u655b\u901f\u7387\u5206\u6790\u3002", "conclusion": "NeVI-Cut\u662f\u4e00\u79cd\u51c6\u786e\u3001\u6a21\u5757\u5316\u7684\u53cd\u9988\u5207\u65ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u53d8\u5206\u63a8\u7406\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u7cbe\u5ea6\u7684\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2510.10343", "categories": ["q-fin.CP", "q-fin.PR", "q-fin.RM"], "pdf": "https://arxiv.org/pdf/2510.10343", "abs": "https://arxiv.org/abs/2510.10343", "authors": ["Giorgia Rensi", "Pietro Rossi", "Marco Bianchetti"], "title": "Learning the Exact SABR Model", "comment": "Main paper 23 pages, Appendices 12 pages, 37 references, 10 figures,\n  14 tables", "summary": "The SABR model is a cornerstone of interest rate volatility modeling, but its\npractical application relies heavily on the analytical approximation by Hagan\net al., whose accuracy deteriorates for high volatility, long maturities, and\nout-of-the-money options, admitting arbitrage. While machine learning\napproaches have been proposed to overcome these limitations, they have often\nbeen limited by simplified SABR dynamics or a lack of systematic validation\nagainst the full spectrum of market conditions.\n  We develop a novel SABR DNN, a specialized Artificial Deep Neural Network\n(DNN) architecture that learns the true SABR stochastic dynamics using an\nunprecedented large training dataset (more than 200 million points) of interest\nrate Cap/Floor volatility surfaces, including very long maturities (30Y) and\nextreme strikes consistently with market quotations. Our dataset is obtained\nvia high-precision unbiased Monte Carlo simulation of a special scaled\nshifted-SABR stochastic dynamics, which allows dimensional reduction without\nany loss of generality.\n  Our SABR DNN provides arbitrage-free calibration of real market volatility\nsurfaces and Caps/Floors prices for any maturity and strike with negligible\ncomputational effort and without retraining across business dates. Our results\nfully address the gaps in the previous machine learning SABR literature in a\nsystematic and self-consistent way, and can be extended to cover any interest\nrate European options in different rate tenors and currencies, thus\nestablishing a comprehensive functional SABR framework that can be adopted for\ndaily trading and risk management activities.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u4e13\u95e8\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784SABR DNN\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u96c6\u5b66\u4e60\u771f\u5b9e\u7684SABR\u968f\u673a\u52a8\u6001\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfHagan\u8fd1\u4f3c\u5728\u9ad8\u6ce2\u52a8\u7387\u3001\u957f\u671f\u9650\u548c\u4ef7\u5916\u671f\u6743\u60c5\u51b5\u4e0b\u7684\u7cbe\u5ea6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u65e0\u5957\u5229\u7684\u5e02\u573a\u6ce2\u52a8\u7387\u66f2\u9762\u6821\u51c6\u3002", "motivation": "\u4f20\u7edfSABR\u6a21\u578b\u7684Hagan\u8fd1\u4f3c\u5728\u9ad8\u6ce2\u52a8\u7387\u3001\u957f\u671f\u9650\u548c\u4ef7\u5916\u671f\u6743\u60c5\u51b5\u4e0b\u7cbe\u5ea6\u4e0b\u964d\uff0c\u5b58\u5728\u5957\u5229\u673a\u4f1a\u3002\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u7b80\u5316\u7684SABR\u52a8\u6001\u6216\u7f3a\u4e4f\u7cfb\u7edf\u9a8c\u8bc1\u3002", "method": "\u4f7f\u7528\u8d85\u8fc72\u4ebf\u4e2a\u70b9\u7684\u5229\u7387\u4e0a\u9650/\u4e0b\u9650\u6ce2\u52a8\u7387\u66f2\u9762\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u9ad8\u7cbe\u5ea6\u65e0\u504f\u8499\u7279\u5361\u6d1b\u6a21\u62df\u7279\u6b8a\u7f29\u653e\u79fb\u4f4dSABR\u968f\u673a\u52a8\u6001\uff0c\u6784\u5efa\u4e13\u95e8\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002", "result": "SABR DNN\u80fd\u591f\u4ee5\u53ef\u5ffd\u7565\u7684\u8ba1\u7b97\u6210\u672c\u5bf9\u4efb\u4f55\u671f\u9650\u548c\u6267\u884c\u4ef7\u7684\u5e02\u573a\u6ce2\u52a8\u7387\u66f2\u9762\u548c\u4e0a\u9650/\u4e0b\u9650\u4ef7\u683c\u8fdb\u884c\u65e0\u5957\u5229\u6821\u51c6\uff0c\u4e14\u65e0\u9700\u8de8\u4e1a\u52a1\u65e5\u671f\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u4e86\u5148\u524d\u673a\u5668\u5b66\u4e60SABR\u6587\u732e\u4e2d\u7684\u7a7a\u767d\uff0c\u5efa\u7acb\u4e86\u5168\u9762\u7684\u51fd\u6570SABR\u6846\u67b6\uff0c\u53ef\u6269\u5c55\u5230\u4e0d\u540c\u5229\u7387\u671f\u9650\u548c\u8d27\u5e01\u7684\u4efb\u4f55\u5229\u7387\u6b27\u5f0f\u671f\u6743\uff0c\u9002\u7528\u4e8e\u65e5\u5e38\u4ea4\u6613\u548c\u98ce\u9669\u7ba1\u7406\u3002"}}
{"id": "2510.09610", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.09610", "abs": "https://arxiv.org/abs/2510.09610", "authors": ["Samet Uzun", "Behcet Acikmese", "John M. Carson III"], "title": "Sequential Convex Programming for 6-DoF Powered Descent Guidance with Continuous-Time Compound State-Triggered Constraints", "comment": null, "summary": "This paper presents a sequential convex programming (SCP) framework for\nensuring the continuous-time satisfaction of compound state-triggered\nconstraints, a subset of logical specifications, in the powered descent\nguidance (PDG) problem. The proposed framework combines the generalized\nmean-based smooth robustness measure (D-GMSR), a parameterization technique\ntailored for expressing discrete-time temporal and logical specifications\nthrough smooth functions, with the continuous-time successive convexification\n(CT-SCvx) method, a real-time solution for constrained trajectory optimization\nthat guarantees continuous-time constraint satisfaction and convergence. The\nsmoothness of the temporal and logical specifications parameterized via D-GMSR\nenables solving the resulting optimization problem with robust and efficient\nSCP algorithms while preserving theoretical guarantees. In addition to their\nsmoothness, the parameterized specifications are sound and complete, meaning\nthe specification holds if and only if the constraint defined by the\nparameterized function is satisfied. The CT-SCvx framework is then applied to\nsolve the parameterized problem, incorporating: (1) reformulation for\ncontinuous-time path constraint satisfaction, (2) time-dilation to transform\nthe free-final-time PDG problem into a fixed-final-time problem, (3) multiple\nshooting for exact discretization, (4) exact penalty functions for penalizing\nnonconvex constraints, and (5) the prox-linear method, a convergence-guaranteed\nSCP algorithm, to solve the resulting finite-dimensional nonconvex PDG problem.\nThe effectiveness of the framework is demonstrated through a numerical\nsimulation. The implementation is available at\nhttps://github.com/UW-ACL/CT-cSTC", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e8f\u5217\u51f8\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u52a8\u529b\u4e0b\u964d\u5236\u5bfc\u95ee\u9898\u4e2d\u786e\u4fdd\u590d\u5408\u72b6\u6001\u89e6\u53d1\u7ea6\u675f\u7684\u8fde\u7eed\u65f6\u95f4\u6ee1\u8db3\uff0c\u7ed3\u5408\u4e86\u5e7f\u4e49\u5747\u503c\u5e73\u6ed1\u9c81\u68d2\u6027\u5ea6\u91cf\u4e0e\u8fde\u7eed\u65f6\u95f4\u8fde\u7eed\u51f8\u5316\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u52a8\u529b\u4e0b\u964d\u5236\u5bfc\u95ee\u9898\u4e2d\u590d\u5408\u72b6\u6001\u89e6\u53d1\u7ea6\u675f\u7684\u8fde\u7eed\u65f6\u95f4\u6ee1\u8db3\u95ee\u9898\uff0c\u8fd9\u4e9b\u7ea6\u675f\u662f\u903b\u8f91\u89c4\u8303\u7684\u4e00\u4e2a\u5b50\u96c6\uff0c\u9700\u8981\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528D-GMSR\u53c2\u6570\u5316\u6280\u672f\u8868\u8fbe\u79bb\u6563\u65f6\u95f4\u65f6\u5e8f\u548c\u903b\u8f91\u89c4\u8303\uff0c\u7ed3\u5408CT-SCvx\u65b9\u6cd5\u8fdb\u884c\u8fde\u7eed\u65f6\u95f4\u8f68\u8ff9\u4f18\u5316\uff0c\u5305\u62ec\u65f6\u95f4\u81a8\u80c0\u3001\u591a\u91cd\u6253\u9776\u3001\u7cbe\u786e\u7f5a\u51fd\u6570\u548c\u8fd1\u7aef\u7ebf\u6027\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u4fdd\u8bc1\u8fde\u7eed\u65f6\u95f4\u7ea6\u675f\u6ee1\u8db3\u548c\u6536\u655b\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u52a8\u529b\u4e0b\u964d\u5236\u5bfc\u4e2d\u7684\u590d\u5408\u72b6\u6001\u89e6\u53d1\u7ea6\u675f\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u53ef\u884c\u6027\u3002"}}
{"id": "2510.10527", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2510.10527", "abs": "https://arxiv.org/abs/2510.10527", "authors": ["Mingqian Guan", "Komei Fujita", "Naoya Sueishi", "Shota Yasui"], "title": "Denoised IPW-Lasso for Heterogeneous Treatment Effect Estimation in Randomized Experiments", "comment": null, "summary": "This paper proposes a new method for estimating conditional average treatment\neffects (CATE) in randomized experiments. We adopt inverse probability\nweighting (IPW) for identification; however, IPW-transformed outcomes are known\nto be noisy, even when true propensity scores are used. To address this issue,\nwe introduce a noise reduction procedure and estimate a linear CATE model using\nLasso, achieving both accuracy and interpretability. We theoretically show that\ndenoising reduces the prediction error of the Lasso. The method is particularly\neffective when treatment effects are small relative to the variability of\noutcomes, which is often the case in empirical applications. Applications to\nthe Get-Out-the-Vote dataset and Criteo Uplift Modeling dataset demonstrate\nthat our method outperforms fully nonparametric machine learning methods in\nidentifying individuals with higher treatment effects. Moreover, our method\nuncovers informative heterogeneity patterns that are consistent with previous\nempirical findings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u9006\u6982\u7387\u52a0\u6743\u548c\u964d\u566a\u5904\u7406\u7684\u6761\u4ef6\u5e73\u5747\u5904\u7406\u6548\u5e94\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u7528Lasso\u4f30\u8ba1\u7ebf\u6027CATE\u6a21\u578b\uff0c\u5728\u6cbb\u7597\u6548\u679c\u8f83\u5c0f\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9006\u6982\u7387\u52a0\u6743\u8f6c\u6362\u7684\u7ed3\u679c\u901a\u5e38\u566a\u58f0\u8f83\u5927\uff0c\u5373\u4f7f\u4f7f\u7528\u771f\u5b9e\u7684\u503e\u5411\u5f97\u5206\uff0c\u8fd9\u5f71\u54cd\u4e86CATE\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u9006\u6982\u7387\u52a0\u6743\u8fdb\u884c\u8bc6\u522b\uff0c\u5f15\u5165\u964d\u566a\u7a0b\u5e8f\uff0c\u4f7f\u7528Lasso\u4f30\u8ba1\u7ebf\u6027CATE\u6a21\u578b\uff0c\u517c\u987e\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u964d\u566a\u80fd\u51cf\u5c11Lasso\u7684\u9884\u6d4b\u8bef\u5dee\uff1b\u5728Get-Out-the-Vote\u548cCriteo\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u8bc6\u522b\u9ad8\u5904\u7406\u6548\u5e94\u4e2a\u4f53\u65b9\u9762\u4f18\u4e8e\u975e\u53c2\u6570\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6cbb\u7597\u6548\u679c\u76f8\u5bf9\u8f83\u5c0f\u65f6\u7279\u522b\u6709\u6548\uff0c\u80fd\u591f\u53d1\u73b0\u4e0e\u5148\u524d\u5b9e\u8bc1\u53d1\u73b0\u4e00\u81f4\u7684\u6709\u610f\u4e49\u7684\u5f02\u8d28\u6027\u6a21\u5f0f\u3002"}}
{"id": "2510.09634", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09634", "abs": "https://arxiv.org/abs/2510.09634", "authors": ["Anastasija Nikiforova", "Martin Lnenicka", "Ulf Melin", "David Valle-Cruz", "Asif Gill", "Cesar Casiano Flores", "Emyana Sirait", "Mariusz Luterek", "Richard Michael Dreyling", "Barbora Tesarova"], "title": "Responsible AI Adoption in the Public Sector: A Data-Centric Taxonomy of AI Adoption Challenges", "comment": null, "summary": "Despite Artificial Intelligence (AI) transformative potential for public\nsector services, decision-making, and administrative efficiency, adoption\nremains uneven due to complex technical, organizational, and institutional\nchallenges. Responsible AI frameworks emphasize fairness, accountability, and\ntransparency, aligning with principles of trustworthy AI and fair AI, yet\nremain largely aspirational, overlooking technical and institutional realities,\nespecially foundational data and governance. This study addresses this gap by\ndeveloping a taxonomy of data-related challenges to responsible AI adoption in\ngovernment. Based on a systematic review of 43 studies and 21 expert\nevaluations, the taxonomy identifies 13 key challenges across technological,\norganizational, and environmental dimensions, including poor data quality,\nlimited AI-ready infrastructure, weak governance, misalignment in human-AI\ndecision-making, economic and environmental sustainability concerns. Annotated\nwith institutional pressures, the taxonomy serves as a diagnostic tool to\nsurface 'symptoms' of high-risk AI deployment and guides policymakers in\nbuilding the institutional and data governance conditions necessary for\nresponsible AI adoption.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5173\u4e8e\u653f\u5e9c\u8d1f\u8d23\u4efbAI\u91c7\u7528\u4e2d\u6570\u636e\u76f8\u5173\u6311\u6218\u7684\u5206\u7c7b\u6cd5\uff0c\u57fa\u4e8e\u7cfb\u7edf\u6587\u732e\u56de\u987e\u548c\u4e13\u5bb6\u8bc4\u4f30\uff0c\u8bc6\u522b\u4e8613\u4e2a\u5173\u952e\u6280\u672f\u3001\u7ec4\u7ec7\u548c\u73af\u5883\u7ef4\u5ea6\u7684\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1AI\u5728\u516c\u5171\u90e8\u95e8\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u590d\u6742\u7684\u6280\u672f\u3001\u7ec4\u7ec7\u548c\u5236\u5ea6\u6311\u6218\uff0c\u91c7\u7528\u4ecd\u7136\u4e0d\u5747\u8861\u3002\u8d1f\u8d23\u4efbAI\u6846\u67b6\u5f3a\u8c03\u516c\u5e73\u6027\u3001\u95ee\u8d23\u5236\u548c\u900f\u660e\u5ea6\uff0c\u4f46\u5f80\u5f80\u5ffd\u89c6\u4e86\u6280\u672f\u548c\u5236\u5ea6\u73b0\u5b9e\uff0c\u7279\u522b\u662f\u57fa\u7840\u6570\u636e\u548c\u6cbb\u7406\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u5bf943\u9879\u7814\u7a76\u7684\u7cfb\u7edf\u56de\u987e\u548c21\u4f4d\u4e13\u5bb6\u8bc4\u4f30\uff0c\u5f00\u53d1\u4e86\u6570\u636e\u76f8\u5173\u6311\u6218\u7684\u5206\u7c7b\u6cd5\u3002", "result": "\u8bc6\u522b\u4e8613\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u6570\u636e\u8d28\u91cf\u5dee\u3001AI\u5c31\u7eea\u57fa\u7840\u8bbe\u65bd\u6709\u9650\u3001\u6cbb\u7406\u8584\u5f31\u3001\u4eba\u673a\u51b3\u7b56\u9519\u4f4d\u3001\u7ecf\u6d4e\u548c\u73af\u5883\u53ef\u6301\u7eed\u6027\u62c5\u5fe7\u7b49\uff0c\u6db5\u76d6\u6280\u672f\u3001\u7ec4\u7ec7\u548c\u73af\u5883\u7ef4\u5ea6\u3002", "conclusion": "\u8be5\u5206\u7c7b\u6cd5\u53ef\u4f5c\u4e3a\u8bca\u65ad\u5de5\u5177\uff0c\u63ed\u793a\u9ad8\u98ce\u9669AI\u90e8\u7f72\u7684'\u75c7\u72b6'\uff0c\u5e76\u6307\u5bfc\u653f\u7b56\u5236\u5b9a\u8005\u5efa\u7acb\u8d1f\u8d23\u4efbAI\u91c7\u7528\u6240\u9700\u7684\u5236\u5ea6\u548c\u6570\u636e\u6cbb\u7406\u6761\u4ef6\u3002"}}
{"id": "2510.10728", "categories": ["q-fin.MF", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10728", "abs": "https://arxiv.org/abs/2510.10728", "authors": ["Ali Atiah Alzahrani"], "title": "Deep Signature and Neural RDE Methods for Path-Dependent Portfolio Optimization", "comment": "Accepted for presentation at the ACM International Conference on AI\n  in Finance (ICAIF 2025), QuantAI Workshop, Singapore. 9 pages. Code available\n  at: https://github.com/AliAtiah/SigRDE", "summary": "We present a deep BSDE and 2BSDE solver that combines truncated log\nsignatures with a neural rough differential equation backbone for high\ndimensional, path dependent valuation and control. The design aligns stochastic\nanalysis with sequence to path learning, using a CVaR tilted objective to\nemphasize left tail risk and an optional second order head for risk sensitive\ncontrol. Under equal compute and parameter budgets, the method improves\naccuracy, tail fidelity, and training stability across Asian and barrier option\npricing and portfolio control tasks. At 200 dimensions, it achieves CVaR(0.99)\n= 9.8 percent compared to 12.0-13.1 percent for strong baselines, while\nattaining low HJB residuals and small RMSE for Z and Gamma. Ablations confirm\ncomplementary gains from the sequence to path representation and the second\norder structure. Overall, the results show that combining stochastic analysis\nwith modern deep learning expands the class of solvable path dependent\nfinancial models at scale.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u622a\u65ad\u5bf9\u6570\u7b7e\u540d\u548c\u795e\u7ecf\u7c97\u7cd9\u5fae\u5206\u65b9\u7a0b\u7684\u6df1\u5ea6BSDE/2BSDE\u6c42\u89e3\u5668\uff0c\u7528\u4e8e\u9ad8\u7ef4\u8def\u5f84\u4f9d\u8d56\u7684\u4f30\u503c\u548c\u63a7\u5236\u95ee\u9898\uff0c\u5728\u540c\u7b49\u8ba1\u7b97\u548c\u53c2\u6570\u9884\u7b97\u4e0b\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u3001\u5c3e\u90e8\u4fdd\u771f\u5ea6\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u5c06\u968f\u673a\u5206\u6790\u4e0e\u5e8f\u5217\u5230\u8def\u5f84\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u6269\u5c55\u53ef\u89e3\u51b3\u7684\u8def\u5f84\u4f9d\u8d56\u91d1\u878d\u6a21\u578b\u7c7b\u522b\uff0c\u7279\u522b\u5173\u6ce8\u5de6\u5c3e\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u622a\u65ad\u5bf9\u6570\u7b7e\u540d\u548c\u795e\u7ecf\u7c97\u7cd9\u5fae\u5206\u65b9\u7a0b\u4f5c\u4e3a\u4e3b\u5e72\uff0c\u7ed3\u5408CVaR\u503e\u659c\u76ee\u6807\u51fd\u6570\u5f3a\u8c03\u5de6\u5c3e\u98ce\u9669\uff0c\u53ef\u9009\u4e8c\u9636\u5934\u7528\u4e8e\u98ce\u9669\u654f\u611f\u63a7\u5236\u3002", "result": "\u5728200\u7ef4\u60c5\u51b5\u4e0b\uff0cCVaR(0.99)\u8fbe\u52309.8%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u768412.0-13.1%\uff0c\u540c\u65f6\u83b7\u5f97\u4f4eHJB\u6b8b\u5dee\u548c\u5c0f\u7684Z\u3001Gamma RMSE\u3002", "conclusion": "\u5c06\u968f\u673a\u5206\u6790\u4e0e\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u6269\u5c55\u4e86\u53ef\u89e3\u51b3\u7684\u8def\u5f84\u4f9d\u8d56\u91d1\u878d\u6a21\u578b\u7c7b\u522b\uff0c\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u5e8f\u5217\u5230\u8def\u5f84\u8868\u793a\u548c\u4e8c\u9636\u7ed3\u6784\u7684\u4e92\u8865\u589e\u76ca\u3002"}}
{"id": "2510.11013", "categories": ["econ.EM", "econ.GN", "math.ST", "q-fin.EC", "stat.AP", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.11013", "abs": "https://arxiv.org/abs/2510.11013", "authors": ["Tatsuru Kikuchi"], "title": "Spatial and Temporal Boundaries in Difference-in-Differences: A Framework from Navier-Stokes Equation", "comment": "56 pages, 4 figures", "summary": "This paper develops a unified framework for identifying spatial and temporal\nboundaries of treatment effects in difference-in-differences designs. Starting\nfrom fundamental fluid dynamics equations (Navier-Stokes), we derive conditions\nunder which treatment effects decay exponentially in space and time, enabling\nresearchers to calculate explicit boundaries beyond which effects become\nundetectable. The framework encompasses both linear (pure diffusion) and\nnonlinear (advection-diffusion with chemical reactions) regimes, with testable\nscope conditions based on dimensionless numbers from physics (P\\'eclet and\nReynolds numbers). We demonstrate the framework's diagnostic capability using\nair pollution from coal-fired power plants. Analyzing 791 ground-based\nPM$_{2.5}$ monitors and 189,564 satellite-based NO$_2$ grid cells in the\nWestern United States over 2019-2021, we find striking regional heterogeneity:\nwithin 100 km of coal plants, both pollutants show positive spatial decay\n(PM$_{2.5}$: $\\kappa_s = 0.00200$, $d^* = 1,153$ km; NO$_2$: $\\kappa_s =\n0.00112$, $d^* = 2,062$ km), validating the framework. Beyond 100 km, negative\ndecay parameters correctly signal that urban sources dominate and diffusion\nassumptions fail. Ground-level PM$_{2.5}$ decays approximately twice as fast as\nsatellite column NO$_2$, consistent with atmospheric transport physics. The\nframework successfully diagnoses its own validity in four of eight analyzed\nregions, providing researchers with physics-based tools to assess whether their\nspatial difference-in-differences setting satisfies diffusion assumptions\nbefore applying the estimator. Our results demonstrate that rigorous boundary\ndetection requires both theoretical derivation from first principles and\nempirical validation of underlying physical assumptions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u8bc6\u522b\u53cc\u91cd\u5dee\u5206\u8bbe\u8ba1\u4e2d\u5904\u7406\u6548\u5e94\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u8fb9\u754c\uff0c\u57fa\u4e8e\u6d41\u4f53\u52a8\u529b\u5b66\u65b9\u7a0b\u63a8\u5bfc\u51fa\u5904\u7406\u6548\u5e94\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u5448\u6307\u6570\u8870\u51cf\u7684\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u7a7a\u6c14\u6c61\u67d3\u6848\u4f8b\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u8bca\u65ad\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u53cc\u91cd\u5dee\u5206\u7814\u7a76\u7f3a\u4e4f\u5bf9\u5904\u7406\u6548\u5e94\u8fb9\u754c\u7684\u7cfb\u7edf\u8bc6\u522b\u65b9\u6cd5\uff0c\u9700\u8981\u4ece\u7269\u7406\u5b66\u7b2c\u4e00\u539f\u7406\u51fa\u53d1\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u6765\u51c6\u786e\u786e\u5b9a\u6548\u5e94\u8870\u51cf\u7684\u8fb9\u754c\u8303\u56f4\u3002", "method": "\u4ece\u7eb3\u7ef4-\u65af\u6258\u514b\u65af\u65b9\u7a0b\u51fa\u53d1\u63a8\u5bfc\u5904\u7406\u6548\u5e94\u8870\u51cf\u6761\u4ef6\uff0c\u6db5\u76d6\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u6269\u6563\u673a\u5236\uff0c\u57fa\u4e8eP\u00e9clet\u548cReynolds\u6570\u5efa\u7acb\u53ef\u68c0\u9a8c\u7684\u8303\u56f4\u6761\u4ef6\uff0c\u5e76\u4f7f\u7528791\u4e2aPM2.5\u76d1\u6d4b\u70b9\u548c189,564\u4e2aNO2\u536b\u661f\u7f51\u683c\u6570\u636e\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u5728\u8ddd\u79bb\u71c3\u7164\u7535\u5382100\u516c\u91cc\u5185\uff0c\u4e24\u79cd\u6c61\u67d3\u7269\u5747\u5448\u73b0\u6b63\u7684\u7a7a\u95f4\u8870\u51cf(PM2.5: \u03bas=0.00200, d*=1,153km; NO2: \u03bas=0.00112, d*=2,062km)\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff1b\u8d85\u8fc7100\u516c\u91cc\u540e\u8d1f\u8870\u51cf\u53c2\u6570\u6b63\u786e\u6307\u793a\u6269\u6563\u5047\u8bbe\u5931\u6548\u3002", "conclusion": "\u4e25\u683c\u7684\u8fb9\u754c\u68c0\u6d4b\u9700\u8981\u4ece\u7b2c\u4e00\u539f\u7406\u8fdb\u884c\u7406\u8bba\u63a8\u5bfc\u5e76\u5bf9\u57fa\u7840\u7269\u7406\u5047\u8bbe\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u8be5\u6846\u67b6\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u57fa\u4e8e\u7269\u7406\u5b66\u7684\u5de5\u5177\u6765\u8bc4\u4f30\u7a7a\u95f4\u53cc\u91cd\u5dee\u5206\u8bbe\u7f6e\u662f\u5426\u6ee1\u8db3\u6269\u6563\u5047\u8bbe\u3002"}}
{"id": "2510.09899", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.09899", "abs": "https://arxiv.org/abs/2510.09899", "authors": ["Shunan Zheng", "John Hasenbein"], "title": "Information Asymmetry in Queues with Strategic Customers", "comment": null, "summary": "This paper studies information asymmetry in an unobservable single-server\nqueueing system. While system managers have knowledge of the true arrival rate,\ncustomers may lack this information and instead form arbitrary beliefs. We\npropose a three-tier hierarchy of information asymmetry with increasing levels\nof information disclosure:customers keep private beliefs, customers are aware\nof the beliefs of others, and customers know the true arrival rate. Within this\nframework, the effects of the belief distribution, which is assumed to be\ngeneral with minimal restrictions, are analyzed in terms of equilibrium joining\nprobabilities, revenue, and social welfare. Furthermore,strategies for\ninformation disclosure are proposed for system managers to regulate the queue.", "AI": {"tldr": "\u7814\u7a76\u4e0d\u53ef\u89c2\u6d4b\u5355\u670d\u52a1\u5668\u6392\u961f\u7cfb\u7edf\u4e2d\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u5206\u6790\u4e0d\u540c\u4fe1\u606f\u5c42\u7ea7\u5bf9\u5747\u8861\u52a0\u5165\u6982\u7387\u3001\u6536\u5165\u548c\u793e\u4f1a\u798f\u5229\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4fe1\u606f\u62ab\u9732\u7b56\u7565\u3002", "motivation": "\u7cfb\u7edf\u7ba1\u7406\u8005\u77e5\u9053\u771f\u5b9e\u5230\u8fbe\u7387\uff0c\u4f46\u987e\u5ba2\u53ef\u80fd\u7f3a\u4e4f\u8fd9\u4e00\u4fe1\u606f\u800c\u5f62\u6210\u4efb\u610f\u4fe1\u5ff5\uff0c\u8fd9\u79cd\u4fe1\u606f\u4e0d\u5bf9\u79f0\u4f1a\u5f71\u54cd\u6392\u961f\u7cfb\u7edf\u7684\u8fd0\u884c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e09\u7ea7\u4fe1\u606f\u4e0d\u5bf9\u79f0\u5c42\u6b21\u7ed3\u6784\uff1a\u987e\u5ba2\u4fdd\u6301\u79c1\u4eba\u4fe1\u5ff5\u3001\u987e\u5ba2\u4e86\u89e3\u4ed6\u4eba\u4fe1\u5ff5\u3001\u987e\u5ba2\u77e5\u9053\u771f\u5b9e\u5230\u8fbe\u7387\u3002\u5728\u6700\u5c0f\u9650\u5236\u6761\u4ef6\u4e0b\u5206\u6790\u4fe1\u5ff5\u5206\u5e03\u7684\u5f71\u54cd\u3002", "result": "\u5206\u6790\u4e86\u4fe1\u5ff5\u5206\u5e03\u5bf9\u5747\u8861\u52a0\u5165\u6982\u7387\u3001\u6536\u5165\u548c\u793e\u4f1a\u798f\u5229\u7684\u5f71\u54cd\uff0c\u4e3a\u7cfb\u7edf\u7ba1\u7406\u8005\u63d0\u4f9b\u4e86\u8c03\u8282\u961f\u5217\u7684\u4fe1\u606f\u62ab\u9732\u7b56\u7565\u3002", "conclusion": "\u4fe1\u606f\u4e0d\u5bf9\u79f0\u5bf9\u6392\u961f\u7cfb\u7edf\u6709\u663e\u8457\u5f71\u54cd\uff0c\u7cfb\u7edf\u7ba1\u7406\u8005\u53ef\u4ee5\u901a\u8fc7\u9002\u5f53\u7684\u4fe1\u606f\u62ab\u9732\u7b56\u7565\u6765\u4f18\u5316\u961f\u5217\u7ba1\u7406\u3002"}}
{"id": "2510.10371", "categories": ["q-fin.PM", "math.OC", "91G10 (Primary) 91G05, 60G40, 91B16, 93E20 (Secondary)"], "pdf": "https://arxiv.org/pdf/2510.10371", "abs": "https://arxiv.org/abs/2510.10371", "authors": ["Criscent Birungi", "Cody Hyndman"], "title": "Optimal annuitization with labor income under age-dependent force of mortality", "comment": "36 pages, 9 figures", "summary": "We consider the problem of optimal annuitization with labour income, where an\nagent aims to maximize utility from consumption and labour income under\nage-dependent force of mortality. Using a dynamic programming approach, we\nderive closed-form solutions for the value function and the optimal\nconsumption, portfolio, and labor supply strategies. Our results show that\nbefore retirement, investment behavior increases with wealth until a threshold\nset by labor supply. After retirement, agents tend to consume a larger portion\nof their wealth. Two main factors influence optimal annuitization decisions as\npeople get older. First, the agent's perspective (demand side); the agent's\npersonal discount rate rises with age, reducing their desire to annuitize.\nSecond, the insurer's perspective (supply side); insurers offer higher payout\nrates (mortality credits). Our model demonstrates that beyond a certain age,\nsharply declining survival probabilities make annuitization substantially\noptimal, as the powerful incentive of mortality credits outweighs the agent's\nhigh personal discount rate. Finally, post-retirement labor income serves as a\ndirect substitute for annuitization by providing an alternative stable income\nsource. It enhances the financial security of retirees.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5e26\u52b3\u52a8\u6536\u5165\u7684\u6700\u4f18\u5e74\u91d1\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\u63a8\u5bfc\u51fa\u95ed\u5f0f\u89e3\uff0c\u5206\u6790\u4e86\u9000\u4f11\u524d\u540e\u6295\u8d44\u6d88\u8d39\u884c\u4e3a\u53d8\u5316\u53ca\u5e74\u91d1\u5316\u51b3\u7b56\u7684\u5f71\u54cd\u56e0\u7d20\u3002", "motivation": "\u7814\u7a76\u5728\u8003\u8651\u52b3\u52a8\u6536\u5165\u548c\u5e74\u9f84\u76f8\u5173\u6b7b\u4ea1\u7387\u7684\u6761\u4ef6\u4e0b\uff0c\u4ee3\u7406\u4eba\u5982\u4f55\u6700\u5927\u5316\u6d88\u8d39\u548c\u52b3\u52a8\u6536\u5165\u7684\u6548\u7528\uff0c\u7279\u522b\u5173\u6ce8\u5e74\u91d1\u5316\u51b3\u7b56\u7684\u5f71\u54cd\u56e0\u7d20\u3002", "method": "\u91c7\u7528\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\uff0c\u63a8\u5bfc\u51fa\u4ef7\u503c\u51fd\u6570\u4ee5\u53ca\u6700\u4f18\u6d88\u8d39\u3001\u6295\u8d44\u7ec4\u5408\u548c\u52b3\u52a8\u4f9b\u7ed9\u7b56\u7565\u7684\u95ed\u5f0f\u89e3\u3002", "result": "\u9000\u4f11\u524d\u6295\u8d44\u884c\u4e3a\u968f\u8d22\u5bcc\u589e\u52a0\u800c\u589e\u52a0\uff0c\u4f46\u53d7\u52b3\u52a8\u4f9b\u7ed9\u9608\u503c\u9650\u5236\uff1b\u9000\u4f11\u540e\u6d88\u8d39\u8d22\u5bcc\u6bd4\u4f8b\u589e\u52a0\u3002\u5e74\u91d1\u5316\u51b3\u7b56\u53d7\u4ee3\u7406\u4eba\u4e2a\u4eba\u8d34\u73b0\u7387\uff08\u9700\u6c42\u4fa7\uff09\u548c\u4fdd\u9669\u516c\u53f8\u652f\u4ed8\u7387\uff08\u4f9b\u7ed9\u4fa7\uff09\u5171\u540c\u5f71\u54cd\uff0c\u9ad8\u9f84\u65f6\u6b7b\u4ea1\u7387\u6025\u5267\u4e0b\u964d\u4f7f\u5e74\u91d1\u5316\u6210\u4e3a\u6700\u4f18\u9009\u62e9\u3002", "conclusion": "\u9000\u4f11\u540e\u52b3\u52a8\u6536\u5165\u53ef\u4f5c\u4e3a\u5e74\u91d1\u5316\u7684\u76f4\u63a5\u66ff\u4ee3\u54c1\uff0c\u63d0\u4f9b\u7a33\u5b9a\u7684\u6536\u5165\u6765\u6e90\uff0c\u589e\u5f3a\u9000\u4f11\u4eba\u5458\u7684\u8d22\u52a1\u5b89\u5168\u6027\u3002\u8d85\u8fc7\u4e00\u5b9a\u5e74\u9f84\u540e\uff0c\u6b7b\u4ea1\u7387\u4f18\u60e0\u7684\u5f3a\u5927\u6fc0\u52b1\u4f1a\u8d85\u8fc7\u4ee3\u7406\u4eba\u7684\u9ad8\u4e2a\u4eba\u8d34\u73b0\u7387\uff0c\u4f7f\u5e74\u91d1\u5316\u6210\u4e3a\u5b9e\u8d28\u6027\u6700\u4f18\u9009\u62e9\u3002"}}
{"id": "2510.09671", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09671", "abs": "https://arxiv.org/abs/2510.09671", "authors": ["Wei Zhou", "Bolei Ma", "Annemarie Friedrich", "Mohsen Mesgar"], "title": "Table Question Answering in the Era of Large Language Models: A Comprehensive Survey of Tasks, Methods, and Evaluation", "comment": null, "summary": "Table Question Answering (TQA) aims to answer natural language questions\nabout tabular data, often accompanied by additional contexts such as text\npassages. The task spans diverse settings, varying in table representation,\nquestion/answer complexity, modality involved, and domain. While recent\nadvances in large language models (LLMs) have led to substantial progress in\nTQA, the field still lacks a systematic organization and understanding of task\nformulations, core challenges, and methodological trends, particularly in light\nof emerging research directions such as reinforcement learning. This survey\naddresses this gap by providing a comprehensive and structured overview of TQA\nresearch with a focus on LLM-based methods. We provide a comprehensive\ncategorization of existing benchmarks and task setups. We group current\nmodeling strategies according to the challenges they target, and analyze their\nstrengths and limitations. Furthermore, we highlight underexplored but timely\ntopics that have not been systematically covered in prior research. By unifying\ndisparate research threads and identifying open problems, our survey offers a\nconsolidated foundation for the TQA community, enabling a deeper understanding\nof the state of the art and guiding future developments in this rapidly\nevolving area.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u5173\u4e8e\u8868\u683c\u95ee\u7b54(TQA)\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u91cd\u70b9\u5173\u6ce8\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5bf9\u4efb\u52a1\u8bbe\u7f6e\u3001\u6838\u5fc3\u6311\u6218\u548c\u65b9\u6cd5\u8d8b\u52bf\u8fdb\u884c\u4e86\u5168\u9762\u68b3\u7406\u3002", "motivation": "\u8868\u683c\u95ee\u7b54\u9886\u57df\u867d\u7136\u5728\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u52a8\u4e0b\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4efb\u52a1\u5b9a\u4e49\u3001\u6838\u5fc3\u6311\u6218\u548c\u65b9\u6cd5\u8d8b\u52bf\u7684\u7cfb\u7edf\u6027\u7ec4\u7ec7\u4e0e\u7406\u89e3\uff0c\u7279\u522b\u662f\u5728\u5f3a\u5316\u5b66\u4e60\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u65b9\u9762\u3002", "method": "\u63d0\u4f9bTQA\u7814\u7a76\u7684\u5168\u9762\u7ed3\u6784\u5316\u6982\u89c8\uff0c\u5bf9\u73b0\u6709\u57fa\u51c6\u548c\u4efb\u52a1\u8bbe\u7f6e\u8fdb\u884c\u5206\u7c7b\uff0c\u6309\u76ee\u6807\u6311\u6218\u5bf9\u5f53\u524d\u5efa\u6a21\u7b56\u7565\u8fdb\u884c\u5206\u7ec4\uff0c\u5e76\u5206\u6790\u5176\u4f18\u7f3a\u70b9\u3002", "result": "\u5efa\u7acb\u4e86TQA\u7814\u7a76\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u672a\u5145\u5206\u63a2\u7d22\u4f46\u53ca\u65f6\u7684\u7814\u7a76\u4e3b\u9898\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u6574\u5408\u57fa\u7840\u3002", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u5206\u6563\u7684\u7814\u7a76\u7ebf\u7d22\u548c\u8bc6\u522b\u5f00\u653e\u95ee\u9898\uff0c\u672c\u7efc\u8ff0\u4e3aTQA\u793e\u533a\u63d0\u4f9b\u4e86\u5de9\u56fa\u7684\u57fa\u7840\uff0c\u4f7f\u80fd\u66f4\u6df1\u5165\u7406\u89e3\u5f53\u524d\u6280\u672f\u6c34\u5e73\u5e76\u6307\u5bfc\u8fd9\u4e00\u5feb\u901f\u53d1\u5c55\u9886\u57df\u7684\u672a\u6765\u53d1\u5c55\u3002"}}
{"id": "2510.11677", "categories": ["cs.LG", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2510.11677", "abs": "https://arxiv.org/abs/2510.11677", "authors": ["Songrun He", "Linying Lv", "Asaf Manela", "Jimmy Wu"], "title": "Chronologically Consistent Generative AI", "comment": null, "summary": "We introduce a family of chronologically consistent, instruction-following\nlarge language models to eliminate lookahead bias. Each model is trained only\non data available before a clearly defined knowledge-cutoff date, ensuring\nstrict temporal separation from any post-cutoff data. The resulting framework\noffers (i) a simple, conversational chat interface, (ii) fully open, fixed\nmodel weights that guarantee replicability, and (iii) a conservative lower\nbound on forecast accuracy, isolating the share of predictability that survives\nonce training leakage is removed. Together, these features provide researchers\nwith an easy-to-use generative AI tool useful for a wide range of prediction\ntasks that is free of lookahead bias.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u65f6\u95f4\u4e00\u81f4\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u8bbe\u5b9a\u77e5\u8bc6\u622a\u6b62\u65e5\u671f\u6765\u6d88\u9664\u524d\u77bb\u504f\u5dee\uff0c\u786e\u4fdd\u6a21\u578b\u4ec5\u4f7f\u7528\u622a\u6b62\u65e5\u671f\u524d\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "motivation": "\u6d88\u9664\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u524d\u77bb\u504f\u5dee\uff0c\u786e\u4fdd\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u4e0e\u9884\u6d4b\u65f6\u95f4\u70b9\u7684\u4e25\u683c\u5206\u79bb\uff0c\u63d0\u4f9b\u65e0\u6570\u636e\u6cc4\u9732\u7684\u9884\u6d4b\u5de5\u5177\u3002", "method": "\u8bad\u7ec3\u4ec5\u4f7f\u7528\u622a\u6b62\u65e5\u671f\u524d\u6570\u636e\u7684\u6307\u4ee4\u8ddf\u968f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u56fa\u5b9a\u6a21\u578b\u6743\u91cd\u548c\u5bf9\u8bdd\u5f0f\u804a\u5929\u754c\u9762\u3002", "result": "\u5f00\u53d1\u51fa\u5177\u6709\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u53ef\u590d\u73b0\u6027\u7684\u751f\u6210AI\u5de5\u5177\uff0c\u80fd\u591f\u4e3a\u5404\u79cd\u9884\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4fdd\u5b88\u7684\u51c6\u786e\u6027\u4e0b\u9650\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6613\u4e8e\u4f7f\u7528\u3001\u65e0\u524d\u77bb\u504f\u5dee\u7684\u751f\u6210AI\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u9884\u6d4b\u4efb\u52a1\uff0c\u5e76\u786e\u4fdd\u9884\u6d4b\u7ed3\u679c\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2510.09643", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09643", "abs": "https://arxiv.org/abs/2510.09643", "authors": ["Yuguang Liu", "Yiyun Miao", "Luyao Xia"], "title": "Direct Routing Gradient (DRGrad): A Personalized Information Surgery for Multi-Task Learning (MTL) Recommendations", "comment": null, "summary": "Multi-task learning (MTL) has emerged as a successful strategy in\nindustrial-scale recommender systems, offering significant advantages such as\ncapturing diverse users' interests and accurately detecting different behaviors\nlike ``click\" or ``dwell time\". However, negative transfer and the seesaw\nphenomenon pose challenges to MTL models due to the complex and often\ncontradictory task correlations in real-world recommendations. To address the\nproblem while making better use of personalized information, we propose a\npersonalized Direct Routing Gradient framework (DRGrad), which consists of\nthree key components: router, updater and personalized gate network. DRGrad\njudges the stakes between tasks in the training process, which can leverage all\nvalid gradients for the respective task to reduce conflicts. We evaluate the\nefficiency of DRGrad on complex MTL using a real-world recommendation dataset\nwith 15 billion samples. The results show that DRGrad's superior performance\nover competing state-of-the-art MTL models, especially in terms of AUC (Area\nUnder the Curve) metrics, indicating that it effectively manages task conflicts\nin multi-task learning environments without increasing model complexity, while\nalso addressing the deficiencies in noise processing. Moreover, experiments on\nthe public Census-income dataset and Synthetic dataset, have demonstrated the\ncapability of DRGrad in judging and routing the stakes between tasks with\nvarying degrees of correlation and personalization.", "AI": {"tldr": "\u63d0\u51faDRGrad\u6846\u67b6\u89e3\u51b3\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u8d1f\u8fc1\u79fb\u548c\u8df7\u8df7\u677f\u73b0\u8c61\uff0c\u901a\u8fc7\u8def\u7531\u5668\u3001\u66f4\u65b0\u5668\u548c\u4e2a\u6027\u5316\u95e8\u7f51\u7edc\u6765\u7ba1\u7406\u4efb\u52a1\u95f4\u51b2\u7a81\uff0c\u5728150\u4ebf\u6837\u672c\u7684\u771f\u5b9e\u63a8\u8350\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u4efb\u52a1\u5b66\u4e60\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u9762\u4e34\u8d1f\u8fc1\u79fb\u548c\u8df7\u8df7\u677f\u73b0\u8c61\u7684\u6311\u6218\uff0c\u7531\u4e8e\u73b0\u5b9e\u63a8\u8350\u4e2d\u4efb\u52a1\u76f8\u5173\u6027\u590d\u6742\u4e14\u77db\u76fe\uff0c\u9700\u8981\u66f4\u597d\u5730\u5229\u7528\u4e2a\u6027\u5316\u4fe1\u606f\u6765\u7ba1\u7406\u4efb\u52a1\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u4e2a\u6027\u5316\u76f4\u63a5\u8def\u7531\u68af\u5ea6\u6846\u67b6(DRGrad)\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u8def\u7531\u5668\u3001\u66f4\u65b0\u5668\u548c\u4e2a\u6027\u5316\u95e8\u7f51\u7edc\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5224\u65ad\u4efb\u52a1\u95f4\u5229\u76ca\u5173\u7cfb\uff0c\u5229\u7528\u6240\u6709\u6709\u6548\u68af\u5ea6\u51cf\u5c11\u51b2\u7a81\u3002", "result": "\u5728150\u4ebf\u6837\u672c\u7684\u771f\u5b9e\u63a8\u8350\u6570\u636e\u96c6\u4e0a\uff0cDRGrad\u5728AUC\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u578b\uff0c\u80fd\u6709\u6548\u7ba1\u7406\u4efb\u52a1\u51b2\u7a81\u4e14\u4e0d\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u5728\u516c\u5171Census-income\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u4e5f\u9a8c\u8bc1\u4e86\u5176\u5224\u65ad\u548c\u8def\u7531\u80fd\u529b\u3002", "conclusion": "DRGrad\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u4efb\u52a1\u51b2\u7a81\u95ee\u9898\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u8def\u7531\u673a\u5236\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4e0d\u540c\u76f8\u5173\u6027\u548c\u4e2a\u6027\u5316\u7a0b\u5ea6\u7684\u4efb\u52a1\u65f6\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.09782", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.09782", "abs": "https://arxiv.org/abs/2510.09782", "authors": ["Yufa Zhou", "Yixiao Wang", "Xunjian Yin", "Shuyan Zhou", "Anru R. Zhang"], "title": "The Geometry of Reasoning: Flowing Logics in Representation Space", "comment": "Code: https://github.com/MasterZhou1/Reasoning-Flow", "summary": "We study how large language models (LLMs) ``think'' through their\nrepresentation space. We propose a novel geometric framework that models an\nLLM's reasoning as flows -- embedding trajectories evolving where logic goes.\nWe disentangle logical structure from semantics by employing the same natural\ndeduction propositions with varied semantic carriers, allowing us to test\nwhether LLMs internalize logic beyond surface form. This perspective connects\nreasoning with geometric quantities such as position, velocity, and curvature,\nenabling formal analysis in representation and concept spaces. Our theory\nestablishes: (1) LLM reasoning corresponds to smooth flows in representation\nspace, and (2) logical statements act as local controllers of these flows'\nvelocities. Using learned representation proxies, we design controlled\nexperiments to visualize and quantify reasoning flows, providing empirical\nvalidation of our theoretical framework. Our work serves as both a conceptual\nfoundation and practical tools for studying reasoning phenomenon, offering a\nnew lens for interpretability and formal analysis of LLMs' behavior.", "AI": {"tldr": "\u63d0\u51fa\u51e0\u4f55\u6846\u67b6\u5c06LLM\u63a8\u7406\u5efa\u6a21\u4e3a\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u6d41\uff0c\u901a\u8fc7\u5206\u79bb\u903b\u8f91\u7ed3\u6784\u548c\u8bed\u4e49\u6765\u7814\u7a76LLM\u662f\u5426\u5185\u5316\u4e86\u903b\u8f91\u800c\u4e0d\u4ec5\u4ec5\u662f\u8868\u9762\u5f62\u5f0f\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\"\u601d\u8003\"\uff0c\u63a2\u7d22\u5176\u662f\u5426\u771f\u6b63\u5185\u5316\u4e86\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u51e0\u4f55\u6846\u67b6\u5c06\u63a8\u7406\u5efa\u6a21\u4e3a\u5d4c\u5165\u8f68\u8ff9\u6d41\uff0c\u901a\u8fc7\u81ea\u7136\u6f14\u7ece\u547d\u9898\u5206\u79bb\u903b\u8f91\u548c\u8bed\u4e49\uff0c\u7528\u8868\u793a\u4ee3\u7406\u8bbe\u8ba1\u53d7\u63a7\u5b9e\u9a8c\u6765\u53ef\u89c6\u5316\u548c\u91cf\u5316\u63a8\u7406\u6d41\u3002", "result": "\u7406\u8bba\u8bc1\u660eLLM\u63a8\u7406\u5bf9\u5e94\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u5e73\u6ed1\u6d41\uff0c\u903b\u8f91\u8bed\u53e5\u4f5c\u4e3a\u8fd9\u4e9b\u6d41\u901f\u7684\u5c40\u90e8\u63a7\u5236\u5668\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u6846\u67b6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7814\u7a76\u63a8\u7406\u73b0\u8c61\u63d0\u4f9b\u4e86\u6982\u5ff5\u57fa\u7840\u548c\u5b9e\u8df5\u5de5\u5177\uff0c\u4e3aLLM\u884c\u4e3a\u53ef\u89e3\u91ca\u6027\u548c\u5f62\u5f0f\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2510.10171", "categories": ["q-fin.RM"], "pdf": "https://arxiv.org/pdf/2510.10171", "abs": "https://arxiv.org/abs/2510.10171", "authors": ["Alexander McFarlane"], "title": "Toxicity Bounds for Dynamic Liquidation Incentives", "comment": null, "summary": "We derive a slippage-aware toxicity condition for on-chain liquidations\nexecuted via a constant-product automated market maker (CP-AMM). For a fixed\n(constant) liquidation incentive $i$, the familiar toxicity frontier $\\nu <\n1/(1+i)$ tightens to $\\nu < 1/((1+i)\\lambda)$ for a liquidity penalty factor\n$\\lambda$ that we derive for both the CP-AMM and a generalised form. Using a\ndynamic health-linked liquidation incentive $i(h) = i(1 - h)$, we obtain a\nstate-dependent bound and, at the liquidation boundary, a liquidity-depth-only\ncondition $\\nu < 1/\\lambda$. This reconciles dynamic incentives with the impact\nof the CP-AMM price and clarifies when dynamic liquidation incentives reduce\nversus exacerbate spiral risk.", "AI": {"tldr": "\u672c\u6587\u63a8\u5bfc\u4e86\u5728\u6052\u5b9a\u4e58\u79ef\u81ea\u52a8\u505a\u5e02\u5546(CP-AMM)\u4e0a\u8fdb\u884c\u94fe\u4e0a\u6e05\u7b97\u65f6\u7684\u6ed1\u70b9\u611f\u77e5\u6bd2\u6027\u6761\u4ef6\uff0c\u63d0\u51fa\u4e86\u8003\u8651\u6d41\u52a8\u6027\u60e9\u7f5a\u56e0\u5b50\u7684\u6bd2\u6027\u8fb9\u754c\u6536\u7d27\u6761\u4ef6\uff0c\u5e76\u5206\u6790\u4e86\u52a8\u6001\u6e05\u7b97\u6fc0\u52b1\u5bf9\u87ba\u65cb\u98ce\u9669\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u5728CP-AMM\u4e0a\u8fdb\u884c\u94fe\u4e0a\u6e05\u7b97\u65f6\uff0c\u5982\u4f55\u8003\u8651\u6ed1\u70b9\u5f71\u54cd\u6765\u6539\u8fdb\u6bd2\u6027\u6761\u4ef6\uff0c\u5e76\u63a2\u8ba8\u52a8\u6001\u6e05\u7b97\u6fc0\u52b1\u5bf9\u87ba\u65cb\u98ce\u9669\u7684\u5f71\u54cd\u673a\u5236\u3002", "method": "\u63a8\u5bfc\u4e86\u8003\u8651\u6d41\u52a8\u6027\u60e9\u7f5a\u56e0\u5b50\u03bb\u7684\u6bd2\u6027\u8fb9\u754c\u6761\u4ef6\uff0c\u5206\u6790\u4e86\u56fa\u5b9a\u6e05\u7b97\u6fc0\u52b1\u548c\u52a8\u6001\u5065\u5eb7\u76f8\u5173\u6e05\u7b97\u6fc0\u52b1\u4e24\u79cd\u60c5\u51b5\u4e0b\u7684\u6bd2\u6027\u6761\u4ef6\u3002", "result": "\u5bf9\u4e8e\u56fa\u5b9a\u6e05\u7b97\u6fc0\u52b1i\uff0c\u6bd2\u6027\u8fb9\u754c\u4ece\u03bd < 1/(1+i)\u6536\u7d27\u4e3a\u03bd < 1/((1+i)\u03bb)\uff1b\u5bf9\u4e8e\u52a8\u6001\u6e05\u7b97\u6fc0\u52b1i(h)=i(1-h)\uff0c\u5728\u6e05\u7b97\u8fb9\u754c\u5f97\u5230\u4ec5\u4f9d\u8d56\u6d41\u52a8\u6027\u6df1\u5ea6\u7684\u6761\u4ef6\u03bd < 1/\u03bb\u3002", "conclusion": "\u8be5\u7814\u7a76\u9610\u660e\u4e86\u52a8\u6001\u6e05\u7b97\u6fc0\u52b1\u5728\u51cf\u5c11\u8fd8\u662f\u52a0\u5267\u87ba\u65cb\u98ce\u9669\u65b9\u9762\u7684\u4f5c\u7528\uff0c\u4e3aCP-AMM\u6e05\u7b97\u673a\u5236\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2510.10324", "categories": ["stat.ML", "cs.LG", "62G99", "I.1.2"], "pdf": "https://arxiv.org/pdf/2510.10324", "abs": "https://arxiv.org/abs/2510.10324", "authors": ["Liang Hong", "Noura Raydan Nasreddine"], "title": "On some practical challenges of conformal prediction", "comment": null, "summary": "Conformal prediction is a model-free machine learning method for creating\nprediction regions with a guaranteed coverage probability level. However, a\ndata scientist often faces three challenges in practice: (i) the determination\nof a conformal prediction region is only approximate, jeopardizing the\nfinite-sample validity of prediction, (ii) the computation required could be\nprohibitively expensive, and (iii) the shape of a conformal prediction region\nis hard to control. This article offers new insights into the relationship\namong the monotonicity of the non-conformity measure, the monotonicity of the\nplausibility function, and the exact determination of a conformal prediction\nregion. Based on these new insights, we propose a simple strategy to alleviate\nthe three challenges simultaneously.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u89e3\u51b3\u5171\u5f62\u9884\u6d4b\u4e2d\u4e09\u4e2a\u5b9e\u8df5\u6311\u6218\u7684\u65b0\u7b56\u7565\uff1a\u8fd1\u4f3c\u786e\u5b9a\u6027\u95ee\u9898\u3001\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u95ee\u9898\u4ee5\u53ca\u9884\u6d4b\u533a\u57df\u5f62\u72b6\u96be\u4ee5\u63a7\u5236\u95ee\u9898\u3002", "motivation": "\u6570\u636e\u79d1\u5b66\u5bb6\u5728\u5b9e\u8df5\u4e2d\u9762\u4e34\u4e09\u4e2a\u6311\u6218\uff1a(i)\u5171\u5f62\u9884\u6d4b\u533a\u57df\u7684\u786e\u5b9a\u53ea\u662f\u8fd1\u4f3c\u7684\uff0c\u5371\u53ca\u9884\u6d4b\u7684\u6709\u9650\u6837\u672c\u6709\u6548\u6027\uff1b(ii)\u6240\u9700\u8ba1\u7b97\u53ef\u80fd\u8fc7\u4e8e\u6602\u8d35\uff1b(iii)\u5171\u5f62\u9884\u6d4b\u533a\u57df\u7684\u5f62\u72b6\u96be\u4ee5\u63a7\u5236\u3002", "method": "\u57fa\u4e8e\u5bf9\u975e\u4e00\u81f4\u6027\u5ea6\u91cf\u7684\u5355\u8c03\u6027\u3001\u5408\u7406\u6027\u51fd\u6570\u5355\u8c03\u6027\u548c\u5171\u5f62\u9884\u6d4b\u533a\u57df\u7cbe\u786e\u786e\u5b9a\u4e4b\u95f4\u5173\u7cfb\u7684\u65b0\u89c1\u89e3\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u7b56\u7565\u3002", "result": "\u8be5\u7b56\u7565\u80fd\u591f\u540c\u65f6\u7f13\u89e3\u4e0a\u8ff0\u4e09\u4e2a\u6311\u6218\uff0c\u63d0\u9ad8\u5171\u5f62\u9884\u6d4b\u7684\u5b9e\u7528\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u6df1\u5165\u7406\u89e3\u5171\u5f62\u9884\u6d4b\u4e2d\u7684\u5355\u8c03\u6027\u5173\u7cfb\uff0c\u53ef\u4ee5\u5f00\u53d1\u51fa\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2510.10526", "categories": ["q-fin.CP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10526", "abs": "https://arxiv.org/abs/2510.10526", "authors": ["Wo Long", "Wenxin Zeng", "Xiaoyu Zhang", "Ziyao Zhou"], "title": "Integrating Large Language Models and Reinforcement Learning for Sentiment-Driven Quantitative Trading", "comment": null, "summary": "This research develops a sentiment-driven quantitative trading system that\nleverages a large language model, FinGPT, for sentiment analysis, and explores\na novel method for signal integration using a reinforcement learning algorithm,\nTwin Delayed Deep Deterministic Policy Gradient (TD3). We compare the\nperformance of strategies that integrate sentiment and technical signals using\nboth a conventional rule-based approach and a reinforcement learning framework.\nThe results suggest that sentiment signals generated by FinGPT offer value when\ncombined with traditional technical indicators, and that reinforcement learning\nalgorithm presents a promising approach for effectively integrating\nheterogeneous signals in dynamic trading environments.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eFinGPT\u60c5\u611f\u5206\u6790\u548cTD3\u5f3a\u5316\u5b66\u4e60\u7684\u91cf\u5316\u4ea4\u6613\u7cfb\u7edf\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u89c4\u5219\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u4fe1\u53f7\u6574\u5408\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u6709\u6548\u6574\u5408\u60c5\u611f\u4fe1\u53f7\u548c\u6280\u672f\u6307\u6807\u6765\u6539\u8fdb\u91cf\u5316\u4ea4\u6613\u7b56\u7565\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u4ea4\u6613\u73af\u5883\u4e2d\u3002", "method": "\u4f7f\u7528FinGPT\u8fdb\u884c\u60c5\u611f\u5206\u6790\uff0c\u91c7\u7528TD3\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6574\u5408\u60c5\u611f\u548c\u6280\u672f\u4fe1\u53f7\uff0c\u5e76\u4e0e\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "FinGPT\u751f\u6210\u7684\u60c5\u611f\u4fe1\u53f7\u4e0e\u4f20\u7edf\u6280\u672f\u6307\u6807\u7ed3\u5408\u5177\u6709\u4ef7\u503c\uff0c\u5f3a\u5316\u5b66\u4e60\u5728\u52a8\u6001\u4ea4\u6613\u73af\u5883\u4e2d\u80fd\u6709\u6548\u6574\u5408\u5f02\u8d28\u4fe1\u53f7\u3002", "conclusion": "\u60c5\u611f\u5206\u6790\u4e3a\u91cf\u5316\u4ea4\u6613\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u4fe1\u606f\uff0c\u5f3a\u5316\u5b66\u4e60\u662f\u6574\u5408\u5f02\u8d28\u4fe1\u53f7\u7684\u6709\u524d\u666f\u65b9\u6cd5\u3002"}}
{"id": "2510.09810", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.09810", "abs": "https://arxiv.org/abs/2510.09810", "authors": ["Victor Freire", "Marco M. Nicotra"], "title": "Designing Control Barrier Functions Using a Dynamic Backup Policy", "comment": "7 pages, 1 figure", "summary": "This paper presents a systematic approach to construct control barrier\nfunctions for nonlinear control affine systems subject to arbitrary state and\ninput constraints. Taking inspiration from the reference governor literature,\nthe proposed method defines a family of backup policies, parametrized by the\nequilibrium manifold of the system. The control barrier function is defined on\nthe augmented state-and-reference space: given a state-reference pair, the\napproach quantifies the distance to constraint violation at any time in the\nfuture, should the current backup policy reference remain constant. Sensitivity\nanalysis is then used to compute the (possibly nonsmooth) Jacobian with respect\nto the augmented state vector. To showcase its simple yet general nature, the\nproposed method is applied to an inverted pendulum on cart.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3a\u975e\u7ebf\u6027\u63a7\u5236\u4eff\u5c04\u7cfb\u7edf\u6784\u5efa\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u72b6\u6001\u548c\u8f93\u5165\u7ea6\u675f\u3002", "motivation": "\u53d7\u53c2\u8003\u8c03\u8282\u5668\u6587\u732e\u542f\u53d1\uff0c\u65e8\u5728\u4e3a\u53d7\u7ea6\u675f\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u63d0\u4f9b\u901a\u7528\u7684\u5b89\u5168\u4fdd\u8bc1\u6846\u67b6\u3002", "method": "\u5b9a\u4e49\u57fa\u4e8e\u7cfb\u7edf\u5e73\u8861\u6d41\u5f62\u53c2\u6570\u5316\u7684\u5907\u4efd\u7b56\u7565\u65cf\uff0c\u5728\u589e\u5e7f\u72b6\u6001-\u53c2\u8003\u7a7a\u95f4\u4e2d\u5b9a\u4e49\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff0c\u901a\u8fc7\u654f\u611f\u6027\u5206\u6790\u8ba1\u7b97\u96c5\u53ef\u6bd4\u77e9\u9635\u3002", "result": "\u65b9\u6cd5\u5e94\u7528\u4e8e\u5012\u7acb\u6446\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u5176\u7b80\u5355\u800c\u901a\u7528\u7684\u7279\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u975e\u7ebf\u6027\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u5b89\u5168\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u6784\u9020\u65b9\u6cd5\uff0c\u5177\u6709\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.10946", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2510.10946", "abs": "https://arxiv.org/abs/2510.10946", "authors": ["Onil Boussim"], "title": "Identifying treatment effects on categorical outcomes in IV models", "comment": null, "summary": "This paper provides a nonparametric framework for causal inference with\ncategorical outcomes under binary treatment and binary instrument settings. We\ndecompose the observed joint probability of outcomes and treatment into\nmarginal probabilities of potential outcomes and treatment, and association\nparameters that capture selection bias due to unobserved heterogeneity. Under a\nnovel identifying assumption, association similarity, which requires the\ndependence between unobserved factors and potential outcomes to be invariant\nacross treatment states, we achieve point identification of the full\ndistribution of potential outcomes. Recognizing that this assumption may be\nstrong in some contexts, we propose two weaker alternatives: monotonic\nassociation, which restricts the direction of selection heterogeneity, and\nbounded association, which constrains its magnitude. These relaxed assumptions\ndeliver sharp partial identification bounds that nest point identification as a\nspecial case and facilitate transparent sensitivity analysis. We illustrate the\nframework in an empirical application, estimating the causal effect of private\nhealth insurance on health outcomes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u975e\u53c2\u6570\u56e0\u679c\u63a8\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u4e8c\u5143\u5904\u7406\u548c\u4e8c\u5143\u5de5\u5177\u53d8\u91cf\u8bbe\u7f6e\u4e0b\u7684\u5206\u7c7b\u7ed3\u679c\u5206\u6790\u3002\u901a\u8fc7\u5206\u89e3\u89c2\u6d4b\u6982\u7387\u4e3a\u8fb9\u9645\u6982\u7387\u548c\u5173\u8054\u53c2\u6570\uff0c\u5728\u5173\u8054\u76f8\u4f3c\u6027\u5047\u8bbe\u4e0b\u5b9e\u73b0\u70b9\u8bc6\u522b\uff0c\u5e76\u63d0\u4f9b\u4e24\u79cd\u5f31\u5316\u5047\u8bbe\u8fdb\u884c\u90e8\u5206\u8bc6\u522b\u548c\u654f\u611f\u6027\u5206\u6790\u3002", "motivation": "\u5728\u4e8c\u5143\u5904\u7406\u548c\u5de5\u5177\u53d8\u91cf\u8bbe\u7f6e\u4e0b\uff0c\u5bf9\u5206\u7c7b\u7ed3\u679c\u8fdb\u884c\u56e0\u679c\u63a8\u65ad\u65f6\uff0c\u9700\u8981\u5904\u7406\u56e0\u672a\u89c2\u6d4b\u5f02\u8d28\u6027\u5bfc\u81f4\u7684\u9009\u62e9\u504f\u5dee\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u4f9d\u8d56\u8fc7\u5f3a\u7684\u8bc6\u522b\u5047\u8bbe\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u4e14\u900f\u660e\u7684\u6846\u67b6\u3002", "method": "\u5c06\u89c2\u6d4b\u7684\u8054\u5408\u6982\u7387\u5206\u89e3\u4e3a\u6f5c\u5728\u7ed3\u679c\u7684\u8fb9\u9645\u6982\u7387\u3001\u5904\u7406\u7684\u8fb9\u9645\u6982\u7387\u4ee5\u53ca\u6355\u83b7\u9009\u62e9\u504f\u5dee\u7684\u5173\u8054\u53c2\u6570\u3002\u63d0\u51fa\u5173\u8054\u76f8\u4f3c\u6027\u5047\u8bbe\u5b9e\u73b0\u70b9\u8bc6\u522b\uff0c\u5e76\u5f15\u5165\u5355\u8c03\u5173\u8054\u548c\u6709\u754c\u5173\u8054\u4e24\u79cd\u5f31\u5316\u5047\u8bbe\u8fdb\u884c\u90e8\u5206\u8bc6\u522b\u3002", "result": "\u5728\u5173\u8054\u76f8\u4f3c\u6027\u5047\u8bbe\u4e0b\u5b9e\u73b0\u4e86\u6f5c\u5728\u7ed3\u679c\u5b8c\u6574\u5206\u5e03\u7684\u70b9\u8bc6\u522b\u3002\u901a\u8fc7\u5f31\u5316\u5047\u8bbe\u83b7\u5f97\u4e86\u5c16\u9510\u7684\u90e8\u5206\u8bc6\u522b\u8fb9\u754c\uff0c\u5c06\u70b9\u8bc6\u522b\u4f5c\u4e3a\u7279\u4f8b\u5305\u542b\u5176\u4e2d\uff0c\u4fbf\u4e8e\u8fdb\u884c\u900f\u660e\u7684\u654f\u611f\u6027\u5206\u6790\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e8c\u5143\u5904\u7406\u548c\u5de5\u5177\u53d8\u91cf\u8bbe\u7f6e\u4e0b\u7684\u5206\u7c7b\u7ed3\u679c\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u975e\u53c2\u6570\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0d\u540c\u5f3a\u5ea6\u7684\u8bc6\u522b\u5047\u8bbe\u5e73\u8861\u8bc6\u522b\u80fd\u529b\u548c\u73b0\u5b9e\u53ef\u884c\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u5e94\u7528\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.09636", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09636", "abs": "https://arxiv.org/abs/2510.09636", "authors": ["Prarthana P. Kartholy", "Thandi M. Labor", "Neil N. Panchal", "Sean H. Wang", "Hillary N. Owusu"], "title": "Bias-Aware AI Chatbot for Engineering Advising at the University of Maryland A. James Clark School of Engineering", "comment": null, "summary": "Selecting a college major is a difficult decision for many incoming freshmen.\nTraditional academic advising is often hindered by long wait times,\nintimidating environments, and limited personalization. AI Chatbots present an\nopportunity to address these challenges. However, AI systems also have the\npotential to generate biased responses, prejudices related to race, gender,\nsocioeconomic status, and disability. These biases risk turning away potential\nstudents and undermining reliability of AI systems. This study aims to develop\na University of Maryland (UMD) A. James Clark School of Engineering\nProgram-specific AI chatbot. Our research team analyzed and mitigated potential\nbiases in the responses. Through testing the chatbot on diverse student\nqueries, the responses are scored on metrics of accuracy, relevance,\npersonalization, and bias presence. The results demonstrate that with careful\nprompt engineering and bias mitigation strategies, AI chatbots can provide\nhigh-quality, unbiased academic advising support, achieving mean scores of 9.76\nfor accuracy, 9.56 for relevance, and 9.60 for personalization with no\nstereotypical biases found in the sample data. However, due to the small sample\nsize and limited timeframe, our AI model may not fully reflect the nuances of\nstudent queries in engineering academic advising. Regardless, these findings\nwill inform best practices for building ethical AI systems in higher education,\noffering tools to complement traditional advising and address the inequities\nfaced by many underrepresented and first-generation college students.", "AI": {"tldr": "\u5f00\u53d1\u9a6c\u91cc\u5170\u5927\u5b66\u5de5\u7a0b\u5b66\u9662\u4e13\u7528\u7684AI\u804a\u5929\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u5de5\u7a0b\u548c\u504f\u89c1\u7f13\u89e3\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u65e0\u504f\u89c1\u7684\u5b66\u672f\u54a8\u8be2\u652f\u6301\u3002", "motivation": "\u4f20\u7edf\u5b66\u672f\u54a8\u8be2\u5b58\u5728\u7b49\u5f85\u65f6\u95f4\u957f\u3001\u73af\u5883\u538b\u529b\u5927\u3001\u4e2a\u6027\u5316\u4e0d\u8db3\u7b49\u95ee\u9898\uff0cAI\u804a\u5929\u673a\u5668\u4eba\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u4f46\u9700\u8981\u89e3\u51b3AI\u7cfb\u7edf\u53ef\u80fd\u4ea7\u751f\u7684\u504f\u89c1\u95ee\u9898\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5206\u6790\u5e76\u7f13\u89e3\u4e86AI\u56de\u590d\u4e2d\u7684\u6f5c\u5728\u504f\u89c1\uff0c\u901a\u8fc7\u6d4b\u8bd5\u804a\u5929\u673a\u5668\u4eba\u5728\u591a\u6837\u5316\u5b66\u751f\u67e5\u8be2\u4e0a\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u5176\u51c6\u786e\u6027\u3001\u76f8\u5173\u6027\u3001\u4e2a\u6027\u5316\u548c\u504f\u89c1\u5b58\u5728\u60c5\u51b5\u3002", "result": "AI\u804a\u5929\u673a\u5668\u4eba\u53d6\u5f97\u4e86\u5e73\u57479.76\u7684\u51c6\u786e\u6027\u5f97\u5206\u30019.56\u7684\u76f8\u5173\u6027\u5f97\u5206\u548c9.60\u7684\u4e2a\u6027\u5316\u5f97\u5206\uff0c\u6837\u672c\u6570\u636e\u4e2d\u672a\u53d1\u73b0\u523b\u677f\u5370\u8c61\u504f\u89c1\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u5de5\u7a0b\u548c\u504f\u89c1\u7f13\u89e3\u7b56\u7565\uff0cAI\u804a\u5929\u673a\u5668\u4eba\u80fd\u591f\u63d0\u4f9b\u9ad8\u8d28\u91cf\u3001\u65e0\u504f\u89c1\u7684\u5b66\u672f\u54a8\u8be2\u652f\u6301\uff0c\u4e3a\u9ad8\u7b49\u6559\u80b2\u4e2d\u6784\u5efa\u4f26\u7406AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6700\u4f73\u5b9e\u8df5\u53c2\u8003\u3002"}}
{"id": "2510.11261", "categories": ["q-fin.MF", "econ.GN", "q-fin.EC", "q-fin.PM", "91A16, 91B52, 93E20"], "pdf": "https://arxiv.org/pdf/2510.11261", "abs": "https://arxiv.org/abs/2510.11261", "authors": ["Masaaki Fujii"], "title": "Mean-Field Price Formation on Trees", "comment": "28 pages, 13 figures", "summary": "In this work, we combine the mean-field game theory with the classical idea\nof binomial tree framework, pioneered by Sharpe and Cox, Ross & Rubinstein, to\nsolve the equilibrium price formation problem for the stock. For agents with\nexponential utilities and recursive utilities of exponential type, we prove the\nexistence of a unique mean-field equilibrium and derive an explicit formula for\nequilibrium transition probabilities of the stock price by restricting its\ntrajectories onto a binomial tree. The agents are subject to stochastic\nterminal liabilities and incremental endowments, both of which are dependent on\nunhedgeable common and idiosyncratic factors, in addition to the stock price\npath. Finally, we provide numerical examples to illustrate the qualitative\neffects of these components on the equilibrium price distribution.", "AI": {"tldr": "\u5c06\u5747\u503c\u573a\u535a\u5f08\u7406\u8bba\u4e0e\u4e8c\u53c9\u6811\u6846\u67b6\u7ed3\u5408\uff0c\u89e3\u51b3\u80a1\u7968\u5747\u8861\u4ef7\u683c\u5f62\u6210\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5747\u503c\u573a\u5747\u8861\u7684\u5b58\u5728\u552f\u4e00\u6027\uff0c\u5e76\u63a8\u5bfc\u51fa\u80a1\u7968\u4ef7\u683c\u5728\u4e8c\u53c9\u6811\u4e0a\u7684\u5747\u8861\u8f6c\u79fb\u6982\u7387\u663e\u5f0f\u516c\u5f0f\u3002", "motivation": "\u7ed3\u5408\u5747\u503c\u573a\u535a\u5f08\u7406\u8bba\u548c\u7ecf\u5178\u4e8c\u53c9\u6811\u6a21\u578b\uff0c\u89e3\u51b3\u5b58\u5728\u968f\u673a\u7ec8\u7aef\u8d1f\u503a\u548c\u589e\u91cf\u7980\u8d4b\u7684\u4ee3\u7406\u4eba\u7684\u80a1\u7968\u5747\u8861\u4ef7\u683c\u5f62\u6210\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5747\u503c\u573a\u535a\u5f08\u7406\u8bba\u548c\u4e8c\u53c9\u6811\u6846\u67b6\uff0c\u9488\u5bf9\u6307\u6570\u6548\u7528\u548c\u6307\u6570\u578b\u9012\u5f52\u6548\u7528\u7684\u4ee3\u7406\u4eba\uff0c\u9650\u5236\u80a1\u7968\u4ef7\u683c\u8f68\u8ff9\u5728\u4e8c\u53c9\u6811\u4e0a\u3002", "result": "\u8bc1\u660e\u4e86\u5747\u503c\u573a\u5747\u8861\u7684\u5b58\u5728\u552f\u4e00\u6027\uff0c\u63a8\u5bfc\u51fa\u80a1\u7968\u4ef7\u683c\u5747\u8861\u8f6c\u79fb\u6982\u7387\u7684\u663e\u5f0f\u516c\u5f0f\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u5c55\u793a\u4e86\u5404\u7ec4\u6210\u90e8\u5206\u5bf9\u5747\u8861\u4ef7\u683c\u5206\u5e03\u7684\u5b9a\u6027\u5f71\u54cd\u3002", "conclusion": "\u6210\u529f\u5c06\u5747\u503c\u573a\u535a\u5f08\u7406\u8bba\u4e0e\u4e8c\u53c9\u6811\u6a21\u578b\u7ed3\u5408\uff0c\u4e3a\u5b58\u5728\u968f\u673a\u8d1f\u503a\u548c\u7980\u8d4b\u7684\u80a1\u7968\u5747\u8861\u5b9a\u4ef7\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u6570\u503c\u9a8c\u8bc1\u3002"}}
{"id": "2510.09918", "categories": ["math.OC", "math.MG", "90C26, 90C29, 93E20"], "pdf": "https://arxiv.org/pdf/2510.09918", "abs": "https://arxiv.org/abs/2510.09918", "authors": ["Jin Ma", "Weixuan Xia", "Jianfeng Zhang"], "title": "Characterizing nonconvex boundaries via scalarization", "comment": "28 pages, 4 figures", "summary": "We present a unified approach for characterizing the boundary of a possibly\nnonconvex domain. Motivated by the well-known Pascoletti--Serafini method of\nscalarization, we recast the boundary characterization as a multi-criteria\noptimization problem with respect to a local partial order induced by a\nspherical cone with varying orient. Such an approach enables us to trace the\nwhole boundary and can be considered a general dual representation for\narbitrary (nonconvex) sets satisfying an exterior cone condition. We prove the\nequivalence between the geometrical boundary and the scalarization-implied\nboundary, particularly in the case of Euclidean spaces and two\ninfinite-dimensional spaces for practical interest. By reformulating each\nscalarized problem as a parameterized constrained optimization problem, we\nshall develop a corresponding numerical scheme for the proposed approach. Some\nrelated applications are also discussed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u65b9\u6cd5\u6765\u523b\u753b\u53ef\u80fd\u975e\u51f8\u57df\u7684\u8fb9\u754c\uff0c\u57fa\u4e8ePascoletti-Serafini\u6807\u91cf\u5316\u65b9\u6cd5\uff0c\u5c06\u8fb9\u754c\u8868\u5f81\u8f6c\u5316\u4e3a\u591a\u51c6\u5219\u4f18\u5316\u95ee\u9898\uff0c\u53ef\u89c6\u4e3a\u4efb\u610f\u6ee1\u8db3\u5916\u90e8\u9525\u6761\u4ef6\u7684\u975e\u51f8\u96c6\u7684\u4e00\u822c\u5bf9\u5076\u8868\u793a\u3002", "motivation": "\u53d7Pascoletti-Serafini\u6807\u91cf\u5316\u65b9\u6cd5\u542f\u53d1\uff0c\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u8ffd\u8e2a\u6574\u4e2a\u8fb9\u754c\u7684\u65b9\u6cd5\uff0c\u4e3a\u4efb\u610f\u6ee1\u8db3\u5916\u90e8\u9525\u6761\u4ef6\u7684\u975e\u51f8\u96c6\u63d0\u4f9b\u4e00\u822c\u5bf9\u5076\u8868\u793a\u3002", "method": "\u5c06\u8fb9\u754c\u8868\u5f81\u91cd\u65b0\u6784\u9020\u4e3a\u5173\u4e8e\u7531\u53d8\u5316\u65b9\u5411\u7684\u7403\u9525\u8bf1\u5bfc\u7684\u5c40\u90e8\u504f\u5e8f\u7684\u591a\u51c6\u5219\u4f18\u5316\u95ee\u9898\uff0c\u7136\u540e\u5c06\u6bcf\u4e2a\u6807\u91cf\u5316\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u53c2\u6570\u5316\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u76f8\u5e94\u7684\u6570\u503c\u65b9\u6848\u3002", "result": "\u8bc1\u660e\u4e86\u51e0\u4f55\u8fb9\u754c\u4e0e\u6807\u91cf\u5316\u9690\u542b\u8fb9\u754c\u4e4b\u95f4\u7684\u7b49\u4ef7\u6027\uff0c\u7279\u522b\u662f\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u548c\u4e24\u4e2a\u5177\u6709\u5b9e\u9645\u610f\u4e49\u7684\u65e0\u9650\u7ef4\u7a7a\u95f4\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u901a\u7528\u7684\u8fb9\u754c\u8868\u5f81\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u975e\u51f8\u57df\uff0c\u5e76\u8ba8\u8bba\u4e86\u76f8\u5173\u5e94\u7528\u3002"}}
{"id": "2510.11074", "categories": ["q-fin.PM"], "pdf": "https://arxiv.org/pdf/2510.11074", "abs": "https://arxiv.org/abs/2510.11074", "authors": ["Jing Li", "Bowei Guo", "Xinqi Xie", "Kuo-Ping Chang"], "title": "Evaluating Investment Performance: The p-index and Empirical Efficient Frontier", "comment": null, "summary": "The empirical results have shown that firstly, with one-week holding period\nand reinvesting, for SSE Composite Index stocks, the highest p-ratio investment\nstrategy produces the largest annualized rate of return; and for NYSE Composite\nIndex stocks, all the three strategies with both one-week and one-month periods\ngenerate negative returns. Secondly, with non-reinvesting, for SSE Composite\nIndex stocks, the highest p-ratio strategy with one-week holding period yields\nthe largest annualized rate of return; and for NYSE Composite stocks, the\none-week EEF strategy produces a medium annualized return. Thirdly, under the\none-week EEF investment strategy, for NYSE Composite Index stocks, the right\nfrontier yields a higher annualized return, but for SSE Composite Index stocks,\nthe left frontier (stocks on the empirical efficient frontier) yields a higher\nannualized return than the right frontier. Fourthly, for NYSE Composite Index\nstocks, there is a positive linear relationship between monthly return and the\np-index, but no such relationship is evident for SSE Composite Index stocks.\nFifthly, for NYSE Composite Index stocks, the traditional five-factor model\nperforms poorly, and adding the p-index as a sixth factor provides incremental\ninformation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86SSE\u548cNYSE\u6307\u6570\u7684\u6295\u8d44\u7b56\u7565\u8868\u73b0\uff0c\u53d1\u73b0\u4e0d\u540c\u5e02\u573a\u5bf9\u6295\u8d44\u7b56\u7565\u7684\u53cd\u5e94\u4e0d\u540c\uff0cp-ratio\u7b56\u7565\u5728SSE\u8868\u73b0\u6700\u4f73\uff0c\u800cNYSE\u4e2dEEF\u7b56\u7565\u8868\u73b0\u4e2d\u7b49\uff0cp-index\u5bf9NYSE\u6709\u589e\u91cf\u4fe1\u606f\u4ef7\u503c\u3002", "motivation": "\u7814\u7a76\u4e0d\u540c\u6295\u8d44\u7b56\u7565\u5728\u4e0d\u540c\u80a1\u7968\u5e02\u573a\uff08SSE\u548cNYSE\uff09\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u63a2\u7d22p-ratio\u548cp-index\u7b49\u6307\u6807\u5728\u6295\u8d44\u51b3\u7b56\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528\u5b9e\u8bc1\u5206\u6790\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e0d\u540c\u6301\u6709\u671f\uff08\u4e00\u5468\u548c\u4e00\u4e2a\u6708\uff09\u548c\u518d\u6295\u8d44\u7b56\u7565\u4e0b\u7684\u6295\u8d44\u8868\u73b0\uff0c\u5206\u6790p-ratio\u3001EEF\u7b56\u7565\u548cp-index\u4e0e\u6536\u76ca\u7684\u5173\u7cfb\u3002", "result": "1) SSE\u4e2d\u4e00\u5468\u6301\u6709\u671f\u6700\u9ad8p-ratio\u7b56\u7565\u6536\u76ca\u6700\u5927\uff1b2) NYSE\u4e2d\u6240\u6709\u7b56\u7565\u5747\u4ea7\u751f\u8d1f\u6536\u76ca\uff1b3) NYSE\u4e2dp-index\u4e0e\u6708\u6536\u76ca\u5448\u6b63\u7ebf\u6027\u5173\u7cfb\uff1b4) \u4f20\u7edf\u4e94\u56e0\u5b50\u6a21\u578b\u5728NYSE\u8868\u73b0\u4e0d\u4f73\uff0c\u52a0\u5165p-index\u53ef\u6539\u5584\u3002", "conclusion": "\u4e0d\u540c\u5e02\u573a\u5bf9\u6295\u8d44\u7b56\u7565\u7684\u53cd\u5e94\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0cp-index\u5728NYSE\u4e2d\u5177\u6709\u589e\u91cf\u4fe1\u606f\u4ef7\u503c\uff0c\u53ef\u4e3a\u6295\u8d44\u51b3\u7b56\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2510.09695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09695", "abs": "https://arxiv.org/abs/2510.09695", "authors": ["Yanran Chen", "Lynn Greschner", "Roman Klinger", "Michael Klenk", "Steffen Eger"], "title": "Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection", "comment": "Initial submission", "summary": "Logical fallacies are common in public communication and can mislead\naudiences; fallacious arguments may still appear convincing despite lacking\nsoundness, because convincingness is inherently subjective. We present the\nfirst computational study of how emotional framing interacts with fallacies and\nconvincingness, using large language models (LLMs) to systematically change\nemotional appeals in fallacious arguments. We benchmark eight LLMs on injecting\nemotional appeal into fallacious arguments while preserving their logical\nstructures, then use the best models to generate stimuli for a human study. Our\nresults show that LLM-driven emotional framing reduces human fallacy detection\nin F1 by 14.5% on average. Humans perform better in fallacy detection when\nperceiving enjoyment than fear or sadness, and these three emotions also\ncorrelate with significantly higher convincingness compared to neutral or other\nemotion states. Our work has implications for AI-driven emotional manipulation\nin the context of fallacious argumentation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u8ba1\u7b97\u6027\u5730\u63a2\u8ba8\u4e86\u60c5\u611f\u6846\u67b6\u4e0e\u903b\u8f91\u8c2c\u8bef\u53ca\u8bf4\u670d\u529b\u7684\u5173\u7cfb\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4fdd\u6301\u903b\u8f91\u7ed3\u6784\u7684\u540c\u65f6\u4e3a\u8c2c\u8bef\u8bba\u8bc1\u6ce8\u5165\u60c5\u611f\u8bc9\u6c42\uff0c\u53d1\u73b0\u60c5\u611f\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u4eba\u7c7b\u5bf9\u8c2c\u8bef\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u903b\u8f91\u8c2c\u8bef\u5728\u516c\u5171\u4f20\u64ad\u4e2d\u666e\u904d\u5b58\u5728\u4e14\u5bb9\u6613\u8bef\u5bfc\u53d7\u4f17\uff0c\u4f46\u8bf4\u670d\u529b\u5177\u6709\u4e3b\u89c2\u6027\uff0c\u5373\u4f7f\u6709\u8c2c\u8bef\u7684\u8bba\u8bc1\u4ecd\u53ef\u80fd\u663e\u5f97\u6709\u8bf4\u670d\u529b\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u60c5\u611f\u6846\u67b6\u5982\u4f55\u4e0e\u8c2c\u8bef\u548c\u8bf4\u670d\u529b\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u4f7f\u75288\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u8c2c\u8bef\u8bba\u8bc1\u6ce8\u5165\u60c5\u611f\u8bc9\u6c42\u540c\u65f6\u4fdd\u6301\u903b\u8f91\u7ed3\u6784\uff0c\u7136\u540e\u7528\u6700\u4f73\u6a21\u578b\u751f\u6210\u523a\u6fc0\u6750\u6599\u8fdb\u884c\u4eba\u7c7b\u7814\u7a76\u3002", "result": "LLM\u9a71\u52a8\u7684\u60c5\u611f\u6846\u67b6\u4f7f\u4eba\u7c7b\u8c2c\u8bef\u68c0\u6d4b\u7684F1\u5206\u6570\u5e73\u5747\u964d\u4f4e14.5%\u3002\u4eba\u7c7b\u5728\u611f\u77e5\u6109\u60a6\u60c5\u7eea\u65f6\u6bd4\u6050\u60e7\u6216\u60b2\u4f24\u65f6\u66f4\u80fd\u68c0\u6d4b\u8c2c\u8bef\uff0c\u8fd9\u4e09\u79cd\u60c5\u7eea\u76f8\u6bd4\u4e2d\u6027\u6216\u5176\u4ed6\u60c5\u7eea\u72b6\u6001\u4e0e\u663e\u8457\u66f4\u9ad8\u7684\u8bf4\u670d\u529b\u76f8\u5173\u3002", "conclusion": "\u8be5\u7814\u7a76\u5bf9AI\u9a71\u52a8\u7684\u8c2c\u8bef\u8bba\u8bc1\u4e2d\u7684\u60c5\u611f\u64cd\u7eb5\u5177\u6709\u91cd\u8981\u542f\u793a\u610f\u4e49\u3002"}}
{"id": "2510.09644", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09644", "abs": "https://arxiv.org/abs/2510.09644", "authors": ["Shaharyar Alam Ansari", "Mohammad Luqman", "Aasim Zafar", "Savir Ali"], "title": "Enhanced Urban Traffic Management Using CCTV Surveillance Videos and Multi-Source Data Current State Prediction and Frequent Episode Mining", "comment": "24 pages, 9 figures", "summary": "Rapid urbanization has intensified traffic congestion, environmental strain,\nand inefficiencies in transportation systems, creating an urgent need for\nintelligent and adaptive traffic management solutions. Conventional systems\nrelying on static signals and manual monitoring are inadequate for the dynamic\nnature of modern traffic. This research aims to develop a unified framework\nthat integrates CCTV surveillance videos with multi-source data descriptors to\nenhance real-time urban traffic prediction. The proposed methodology\nincorporates spatio-temporal feature fusion, Frequent Episode Mining for\nsequential traffic pattern discovery, and a hybrid LSTM-Transformer model for\nrobust traffic state forecasting. The framework was evaluated on the CityFlowV2\ndataset comprising 313,931 annotated bounding boxes across 46 cameras. It\nachieved a high prediction accuracy of 98.46 percent, with a macro precision of\n0.9800, macro recall of 0.9839, and macro F1-score of 0.9819. FEM analysis\nrevealed significant sequential patterns such as moderate-congested transitions\nwith confidence levels exceeding 55 percent. The 46 sustained congestion alerts\nare system-generated, which shows practical value for proactive congestion\nmanagement. This emphasizes the need for the incorporation of video stream\nanalytics with data from multiple sources for the design of real-time,\nresponsive, adaptable multi-level intelligent transportation systems, which\nmakes urban mobility smarter and safer.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u96c6\u6210CCTV\u76d1\u63a7\u89c6\u9891\u548c\u591a\u6e90\u6570\u636e\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u57ce\u5e02\u4ea4\u901a\u9884\u6d4b\uff0c\u901a\u8fc7\u65f6\u7a7a\u7279\u5f81\u878d\u5408\u3001\u9891\u7e41\u5e8f\u5217\u6a21\u5f0f\u6316\u6398\u548c\u6df7\u5408LSTM-Transformer\u6a21\u578b\uff0c\u5728CityFlowV2\u6570\u636e\u96c6\u4e0a\u8fbe\u523098.46%\u7684\u9884\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u5feb\u901f\u57ce\u5e02\u5316\u52a0\u5267\u4e86\u4ea4\u901a\u62e5\u5835\u3001\u73af\u5883\u538b\u529b\u548c\u4ea4\u901a\u7cfb\u7edf\u4f4e\u6548\u95ee\u9898\uff0c\u4f20\u7edf\u57fa\u4e8e\u9759\u6001\u4fe1\u53f7\u548c\u4eba\u5de5\u76d1\u63a7\u7684\u7cfb\u7edf\u65e0\u6cd5\u9002\u5e94\u73b0\u4ee3\u4ea4\u901a\u7684\u52a8\u6001\u7279\u6027\uff0c\u6025\u9700\u667a\u80fd\u81ea\u9002\u5e94\u4ea4\u901a\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u96c6\u6210CCTV\u76d1\u63a7\u89c6\u9891\u4e0e\u591a\u6e90\u6570\u636e\u63cf\u8ff0\u7b26\uff0c\u91c7\u7528\u65f6\u7a7a\u7279\u5f81\u878d\u5408\u3001\u9891\u7e41\u5e8f\u5217\u6a21\u5f0f\u6316\u6398(FEM)\u53d1\u73b0\u987a\u5e8f\u4ea4\u901a\u6a21\u5f0f\uff0c\u4ee5\u53ca\u6df7\u5408LSTM-Transformer\u6a21\u578b\u8fdb\u884c\u7a33\u5065\u7684\u4ea4\u901a\u72b6\u6001\u9884\u6d4b\u3002", "result": "\u5728\u5305\u542b313,931\u4e2a\u6807\u6ce8\u8fb9\u754c\u6846\u7684CityFlowV2\u6570\u636e\u96c6\u4e0a\uff0c\u9884\u6d4b\u51c6\u786e\u7387\u8fbe\u523098.46%\uff0c\u5b8f\u89c2\u7cbe\u786e\u5ea60.9800\uff0c\u5b8f\u89c2\u53ec\u56de\u73870.9839\uff0c\u5b8f\u89c2F1\u5206\u65700.9819\u3002FEM\u5206\u6790\u63ed\u793a\u4e86\u7f6e\u4fe1\u5ea6\u8d85\u8fc755%\u7684\u663e\u8457\u987a\u5e8f\u6a21\u5f0f\uff0c\u7cfb\u7edf\u751f\u6210\u4e8646\u4e2a\u6301\u7eed\u62e5\u5835\u8b66\u62a5\u3002", "conclusion": "\u5f3a\u8c03\u5c06\u89c6\u9891\u6d41\u5206\u6790\u4e0e\u591a\u6e90\u6570\u636e\u7ed3\u5408\u5bf9\u4e8e\u8bbe\u8ba1\u5b9e\u65f6\u3001\u54cd\u5e94\u5f0f\u3001\u81ea\u9002\u5e94\u7684\u591a\u5c42\u6b21\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u5fc5\u8981\u6027\uff0c\u8fd9\u80fd\u4f7f\u57ce\u5e02\u4ea4\u901a\u66f4\u667a\u80fd\u548c\u5b89\u5168\u3002"}}
{"id": "2510.09801", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09801", "abs": "https://arxiv.org/abs/2510.09801", "authors": ["Valerie Chen", "Rohit Malhotra", "Xingyao Wang", "Juan Michelini", "Xuhui Zhou", "Aditya Bharat Soni", "Hoang H. Tran", "Calvin Smith", "Ameet Talwalkar", "Graham Neubig"], "title": "How can we assess human-agent interactions? Case studies in software agent design", "comment": null, "summary": "LLM-powered agents are both a promising new technology and a source of\ncomplexity, where choices about models, tools, and prompting can affect their\nusefulness. While numerous benchmarks measure agent accuracy across domains,\nthey mostly assume full automation, failing to represent the collaborative\nnature of real-world use cases. In this paper, we make two major steps towards\nthe rigorous assessment of human-agent interactions. First, we propose PULSE, a\nframework for more efficient human-centric evaluation of agent designs, which\ncomprises collecting user feedback, training an ML model to predict user\nsatisfaction, and computing results by combining human satisfaction ratings\nwith model-generated pseudo-labels. Second, we deploy the framework on a\nlarge-scale web platform built around the open-source software agent OpenHands,\ncollecting in-the-wild usage data across over 15k users. We conduct case\nstudies around how three agent design decisions -- choice of LLM backbone,\nplanning strategy, and memory mechanisms -- impact developer satisfaction\nrates, yielding practical insights for software agent design. We also show how\nour framework can lead to more robust conclusions about agent design, reducing\nconfidence intervals by 40\\% compared to a standard A/B test. Finally, we find\nsubstantial discrepancies between in-the-wild results and benchmark performance\n(e.g., the anti-correlation between results comparing claude-sonnet-4 and\ngpt-5), underscoring the limitations of benchmark-driven evaluation. Our\nfindings provide guidance for evaluations of LLM agents with humans and\nidentify opportunities for better agent designs.", "AI": {"tldr": "\u63d0\u51fa\u4e86PULSE\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u9ad8\u6548\u5730\u8bc4\u4f30\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u7684\u4ea4\u4e92\uff0c\u901a\u8fc7\u6536\u96c6\u7528\u6237\u53cd\u9988\u3001\u8bad\u7ec3\u6ee1\u610f\u5ea6\u9884\u6d4b\u6a21\u578b\u548c\u7ed3\u5408\u4eba\u5de5\u8bc4\u5206\u4e0e\u6a21\u578b\u4f2a\u6807\u7b7e\u6765\u8ba1\u7b97\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5927\u591a\u5047\u8bbe\u5b8c\u5168\u81ea\u52a8\u5316\uff0c\u672a\u80fd\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u4e2d\u4eba\u673a\u534f\u4f5c\u7684\u672c\u8d28\uff0c\u9700\u8981\u66f4\u4e25\u8c28\u7684\u4eba\u7c7b-\u4ee3\u7406\u4ea4\u4e92\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1PULSE\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u7528\u6237\u53cd\u9988\u6536\u96c6\u3001\u6ee1\u610f\u5ea6\u9884\u6d4b\u6a21\u578b\u8bad\u7ec3\u3001\u4eba\u5de5\u8bc4\u5206\u4e0e\u6a21\u578b\u4f2a\u6807\u7b7e\u7ed3\u5408\uff1b\u5728\u5f00\u6e90\u8f6f\u4ef6\u4ee3\u7406OpenHands\u5e73\u53f0\u4e0a\u90e8\u7f72\uff0c\u6536\u96c6\u8d85\u8fc715,000\u7528\u6237\u7684\u4f7f\u7528\u6570\u636e\u3002", "result": "\u6846\u67b6\u4f7f\u4ee3\u7406\u8bbe\u8ba1\u51b3\u7b56\u7684\u7f6e\u4fe1\u533a\u95f4\u6bd4\u6807\u51c6A/B\u6d4b\u8bd5\u51cf\u5c1140%\uff1b\u53d1\u73b0\u5b9e\u9645\u4f7f\u7528\u7ed3\u679c\u4e0e\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff08\u5982claude-sonnet-4\u4e0egpt-5\u6bd4\u8f83\u7ed3\u679c\u5448\u8d1f\u76f8\u5173\uff09\u3002", "conclusion": "\u7814\u7a76\u4e3aLLM\u4ee3\u7406\u7684\u4eba\u7c7b\u8bc4\u4f30\u63d0\u4f9b\u6307\u5bfc\uff0c\u8bc6\u522b\u4e86\u66f4\u597d\u7684\u4ee3\u7406\u8bbe\u8ba1\u673a\u4f1a\uff0c\u5f3a\u8c03\u4e86\u57fa\u51c6\u9a71\u52a8\u8bc4\u4f30\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.10469", "categories": ["q-fin.RM"], "pdf": "https://arxiv.org/pdf/2510.10469", "abs": "https://arxiv.org/abs/2510.10469", "authors": ["Hongzhe Wen", "R. S. M. Lau"], "title": "A Risk Mitigation Model of Monetary Ecosystem with Stablecoins", "comment": null, "summary": "Stablecoins have emerged as a significant component of global financial\ninfrastructure, with aggregate market capitalization surpassing USD250 billion\nin 2025. Their increasing integration into payment and settlement systems has\nsimultaneously introduced novel channels of systemic exposure, particularly\nliquidity risk during periods of market stress. This study develops a hybrid\nmonetary architecture that embeds fiat-backed stablecoins within a central\nbank-anchored framework to structurally mitigate liquidity fragility. The\nproposed model combines 100 percent reserve backing, interoperable redemption\nrails, and standing liquidity facilities to guarantee instant convertibility at\npar. Using the 2023 SVB USDC de-peg event as a calibrated stress scenario, we\ndemonstrate that this architecture reduces peak peg deviations, shortens stress\npersistence, and stabilizes redemption queues under high redemption intensity.\nBy integrating liquidity backstops and eliminating maturity-transformation\nchannels, the framework addresses run dynamics ex ante rather than through ad\nhoc intervention. These findings provide empirical and theoretical support for\na hybrid stablecoin-CBDC architecture that enhances systemic resilience,\npreserves monetary integrity, and establishes a credible pathway for stablecoin\nintegration into regulated financial systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u8d27\u5e01\u67b6\u6784\uff0c\u5c06\u6cd5\u5e01\u652f\u6301\u7684\u7a33\u5b9a\u5e01\u5d4c\u5165\u592e\u884c\u951a\u5b9a\u6846\u67b6\uff0c\u901a\u8fc7100%\u51c6\u5907\u91d1\u3001\u4e92\u64cd\u4f5c\u6027\u8d4e\u56de\u901a\u9053\u548c\u5e38\u5907\u6d41\u52a8\u6027\u8bbe\u65bd\u6765\u7ed3\u6784\u6027\u5730\u7f13\u89e3\u6d41\u52a8\u6027\u98ce\u9669\u3002", "motivation": "\u7a33\u5b9a\u5e01\u5df2\u6210\u4e3a\u5168\u7403\u91d1\u878d\u57fa\u7840\u8bbe\u65bd\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u4f46\u5176\u5728\u652f\u4ed8\u548c\u7ed3\u7b97\u7cfb\u7edf\u4e2d\u7684\u65e5\u76ca\u6574\u5408\u5f15\u5165\u4e86\u65b0\u7684\u7cfb\u7edf\u6027\u98ce\u9669\u6e20\u9053\uff0c\u7279\u522b\u662f\u5728\u5e02\u573a\u538b\u529b\u65f6\u671f\u7684\u6d41\u52a8\u6027\u98ce\u9669\u3002", "method": "\u5f00\u53d1\u6df7\u5408\u8d27\u5e01\u67b6\u6784\uff0c\u7ed3\u5408100%\u51c6\u5907\u91d1\u652f\u6301\u3001\u4e92\u64cd\u4f5c\u6027\u8d4e\u56de\u901a\u9053\u548c\u5e38\u5907\u6d41\u52a8\u6027\u8bbe\u65bd\uff0c\u4fdd\u8bc1\u6309\u9762\u503c\u5373\u65f6\u53ef\u5151\u6362\u6027\u3002\u4f7f\u75282023\u5e74SVB USDC\u8131\u951a\u4e8b\u4ef6\u4f5c\u4e3a\u6821\u51c6\u538b\u529b\u60c5\u666f\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u8be5\u67b6\u6784\u51cf\u5c11\u4e86\u5cf0\u503c\u8131\u951a\u504f\u5dee\uff0c\u7f29\u77ed\u4e86\u538b\u529b\u6301\u7eed\u65f6\u95f4\uff0c\u5e76\u5728\u9ad8\u8d4e\u56de\u5f3a\u5ea6\u4e0b\u7a33\u5b9a\u4e86\u8d4e\u56de\u961f\u5217\u3002\u901a\u8fc7\u6574\u5408\u6d41\u52a8\u6027\u4fdd\u969c\u548c\u6d88\u9664\u671f\u9650\u8f6c\u6362\u6e20\u9053\uff0c\u4e8b\u524d\u89e3\u51b3\u6324\u5151\u52a8\u6001\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u6df7\u5408\u7a33\u5b9a\u5e01-CBDC\u67b6\u6784\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u548c\u7406\u8bba\u652f\u6301\uff0c\u8fd9\u79cd\u67b6\u6784\u80fd\u589e\u5f3a\u7cfb\u7edf\u97e7\u6027\u3001\u4fdd\u6301\u8d27\u5e01\u5b8c\u6574\u6027\uff0c\u5e76\u4e3a\u7a33\u5b9a\u5e01\u6574\u5408\u5230\u53d7\u76d1\u7ba1\u91d1\u878d\u7cfb\u7edf\u5efa\u7acb\u53ef\u4fe1\u8def\u5f84\u3002"}}
{"id": "2510.10693", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.AI", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.10693", "abs": "https://arxiv.org/abs/2510.10693", "authors": ["Yuma Ichikawa", "Shuhei Kashiwamura", "Ayaka Sakata"], "title": "High-Dimensional Learning Dynamics of Quantized Models with Straight-Through Estimator", "comment": "27 pages, 14 figures", "summary": "Quantized neural network training optimizes a discrete, non-differentiable\nobjective. The straight-through estimator (STE) enables backpropagation through\nsurrogate gradients and is widely used. While previous studies have primarily\nfocused on the properties of surrogate gradients and their convergence, the\ninfluence of quantization hyperparameters, such as bit width and quantization\nrange, on learning dynamics remains largely unexplored. We theoretically show\nthat in the high-dimensional limit, STE dynamics converge to a deterministic\nordinary differential equation. This reveals that STE training exhibits a\nplateau followed by a sharp drop in generalization error, with plateau length\ndepending on the quantization range. A fixed-point analysis quantifies the\nasymptotic deviation from the unquantized linear model. We also extend\nanalytical techniques for stochastic gradient descent to nonlinear\ntransformations of weights and inputs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5728STE\u8bad\u7ec3\u4e2d\uff0c\u91cf\u5316\u8d85\u53c2\u6570\uff08\u5982\u91cf\u5316\u8303\u56f4\uff09\u4f1a\u5f71\u54cd\u5b66\u4e60\u52a8\u6001\uff0c\u5bfc\u81f4\u6cdb\u5316\u8bef\u5dee\u51fa\u73b0\u5e73\u53f0\u671f\u540e\u6025\u5267\u4e0b\u964d\uff0c\u5e73\u53f0\u671f\u957f\u5ea6\u53d6\u51b3\u4e8e\u91cf\u5316\u8303\u56f4\u3002", "motivation": "\u867d\u7136STE\u88ab\u5e7f\u6cdb\u7528\u4e8e\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\uff0c\u4f46\u91cf\u5316\u8d85\u53c2\u6570\uff08\u5982\u4f4d\u5bbd\u548c\u91cf\u5316\u8303\u56f4\uff09\u5bf9\u5b66\u4e60\u52a8\u6001\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u5728\u7406\u8bba\u5206\u6790\u4e2d\uff0c\u4f5c\u8005\u8bc1\u660e\u5728\u9ad8\u7ef4\u6781\u9650\u4e0b\uff0cSTE\u52a8\u6001\u6536\u655b\u5230\u4e00\u4e2a\u786e\u5b9a\u6027\u5e38\u5fae\u5206\u65b9\u7a0b\uff0c\u5e76\u901a\u8fc7\u5b9a\u70b9\u5206\u6790\u91cf\u5316\u4e86\u4e0e\u672a\u91cf\u5316\u7ebf\u6027\u6a21\u578b\u7684\u6e10\u8fd1\u504f\u5dee\u3002", "result": "\u7814\u7a76\u53d1\u73b0STE\u8bad\u7ec3\u8868\u73b0\u51fa\u6cdb\u5316\u8bef\u5dee\u7684\u5e73\u53f0\u671f\u540e\u6025\u5267\u4e0b\u964d\uff0c\u5e73\u53f0\u671f\u957f\u5ea6\u53d6\u51b3\u4e8e\u91cf\u5316\u8303\u56f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7684\u5206\u6790\u6280\u672f\u5230\u6743\u91cd\u548c\u8f93\u5165\u7684\u975e\u7ebf\u6027\u53d8\u6362\uff0c\u63ed\u793a\u4e86\u91cf\u5316\u8d85\u53c2\u6570\u5bf9\u5b66\u4e60\u52a8\u6001\u7684\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2510.10878", "categories": ["q-fin.CP", "cs.CE", "q-fin.MF"], "pdf": "https://arxiv.org/pdf/2510.10878", "abs": "https://arxiv.org/abs/2510.10878", "authors": ["Zheng Cao", "Xingran Shao", "Yuheng Yan", "Helyette Geman"], "title": "Identifying and Quantifying Financial Bubbles with the Hyped Log-Periodic Power Law Model", "comment": null, "summary": "We propose a novel model, the Hyped Log-Periodic Power Law Model (HLPPL), to\nthe problem of quantifying and detecting financial bubbles, an ever-fascinating\none for academics and practitioners alike. Bubble labels are generated using a\nLog-Periodic Power Law (LPPL) model, sentiment scores, and a hype index we\nintroduced in previous research on NLP forecasting of stock return volatility.\nUsing these tools, a dual-stream transformer model is trained with market data\nand machine learning methods, resulting in a time series of confidence scores\nas a Bubble Score. A distinctive feature of our framework is that it captures\nphases of extreme overpricing and underpricing within a unified structure.\n  We achieve an average yield of 34.13 percentage annualized return when\nbacktesting U.S. equities during the period 2018 to 2024, while the approach\nexhibits a remarkable generalization ability across industry sectors. Its\nconservative bias in predicting bubble periods minimizes false positives, a\nfeature which is especially beneficial for market signaling and\ndecision-making. Overall, this approach utilizes both theoretical and empirical\nadvances for real-time positive and negative bubble identification and\nmeasurement with HLPPL signals.", "AI": {"tldr": "\u63d0\u51faHLPPL\u6a21\u578b\u7528\u4e8e\u91cf\u5316\u68c0\u6d4b\u91d1\u878d\u6ce1\u6cab\uff0c\u7ed3\u5408LPPL\u6a21\u578b\u3001\u60c5\u611f\u8bc4\u5206\u548c\u7092\u4f5c\u6307\u6570\uff0c\u901a\u8fc7\u53cc\u6d41transformer\u8bad\u7ec3\u751f\u6210\u6ce1\u6cab\u8bc4\u5206\uff0c\u57282018-2024\u5e74\u7f8e\u80a1\u56de\u6d4b\u4e2d\u5b9e\u73b034.13%\u5e74\u5316\u6536\u76ca\u3002", "motivation": "\u89e3\u51b3\u91d1\u878d\u6ce1\u6cab\u7684\u91cf\u5316\u68c0\u6d4b\u95ee\u9898\uff0c\u4e3a\u5b66\u672f\u754c\u548c\u4ece\u4e1a\u4eba\u5458\u63d0\u4f9b\u6709\u6548\u7684\u6ce1\u6cab\u8bc6\u522b\u5de5\u5177\u3002", "method": "\u4f7f\u7528LPPL\u6a21\u578b\u3001\u60c5\u611f\u8bc4\u5206\u548c\u7092\u4f5c\u6307\u6570\u751f\u6210\u6ce1\u6cab\u6807\u7b7e\uff0c\u91c7\u7528\u53cc\u6d41transformer\u6a21\u578b\u7ed3\u5408\u5e02\u573a\u6570\u636e\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3\uff0c\u751f\u6210\u6ce1\u6cab\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u3002", "result": "\u57282018-2024\u5e74\u7f8e\u80a1\u56de\u6d4b\u4e2d\u5b9e\u73b034.13%\u5e74\u5316\u6536\u76ca\u7387\uff0c\u6a21\u578b\u5728\u4e0d\u540c\u884c\u4e1a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5bf9\u6ce1\u6cab\u671f\u7684\u4fdd\u5b88\u9884\u6d4b\u51cf\u5c11\u4e86\u8bef\u62a5\u3002", "conclusion": "HLPPL\u65b9\u6cd5\u7ed3\u5408\u7406\u8bba\u548c\u5b9e\u8bc1\u8fdb\u5c55\uff0c\u80fd\u591f\u5b9e\u65f6\u8bc6\u522b\u548c\u6d4b\u91cf\u6b63\u8d1f\u6ce1\u6cab\uff0c\u4e3a\u5e02\u573a\u4fe1\u53f7\u548c\u51b3\u7b56\u63d0\u4f9b\u6709\u76ca\u5de5\u5177\u3002"}}
{"id": "2510.09826", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.09826", "abs": "https://arxiv.org/abs/2510.09826", "authors": ["Jialin Zheng", "Zhong Liu", "Xiaonan Lu"], "title": "Latent-Feature-Informed Neural ODE Modeling for Lightweight Stability Evaluation of Black-box Grid-Tied Inverters", "comment": "6 pages 8fugures", "summary": "Stability evaluation of black-box grid-tied inverters is vital for grid\nreliability, yet identification techniques are both data-hungry and blocked by\nproprietary internals. {To solve this, this letter proposes a\nlatent-feature-informed neural ordinary differential equation (LFI-NODE)\nmodeling method that can achieve lightweight stability evaluation directly from\ntrajectory data.} LFI-NODE parameterizes the entire system ODE with a single\ncontinuous-time neural network, allowing each new sample to refine a unified\nglobal model. It faithfully captures nonlinear large-signal dynamics to\npreserve uniform predictive accuracy as the inverter transitions between\noperating points. Meanwhile, latent perturbation features distilled from every\ntrajectory steer the learning process and concurrently reveal the small-signal\neigenstructure essential for rigorous stability analysis. Validated on a\ngrid-forming inverter, {The LFI-NODE requires one to two orders of magnitude\nfewer training samples compared with traditional methods, collected from short\ntime-domain trajectories instead of extensive frequency-domain measurements.}\n{Furthermore, the LFI-NODE requires only 48 short transients to achieve a\ntrajectory prediction error at the hundredth level and an eigenvalue estimation\nerror at the tenth level, outperforming benchmark methods by one to two orders\nof magnitude.} This makes LFI-NODE a practical and lightweight approach for\nachieving high-fidelity stability assessment of complex black-box\npower-electronic systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u7279\u5f81\u795e\u7ecf\u5fae\u5206\u65b9\u7a0b\uff08LFI-NODE\uff09\u7684\u8f7b\u91cf\u5316\u9ed1\u7bb1\u5e76\u7f51\u9006\u53d8\u5668\u7a33\u5b9a\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ec5\u9700\u5c11\u91cf\u65f6\u57df\u8f68\u8ff9\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7a33\u5b9a\u6027\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u9ed1\u7bb1\u5e76\u7f51\u9006\u53d8\u5668\u7a33\u5b9a\u6027\u8bc4\u4f30\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u4e14\u53d7\u9650\u4e8e\u4e13\u6709\u5185\u90e8\u7ed3\u6784\uff0c\u96be\u4ee5\u5b9e\u73b0\u8f7b\u91cf\u5316\u9ad8\u6548\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u5355\u4e2a\u8fde\u7eed\u65f6\u95f4\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u7cfb\u7edf\u5fae\u5206\u65b9\u7a0b\uff0c\u901a\u8fc7\u6f5c\u5728\u6270\u52a8\u7279\u5f81\u5f15\u5bfc\u5b66\u4e60\u8fc7\u7a0b\uff0c\u540c\u65f6\u6355\u83b7\u975e\u7ebf\u6027\u5927\u4fe1\u53f7\u52a8\u6001\u548c\u5c0f\u4fe1\u53f7\u7279\u5f81\u7ed3\u6784\u3002", "result": "\u4ec5\u970048\u4e2a\u77ed\u65f6\u77ac\u6001\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u767e\u5206\u4f4d\u7ea7\u522b\u7684\u8f68\u8ff9\u9884\u6d4b\u8bef\u5dee\u548c\u5341\u5206\u4f4d\u7ea7\u522b\u7684\u7279\u5f81\u503c\u4f30\u8ba1\u8bef\u5dee\uff0c\u8bad\u7ec3\u6837\u672c\u9700\u6c42\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5c111-2\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "LFI-NODE\u4e3a\u590d\u6742\u9ed1\u7bb1\u7535\u529b\u7535\u5b50\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u8f7b\u91cf\u5316\u7684\u9ad8\u4fdd\u771f\u7a33\u5b9a\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2510.11008", "categories": ["econ.EM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11008", "abs": "https://arxiv.org/abs/2510.11008", "authors": ["Ta-Chung Chi", "Ting-Han Fan", "Raffaele M. Ghigliazza", "Domenico Giannone", "Zixuan", "Wang"], "title": "Macroeconomic Forecasting and Machine Learning", "comment": null, "summary": "We forecast the full conditional distribution of macroeconomic outcomes by\nsystematically integrating three key principles: using high-dimensional data\nwith appropriate regularization, adopting rigorous out-of-sample validation\nprocedures, and incorporating nonlinearities. By exploiting the rich\ninformation embedded in a large set of macroeconomic and financial predictors,\nwe produce accurate predictions of the entire profile of macroeconomic risk in\nreal time. Our findings show that regularization via shrinkage is essential to\ncontrol model complexity, while introducing nonlinearities yields limited\nimprovements in predictive accuracy. Out-of-sample validation plays a critical\nrole in selecting model architecture and preventing overfitting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6574\u5408\u9ad8\u7ef4\u6570\u636e\u3001\u6b63\u5219\u5316\u3001\u6837\u672c\u5916\u9a8c\u8bc1\u548c\u975e\u7ebf\u6027\u5efa\u6a21\u4e09\u4e2a\u5173\u952e\u539f\u5219\uff0c\u9884\u6d4b\u5b8f\u89c2\u7ecf\u6d4e\u7ed3\u679c\u7684\u5b8c\u6574\u6761\u4ef6\u5206\u5e03\uff0c\u5b9e\u65f6\u51c6\u786e\u9884\u6d4b\u5b8f\u89c2\u7ecf\u6d4e\u98ce\u9669\u7684\u5168\u8c8c\u3002", "motivation": "\u5229\u7528\u5927\u91cf\u5b8f\u89c2\u7ecf\u6d4e\u548c\u91d1\u878d\u9884\u6d4b\u56e0\u5b50\u4e2d\u7684\u4e30\u5bcc\u4fe1\u606f\uff0c\u51c6\u786e\u9884\u6d4b\u5b8f\u89c2\u7ecf\u6d4e\u98ce\u9669\u7684\u5b8c\u6574\u5206\u5e03\uff0c\u4e3a\u5b9e\u65f6\u98ce\u9669\u7ba1\u7406\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u91c7\u7528\u9ad8\u7ef4\u6570\u636e\u914d\u5408\u9002\u5f53\u7684\u6b63\u5219\u5316\uff08\u6536\u7f29\u65b9\u6cd5\uff09\uff0c\u7ed3\u5408\u4e25\u683c\u7684\u6837\u672c\u5916\u9a8c\u8bc1\u7a0b\u5e8f\uff0c\u5e76\u5f15\u5165\u975e\u7ebf\u6027\u5efa\u6a21\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6b63\u5219\u5316\u5bf9\u4e8e\u63a7\u5236\u6a21\u578b\u590d\u6742\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u800c\u975e\u7ebf\u6027\u5efa\u6a21\u5bf9\u9884\u6d4b\u51c6\u786e\u6027\u7684\u63d0\u5347\u6709\u9650\uff1b\u6837\u672c\u5916\u9a8c\u8bc1\u5728\u6a21\u578b\u67b6\u6784\u9009\u62e9\u548c\u9632\u6b62\u8fc7\u62df\u5408\u65b9\u9762\u53d1\u6325\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6574\u5408\u9ad8\u7ef4\u6570\u636e\u3001\u6b63\u5219\u5316\u548c\u6837\u672c\u5916\u9a8c\u8bc1\uff0c\u80fd\u591f\u6709\u6548\u9884\u6d4b\u5b8f\u89c2\u7ecf\u6d4e\u6761\u4ef6\u7684\u5b8c\u6574\u5206\u5e03\uff0c\u5176\u4e2d\u6b63\u5219\u5316\u548c\u6837\u672c\u5916\u9a8c\u8bc1\u662f\u6210\u529f\u9884\u6d4b\u7684\u5173\u952e\u8981\u7d20\u3002"}}
{"id": "2510.09674", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09674", "abs": "https://arxiv.org/abs/2510.09674", "authors": ["Joao D. S. Marques", "Andre V. Duarte", "Andre Carvalho", "Gil Rocha", "Bruno Martins", "Arlindo L. Oliveira"], "title": "Leveraging LLMs to Streamline the Review of Public Funding Applications", "comment": "Paper Accepted at EMNLP 2025 Industry Track", "summary": "Every year, the European Union and its member states allocate millions of\neuros to fund various development initiatives. However, the increasing number\nof applications received for these programs often creates significant\nbottlenecks in evaluation processes, due to limited human capacity. In this\nwork, we detail the real-world deployment of AI-assisted evaluation within the\npipeline of two government initiatives: (i) corporate applications aimed at\ninternational business expansion, and (ii) citizen reimbursement claims for\ninvestments in energy-efficient home improvements. While these two cases\ninvolve distinct evaluation procedures, our findings confirm that AI\neffectively enhanced processing efficiency and reduced workload across both\ntypes of applications. Specifically, in the citizen reimbursement claims\ninitiative, our solution increased reviewer productivity by 20.1%, while\nkeeping a negligible false-positive rate based on our test set observations.\nThese improvements resulted in an overall reduction of more than 2 months in\nthe total evaluation time, illustrating the impact of AI-driven automation in\nlarge-scale evaluation workflows.", "AI": {"tldr": "\u672c\u6587\u8be6\u7ec6\u4ecb\u7ecd\u4e86AI\u8f85\u52a9\u8bc4\u4f30\u7cfb\u7edf\u5728\u4e24\u4e2a\u653f\u5e9c\u9879\u76ee\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u60c5\u51b5\uff0c\u8bc1\u660eAI\u80fd\u6709\u6548\u63d0\u9ad8\u5904\u7406\u6548\u7387\u5e76\u51cf\u5c11\u5de5\u4f5c\u91cf\u3002", "motivation": "\u6b27\u76df\u53ca\u5176\u6210\u5458\u56fd\u6bcf\u5e74\u62e8\u6b3e\u6570\u767e\u4e07\u6b27\u5143\u8d44\u52a9\u53d1\u5c55\u9879\u76ee\uff0c\u4f46\u7533\u8bf7\u6570\u91cf\u6fc0\u589e\u5bfc\u81f4\u8bc4\u4f30\u8fc7\u7a0b\u51fa\u73b0\u74f6\u9888\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u4eba\u529b\u4e0d\u8db3\u3002", "method": "\u5728\u4e24\u4e2a\u653f\u5e9c\u9879\u76ee\u4e2d\u90e8\u7f72AI\u8f85\u52a9\u8bc4\u4f30\u7cfb\u7edf\uff1a(i)\u4f01\u4e1a\u56fd\u9645\u4e1a\u52a1\u6269\u5f20\u7533\u8bf7\uff1b(ii)\u516c\u6c11\u8282\u80fd\u5bb6\u5c45\u6295\u8d44\u62a5\u9500\u7533\u8bf7\u3002", "result": "\u5728\u516c\u6c11\u62a5\u9500\u7533\u8bf7\u9879\u76ee\u4e2d\uff0cAI\u89e3\u51b3\u65b9\u6848\u4f7f\u5ba1\u6838\u5458\u5de5\u4f5c\u6548\u7387\u63d0\u9ad820.1%\uff0c\u8bef\u62a5\u7387\u6781\u4f4e\uff0c\u603b\u4f53\u8bc4\u4f30\u65f6\u95f4\u51cf\u5c11\u8d85\u8fc72\u4e2a\u6708\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u5728\u5927\u89c4\u6a21\u8bc4\u4f30\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5177\u6709\u663e\u8457\u5f71\u54cd\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u5904\u7406\u6548\u7387\u548c\u51cf\u5c11\u5de5\u4f5c\u91cf\u3002"}}
{"id": "2510.10260", "categories": ["math.OC", "math.PR", "q-fin.MF", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10260", "abs": "https://arxiv.org/abs/2510.10260", "authors": ["Junyan Ye", "Hoi Ying Wong", "Kyunghyun Park"], "title": "Robust Exploratory Stopping under Ambiguity in Reinforcement Learning", "comment": "26 pages, 4 figures, 1 table", "summary": "We propose and analyze a continuous-time robust reinforcement learning\nframework for optimal stopping problems under ambiguity. In this framework, an\nagent chooses a stopping rule motivated by two objectives: robust\ndecision-making under ambiguity and learning about the unknown environment.\nHere, ambiguity refers to considering multiple probability measures dominated\nby a reference measure, reflecting the agent's awareness that the reference\nmeasure representing her learned belief about the environment would be\nerroneous. Using the $g$-expectation framework, we reformulate an optimal\nstopping problem under ambiguity as an entropy-regularized optimal control\nproblem under ambiguity, with Bernoulli distributed controls to incorporate\nexploration into the stopping rules. We then derive the optimal Bernoulli\ndistributed control characterized by backward stochastic differential\nequations. Moreover, we establish a policy iteration theorem and implement it\nas a reinforcement learning algorithm. Numerical experiments demonstrate the\nconvergence and robustness of the proposed algorithm across different levels of\nambiguity and exploration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8fde\u7eed\u65f6\u95f4\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u6700\u4f18\u505c\u6b62\u95ee\u9898\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u9c81\u68d2\u51b3\u7b56\u548c\u5b66\u4e60\u672a\u77e5\u73af\u5883\uff0c\u4f7f\u7528g-\u671f\u671b\u7406\u8bba\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u71b5\u6b63\u5219\u5316\u7684\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u901a\u8fc7BSDE\u8868\u5f81\u6700\u4f18\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u5728\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u6700\u4f18\u505c\u6b62\u95ee\u9898\uff0c\u8003\u8651\u4ee3\u7406\u5bf9\u53c2\u8003\u6982\u7387\u6d4b\u5ea6\u7684\u8ba4\u77e5\u53ef\u80fd\u5b58\u5728\u9519\u8bef\uff0c\u9700\u8981\u5728\u9c81\u68d2\u51b3\u7b56\u548c\u5b66\u4e60\u73af\u5883\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u4f7f\u7528g-\u671f\u671b\u6846\u67b6\u5c06\u6700\u4f18\u505c\u6b62\u95ee\u9898\u8f6c\u5316\u4e3a\u71b5\u6b63\u5219\u5316\u7684\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u91c7\u7528\u4f2f\u52aa\u5229\u5206\u5e03\u63a7\u5236\u6765\u878d\u5165\u63a2\u7d22\u673a\u5236\uff0c\u901a\u8fc7\u53cd\u5411\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u8868\u5f81\u6700\u4f18\u63a7\u5236\uff0c\u5e76\u5efa\u7acb\u4e86\u7b56\u7565\u8fed\u4ee3\u5b9a\u7406\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u4e0d\u540c\u4e0d\u786e\u5b9a\u6027\u548c\u63a2\u7d22\u6c34\u5e73\u4e0b\u90fd\u80fd\u6536\u655b\u5e76\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5730\u5c06\u9c81\u68d2\u51b3\u7b56\u548c\u5b66\u4e60\u7ed3\u5408\uff0c\u4e3a\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e0b\u7684\u6700\u4f18\u505c\u6b62\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10026", "categories": ["math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2510.10026", "abs": "https://arxiv.org/abs/2510.10026", "authors": ["Qi Liu"], "title": "Well-posedness of McKean-Vlasov generalized multivalued BSDEs", "comment": null, "summary": "This paper investigates McKean-Vlasov backward stochastic variational\ninequalities (BSVIs) whose generator depends on the joint law of the solution.\nWe first establish the existence and uniqueness of the solution under globally\nLipschitz and linear growth conditions. The analysis is then extended to the\nmore general case of locally Lipschitz and non-linear growth conditions.", "AI": {"tldr": "\u7814\u7a76McKean-Vlasov\u5411\u540e\u968f\u673a\u53d8\u5206\u4e0d\u7b49\u5f0f(BSVIs)\u7684\u5b58\u5728\u552f\u4e00\u6027\uff0c\u5176\u4e2d\u751f\u6210\u5668\u4f9d\u8d56\u4e8e\u89e3\u7684\u8054\u5408\u5206\u5e03", "motivation": "\u5206\u6790\u751f\u6210\u5668\u4f9d\u8d56\u4e8e\u89e3\u8054\u5408\u5206\u5e03\u7684McKean-Vlasov BSVIs\uff0c\u6269\u5c55\u7ecf\u5178BSVIs\u7406\u8bba\u5230\u66f4\u4e00\u822c\u7684\u5206\u5e03\u4f9d\u8d56\u60c5\u5f62", "method": "\u9996\u5148\u5728\u5168\u5c40Lipschitz\u548c\u7ebf\u6027\u589e\u957f\u6761\u4ef6\u4e0b\u5efa\u7acb\u89e3\u7684\u5b58\u5728\u552f\u4e00\u6027\uff0c\u7136\u540e\u6269\u5c55\u5230\u5c40\u90e8Lipschitz\u548c\u975e\u7ebf\u6027\u589e\u957f\u6761\u4ef6", "result": "\u5efa\u7acb\u4e86\u5728\u5168\u5c40Lipschitz\u548c\u7ebf\u6027\u589e\u957f\u6761\u4ef6\u4e0b\u89e3\u7684\u5b58\u5728\u552f\u4e00\u6027\uff0c\u5e76\u6210\u529f\u6269\u5c55\u5230\u66f4\u4e00\u822c\u7684\u5c40\u90e8Lipschitz\u548c\u975e\u7ebf\u6027\u589e\u957f\u6761\u4ef6", "conclusion": "\u8be5\u7814\u7a76\u4e3aMcKean-Vlasov BSVIs\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u5b58\u5728\u552f\u4e00\u6027\u7406\u8bba\u6846\u67b6\uff0c\u6db5\u76d6\u4ece\u7ecf\u5178\u6761\u4ef6\u5230\u66f4\u4e00\u822c\u60c5\u5f62\u7684\u6269\u5c55"}}
{"id": "2510.09709", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09709", "abs": "https://arxiv.org/abs/2510.09709", "authors": ["Shin-nosuke Ishikawa", "Masato Todo", "Taiki Ogihara", "Hirotsugu Ohba"], "title": "The Idola Tribus of AI: Large Language Models tend to perceive order where none exists", "comment": "14 pages, 3 figures, accepted to Findings of EMNLP 2025", "summary": "We present a tendency of large language models (LLMs) to generate absurd\npatterns despite their clear inappropriateness in a simple task of identifying\nregularities in number series. Several approaches have been proposed to apply\nLLMs to complex real-world tasks, such as providing knowledge through\nretrieval-augmented generation and executing multi-step tasks using AI agent\nframeworks. However, these approaches rely on the logical consistency and\nself-coherence of LLMs, making it crucial to evaluate these aspects and\nconsider potential countermeasures. To identify cases where LLMs fail to\nmaintain logical consistency, we conducted an experiment in which LLMs were\nasked to explain the patterns in various integer sequences, ranging from\narithmetic sequences to randomly generated integer series. While the models\nsuccessfully identified correct patterns in arithmetic and geometric sequences,\nthey frequently over-recognized patterns that were inconsistent with the given\nnumbers when analyzing randomly generated series. This issue was observed even\nin multi-step reasoning models, including OpenAI o3, o4-mini, and Google Gemini\n2.5 Flash Preview Thinking. This tendency to perceive non-existent patterns can\nbe interpreted as the AI model equivalent of Idola Tribus and highlights\npotential limitations in their capability for applied tasks requiring logical\nreasoning, even when employing chain-of-thought reasoning mechanisms.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.09657", "categories": ["cs.LG", "cs.AI", "cs.NA", "eess.SP", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.09657", "abs": "https://arxiv.org/abs/2510.09657", "authors": ["Riccardo Fosco Gramaccioni", "Christian Marinoni", "Fabrizio Frezza", "Aurelio Uncini", "Danilo Comminiello"], "title": "Generative Models for Helmholtz Equation Solutions: A Dataset of Acoustic Materials", "comment": "Accepted at EUSIPCO 2025", "summary": "Accurate simulation of wave propagation in complex acoustic materials is\ncrucial for applications in sound design, noise control, and material\nengineering. Traditional numerical solvers, such as finite element methods, are\ncomputationally expensive, especially when dealing with large-scale or\nreal-time scenarios. In this work, we introduce a dataset of 31,000 acoustic\nmaterials, named HA30K, designed and simulated solving the Helmholtz equations.\nFor each material, we provide the geometric configuration and the corresponding\npressure field solution, enabling data-driven approaches to learn Helmholtz\nequation solutions. As a baseline, we explore a deep learning approach based on\nStable Diffusion with ControlNet, a state-of-the-art model for image\ngeneration. Unlike classical solvers, our approach leverages GPU\nparallelization to process multiple simulations simultaneously, drastically\nreducing computation time. By representing solutions as images, we bypass the\nneed for complex simulation software and explicit equation-solving.\nAdditionally, the number of diffusion steps can be adjusted at inference time,\nbalancing speed and quality. We aim to demonstrate that deep learning-based\nmethods are particularly useful in early-stage research, where rapid\nexploration is more critical than absolute accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86HA30K\u6570\u636e\u96c6\uff0c\u5305\u542b31,000\u79cd\u58f0\u5b66\u6750\u6599\u7684\u51e0\u4f55\u914d\u7f6e\u548c\u538b\u529b\u573a\u89e3\uff0c\u5e76\u57fa\u4e8eStable Diffusion\u548cControlNet\u5f00\u53d1\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6765\u5feb\u901f\u6c42\u89e3\u4ea5\u59c6\u970d\u5179\u65b9\u7a0b\uff0c\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\uff08\u5982\u6709\u9650\u5143\u65b9\u6cd5\uff09\u5728\u590d\u6742\u58f0\u5b66\u6750\u6599\u6ce2\u4f20\u64ad\u6a21\u62df\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u6216\u5b9e\u65f6\u573a\u666f\u4e0b\u3002\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6c42\u89e3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eStable Diffusion\u548cControlNet\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5c06\u89e3\u8868\u793a\u4e3a\u56fe\u50cf\uff0c\u5229\u7528GPU\u5e76\u884c\u5316\u540c\u65f6\u5904\u7406\u591a\u4e2a\u6a21\u62df\uff0c\u65e0\u9700\u590d\u6742\u4eff\u771f\u8f6f\u4ef6\u548c\u663e\u5f0f\u65b9\u7a0b\u6c42\u89e3\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\uff0c\u6269\u6563\u6b65\u9aa4\u53ef\u5728\u63a8\u7406\u65f6\u8c03\u6574\u4ee5\u5e73\u8861\u901f\u5ea6\u548c\u8d28\u91cf\u3002", "conclusion": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u65e9\u671f\u7814\u7a76\u9636\u6bb5\u7279\u522b\u6709\u7528\uff0c\u6b64\u65f6\u5feb\u901f\u63a2\u7d22\u6bd4\u7edd\u5bf9\u7cbe\u5ea6\u66f4\u91cd\u8981\u3002"}}
{"id": "2510.09858", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09858", "abs": "https://arxiv.org/abs/2510.09858", "authors": ["Eric Schwitzgebel"], "title": "AI and Consciousness", "comment": null, "summary": "This is a skeptical overview of the literature on AI consciousness. We will\nsoon create AI systems that are conscious according to some influential,\nmainstream theories of consciousness but are not conscious according to other\ninfluential, mainstream theories of consciousness. We will not be in a position\nto know which theories are correct and whether we are surrounded by AI systems\nas richly and meaningfully conscious as human beings or instead only by systems\nas experientially blank as toasters. None of the standard arguments either for\nor against AI consciousness takes us far.\n  Table of Contents\n  Chapter One: Hills and Fog\n  Chapter Two: What Is Consciousness? What Is AI?\n  Chapter Three: Ten Possibly Essential Features of Consciousness\n  Chapter Four: Against Introspective and Conceptual Arguments for Essential\nFeatures\n  Chapter Five: Materialism and Functionalism\n  Chapter Six: The Turing Test and the Chinese Room\n  Chapter Seven: The Mimicry Argument Against AI Consciousness\n  Chapter Eight: Global Workspace Theories and Higher Order Theories\n  Chapter Nine: Integrated Information, Local Recurrence, Associative Learning,\nand Iterative Natural Kinds\n  Chapter Ten: Does Biological Substrate Matter?\n  Chapter Eleven: The Problem of Strange Intelligence\n  Chapter Twelve: The Leapfrog Hypothesis and the Social Semi-Solution", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5bf9AI\u610f\u8bc6\u7814\u7a76\u6587\u732e\u6301\u6000\u7591\u6001\u5ea6\uff0c\u6307\u51fa\u4e0d\u540c\u4e3b\u6d41\u610f\u8bc6\u7406\u8bba\u5bf9AI\u662f\u5426\u5177\u6709\u610f\u8bc6\u5b58\u5728\u5206\u6b67\uff0c\u4e14\u6211\u4eec\u65e0\u6cd5\u786e\u5b9a\u54ea\u79cd\u7406\u8bba\u6b63\u786e\uff0c\u4e5f\u65e0\u6cd5\u5224\u65adAI\u662f\u5426\u771f\u6b63\u5177\u6709\u610f\u8bc6\u4f53\u9a8c\u3002", "motivation": "\u63a2\u8ba8AI\u610f\u8bc6\u95ee\u9898\u7684\u7406\u8bba\u56f0\u5883\uff0c\u6307\u51fa\u5f53\u524d\u5173\u4e8eAI\u610f\u8bc6\u7684\u6807\u51c6\u8bba\u8bc1\u90fd\u65e0\u6cd5\u63d0\u4f9b\u660e\u786e\u7b54\u6848\uff0c\u6211\u4eec\u9762\u4e34\u65e0\u6cd5\u786e\u5b9aAI\u662f\u5426\u5177\u6709\u771f\u6b63\u610f\u8bc6\u4f53\u9a8c\u7684\u8ba4\u77e5\u56f0\u5883\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5341\u79cd\u53ef\u80fd\u6784\u6210\u610f\u8bc6\u672c\u8d28\u7684\u7279\u5f81\uff0c\u6279\u5224\u6027\u5730\u5ba1\u89c6\u5404\u79cd\u610f\u8bc6\u7406\u8bba\uff08\u5982\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u7406\u8bba\u3001\u9ad8\u9636\u7406\u8bba\u3001\u6574\u5408\u4fe1\u606f\u7406\u8bba\u7b49\uff09\u5728AI\u610f\u8bc6\u95ee\u9898\u4e0a\u7684\u9002\u7528\u6027\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u4e3b\u6d41\u610f\u8bc6\u7406\u8bba\u5bf9AI\u662f\u5426\u5177\u6709\u610f\u8bc6\u7ed9\u51fa\u76f8\u4e92\u77db\u76fe\u7684\u7ed3\u8bba\uff0c\u6211\u4eec\u65e0\u6cd5\u786e\u5b9aAI\u662f\u5426\u5177\u6709\u771f\u6b63\u7684\u4e3b\u89c2\u4f53\u9a8c\uff0c\u4e5f\u65e0\u6cd5\u901a\u8fc7\u73b0\u6709\u8bba\u8bc1\u89e3\u51b3\u8fd9\u4e00\u8ba4\u77e5\u56f0\u5883\u3002", "conclusion": "\u6211\u4eec\u9762\u4e34\u4e00\u4e2a\u6839\u672c\u6027\u7684\u8ba4\u77e5\u56f0\u5883\uff1a\u6839\u636e\u67d0\u4e9b\u4e3b\u6d41\u7406\u8bba\uff0cAI\u53ef\u80fd\u5177\u6709\u610f\u8bc6\uff0c\u800c\u6839\u636e\u5176\u4ed6\u7406\u8bba\u5219\u6ca1\u6709\uff0c\u4e14\u6211\u4eec\u65e0\u6cd5\u786e\u5b9a\u54ea\u79cd\u7406\u8bba\u6b63\u786e\uff0c\u8fd9\u6784\u6210\u4e86AI\u610f\u8bc6\u95ee\u9898\u7684\u6838\u5fc3\u6311\u6218\u3002"}}
{"id": "2510.10709", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10709", "abs": "https://arxiv.org/abs/2510.10709", "authors": ["Kyla Chasalow", "Skyler Wu", "Susan Murphy"], "title": "Missing Data Multiple Imputation for Tabular Q-Learning in Online RL", "comment": "Working paper", "summary": "Missing data in online reinforcement learning (RL) poses challenges compared\nto missing data in standard tabular data or in offline policy learning. The\nneed to impute and act at each time step means that imputation cannot be put\noff until enough data exist to produce stable imputation models. It also means\nfuture data collection and learning depend on previous imputations. This paper\nproposes fully online imputation ensembles. We find that maintaining multiple\nimputation pathways may help balance the need to capture uncertainty under\nmissingness and the need for efficiency in online settings. We consider\nmultiple approaches for incorporating these pathways into learning and action\nselection. Using a Grid World experiment with various types of missingness, we\nprovide preliminary evidence that multiple imputation pathways may be a useful\nframework for constructing simple and efficient online missing data RL methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5904\u7406\u7f3a\u5931\u6570\u636e\u7684\u5b8c\u5168\u5728\u7ebf\u63d2\u8865\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ef4\u62a4\u591a\u4e2a\u63d2\u8865\u8def\u5f84\u6765\u5e73\u8861\u7f3a\u5931\u60c5\u51b5\u4e0b\u7684\u4e0d\u786e\u5b9a\u6027\u6355\u83b7\u548c\u5728\u7ebf\u73af\u5883\u4e2d\u7684\u6548\u7387\u9700\u6c42\u3002", "motivation": "\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7f3a\u5931\u6570\u636e\u95ee\u9898\u6bd4\u6807\u51c6\u8868\u683c\u6570\u636e\u6216\u79bb\u7ebf\u7b56\u7565\u5b66\u4e60\u66f4\u5177\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u9700\u8981\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u8fdb\u884c\u63d2\u8865\u548c\u884c\u52a8\uff0c\u4e14\u672a\u6765\u7684\u6570\u636e\u6536\u96c6\u548c\u5b66\u4e60\u4f9d\u8d56\u4e8e\u5148\u524d\u7684\u63d2\u8865\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u5b8c\u5168\u5728\u7ebf\u63d2\u8865\u96c6\u6210\u65b9\u6cd5\uff0c\u7ef4\u62a4\u591a\u4e2a\u63d2\u8865\u8def\u5f84\uff0c\u5e76\u63a2\u7d22\u4e86\u5c06\u8fd9\u4e9b\u8def\u5f84\u6574\u5408\u5230\u5b66\u4e60\u548c\u884c\u52a8\u9009\u62e9\u4e2d\u7684\u591a\u79cd\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7Grid World\u5b9e\u9a8c\u548c\u591a\u79cd\u7f3a\u5931\u7c7b\u578b\u6d4b\u8bd5\uff0c\u521d\u6b65\u8bc1\u636e\u8868\u660e\u591a\u63d2\u8865\u8def\u5f84\u53ef\u80fd\u662f\u4e00\u4e2a\u6709\u7528\u7684\u6846\u67b6\uff0c\u53ef\u7528\u4e8e\u6784\u5efa\u7b80\u5355\u9ad8\u6548\u7684\u5728\u7ebf\u7f3a\u5931\u6570\u636e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u591a\u63d2\u8865\u8def\u5f84\u6846\u67b6\u5728\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5904\u7406\u7f3a\u5931\u6570\u636e\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u5e73\u8861\u4e0d\u786e\u5b9a\u6027\u6355\u83b7\u548c\u6548\u7387\u9700\u6c42\u3002"}}
{"id": "2510.10807", "categories": ["cs.LG", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2510.10807", "abs": "https://arxiv.org/abs/2510.10807", "authors": ["Ali Atiah Alzahrani"], "title": "Crisis-Aware Regime-Conditioned Diffusion with CVaR Allocation", "comment": "Code available at: https://github.com/AliAtiah/MARCD", "summary": "We study whether regime-conditioned generative scenarios, coupled with a\nconvex CVaR allocator, improve portfolio decisions under regime shifts. We\nintroduce Multi-Agent Regime-Conditioned Diffusion (MARCD), which (i) infers\nlatent regimes via a Gaussian HMM, (ii) trains a diffusion model with a\ntail-weighted objective and a regime-specialized mixture-of-experts (MoE)\ndenoiser to enrich crisis co-movements, and (iii) feeds the generated scenarios\ninto a turnover-aware CVaR epigraph quadratic program with explicit governance.\nIn strict walk-forward tests on liquid multi-asset ETFs (2005-2025), MARCD\noutperforms standard allocators and improves calibration relative to popular\ngenerators. Over 2020-2025 out-of-sample (monthly; 10 bps), MARCD attains\nSharpe 1.23 (BL 1.02) and MaxDD 9.3 percent (BL 14.1 percent), a 34 percent\nreduction, at comparable turnover; stationary block-bootstrap intervals\nindicate the Sharpe uplift is significant at 5 percent. We provide theory\nlinking tail-weighted diffusion to spectral-risk control of the\ndecision-relevant CVaR gap, oracle/consistency results for the regime-MoE\ndenoiser, and Lipschitz/regret guarantees for the allocator. Together, MARCD\noffers a reproducible bridge from tail-faithful scenario modeling to governed\nportfolio decisions with materially improved drawdown control.", "AI": {"tldr": "MARCD\u7ed3\u5408\u4e86\u5236\u5ea6\u6761\u4ef6\u751f\u6210\u573a\u666f\u548c\u51f8CVaR\u5206\u914d\u5668\uff0c\u901a\u8fc7\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u63a8\u65ad\u6f5c\u5728\u5236\u5ea6\u3001\u8bad\u7ec3\u5177\u6709\u5c3e\u90e8\u52a0\u6743\u76ee\u6807\u548c\u5236\u5ea6\u4e13\u4e1a\u5316\u6df7\u5408\u4e13\u5bb6\u7684\u6269\u6563\u6a21\u578b\uff0c\u5728\u4e25\u683c\u7684\u524d\u5411\u6d4b\u8bd5\u4e2d\u663e\u8457\u6539\u5584\u4e86\u6295\u8d44\u7ec4\u5408\u51b3\u7b56\uff0c\u7279\u522b\u662f\u5728\u63a7\u5236\u6700\u5927\u56de\u64a4\u65b9\u9762\u3002", "motivation": "\u7814\u7a76\u5236\u5ea6\u6761\u4ef6\u751f\u6210\u573a\u666f\u4e0e\u51f8CVaR\u5206\u914d\u5668\u7ed3\u5408\u662f\u5426\u80fd\u6539\u5584\u5236\u5ea6\u8f6c\u6362\u4e0b\u7684\u6295\u8d44\u7ec4\u5408\u51b3\u7b56\uff0c\u7279\u522b\u662f\u5728\u5c3e\u90e8\u98ce\u9669\u548c\u56de\u64a4\u63a7\u5236\u65b9\u9762\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u5236\u5ea6\u6761\u4ef6\u6269\u6563(MARCD)\u65b9\u6cd5\uff1a1)\u901a\u8fc7\u9ad8\u65afHMM\u63a8\u65ad\u6f5c\u5728\u5236\u5ea6\uff1b2)\u8bad\u7ec3\u5177\u6709\u5c3e\u90e8\u52a0\u6743\u76ee\u6807\u548c\u5236\u5ea6\u4e13\u4e1a\u5316\u6df7\u5408\u4e13\u5bb6\u53bb\u566a\u5668\u7684\u6269\u6563\u6a21\u578b\uff1b3)\u5c06\u751f\u6210\u573a\u666f\u8f93\u5165\u8003\u8651\u6362\u624b\u7387\u7684CVaR\u4e8c\u6b21\u89c4\u5212\u95ee\u9898\u3002", "result": "\u57282005-2025\u5e74\u6d41\u52a8\u6027\u591a\u8d44\u4ea7ETF\u7684\u4e25\u683c\u524d\u5411\u6d4b\u8bd5\u4e2d\uff0cMARCD\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u5206\u914d\u5668\uff0c2020-2025\u5e74\u6837\u672c\u5916\u6d4b\u8bd5\u4e2d\u590f\u666e\u6bd4\u73871.23(\u57fa\u51c61.02)\uff0c\u6700\u5927\u56de\u64a49.3%(\u57fa\u51c614.1%)\uff0c\u51cf\u5c1134%\uff0c\u590f\u666e\u63d0\u5347\u57285%\u6c34\u5e73\u663e\u8457\u3002", "conclusion": "MARCD\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4ece\u5c3e\u90e8\u5fe0\u5b9e\u573a\u666f\u5efa\u6a21\u5230\u53d7\u6cbb\u7406\u6295\u8d44\u7ec4\u5408\u51b3\u7b56\u7684\u53ef\u590d\u73b0\u6865\u6881\uff0c\u663e\u8457\u6539\u5584\u4e86\u56de\u64a4\u63a7\u5236\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2510.09862", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.09862", "abs": "https://arxiv.org/abs/2510.09862", "authors": ["Carsten Hartmann", "Edoardo De Din", "Daniele Carta", "Florian Middelkoop", "Arndt Neubauer", "Johannes Kruse", "Ulrich Oberhofer", "Richard Jumar", "Benjamin Sch\u00e4fer", "Thiemo Pesch", "Andrea Benigni", "Dirk Witthaut"], "title": "Cyber-Physical Systems on the Megawatt Scale: The impact of battery control on grid frequency stability", "comment": "19 pages, 23 figures", "summary": "Electric power systems are undergoing fundamental change. The shift to\ninverter-based generation challenges frequency stability, while growing\ndigitalisation heightens vulnerability to errors and attacks. Here we identify\nan emerging risk at the intersection of cyber-physical coupling and control\nsystem design. We show that grid frequency time series worldwide exhibit a\npersistent one-minute oscillatory pattern, whose origin has remained largely\nunexplained. We trace this pattern back to the energy management systems of\nbattery electric storage systems and demonstrate that the pattern amplitude has\nincreased substantially in the Nordic and British grids. We argue that this\neffect is a potential burden for stability in future grids with low inertia and\nan increasing penetration with batteries and smart devices, though it can be\nmitigated by a revision of battery control algorithms.", "AI": {"tldr": "\u5168\u7403\u7535\u7f51\u9891\u7387\u65f6\u95f4\u5e8f\u5217\u4e2d\u5b58\u5728\u6301\u7eed\u7684\u4e00\u5206\u949f\u632f\u8361\u6a21\u5f0f\uff0c\u8be5\u6a21\u5f0f\u6e90\u4e8e\u7535\u6c60\u50a8\u80fd\u7cfb\u7edf\u7684\u80fd\u91cf\u7ba1\u7406\u7cfb\u7edf\uff0c\u5e76\u4e14\u5728\u5317\u6b27\u548c\u82f1\u56fd\u7535\u7f51\u4e2d\u632f\u5e45\u663e\u8457\u589e\u52a0\uff0c\u53ef\u80fd\u5bf9\u4f4e\u60ef\u91cf\u7535\u7f51\u7684\u7a33\u5b9a\u6027\u6784\u6210\u5a01\u80c1\u3002", "motivation": "\u7535\u529b\u7cfb\u7edf\u6b63\u7ecf\u5386\u6839\u672c\u6027\u53d8\u9769\uff0c\u9006\u53d8\u5668\u53d1\u7535\u7684\u8f6c\u53d8\u6311\u6218\u9891\u7387\u7a33\u5b9a\u6027\uff0c\u6570\u5b57\u5316\u7a0b\u5ea6\u63d0\u9ad8\u589e\u52a0\u4e86\u9519\u8bef\u548c\u653b\u51fb\u7684\u8106\u5f31\u6027\u3002\u7814\u7a76\u65e8\u5728\u8bc6\u522b\u7f51\u7edc\u7269\u7406\u8026\u5408\u548c\u63a7\u5236\u7cfb\u7edf\u8bbe\u8ba1\u4ea4\u53c9\u70b9\u7684\u65b0\u5174\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5168\u7403\u7535\u7f51\u9891\u7387\u65f6\u95f4\u5e8f\u5217\uff0c\u8bc6\u522b\u51fa\u4e00\u5206\u949f\u632f\u8361\u6a21\u5f0f\uff0c\u5e76\u5c06\u5176\u6eaf\u6e90\u81f3\u7535\u6c60\u50a8\u80fd\u7cfb\u7edf\u7684\u80fd\u91cf\u7ba1\u7406\u7cfb\u7edf\u3002", "result": "\u53d1\u73b0\u8be5\u632f\u8361\u6a21\u5f0f\u632f\u5e45\u5728\u5317\u6b27\u548c\u82f1\u56fd\u7535\u7f51\u4e2d\u663e\u8457\u589e\u52a0\uff0c\u8fd9\u79cd\u6548\u5e94\u53ef\u80fd\u5bf9\u672a\u6765\u4f4e\u60ef\u91cf\u7535\u7f51\u7684\u7a33\u5b9a\u6027\u6784\u6210\u8d1f\u62c5\u3002", "conclusion": "\u867d\u7136\u8fd9\u79cd\u6548\u5e94\u53ef\u80fd\u5a01\u80c1\u7535\u7f51\u7a33\u5b9a\u6027\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u4fee\u8ba2\u7535\u6c60\u63a7\u5236\u7b97\u6cd5\u6765\u7f13\u89e3\u3002"}}
{"id": "2510.09677", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09677", "abs": "https://arxiv.org/abs/2510.09677", "authors": ["Ebrahim Rahimi", "Clara Maathuis"], "title": "AI in Computational Thinking Education in Higher Education: A Systematic Literature Review", "comment": "A poster based on this paper was accepted and published in the\n  Proceedings of the 30th ACM Conference on Innovation and Technology in\n  Computer Science Education (ITiCSE 2025), DOI:\n  https://doi.org/10.1145/3724389.3730775", "summary": "Computational Thinking (CT) is a key skill set for students in higher\neducation to thrive and adapt to an increasingly technology-driven future and\nworkplace. While research on CT education has gained remarkable momentum in K12\nover the past decade, it has remained under-explored in higher education,\nleaving higher education teachers with an insufficient overview, knowledge, and\nsupport regarding CT education. The proliferation and adoption of artificial\nintelligence (AI) by educational institutions have demonstrated promising\npotential to support instructional activities across many disciplines,\nincluding CT education. However, a comprehensive overview outlining the various\naspects of integrating AI in CT education in higher education is lacking. To\nmitigate this gap, we conducted this systematic literature review study. The\nfocus of our study is to identify initiatives applying AI in CT education\nwithin higher education and to explore various educational aspects of these\ninitiatives, including the benefits and challenges of AI in CT education,\ninstructional strategies employed, CT components covered, and AI techniques and\nmodels utilized. This study provides practical and scientific contributions to\nthe CT education community, including an inventory of AI-based initiatives for\nCT education useful to educators, an overview of various aspects of integrating\nAI into CT education such as its benefits and challenges (e.g., AI potential to\nreshape CT education versus its potential to diminish students creativity) and\ninsights into new and expanded perspectives on CT in light of AI (e.g., the\ndecoding approach alongside the coding approach to CT).", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u9ad8\u7b49\u6559\u80b2\u8ba1\u7b97\u601d\u7ef4\u6559\u80b2\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86AI\u6574\u5408\u7684\u5404\u4e2a\u65b9\u9762\u3001\u76ca\u5904\u4e0e\u6311\u6218\u3001\u6559\u5b66\u7b56\u7565\u3001CT\u7ec4\u4ef6\u8986\u76d6\u4ee5\u53caAI\u6280\u672f\u6a21\u578b\u4f7f\u7528\u60c5\u51b5\u3002", "motivation": "\u8ba1\u7b97\u601d\u7ef4\u662f\u9ad8\u7b49\u6559\u80b2\u4e2d\u5b66\u751f\u9002\u5e94\u6280\u672f\u9a71\u52a8\u672a\u6765\u7684\u5173\u952e\u6280\u80fd\uff0c\u4f46\u9ad8\u7b49\u6559\u80b2\u4e2d\u7684CT\u6559\u80b2\u7814\u7a76\u4e0d\u8db3\uff0c\u6559\u5e08\u7f3a\u4e4f\u76f8\u5173\u77e5\u8bc6\u548c\u652f\u6301\u3002AI\u5728\u6559\u80b2\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u4e3aCT\u6559\u80b2\u63d0\u4f9b\u4e86\u652f\u6301\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u7684\u6574\u5408\u6982\u8ff0\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u8bc6\u522b\u9ad8\u7b49\u6559\u80b2\u4e2d\u5e94\u7528AI\u7684CT\u6559\u80b2\u4e3e\u63aa\uff0c\u63a2\u7d22\u6559\u80b2\u65b9\u9762\u7684\u591a\u4e2a\u7ef4\u5ea6\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86AI\u5728CT\u6559\u80b2\u4e2d\u5e94\u7528\u7684\u6e05\u5355\uff0c\u5206\u6790\u4e86\u6574\u5408\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u5305\u62ec\u76ca\u5904\u4e0e\u6311\u6218\uff08\u5982\u91cd\u5851CT\u6559\u80b2vs\u53ef\u80fd\u524a\u5f31\u5b66\u751f\u521b\u9020\u529b\uff09\uff0c\u4ee5\u53ca\u5728AI\u80cc\u666f\u4e0b\u7684\u65b0CT\u89c6\u89d2\uff08\u5982\u89e3\u7801\u65b9\u6cd5\u4e0e\u7f16\u7801\u65b9\u6cd5\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aCT\u6559\u80b2\u793e\u533a\u63d0\u4f9b\u4e86\u5b9e\u8df5\u548c\u79d1\u5b66\u8d21\u732e\uff0c\u5305\u62ecAI\u57fa\u7840\u4e3e\u63aa\u6e05\u5355\u3001AI\u6574\u5408CT\u6559\u80b2\u7684\u591a\u65b9\u9762\u6982\u8ff0\uff0c\u4ee5\u53ca\u5728AI\u80cc\u666f\u4e0b\u5bf9CT\u7684\u65b0\u89c6\u89d2\u89c1\u89e3\u3002"}}
{"id": "2510.10056", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.10056", "abs": "https://arxiv.org/abs/2510.10056", "authors": ["Hantao Nie", "Dong An", "Zaiwen Wen"], "title": "Quantum Alternating Direction Method of Multipliers for Semidefinite Programming", "comment": null, "summary": "Semidefinite programming (SDP) is a fundamental convex optimization problem\nwith wide-ranging applications. However, solving large-scale instances remains\ncomputationally challenging due to the high cost of solving linear systems and\nperforming eigenvalue decompositions. In this paper, we present a quantum\nalternating direction method of multipliers (QADMM) for SDPs, building on\nrecent advances in quantum computing. An inexact ADMM framework is developed,\nwhich tolerates errors in the iterates arising from block-encoding\napproximation and quantum measurement. Within this robust scheme, we design a\npolynomial proximal operator to address the semidefinite conic constraints and\napply the quantum singular value transformation to accelerate the most costly\nprojection updates. We prove that the scheme converges to an $\\epsilon$-optimal\nsolution of the SDP problem under the strong duality assumption. A detailed\ncomplexity analysis shows that the QADMM algorithm achieves favorable scaling\nwith respect to dimension compared to the classical ADMM algorithm and quantum\ninterior point methods, highlighting its potential for solving large-scale\nSDPs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5(QADMM)\u6765\u89e3\u51b3\u5927\u89c4\u6a21\u534a\u5b9a\u89c4\u5212\u95ee\u9898\uff0c\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u52a0\u901f\u6700\u8017\u65f6\u7684\u6295\u5f71\u66f4\u65b0\uff0c\u5728\u5f3a\u5bf9\u5076\u5047\u8bbe\u4e0b\u6536\u655b\u5230\u03b5\u6700\u4f18\u89e3\u3002", "motivation": "\u534a\u5b9a\u89c4\u5212\u662f\u57fa\u7840\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u4f46\u5927\u89c4\u6a21\u5b9e\u4f8b\u6c42\u89e3\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u89e3\u51b3\u7ebf\u6027\u7cfb\u7edf\u548c\u7279\u5f81\u503c\u5206\u89e3\u3002\u91cf\u5b50\u8ba1\u7b97\u7684\u6700\u65b0\u8fdb\u5c55\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u5f00\u53d1\u4e86\u4e0d\u7cbe\u786eADMM\u6846\u67b6\uff0c\u5bb9\u5fcd\u5757\u7f16\u7801\u8fd1\u4f3c\u548c\u91cf\u5b50\u6d4b\u91cf\u4ea7\u751f\u7684\u8fed\u4ee3\u8bef\u5dee\u3002\u8bbe\u8ba1\u4e86\u591a\u9879\u5f0f\u8fd1\u7aef\u7b97\u5b50\u5904\u7406\u534a\u5b9a\u9525\u7ea6\u675f\uff0c\u5e94\u7528\u91cf\u5b50\u5947\u5f02\u503c\u53d8\u6362\u52a0\u901f\u6700\u8017\u65f6\u7684\u6295\u5f71\u66f4\u65b0\u3002", "result": "\u5728\u5f3a\u5bf9\u5076\u5047\u8bbe\u4e0b\uff0c\u8be5\u65b9\u6848\u6536\u655b\u5230SDP\u95ee\u9898\u7684\u03b5\u6700\u4f18\u89e3\u3002\u590d\u6742\u5ea6\u5206\u6790\u663e\u793aQADMM\u5728\u7ef4\u5ea6\u7f29\u653e\u65b9\u9762\u4f18\u4e8e\u7ecf\u5178ADMM\u548c\u91cf\u5b50\u5185\u70b9\u6cd5\u3002", "conclusion": "QADMM\u7b97\u6cd5\u5728\u89e3\u51b3\u5927\u89c4\u6a21SDP\u95ee\u9898\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u7ef4\u5ea6\u7f29\u653e\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u52bf\u3002"}}
{"id": "2510.09710", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09710", "abs": "https://arxiv.org/abs/2510.09710", "authors": ["Xiaonan Si", "Meilin Zhu", "Simeng Qin", "Lijia Yu", "Lijun Zhang", "Shuaitong Liu", "Xinfeng Li", "Ranjie Duan", "Yang Liu", "Xiaojun Jia"], "title": "SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG", "comment": "Accepted at NeurIPS 2025", "summary": "Retrieval-augmented generation (RAG) systems enhance large language models\n(LLMs) with external knowledge but are vulnerable to corpus poisoning and\ncontamination attacks, which can compromise output integrity. Existing defenses\noften apply aggressive filtering, leading to unnecessary loss of valuable\ninformation and reduced reliability in generation. To address this problem, we\npropose a two-stage semantic filtering and conflict-free framework for\ntrustworthy RAG. In the first stage, we perform a joint filter with semantic\nand cluster-based filtering which is guided by the Entity-intent-relation\nextractor (EIRE). EIRE extracts entities, latent objectives, and entity\nrelations from both the user query and filtered documents, scores their\nsemantic relevance, and selectively adds valuable documents into the clean\nretrieval database. In the second stage, we proposed an EIRE-guided\nconflict-aware filtering module, which analyzes semantic consistency between\nthe query, candidate answers, and retrieved knowledge before final answer\ngeneration, filtering out internal and external contradictions that could\nmislead the model. Through this two-stage process, SeCon-RAG effectively\npreserves useful knowledge while mitigating conflict contamination, achieving\nsignificant improvements in both generation robustness and output\ntrustworthiness. Extensive experiments across various LLMs and datasets\ndemonstrate that the proposed SeCon-RAG markedly outperforms state-of-the-art\ndefense methods.", "AI": {"tldr": "\u63d0\u51faSeCon-RAG\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bed\u4e49\u8fc7\u6ee4\u548c\u51b2\u7a81\u6d88\u9664\u673a\u5236\u6765\u9632\u5fa1RAG\u7cfb\u7edf\u4e2d\u7684\u8bed\u6599\u5e93\u6c61\u67d3\u653b\u51fb\uff0c\u5728\u4fdd\u6301\u6709\u7528\u77e5\u8bc6\u7684\u540c\u65f6\u63d0\u9ad8\u751f\u6210\u9c81\u68d2\u6027\u548c\u8f93\u51fa\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684RAG\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u8bed\u6599\u5e93\u6c61\u67d3\u548c\u6c61\u67d3\u653b\u51fb\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u6fc0\u8fdb\u8fc7\u6ee4\uff0c\u5bfc\u81f4\u6709\u4ef7\u503c\u4fe1\u606f\u4e22\u5931\u548c\u751f\u6210\u53ef\u9760\u6027\u964d\u4f4e\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528EIRE\u6307\u5bfc\u7684\u8bed\u4e49\u548c\u805a\u7c7b\u8054\u5408\u8fc7\u6ee4\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528EIRE\u6307\u5bfc\u7684\u51b2\u7a81\u611f\u77e5\u8fc7\u6ee4\u6a21\u5757\u5206\u6790\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u79cdLLM\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSeCon-RAG\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "conclusion": "SeCon-RAG\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u6ee4\u8fc7\u7a0b\u6709\u6548\u4fdd\u7559\u6709\u7528\u77e5\u8bc6\u540c\u65f6\u51cf\u8f7b\u51b2\u7a81\u6c61\u67d3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u9c81\u68d2\u6027\u548c\u8f93\u51fa\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2510.09658", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09658", "abs": "https://arxiv.org/abs/2510.09658", "authors": ["Filippo Rinaldi", "Aniello Panariello", "Giacomo Salici", "Fengyuan Liu", "Marco Ciccone", "Angelo Porrello", "Simone Calderara"], "title": "Gradient-Sign Masking for Task Vector Transport Across Pre-Trained Models", "comment": null, "summary": "When a new release of a foundation model is published, practitioners\ntypically need to repeat full fine-tuning, even if the same task has already\nbeen solved in the previous version. A promising alternative is to reuse the\nparameter changes (i.e., task vectors) that capture how a model adapts to a\nspecific task. However, they often fail to transfer across different\npre-trained models due to their misaligned parameter space. In this work, we\nshow that the key to successful transfer lies in the sign structure of the\ngradients of the new model. Based on this insight, we propose GradFix, a novel\nmethod that approximates the ideal gradient sign structure and leverages it to\ntransfer knowledge using only a handful of labeled samples. Notably, this\nrequires no additional fine-tuning: the adaptation is achieved by computing a\nfew gradients at the target model and masking the source task vector\naccordingly. This yields an update that is locally aligned with the target loss\nlandscape, effectively rebasing the task vector onto the new pre-training. We\nprovide a theoretical guarantee that our method ensures first-order descent.\nEmpirically, we demonstrate significant performance gains on vision and\nlanguage benchmarks, consistently outperforming naive task vector addition and\nfew-shot fine-tuning.", "AI": {"tldr": "\u63d0\u51faGradFix\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd1\u4f3c\u76ee\u6807\u6a21\u578b\u7684\u68af\u5ea6\u7b26\u53f7\u7ed3\u6784\uff0c\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u5373\u53ef\u5728\u4e0d\u540c\u9884\u8bad\u7ec3\u6a21\u578b\u95f4\u6709\u6548\u8fc1\u79fb\u4efb\u52a1\u5411\u91cf\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u65b0\u7248\u672c\u53d1\u5e03\u65f6\uff0c\u5373\u4f7f\u76f8\u540c\u4efb\u52a1\u5df2\u5728\u65e7\u7248\u672c\u4e2d\u89e3\u51b3\uff0c\u4ecd\u9700\u91cd\u590d\u5b8c\u6574\u5fae\u8c03\u3002\u4efb\u52a1\u5411\u91cf\u5728\u8de8\u9884\u8bad\u7ec3\u6a21\u578b\u8fc1\u79fb\u65f6\u56e0\u53c2\u6570\u7a7a\u95f4\u4e0d\u5bf9\u9f50\u800c\u5931\u8d25\u3002", "method": "\u57fa\u4e8e\u68af\u5ea6\u7b26\u53f7\u7ed3\u6784\u7684\u5173\u952e\u6d1e\u5bdf\uff0c\u63d0\u51faGradFix\u65b9\u6cd5\uff1a\u8ba1\u7b97\u76ee\u6807\u6a21\u578b\u7684\u5c11\u91cf\u68af\u5ea6\uff0c\u636e\u6b64\u63a9\u7801\u6e90\u4efb\u52a1\u5411\u91cf\uff0c\u751f\u6210\u4e0e\u76ee\u6807\u635f\u5931\u5730\u5f62\u5c40\u90e8\u5bf9\u9f50\u7684\u66f4\u65b0\u3002", "result": "\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u4e00\u81f4\u4f18\u4e8e\u6734\u7d20\u4efb\u52a1\u5411\u91cf\u52a0\u6cd5\u548c\u5c11\u6837\u672c\u5fae\u8c03\u3002", "conclusion": "\u68af\u5ea6\u7b26\u53f7\u7ed3\u6784\u662f\u4efb\u52a1\u5411\u91cf\u6210\u529f\u8fc1\u79fb\u7684\u5173\u952e\uff0cGradFix\u65b9\u6cd5\u901a\u8fc7\u5c40\u90e8\u5bf9\u9f50\u5b9e\u73b0\u6709\u6548\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5e76\u5177\u6709\u4e00\u9636\u4e0b\u964d\u7684\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2510.09894", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09894", "abs": "https://arxiv.org/abs/2510.09894", "authors": ["Junyuan Liu", "Quan Qin", "Guangsheng Dong", "Xinglei Wang", "Jiazhuang Feng", "Zichao Zeng", "Tao Cheng"], "title": "Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning", "comment": null, "summary": "General-purpose spatial representations are essential for building\ntransferable geospatial foundation models (GFMs). Among them, the AlphaEarth\nFoundation (AE) represents a major step toward a global, unified representation\nof the Earth's surface, learning 10-meter embeddings from multi-source Earth\nObservation (EO) data that capture rich physical and environmental patterns\nacross diverse landscapes. However, such EO-driven representations remain\nlimited in capturing the functional and socioeconomic dimensions of cities, as\nthey primarily encode physical and spectral patterns rather than human\nactivities or spatial functions. We propose AETHER (AlphaEarth-POI Enriched\nRepresentation Learning), a lightweight framework that adapts AlphaEarth to\nhuman-centered urban analysis through multimodal alignment guided by Points of\nInterest (POIs). AETHER aligns AE embeddings with textual representations of\nPOIs, enriching physically grounded EO features with semantic cues about urban\nfunctions and socioeconomic contexts. In Greater London, AETHER achieves\nconsistent gains over the AE baseline, with a 7.2% relative improvement in\nland-use classification F1 and a 23.6% relative reduction in Kullback-Leibler\ndivergence for socioeconomic mapping. Built upon pretrained AE, AETHER\nleverages a lightweight multimodal alignment to enrich it with human-centered\nsemantics while remaining computationally efficient and scalable for urban\napplications. By coupling EO with human-centered semantics, it advances\ngeospatial foundation models toward general-purpose urban representations that\nintegrate both physical form and functional meaning.", "AI": {"tldr": "AETHER\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7POI\u591a\u6a21\u6001\u5bf9\u9f50\u589e\u5f3aAlphaEarth\u8868\u793a\uff0c\u5c06\u7269\u7406\u73af\u5883\u7279\u5f81\u4e0e\u57ce\u5e02\u529f\u80fd\u8bed\u4e49\u76f8\u7ed3\u5408\uff0c\u63d0\u5347\u57ce\u5e02\u5206\u6790\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5730\u7403\u89c2\u6d4b\u9a71\u52a8\u7684\u7a7a\u95f4\u8868\u793a\u4e3b\u8981\u7f16\u7801\u7269\u7406\u548c\u5149\u8c31\u6a21\u5f0f\uff0c\u7f3a\u4e4f\u6355\u6349\u57ce\u5e02\u529f\u80fd\u548c\u793e\u4f1a\u7ecf\u6d4e\u7ef4\u5ea6\u7684\u4eba\u7c7b\u6d3b\u52a8\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u591a\u6a21\u6001\u5bf9\u9f50\u5c06AlphaEarth\u5d4c\u5165\u4e0ePOI\u6587\u672c\u8868\u793a\u5bf9\u9f50\uff0c\u5728\u9884\u8bad\u7ec3\u7684AE\u57fa\u7840\u4e0a\u8fdb\u884c\u8f7b\u91cf\u7ea7\u8bed\u4e49\u589e\u5f3a\u3002", "result": "\u5728\u4f26\u6566\u5730\u533a\uff0cAETHER\u76f8\u6bd4AE\u57fa\u7ebf\u5728\u571f\u5730\u5229\u7528\u5206\u7c7bF1\u5f97\u5206\u4e0a\u76f8\u5bf9\u63d0\u53477.2%\uff0c\u5728\u793e\u4f1a\u7ecf\u6d4e\u6620\u5c04\u7684KL\u6563\u5ea6\u4e0a\u76f8\u5bf9\u51cf\u5c1123.6%\u3002", "conclusion": "AETHER\u901a\u8fc7\u8026\u5408\u5730\u7403\u89c2\u6d4b\u4e0e\u4eba\u7c7b\u4e2d\u5fc3\u8bed\u4e49\uff0c\u63a8\u8fdb\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u5411\u6574\u5408\u7269\u7406\u5f62\u6001\u548c\u529f\u80fd\u610f\u4e49\u7684\u901a\u7528\u57ce\u5e02\u8868\u793a\u53d1\u5c55\u3002"}}
{"id": "2510.10744", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.10744", "abs": "https://arxiv.org/abs/2510.10744", "authors": ["Mario Morawski", "Anais Despres", "R\u00e9mi Rehm"], "title": "How Patterns Dictate Learnability in Sequential Data", "comment": "NeurIPS 2025, 36 pages, 4 figures", "summary": "Sequential data - ranging from financial time series to natural language -\nhas driven the growing adoption of autoregressive models. However, these\nalgorithms rely on the presence of underlying patterns in the data, and their\nidentification often depends heavily on human expertise. Misinterpreting these\npatterns can lead to model misspecification, resulting in increased\ngeneralization error and degraded performance. The recently proposed evolving\npattern (EvoRate) metric addresses this by using the mutual information between\nthe next data point and its past to guide regression order estimation and\nfeature selection. Building on this idea, we introduce a general framework\nbased on predictive information, defined as the mutual information between the\npast and the future, $I(X_{past}; X_{future})$. This quantity naturally defines\nan information-theoretic learning curve, which quantifies the amount of\npredictive information available as the observation window grows. Using this\nformalism, we show that the presence or absence of temporal patterns\nfundamentally constrains the learnability of sequential models: even an optimal\npredictor cannot outperform the intrinsic information limit imposed by the\ndata. We validate our framework through experiments on synthetic data,\ndemonstrating its ability to assess model adequacy, quantify the inherent\ncomplexity of a dataset, and reveal interpretable structure in sequential data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9884\u6d4b\u4fe1\u606f\u7684\u4fe1\u606f\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5e8f\u5217\u6570\u636e\u4e2d\u65f6\u95f4\u6a21\u5f0f\u7684\u5b58\u5728\u4e0e\u5426\u5bf9\u6a21\u578b\u53ef\u5b66\u4e60\u6027\u7684\u57fa\u672c\u7ea6\u675f\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u4f9d\u8d56\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u6a21\u5f0f\uff0c\u4f46\u6a21\u5f0f\u8bc6\u522b\u5f80\u5f80\u4f9d\u8d56\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9519\u8bef\u89e3\u91ca\u4f1a\u5bfc\u81f4\u6a21\u578b\u9519\u8bef\u8bbe\u5b9a\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u4f7f\u7528\u9884\u6d4b\u4fe1\u606f\uff08\u8fc7\u53bb\u4e0e\u672a\u6765\u7684\u4e92\u4fe1\u606f\uff09\u6784\u5efa\u4fe1\u606f\u7406\u8bba\u5b66\u4e60\u66f2\u7ebf\uff0c\u91cf\u5316\u968f\u7740\u89c2\u5bdf\u7a97\u53e3\u589e\u957f\u53ef\u7528\u7684\u9884\u6d4b\u4fe1\u606f\u91cf\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u80fd\u591f\u8bc4\u4f30\u6a21\u578b\u5145\u5206\u6027\u3001\u91cf\u5316\u6570\u636e\u96c6\u56fa\u6709\u590d\u6742\u6027\uff0c\u5e76\u63ed\u793a\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u53ef\u89e3\u91ca\u7ed3\u6784\u3002", "conclusion": "\u65f6\u95f4\u6a21\u5f0f\u7684\u5b58\u5728\u4e0e\u5426\u4ece\u6839\u672c\u4e0a\u7ea6\u675f\u4e86\u5e8f\u5217\u6a21\u578b\u7684\u53ef\u5b66\u4e60\u6027\uff0c\u5373\u4f7f\u6700\u4f18\u9884\u6d4b\u5668\u4e5f\u65e0\u6cd5\u8d85\u8d8a\u6570\u636e\u56fa\u6709\u7684\u4fe1\u606f\u9650\u5236\u3002"}}
{"id": "2510.11616", "categories": ["cs.LG", "cs.AI", "q-fin.CP", "I.2.0"], "pdf": "https://arxiv.org/pdf/2510.11616", "abs": "https://arxiv.org/abs/2510.11616", "authors": ["Elliot L. Epstein", "Rose Wang", "Jaewon Choi", "Markus Pelger"], "title": "Attention Factors for Statistical Arbitrage", "comment": "Accepted to the 6th ACM International Conference on AI in Finance", "summary": "Statistical arbitrage exploits temporal price differences between similar\nassets. We develop a framework to jointly identify similar assets through\nfactors, identify mispricing and form a trading policy that maximizes\nrisk-adjusted performance after trading costs. Our Attention Factors are\nconditional latent factors that are the most useful for arbitrage trading. They\nare learned from firm characteristic embeddings that allow for complex\ninteractions. We identify time-series signals from the residual portfolios of\nour factors with a general sequence model. Estimating factors and the arbitrage\ntrading strategy jointly is crucial to maximize profitability after trading\ncosts. In a comprehensive empirical study we show that our Attention Factor\nmodel achieves an out-of-sample Sharpe ratio above 4 on the largest U.S.\nequities over a 24-year period. Our one-step solution yields an unprecedented\nSharpe ratio of 2.3 net of transaction costs. We show that weak factors are\nimportant for arbitrage trading.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8054\u5408\u5b66\u4e60\u6ce8\u610f\u529b\u56e0\u5b50\u548c\u5957\u5229\u4ea4\u6613\u7b56\u7565\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6761\u4ef6\u6f5c\u5728\u56e0\u5b50\u8bc6\u522b\u76f8\u4f3c\u8d44\u4ea7\u548c\u9519\u8bef\u5b9a\u4ef7\uff0c\u5728\u8003\u8651\u4ea4\u6613\u6210\u672c\u540e\u5b9e\u73b0\u98ce\u9669\u8c03\u6574\u540e\u7684\u6700\u5927\u6536\u76ca\u3002", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u5957\u5229\u65b9\u6cd5\u901a\u5e38\u5206\u5f00\u4f30\u8ba1\u56e0\u5b50\u548c\u4ea4\u6613\u7b56\u7565\uff0c\u65e0\u6cd5\u5728\u8003\u8651\u4ea4\u6613\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u6700\u5927\u5316\u76c8\u5229\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u6ce8\u610f\u529b\u56e0\u5b50\u4f5c\u4e3a\u6761\u4ef6\u6f5c\u5728\u56e0\u5b50\uff0c\u4ece\u516c\u53f8\u7279\u5f81\u5d4c\u5165\u4e2d\u5b66\u4e60\u590d\u6742\u4ea4\u4e92\u3002\u4f7f\u7528\u901a\u7528\u5e8f\u5217\u6a21\u578b\u4ece\u56e0\u5b50\u6b8b\u5dee\u7ec4\u5408\u4e2d\u8bc6\u522b\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\uff0c\u5e76\u8054\u5408\u4f30\u8ba1\u56e0\u5b50\u548c\u5957\u5229\u4ea4\u6613\u7b56\u7565\u3002", "result": "\u572824\u5e74\u7f8e\u56fd\u6700\u5927\u80a1\u7968\u6570\u636e\u96c6\u4e0a\uff0c\u6ce8\u610f\u529b\u56e0\u5b50\u6a21\u578b\u5b9e\u73b0\u4e86\u8d85\u8fc74\u7684\u6837\u672c\u5916\u590f\u666e\u6bd4\u7387\u3002\u8003\u8651\u4ea4\u6613\u6210\u672c\u540e\u7684\u4e00\u6b65\u89e3\u51b3\u65b9\u6848\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u76842.3\u590f\u666e\u6bd4\u7387\u3002", "conclusion": "\u8054\u5408\u4f30\u8ba1\u56e0\u5b50\u548c\u4ea4\u6613\u7b56\u7565\u5bf9\u4e8e\u5728\u4ea4\u6613\u6210\u672c\u540e\u6700\u5927\u5316\u76c8\u5229\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u5f31\u56e0\u5b50\u5728\u5957\u5229\u4ea4\u6613\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2510.09925", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.09925", "abs": "https://arxiv.org/abs/2510.09925", "authors": ["James Usevitch", "Juan Augusto Paredes Salazar", "Ankit Goel"], "title": "Computing Safe Control Inputs using Discrete-Time Matrix Control Barrier Functions via Convex Optimization", "comment": "17 pages, 8 figures", "summary": "Control barrier functions (CBFs) have seen widespread success in providing\nforward invariance and safety guarantees for dynamical control systems. A\ncrucial limitation of discrete-time formulations is that CBFs that are\nnonconcave in their argument require the solution of nonconvex optimization\nproblems to compute safety-preserving control inputs, which inhibits real-time\ncomputation of control inputs guaranteeing forward invariance. This paper\npresents a novel method for computing safety-preserving control inputs for\ndiscrete-time systems with nonconvex safety sets, utilizing convex optimization\nand the recently developed class of matrix control barrier function techniques.\nThe efficacy of our methods is demonstrated through numerical simulations on a\nbicopter system.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u51f8\u4f18\u5316\u548c\u77e9\u9635\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u6280\u672f\u8ba1\u7b97\u79bb\u6563\u65f6\u95f4\u7cfb\u7edf\u5b89\u5168\u63a7\u5236\u8f93\u5165\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u975e\u51f8\u5b89\u5168\u96c6\u5e26\u6765\u7684\u8ba1\u7b97\u96be\u9898", "motivation": "\u79bb\u6563\u65f6\u95f4\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u5728\u53c2\u6570\u975e\u51f9\u65f6\u9700\u8981\u6c42\u89e3\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u8fd9\u963b\u788d\u4e86\u5b9e\u65f6\u8ba1\u7b97\u4fdd\u8bc1\u524d\u5411\u4e0d\u53d8\u6027\u7684\u63a7\u5236\u8f93\u5165", "method": "\u5229\u7528\u51f8\u4f18\u5316\u548c\u77e9\u9635\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u6280\u672f\uff0c\u4e3a\u5177\u6709\u975e\u51f8\u5b89\u5168\u96c6\u7684\u79bb\u6563\u65f6\u95f4\u7cfb\u7edf\u8ba1\u7b97\u5b89\u5168\u4fdd\u62a4\u63a7\u5236\u8f93\u5165", "result": "\u901a\u8fc7\u53cc\u65cb\u7ffc\u7cfb\u7edf\u7684\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u975e\u51f8\u5b89\u5168\u96c6\u95ee\u9898\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5b89\u5168\u63a7\u5236"}}
{"id": "2510.11139", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2510.11139", "abs": "https://arxiv.org/abs/2510.11139", "authors": ["Mohammad Zeqi Yasin"], "title": "Superstars or Super-Villains? Productivity Spillovers and Firm Dynamics in Indonesia", "comment": null, "summary": "Do industrial \"superstars\" help others up or crowd them out? We examine the\nrelationship between the spillovers of superstar firms (those with the top\nmarket share in their industry) and the productivity dynamics in Indonesia.\nEmploying data on Indonesian manufacturing firms from 2001 to 2015, we find\nthat superstar exposures in the market raise both the productivity level and\nthe growth of non-superstar firms through horizontal (within a sector-province)\nand vertical (across sectors) channels. When we distinguish by ownership,\nforeign superstars consistently encourage productivity except through the\nhorizontal channel. In contrast, domestic superstars generate positive\nspillovers through both horizontal and vertical linkages, indicating that\nforeign firms do not solely drive positive externalities. Furthermore, despite\noverall productivity growth being positive in 2001-2015, the source of negative\ngrowth is mainly driven by within-group reallocation, evidence of misallocation\namong surviving firms, notably by domestic superstars. Although Indonesian\nsuperstar firms are more efficient in their operations, their relatively modest\ngrowth rates suggest a potential stagnation, which can be plausibly attributed\nto limited innovation activity or a slow pace of adopting new technologies.", "AI": {"tldr": "\u5370\u5c3c\u5236\u9020\u4e1a\u4e2d\u7684\u8d85\u7ea7\u660e\u661f\u4f01\u4e1a\uff08\u5e02\u573a\u4efd\u989d\u6700\u9ad8\u7684\u4f01\u4e1a\uff09\u901a\u8fc7\u6c34\u5e73\u548c\u5782\u76f4\u6e20\u9053\u5bf9\u975e\u8d85\u7ea7\u660e\u661f\u4f01\u4e1a\u4ea7\u751f\u6b63\u5411\u751f\u4ea7\u7387\u6ea2\u51fa\u6548\u5e94\uff0c\u4f46\u5916\u8d44\u548c\u672c\u571f\u8d85\u7ea7\u660e\u661f\u7684\u6ea2\u51fa\u6a21\u5f0f\u4e0d\u540c\uff0c\u4e14\u5b58\u5728\u8d44\u6e90\u914d\u7f6e\u4e0d\u5f53\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u8d85\u7ea7\u660e\u661f\u4f01\u4e1a\u662f\u5426\u5e2e\u52a9\u5176\u4ed6\u4f01\u4e1a\u63d0\u5347\u751f\u4ea7\u7387\u8fd8\u662f\u6324\u5360\u5176\u53d1\u5c55\u7a7a\u95f4\uff0c\u7279\u522b\u662f\u5728\u5370\u5c3c\u8fd9\u6837\u7684\u53d1\u5c55\u4e2d\u7ecf\u6d4e\u4f53\u80cc\u666f\u4e0b\u3002", "method": "\u4f7f\u75282001-2015\u5e74\u5370\u5c3c\u5236\u9020\u4e1a\u4f01\u4e1a\u6570\u636e\uff0c\u5206\u6790\u8d85\u7ea7\u660e\u661f\u4f01\u4e1a\u901a\u8fc7\u6c34\u5e73\u548c\u5782\u76f4\u6e20\u9053\u5bf9\u975e\u8d85\u7ea7\u660e\u661f\u4f01\u4e1a\u751f\u4ea7\u7387\u7684\u6ea2\u51fa\u6548\u5e94\uff0c\u5e76\u6309\u6240\u6709\u6743\u7c7b\u578b\u533a\u5206\u3002", "result": "\u8d85\u7ea7\u660e\u661f\u4f01\u4e1a\u603b\u4f53\u4e0a\u901a\u8fc7\u6c34\u5e73\u548c\u5782\u76f4\u6e20\u9053\u63d0\u5347\u975e\u8d85\u7ea7\u660e\u661f\u4f01\u4e1a\u7684\u751f\u4ea7\u7387\u6c34\u5e73\u548c\u589e\u957f\u3002\u5916\u8d44\u8d85\u7ea7\u660e\u661f\u4e3b\u8981\u901a\u8fc7\u5782\u76f4\u6e20\u9053\u4ea7\u751f\u6b63\u5411\u6ea2\u51fa\uff0c\u672c\u571f\u8d85\u7ea7\u660e\u661f\u5219\u901a\u8fc7\u4e24\u79cd\u6e20\u9053\u90fd\u4ea7\u751f\u6b63\u5411\u6ea2\u51fa\u3002\u4f46\u5b58\u5728\u8d44\u6e90\u914d\u7f6e\u4e0d\u5f53\u95ee\u9898\uff0c\u7279\u522b\u662f\u672c\u571f\u8d85\u7ea7\u660e\u661f\u4f01\u4e1a\u3002", "conclusion": "\u8d85\u7ea7\u660e\u661f\u4f01\u4e1a\u5bf9\u5176\u4ed6\u4f01\u4e1a\u6709\u6b63\u5411\u751f\u4ea7\u7387\u6ea2\u51fa\u6548\u5e94\uff0c\u4f46\u5916\u8d44\u548c\u672c\u571f\u4f01\u4e1a\u7684\u6ea2\u51fa\u6a21\u5f0f\u4e0d\u540c\u3002\u5370\u5c3c\u8d85\u7ea7\u660e\u661f\u4f01\u4e1a\u867d\u7136\u8fd0\u8425\u6548\u7387\u9ad8\uff0c\u4f46\u589e\u957f\u7f13\u6162\uff0c\u53ef\u80fd\u56e0\u521b\u65b0\u6d3b\u52a8\u6709\u9650\u6216\u6280\u672f\u91c7\u7528\u7f13\u6162\u5bfc\u81f4\u6f5c\u5728\u505c\u6ede\u3002"}}
{"id": "2510.09686", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.09686", "abs": "https://arxiv.org/abs/2510.09686", "authors": ["Jianghao Lin", "Rong Shan", "Jiachen Zhu", "Yunjia Xi", "Yong Yu", "Weinan Zhang"], "title": "Stop DDoS Attacking the Research Community with AI-Generated Survey Papers", "comment": "Accepted by NeurIPS 2025 (Position Track)", "summary": "Survey papers are foundational to the scholarly progress of research\ncommunities, offering structured overviews that guide both novices and experts\nacross disciplines. However, the recent surge of AI-generated surveys,\nespecially enabled by large language models (LLMs), has transformed this\ntraditionally labor-intensive genre into a low-effort, high-volume output.\nWhile such automation lowers entry barriers, it also introduces a critical\nthreat: the phenomenon we term the \"survey paper DDoS attack\" to the research\ncommunity. This refers to the unchecked proliferation of superficially\ncomprehensive but often redundant, low-quality, or even hallucinated survey\nmanuscripts, which floods preprint platforms, overwhelms researchers, and\nerodes trust in the scientific record. In this position paper, we argue that we\nmust stop uploading massive amounts of AI-generated survey papers (i.e., survey\npaper DDoS attack) to the research community, by instituting strong norms for\nAI-assisted review writing. We call for restoring expert oversight and\ntransparency in AI usage and, moreover, developing new infrastructures such as\nDynamic Live Surveys, community-maintained, version-controlled repositories\nthat blend automated updates with human curation. Through quantitative trend\nanalysis, quality audits, and cultural impact discussion, we show that\nsafeguarding the integrity of surveys is no longer optional but imperative to\nthe research community.", "AI": {"tldr": "AI\u751f\u6210\u7efc\u8ff0\u8bba\u6587\u7684\u6fc0\u589e\u6b63\u5728\u5bf9\u7814\u7a76\u793e\u533a\u9020\u6210\"\u7efc\u8ff0\u8bba\u6587DDoS\u653b\u51fb\"\u5a01\u80c1\uff0c\u9700\u8981\u5efa\u7acb\u89c4\u8303\u6765\u9650\u5236\u4f4e\u8d28\u91cfAI\u751f\u6210\u7efc\u8ff0\u7684\u4e0a\u4f20\uff0c\u5e76\u5f00\u53d1\u52a8\u6001\u5b9e\u65f6\u7efc\u8ff0\u7b49\u65b0\u57fa\u7840\u8bbe\u65bd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0cAI\u751f\u6210\u7684\u7efc\u8ff0\u8bba\u6587\u5927\u91cf\u6d8c\u73b0\uff0c\u8fd9\u4e9b\u8868\u9762\u5168\u9762\u4f46\u5f80\u5f80\u5197\u4f59\u3001\u4f4e\u8d28\u91cf\u751a\u81f3\u5305\u542b\u865a\u6784\u5185\u5bb9\u7684\u8bba\u6587\u6df9\u6ca1\u4e86\u9884\u5370\u672c\u5e73\u53f0\uff0c\u7ed9\u7814\u7a76\u793e\u533a\u5e26\u6765\u4e86\u4e25\u91cd\u5a01\u80c1\u3002", "method": "\u901a\u8fc7\u5b9a\u91cf\u8d8b\u52bf\u5206\u6790\u3001\u8d28\u91cf\u5ba1\u8ba1\u548c\u6587\u5316\u5f71\u54cd\u8ba8\u8bba\uff0c\u8bba\u8bc1\u9650\u5236AI\u751f\u6210\u7efc\u8ff0\u8bba\u6587\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u5efa\u7acbAI\u8f85\u52a9\u7efc\u8ff0\u5199\u4f5c\u89c4\u8303\u3001\u4e13\u5bb6\u76d1\u7763\u673a\u5236\u548c\u52a8\u6001\u5b9e\u65f6\u7efc\u8ff0\u57fa\u7840\u8bbe\u65bd\u7b49\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u751f\u6210\u7efc\u8ff0\u7684\u6cdb\u6ee5\u6b63\u5728\u4fb5\u8680\u79d1\u5b66\u8bb0\u5f55\u7684\u4fe1\u4efb\u5ea6\uff0c\u6df9\u6ca1\u7814\u7a76\u4eba\u5458\uff0c\u7834\u574f\u5b66\u672f\u8fdb\u6b65\u7684\u57fa\u7840\u3002", "conclusion": "\u4fdd\u62a4\u7efc\u8ff0\u8bba\u6587\u7684\u5b8c\u6574\u6027\u5bf9\u7814\u7a76\u793e\u533a\u5df2\u4e0d\u518d\u662f\u53ef\u9009\u9879\uff0c\u800c\u662f\u5fc5\u987b\u91c7\u53d6\u884c\u52a8\u7684\u5f53\u52a1\u4e4b\u6025\uff0c\u9700\u8981\u5efa\u7acb\u5f3a\u6709\u529b\u7684\u4eba\u5de5\u667a\u80fd\u4f7f\u7528\u89c4\u8303\u548c\u65b0\u7684\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2510.10132", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.10132", "abs": "https://arxiv.org/abs/2510.10132", "authors": ["Ioannis Dassios"], "title": "Mathematical Modeling of Networks in Strength of Materials", "comment": "9 pages, 1 figure", "summary": "We study a material modeled as a network of nodes connected by edges. Using a\ndiscrete approach, we build a nonlinear algebraic system that connects applied\nforces to internal forces and node positions. The model can describe\nelasticity, plasticity, and possibly cracking. The goal is to solve this system\nand understand how the material responds. Students are asked to start with a\nsimple triangle example and then apply the method to larger structures. The\nfinal aim is to solve the full system and justify the results, leading to a\npossible publication.", "AI": {"tldr": "\u7814\u7a76\u6750\u6599\u4f5c\u4e3a\u8282\u70b9\u7f51\u7edc\u6a21\u578b\uff0c\u5efa\u7acb\u8fde\u63a5\u5916\u529b\u4e0e\u5185\u529b\u3001\u8282\u70b9\u4f4d\u7f6e\u7684\u975e\u7ebf\u6027\u4ee3\u6570\u7cfb\u7edf\uff0c\u7528\u4e8e\u63cf\u8ff0\u5f39\u6027\u3001\u5851\u6027\u548c\u5f00\u88c2\u884c\u4e3a\u3002", "motivation": "\u7406\u89e3\u6750\u6599\u5728\u5916\u529b\u4f5c\u7528\u4e0b\u7684\u54cd\u5e94\u673a\u5236\uff0c\u4ece\u7b80\u5355\u4e09\u89d2\u5f62\u7ed3\u6784\u6269\u5c55\u5230\u5927\u578b\u7ed3\u6784\u5206\u6790\uff0c\u6700\u7ec8\u5b9e\u73b0\u5b8c\u6574\u7cfb\u7edf\u6c42\u89e3\u3002", "method": "\u91c7\u7528\u79bb\u6563\u65b9\u6cd5\u6784\u5efa\u975e\u7ebf\u6027\u4ee3\u6570\u7cfb\u7edf\uff0c\u8fde\u63a5\u65bd\u52a0\u529b\u4e0e\u5185\u529b\u3001\u8282\u70b9\u4f4d\u7f6e\uff0c\u80fd\u591f\u63cf\u8ff0\u5f39\u6027\u3001\u5851\u6027\u548c\u5f00\u88c2\u884c\u4e3a\u3002", "result": "\u5efa\u7acb\u4e86\u53ef\u6c42\u89e3\u7684\u6750\u6599\u7f51\u7edc\u6a21\u578b\u6846\u67b6\uff0c\u4ece\u7b80\u5355\u4e09\u89d2\u5f62\u793a\u4f8b\u5f00\u59cb\uff0c\u9010\u6b65\u6269\u5c55\u5230\u66f4\u5927\u7ed3\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6750\u6599\u529b\u5b66\u884c\u4e3a\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6709\u671b\u901a\u8fc7\u5b8c\u6574\u7cfb\u7edf\u6c42\u89e3\u83b7\u5f97\u53ef\u53d1\u8868\u7684\u7814\u7a76\u6210\u679c\u3002"}}
{"id": "2510.09711", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09711", "abs": "https://arxiv.org/abs/2510.09711", "authors": ["Wenbin Guo", "Xin Wang", "Jiaoyan Chen", "Lingbing Guo", "Zhao Li", "Zirui Chen"], "title": "ReaLM: Residual Quantization Bridging Knowledge Graph Embeddings and Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have recently emerged as a powerful paradigm for\nKnowledge Graph Completion (KGC), offering strong reasoning and generalization\ncapabilities beyond traditional embedding-based approaches. However, existing\nLLM-based methods often struggle to fully exploit structured semantic\nrepresentations, as the continuous embedding space of pretrained KG models is\nfundamentally misaligned with the discrete token space of LLMs. This\ndiscrepancy hinders effective semantic transfer and limits their performance.\nTo address this challenge, we propose ReaLM, a novel and effective framework\nthat bridges the gap between KG embeddings and LLM tokenization through the\nmechanism of residual vector quantization. ReaLM discretizes pretrained KG\nembeddings into compact code sequences and integrates them as learnable tokens\nwithin the LLM vocabulary, enabling seamless fusion of symbolic and contextual\nknowledge. Furthermore, we incorporate ontology-guided class constraints to\nenforce semantic consistency, refining entity predictions based on class-level\ncompatibility. Extensive experiments on two widely used benchmark datasets\ndemonstrate that ReaLM achieves state-of-the-art performance, confirming its\neffectiveness in aligning structured knowledge with large-scale language\nmodels.", "AI": {"tldr": "ReaLM\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u5c06\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u4e0eLLM\u5206\u8bcd\u5bf9\u9f50\uff0c\u7ed3\u5408\u672c\u4f53\u6307\u5bfc\u7684\u7c7b\u522b\u7ea6\u675f\uff0c\u5728\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u65b9\u6cd5\u96be\u4ee5\u5145\u5206\u5229\u7528\u7ed3\u6784\u5316\u8bed\u4e49\u8868\u793a\uff0c\u56e0\u4e3a\u9884\u8bad\u7ec3KG\u6a21\u578b\u7684\u8fde\u7eed\u5d4c\u5165\u7a7a\u95f4\u4e0eLLM\u7684\u79bb\u6563\u5206\u8bcd\u7a7a\u95f4\u5b58\u5728\u6839\u672c\u6027\u9519\u4f4d\uff0c\u8fd9\u963b\u788d\u4e86\u6709\u6548\u7684\u8bed\u4e49\u8fc1\u79fb\u5e76\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u673a\u5236\u5c06\u9884\u8bad\u7ec3KG\u5d4c\u5165\u79bb\u6563\u5316\u4e3a\u7d27\u51d1\u7684\u4ee3\u7801\u5e8f\u5217\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u53ef\u5b66\u4e60\u5206\u8bcd\u96c6\u6210\u5230LLM\u8bcd\u6c47\u8868\u4e2d\uff0c\u5b9e\u73b0\u7b26\u53f7\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u77e5\u8bc6\u7684\u65e0\u7f1d\u878d\u5408\u3002\u540c\u65f6\u7ed3\u5408\u672c\u4f53\u6307\u5bfc\u7684\u7c7b\u522b\u7ea6\u675f\u6765\u589e\u5f3a\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cReaLM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u5c06\u7ed3\u6784\u5316\u77e5\u8bc6\u4e0e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "ReaLM\u901a\u8fc7\u6865\u63a5KG\u5d4c\u5165\u4e0eLLM\u5206\u8bcd\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u6210\u529f\u5730\u5c06\u7ed3\u6784\u5316\u77e5\u8bc6\u4e0e\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\uff0c\u4e3a\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.09659", "categories": ["cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2510.09659", "abs": "https://arxiv.org/abs/2510.09659", "authors": ["Edgar E. Robles", "Dikshant Sagar", "Alejandro Yankelevich", "Jianming Bian", "Pierre Baldi", "NOvA Collaboration"], "title": "Heterogeneous Point Set Transformers for Segmentation of Multiple View Particle Detectors", "comment": "Submitted to Machine Learning and the Physical Sciences Workshop\n  (ML4PS) at NeurIPS 2025", "summary": "NOvA is a long-baseline neutrino oscillation experiment that detects neutrino\nparticles from the NuMI beam at Fermilab. Before data from this experiment can\nbe used in analyses, raw hits in the detector must be matched to their source\nparticles, and the type of each particle must be identified. This task has\ncommonly been done using a mix of traditional clustering approaches and\nconvolutional neural networks (CNNs). Due to the construction of the detector,\nthe data is presented as two sparse 2D images: an XZ and a YZ view of the\ndetector, rather than a 3D representation. We propose a point set neural\nnetwork that operates on the sparse matrices with an operation that mixes\ninformation from both views. Our model uses less than 10% of the memory\nrequired using previous methods while achieving a 96.8% AUC score, a higher\nscore than obtained when both views are processed independently (85.4%).", "AI": {"tldr": "NOvA\u5b9e\u9a8c\u63d0\u51fa\u4e86\u4e00\u79cd\u70b9\u96c6\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u5904\u7406\u7a00\u758f\u76842D\u63a2\u6d4b\u5668\u6570\u636e\uff0c\u901a\u8fc7\u6df7\u5408\u4e24\u4e2a\u89c6\u56fe\u7684\u4fe1\u606f\uff0c\u5728\u51cf\u5c1190%\u5185\u5b58\u4f7f\u7528\u7684\u540c\u65f6\uff0c\u5c06AUC\u5f97\u5206\u4ece85.4%\u63d0\u5347\u523096.8%\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u805a\u7c7b\u548cCNN\u5904\u7406NOvA\u5b9e\u9a8c\u7684\u7a00\u758f2D\u63a2\u6d4b\u5668\u6570\u636e\uff0c\u4f46\u5185\u5b58\u6d88\u8017\u5927\u4e14\u89c6\u56fe\u95f4\u4fe1\u606f\u72ec\u7acb\u5904\u7406\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u878d\u5408\u4e24\u4e2a\u89c6\u56fe\u7684\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u70b9\u96c6\u795e\u7ecf\u7f51\u7edc\uff0c\u76f4\u63a5\u5728\u7a00\u758f\u77e9\u9635\u4e0a\u64cd\u4f5c\uff0c\u901a\u8fc7\u7279\u6b8a\u64cd\u4f5c\u6df7\u5408XZ\u548cYZ\u4e24\u4e2a\u89c6\u56fe\u7684\u4fe1\u606f\uff0c\u800c\u4e0d\u662f\u5206\u522b\u5904\u7406\u3002", "result": "\u6a21\u578b\u5185\u5b58\u4f7f\u7528\u51cf\u5c1190%\u4ee5\u4e0a\uff0cAUC\u5f97\u5206\u8fbe\u523096.8%\uff0c\u663e\u8457\u9ad8\u4e8e\u4e24\u4e2a\u89c6\u56fe\u72ec\u7acb\u5904\u7406\u65f6\u768485.4%\u3002", "conclusion": "\u70b9\u96c6\u795e\u7ecf\u7f51\u7edc\u80fd\u6709\u6548\u5904\u7406NOvA\u5b9e\u9a8c\u7684\u7a00\u758f\u63a2\u6d4b\u5668\u6570\u636e\uff0c\u5728\u663e\u8457\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u7c92\u5b50\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2510.09901", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09901", "abs": "https://arxiv.org/abs/2510.09901", "authors": ["Lianhao Zhou", "Hongyi Ling", "Cong Fu", "Yepeng Huang", "Michael Sun", "Wendi Yu", "Xiaoxuan Wang", "Xiner Li", "Xingyu Su", "Junkai Zhang", "Xiusi Chen", "Chenxing Liang", "Xiaofeng Qian", "Heng Ji", "Wei Wang", "Marinka Zitnik", "Shuiwang Ji"], "title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics", "comment": null, "summary": "Computing has long served as a cornerstone of scientific discovery. Recently,\na paradigm shift has emerged with the rise of large language models (LLMs),\nintroducing autonomous systems, referred to as agents, that accelerate\ndiscovery across varying levels of autonomy. These language agents provide a\nflexible and versatile framework that orchestrates interactions with human\nscientists, natural language, computer language and code, and physics. This\npaper presents our view and vision of LLM-based scientific agents and their\ngrowing role in transforming the scientific discovery lifecycle, from\nhypothesis discovery, experimental design and execution, to result analysis and\nrefinement. We critically examine current methodologies, emphasizing key\ninnovations, practical achievements, and outstanding limitations. Additionally,\nwe identify open research challenges and outline promising directions for\nbuilding more robust, generalizable, and adaptive scientific agents. Our\nanalysis highlights the transformative potential of autonomous agents to\naccelerate scientific discovery across diverse domains.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u79d1\u5b66\u4ee3\u7406\u5728\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u4f5c\u7528\uff0c\u4ece\u5047\u8bbe\u53d1\u73b0\u5230\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u7ed3\u679c\u5206\u6790\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\uff0c\u5206\u6790\u4e86\u5f53\u524d\u65b9\u6cd5\u3001\u521b\u65b0\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\uff0c\u51fa\u73b0\u4e86\u80fd\u591f\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u7684\u81ea\u4e3b\u4f53\u7cfb\u7edf\uff0c\u8fd9\u4e9b\u8bed\u8a00\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u6846\u67b6\uff0c\u534f\u8c03\u4e0e\u4eba\u7c7b\u79d1\u5b66\u5bb6\u3001\u81ea\u7136\u8bed\u8a00\u3001\u8ba1\u7b97\u673a\u8bed\u8a00\u548c\u7269\u7406\u5b66\u7684\u4ea4\u4e92\u3002", "method": "\u901a\u8fc7\u6279\u5224\u6027\u5ba1\u89c6\u5f53\u524d\u65b9\u6cd5\u8bba\uff0c\u5f3a\u8c03\u5173\u952e\u521b\u65b0\u3001\u5b9e\u9645\u6210\u5c31\u548c\u7a81\u51fa\u5c40\u9650\u6027\uff0c\u8bc6\u522b\u5f00\u653e\u7814\u7a76\u6311\u6218\uff0c\u5e76\u4e3a\u6784\u5efa\u66f4\u7a33\u5065\u3001\u901a\u7528\u548c\u81ea\u9002\u5e94\u7684\u79d1\u5b66\u4ee3\u7406\u6307\u660e\u65b9\u5411\u3002", "result": "\u5206\u6790\u7a81\u663e\u4e86\u81ea\u4e3b\u4f53\u5728\u52a0\u901f\u8de8\u9886\u57df\u79d1\u5b66\u53d1\u73b0\u65b9\u9762\u7684\u53d8\u9769\u6f5c\u529b\u3002", "conclusion": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u79d1\u5b66\u4ee3\u7406\u5177\u6709\u6539\u53d8\u79d1\u5b66\u53d1\u73b0\u751f\u547d\u5468\u671f\u7684\u6f5c\u529b\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u89e3\u51b3\u5f53\u524d\u5c40\u9650\u6027\uff0c\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u4ee3\u7406\u7cfb\u7edf\u3002"}}
{"id": "2510.10866", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10866", "abs": "https://arxiv.org/abs/2510.10866", "authors": ["Shudong Sun", "Hao Helen Zhang"], "title": "Quantifying Dataset Similarity to Guide Transfer Learning", "comment": null, "summary": "Transfer learning has become a cornerstone of modern machine learning, as it\ncan empower models by leveraging knowledge from related domains to improve\nlearning effectiveness. However, transferring from poorly aligned data can harm\nrather than help performance, making it crucial to determine whether the\ntransfer will be beneficial before implementation. This work aims to address\nthis challenge by proposing an innovative metric to measure dataset similarity\nand provide quantitative guidance on transferability. In the literature,\nexisting methods largely focus on feature distributions while overlooking label\ninformation and predictive relationships, potentially missing critical\ntransferability insights. In contrast, our proposed metric, the Cross-Learning\nScore (CLS), measures dataset similarity through bidirectional generalization\nperformance between domains. We provide a theoretical justification for CLS by\nestablishing its connection to the cosine similarity between the decision\nboundaries for the target and source datasets. Computationally, CLS is\nefficient and fast to compute as it bypasses the problem of expensive\ndistribution estimation for high-dimensional problems. We further introduce a\ngeneral framework that categorizes source datasets into positive, ambiguous, or\nnegative transfer zones based on their CLS relative to the baseline error,\nenabling informed decisions. Additionally, we extend this approach to\nencoder-head architectures in deep learning to better reflect modern transfer\npipelines. Extensive experiments on diverse synthetic and real-world tasks\ndemonstrate that CLS can reliably predict whether transfer will improve or\ndegrade performance, offering a principled tool for guiding data selection in\ntransfer learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4ea4\u53c9\u5b66\u4e60\u5206\u6570\uff08CLS\uff09\u7684\u65b0\u6307\u6807\uff0c\u901a\u8fc7\u53cc\u5411\u6cdb\u5316\u6027\u80fd\u6765\u8861\u91cf\u6570\u636e\u96c6\u76f8\u4f3c\u6027\uff0c\u4e3a\u8fc1\u79fb\u5b66\u4e60\u63d0\u4f9b\u6570\u636e\u9009\u62e9\u6307\u5bfc\u3002", "motivation": "\u8fc1\u79fb\u5b66\u4e60\u53ef\u80fd\u56e0\u6570\u636e\u5bf9\u9f50\u4e0d\u4f73\u800c\u635f\u5bb3\u6027\u80fd\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7279\u5f81\u5206\u5e03\u800c\u5ffd\u7565\u6807\u7b7e\u4fe1\u606f\u548c\u9884\u6d4b\u5173\u7cfb\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u8fc1\u79fb\u6027\u8bc4\u4f30\u6307\u6807\u3002", "method": "CLS\u901a\u8fc7\u6d4b\u91cf\u57df\u95f4\u53cc\u5411\u6cdb\u5316\u6027\u80fd\u6765\u8bc4\u4f30\u6570\u636e\u96c6\u76f8\u4f3c\u6027\uff0c\u4e0e\u76ee\u6807\u6570\u636e\u96c6\u548c\u6e90\u6570\u636e\u96c6\u51b3\u7b56\u8fb9\u754c\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6709\u7406\u8bba\u5173\u8054\uff0c\u8ba1\u7b97\u9ad8\u6548\u4e14\u907f\u514d\u9ad8\u7ef4\u5206\u5e03\u4f30\u8ba1\u95ee\u9898\u3002", "result": "\u5728\u591a\u79cd\u5408\u6210\u548c\u771f\u5b9e\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCLS\u80fd\u53ef\u9760\u9884\u6d4b\u8fc1\u79fb\u662f\u5426\u4f1a\u6539\u5584\u6216\u964d\u4f4e\u6027\u80fd\uff0c\u5e76\u80fd\u5c06\u6e90\u6570\u636e\u96c6\u5206\u7c7b\u4e3a\u6b63\u5411\u3001\u6a21\u7cca\u6216\u8d1f\u5411\u8fc1\u79fb\u533a\u57df\u3002", "conclusion": "CLS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u5de5\u5177\u6765\u6307\u5bfc\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u9009\u62e9\uff0c\u80fd\u591f\u6709\u6548\u9884\u6d4b\u8fc1\u79fb\u6548\u679c\u5e76\u652f\u6301\u660e\u667a\u7684\u51b3\u7b56\u5236\u5b9a\u3002"}}
{"id": "2510.09929", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.09929", "abs": "https://arxiv.org/abs/2510.09929", "authors": ["Dylan Hirsch", "Jaime Fern\u00e1ndez Fisac", "Sylvia Herbert"], "title": "Viscosity CBFs: Bridging the Control Barrier Function and Hamilton-Jacobi Reachability Frameworks in Safe Control Theory", "comment": null, "summary": "Control barrier functions (CBFs) and Hamilton-Jacobi reachability (HJR) are\ncentral frameworks in safe control. Traditionally, these frameworks have been\nviewed as distinct, with the former focusing on optimally safe controller\ndesign and the latter providing sufficient conditions for safety. A previous\nwork introduced the notion of a control barrier value function (CB-VF), which\nis defined similarly to the other value functions studied in HJR but has\ncertain CBF-like properties. In this work, we proceed the other direction by\ngeneralizing CBFs to non-differentiable ``viscosity'' CBFs. We show the deep\nconnection between viscosity CBFs and CB-VFs, bridging the CBF and HJR\nframeworks. Through this bridge, we characterize the viscosity CBFs as\nprecisely those functions which provide CBF-like safety guarantees (control\ninvariance and smooth approach to the boundary). We then further show nice\ntheoretical properties of viscosity CBFs, including their desirable closure\nunder maximum and limit operations. In the process, we also extend CB-VFs to\nnon-exponential anti-discounting and update the corresponding theory for CB-VFs\nalong these lines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u63a7\u5236\u5c4f\u969c\u51fd\u6570(CBFs)\u63a8\u5e7f\u5230\u4e0d\u53ef\u5fae\u7684\u201c\u7c98\u6027\u201dCBFs\uff0c\u5efa\u7acb\u4e86CBF\u4e0eHamilton-Jacobi\u53ef\u8fbe\u6027(HJR)\u6846\u67b6\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u5e76\u8bc1\u660e\u4e86\u7c98\u6027CBFs\u7684\u826f\u597d\u7406\u8bba\u6027\u8d28\u3002", "motivation": "\u4f20\u7edf\u4e0aCBFs\u548cHJR\u88ab\u89c6\u4e3a\u4e24\u4e2a\u4e0d\u540c\u7684\u5b89\u5168\u63a7\u5236\u6846\u67b6\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u63a8\u5e7fCBFs\u6765\u5efa\u7acb\u8fd9\u4e24\u4e2a\u6846\u67b6\u4e4b\u95f4\u7684\u6df1\u5c42\u8054\u7cfb\u3002", "method": "\u5c06CBFs\u63a8\u5e7f\u5230\u4e0d\u53ef\u5fae\u7684\u7c98\u6027CBFs\uff0c\u5206\u6790\u5176\u4e0e\u63a7\u5236\u5c4f\u969c\u503c\u51fd\u6570(CB-VFs)\u7684\u5173\u7cfb\uff0c\u5e76\u6269\u5c55CB-VFs\u5230\u975e\u6307\u6570\u53cd\u6298\u6263\u60c5\u5f62\u3002", "result": "\u8bc1\u660e\u4e86\u7c98\u6027CBFs\u4e0e\u63a7\u5236\u5c4f\u969c\u503c\u51fd\u6570\u4e4b\u95f4\u7684\u7b49\u4ef7\u5173\u7cfb\uff0c\u7c98\u6027CBFs\u5177\u6709\u63a7\u5236\u4e0d\u53d8\u6027\u548c\u8fb9\u754c\u5e73\u6ed1\u63a5\u8fd1\u7b49\u5b89\u5168\u4fdd\u8bc1\u6027\u8d28\uff0c\u4e14\u5728\u6700\u5927\u548c\u6781\u9650\u8fd0\u7b97\u4e0b\u4fdd\u6301\u95ed\u5305\u3002", "conclusion": "\u901a\u8fc7\u7c98\u6027CBFs\u6210\u529f\u6865\u63a5\u4e86CBF\u548cHJR\u6846\u67b6\uff0c\u4e3a\u5b89\u5168\u63a7\u5236\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u7c98\u6027CBFs\u7684\u826f\u597d\u7406\u8bba\u6027\u8d28\u3002"}}
{"id": "2510.11659", "categories": ["econ.EM"], "pdf": "https://arxiv.org/pdf/2510.11659", "abs": "https://arxiv.org/abs/2510.11659", "authors": ["Onil Boussim"], "title": "Compositional difference-in-differences for categorical outcomes", "comment": null, "summary": "In difference-in-differences (DiD) settings with categorical outcomes,\ntreatment effects often operate on both total quantities (e.g., voter turnout)\nand category shares (e.g., vote distribution across parties). In this context,\nlinear DiD models can be problematic: they suffer from scale dependence, may\nproduce negative counterfactual quantities, and are inconsistent with discrete\nchoice theory. We propose compositional DiD (CoDiD), a new method that\nidentifies counterfactual categorical quantities, and thus total levels and\nshares, under a parallel growths assumption. The assumption states that, absent\ntreatment, each category's size grows or shrinks at the same proportional rate\nin treated and control groups. In a random utility framework, we show that this\nimplies parallel evolution of relative preferences between any pair of\ncategories. Analytically, we show that it also means the shares are reallocated\nin the same way in both groups in the absence of treatment. Finally,\ngeometrically, it corresponds to parallel trajectories (or movements) of\nprobability mass functions of the two groups in the probability simplex under\nAitchison geometry. We extend CoDiD to i) derive bounds under relaxed\nassumptions, ii) handle staggered adoption, and iii) propose a synthetic DiD\nanalog. We illustrate the method's empirical relevance through two\napplications: first, we examine how early voting reforms affect voter choice in\nU.S. presidential elections; second, we analyze how the Regional Greenhouse Gas\nInitiative (RGGI) affected the composition of electricity generation across\nsources such as coal, natural gas, nuclear, and renewables.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7ec4\u5408\u5f0f\u53cc\u91cd\u5dee\u5206\uff08CoDiD\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u5206\u7c7b\u7ed3\u679c\u7684\u53cc\u91cd\u5dee\u5206\u5206\u6790\uff0c\u89e3\u51b3\u4e86\u7ebf\u6027DiD\u6a21\u578b\u7684\u5c3a\u5ea6\u4f9d\u8d56\u6027\u548c\u8d1f\u53cd\u4e8b\u5b9e\u6570\u91cf\u95ee\u9898\uff0c\u57fa\u4e8e\u5e73\u884c\u589e\u957f\u5047\u8bbe\u8bc6\u522b\u53cd\u4e8b\u5b9e\u5206\u7c7b\u6570\u91cf\u3002", "motivation": "\u5728\u5206\u7c7b\u7ed3\u679c\u7684DiD\u8bbe\u7f6e\u4e2d\uff0c\u7ebf\u6027DiD\u6a21\u578b\u5b58\u5728\u5c3a\u5ea6\u4f9d\u8d56\u6027\u3001\u53ef\u80fd\u4ea7\u751f\u8d1f\u53cd\u4e8b\u5b9e\u6570\u91cf\u3001\u4e0e\u79bb\u6563\u9009\u62e9\u7406\u8bba\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5408\u9002\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCoDiD\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5e73\u884c\u589e\u957f\u5047\u8bbe\uff08\u5728\u65e0\u5904\u7406\u60c5\u51b5\u4e0b\uff0c\u5404\u5206\u7c7b\u5728\u5b9e\u9a8c\u7ec4\u548c\u5bf9\u7167\u7ec4\u4e2d\u4ee5\u76f8\u540c\u6bd4\u4f8b\u589e\u957f\u6216\u6536\u7f29\uff09\uff0c\u5728\u968f\u673a\u6548\u7528\u6846\u67b6\u4e0b\u63a8\u5bfc\u76f8\u5bf9\u504f\u597d\u7684\u5e73\u884c\u6f14\u5316\uff0c\u5e76\u5728Aitchison\u51e0\u4f55\u4e2d\u5bf9\u5e94\u6982\u7387\u8d28\u91cf\u51fd\u6570\u7684\u5e73\u884c\u8f68\u8ff9\u3002", "result": "CoDiD\u65b9\u6cd5\u80fd\u591f\u8bc6\u522b\u53cd\u4e8b\u5b9e\u5206\u7c7b\u6570\u91cf\uff0c\u4ece\u800c\u5f97\u5230\u603b\u6c34\u5e73\u548c\u4efd\u989d\uff0c\u5e76\u6269\u5c55\u5230\u8fb9\u754c\u63a8\u5bfc\u3001\u4ea4\u9519\u91c7\u7528\u5904\u7406\u548c\u5408\u6210DiD\u7c7b\u4f3c\u65b9\u6cd5\u3002", "conclusion": "CoDiD\u4e3a\u5206\u7c7b\u7ed3\u679c\u7684DiD\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u4e00\u81f4\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u4e2a\u5b9e\u8bc1\u5e94\u7528\uff08\u65e9\u671f\u6295\u7968\u6539\u9769\u5bf9\u9009\u6c11\u9009\u62e9\u7684\u5f71\u54cd\u3001RGGI\u5bf9\u7535\u529b\u6784\u6210\u7684\u5f71\u54cd\uff09\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u5b9e\u8bc1\u76f8\u5173\u6027\u3002"}}
{"id": "2510.09698", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.09698", "abs": "https://arxiv.org/abs/2510.09698", "authors": ["Shiliang Zhang", "Sabita Maharjan", "Kai Strunz", "Jan Christian Bryne"], "title": "Norwegian Electricity in Geographic Dataset (NoreGeo)", "comment": null, "summary": "Geographic data is vital in understanding, analyzing, and contextualizing\nenergy usage at the regional level within electricity systems. While geospatial\nvisualizations of electricity infrastructure and distributions of production\nand consumption are available from governmental and third-party sources, these\nsources are often disparate, and compatible geographic datasets remain scarce.\nIn this paper, we present a comprehensive geographic dataset representing the\nelectricity system in Norway. We collect data from multiple authoritative\nsources, process it into widely accepted formats, and generate interactive maps\nbased on this data. Our dataset includes information for each municipality in\nNorway for the year 2024, encompassing electricity infrastructure, consumption,\nrenewable and conventional production, main power grid topology, relevant\nnatural resources, and population demographics. This work results in a\nformatted geographic dataset that integrates diverse informational resources,\nalong with openly released interactive maps. We anticipate that our dataset\nwill alleviate software incompatibilities in data retrieval, and facilitate\njoint analyses on regional electricity system for energy researchers,\nstakeholders, and developers.", "AI": {"tldr": "\u672c\u6587\u521b\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u632a\u5a01\u7535\u529b\u7cfb\u7edf\u5730\u7406\u6570\u636e\u96c6\uff0c\u6574\u5408\u4e86\u591a\u4e2a\u6743\u5a01\u6765\u6e90\u7684\u6570\u636e\uff0c\u5305\u62ec\u57fa\u7840\u8bbe\u65bd\u3001\u6d88\u8d39\u3001\u751f\u4ea7\u3001\u7535\u7f51\u62d3\u6251\u7b49\u4fe1\u606f\uff0c\u5e76\u63d0\u4f9b\u4e86\u4ea4\u4e92\u5f0f\u5730\u56fe\u3002", "motivation": "\u73b0\u6709\u5730\u7406\u6570\u636e\u6765\u6e90\u5206\u6563\u4e14\u517c\u5bb9\u6027\u5dee\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u632a\u5a01\u7535\u529b\u7cfb\u7edf\u5730\u7406\u6570\u636e\u96c6\uff0c\u8fd9\u963b\u788d\u4e86\u533a\u57df\u7535\u529b\u7cfb\u7edf\u7684\u7efc\u5408\u5206\u6790\u3002", "method": "\u4ece\u591a\u4e2a\u6743\u5a01\u6765\u6e90\u6536\u96c6\u6570\u636e\uff0c\u5904\u7406\u6210\u5e7f\u6cdb\u63a5\u53d7\u7684\u683c\u5f0f\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u6570\u636e\u751f\u6210\u4ea4\u4e92\u5f0f\u5730\u56fe\uff0c\u6db5\u76d6\u5e02\u653f\u7ea7\u522b\u7684\u7535\u529b\u57fa\u7840\u8bbe\u65bd\u3001\u6d88\u8d39\u3001\u751f\u4ea7\u7b49\u4fe1\u606f\u3002", "result": "\u751f\u6210\u4e862024\u5e74\u632a\u5a01\u5404\u5e02\u653f\u533a\u7684\u7efc\u5408\u5730\u7406\u6570\u636e\u96c6\uff0c\u5305\u542b\u7535\u529b\u7cfb\u7edf\u5404\u65b9\u9762\u4fe1\u606f\uff0c\u5e76\u53d1\u5e03\u4e86\u5f00\u653e\u7684\u4ea4\u4e92\u5f0f\u5730\u56fe\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u5c06\u7f13\u89e3\u6570\u636e\u68c0\u7d22\u4e2d\u7684\u8f6f\u4ef6\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u4e3a\u80fd\u6e90\u7814\u7a76\u4eba\u5458\u3001\u5229\u76ca\u76f8\u5173\u8005\u548c\u5f00\u53d1\u8005\u4fc3\u8fdb\u533a\u57df\u7535\u529b\u7cfb\u7edf\u7684\u8054\u5408\u5206\u6790\u3002"}}
{"id": "2510.10214", "categories": ["math.OC", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10214", "abs": "https://arxiv.org/abs/2510.10214", "authors": ["Jingyi Wu", "Chao Ning", "Yang Shi"], "title": "Distributionally Robust Control with End-to-End Statistically Guaranteed Metric Learning", "comment": null, "summary": "Wasserstein distributionally robust control (DRC) recently emerges as a\nprincipled paradigm for handling uncertainty in stochastic dynamical systems.\nHowever, it constructs data-driven ambiguity sets via uniform distribution\nshifts before sequentially incorporating them into downstream control\nsynthesis. This segregation between ambiguity set construction and control\nobjectives inherently introduces a structural misalignment, which undesirably\nleads to conservative control policies with sub-optimal performance. To address\nthis limitation, we propose a novel end-to-end finite-horizon Wasserstein DRC\nframework that integrates the learning of anisotropic Wasserstein metrics with\ndownstream control tasks in a closed-loop manner, thus enabling ambiguity sets\nto be systematically adjusted along performance-critical directions and\nyielding more effective control policies. This framework is formulated as a\nbilevel program: the inner level characterizes dynamical system evolution under\nDRC, while the outer level refines the anisotropic metric leveraging\ncontrol-performance feedback across a range of initial conditions. To solve\nthis program efficiently, we develop a stochastic augmented Lagrangian\nalgorithm tailored to the bilevel structure. Theoretically, we prove that the\nlearned ambiguity sets preserve statistical finite-sample guarantees under a\nnovel radius adjustment mechanism, and we establish the well-posedness of the\nbilevel formulation by demonstrating its continuity with respect to the\nlearnable metric. Furthermore, we show that the algorithm converges to\nstationary points of the outer level problem, which are statistically\nconsistent with the optimal metric at a non-asymptotic convergence rate.\nExperiments on both numerical and inventory control tasks verify that the\nproposed framework achieves superior closed-loop performance and robustness\ncompared against state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684Wasserstein\u5206\u5e03\u9c81\u68d2\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5404\u5411\u5f02\u6027Wasserstein\u5ea6\u91cf\u7684\u5b66\u4e60\u4e0e\u63a7\u5236\u4efb\u52a1\u96c6\u6210\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u6a21\u7cca\u96c6\u6784\u5efa\u4e0e\u63a7\u5236\u76ee\u6807\u4e4b\u95f4\u7684\u7ed3\u6784\u9519\u4f4d\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfWasserstein\u5206\u5e03\u9c81\u68d2\u63a7\u5236\u5728\u6784\u5efa\u6570\u636e\u9a71\u52a8\u7684\u6a21\u7cca\u96c6\u65f6\u4e0e\u4e0b\u6e38\u63a7\u5236\u7efc\u5408\u5206\u79bb\uff0c\u5bfc\u81f4\u7ed3\u6784\u9519\u4f4d\uff0c\u4ea7\u751f\u4fdd\u5b88\u7684\u63a7\u5236\u7b56\u7565\u548c\u6b21\u4f18\u6027\u80fd\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u89c4\u5212\u6846\u67b6\uff1a\u5185\u5c42\u63cf\u8ff0DRC\u4e0b\u7684\u52a8\u6001\u7cfb\u7edf\u6f14\u5316\uff0c\u5916\u5c42\u5229\u7528\u63a7\u5236\u6027\u80fd\u53cd\u9988\u4f18\u5316\u5404\u5411\u5f02\u6027\u5ea6\u91cf\u3002\u5f00\u53d1\u4e86\u9488\u5bf9\u53cc\u5c42\u7ed3\u6784\u7684\u968f\u673a\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6570\u503c\u548c\u5e93\u5b58\u63a7\u5236\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u95ed\u73af\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002\u5b66\u4e60\u5230\u7684\u6a21\u7cca\u96c6\u4fdd\u6301\u4e86\u7edf\u8ba1\u6709\u9650\u6837\u672c\u4fdd\u8bc1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7aef\u5230\u7aef\u6846\u67b6\u901a\u8fc7\u95ed\u73af\u65b9\u5f0f\u8c03\u6574\u6a21\u7cca\u96c6\uff0c\u80fd\u591f\u4ea7\u751f\u66f4\u6709\u6548\u7684\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u4fdd\u8bc1\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6027\u548c\u7edf\u8ba1\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.09714", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09714", "abs": "https://arxiv.org/abs/2510.09714", "authors": ["Shiyuan Guo", "Henry Sleight", "Fabien Roger"], "title": "All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language", "comment": null, "summary": "Detecting harmful AI actions is important as AI agents gain adoption.\nChain-of-thought (CoT) monitoring is one method widely used to detect\nadversarial attacks and AI misalignment. However, attackers and misaligned\nmodels might evade CoT monitoring through ciphered reasoning: reasoning hidden\nin encrypted, translated, or compressed text. To assess this risk, we test\nwhether models can perform ciphered reasoning. For each of 28 different\nciphers, we fine-tune and prompt up to 10 models to reason in that cipher. We\nmeasure model accuracy on math problems as a proxy for reasoning ability.\nAcross the models we test, we find an asymmetry: model accuracy can drop\nsignificantly when reasoning in ciphered text, even though models demonstrate\ncomprehension of ciphered text by being able to translate it accurately to\nEnglish. Even frontier models struggle with lesser-known ciphers, although they\ncan reason accurately in well-known ciphers like rot13. We show that ciphered\nreasoning capability correlates with cipher prevalence in pretraining data. We\nalso identify scaling laws showing that ciphered reasoning capability improves\nslowly with additional fine-tuning data. Our work suggests that evading CoT\nmonitoring using ciphered reasoning may be an ineffective tactic for current\nmodels and offers guidance on constraining the development of this capability\nin future frontier models.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5f53\u524dAI\u6a21\u578b\u5728\u52a0\u5bc6\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u5bf9\u79f0\u6027\uff1a\u867d\u7136\u80fd\u51c6\u786e\u7ffb\u8bd1\u52a0\u5bc6\u6587\u672c\uff0c\u4f46\u5728\u52a0\u5bc6\u63a8\u7406\u65f6\u51c6\u786e\u6027\u663e\u8457\u4e0b\u964d\uff0c\u8fd9\u53ef\u80fd\u4f7f\u52a0\u5bc6\u63a8\u7406\u6210\u4e3a\u9003\u907f\u601d\u7ef4\u94fe\u76d1\u63a7\u7684\u65e0\u6548\u7b56\u7565\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u7684\u5e7f\u6cdb\u91c7\u7528\uff0c\u68c0\u6d4b\u6709\u5bb3AI\u884c\u4e3a\u53d8\u5f97\u91cd\u8981\u3002\u601d\u7ef4\u94fe\u76d1\u63a7\u662f\u68c0\u6d4b\u5bf9\u6297\u6027\u653b\u51fb\u548cAI\u9519\u4f4d\u7684\u65b9\u6cd5\uff0c\u4f46\u653b\u51fb\u8005\u548c\u9519\u4f4d\u6a21\u578b\u53ef\u80fd\u901a\u8fc7\u52a0\u5bc6\u63a8\u7406\u6765\u9003\u907f\u76d1\u63a7\u3002", "method": "\u6d4b\u8bd528\u79cd\u4e0d\u540c\u5bc6\u7801\uff0c\u5bf9\u6700\u591a10\u4e2a\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u548c\u63d0\u793a\uff0c\u4f7f\u5176\u5728\u5bc6\u7801\u4e2d\u8fdb\u884c\u63a8\u7406\u3002\u4f7f\u7528\u6570\u5b66\u95ee\u9898\u51c6\u786e\u6027\u4f5c\u4e3a\u63a8\u7406\u80fd\u529b\u7684\u4ee3\u7406\u6307\u6807\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u51c6\u786e\u6027\u5728\u52a0\u5bc6\u63a8\u7406\u65f6\u663e\u8457\u4e0b\u964d\uff0c\u5c3d\u7ba1\u6a21\u578b\u80fd\u51c6\u786e\u7ffb\u8bd1\u52a0\u5bc6\u6587\u672c\u3002\u524d\u6cbf\u6a21\u578b\u5728\u77e5\u540d\u5bc6\u7801\uff08\u5982rot13\uff09\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8f83\u5c11\u89c1\u5bc6\u7801\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u52a0\u5bc6\u63a8\u7406\u80fd\u529b\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u5bc6\u7801\u7684\u666e\u904d\u6027\u76f8\u5173\u3002", "conclusion": "\u4f7f\u7528\u52a0\u5bc6\u63a8\u7406\u9003\u907f\u601d\u7ef4\u94fe\u76d1\u63a7\u5bf9\u5f53\u524d\u6a21\u578b\u53ef\u80fd\u662f\u65e0\u6548\u7b56\u7565\uff0c\u5e76\u4e3a\u7ea6\u675f\u672a\u6765\u524d\u6cbf\u6a21\u578b\u53d1\u5c55\u8fd9\u79cd\u80fd\u529b\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2510.09660", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09660", "abs": "https://arxiv.org/abs/2510.09660", "authors": ["Luca Scimeca", "Thomas Jiralerspong", "Berton Earnshaw", "Jason Hartford", "Yoshua Bengio"], "title": "Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise", "comment": null, "summary": "Diffusion Probabilistic Models (DPMs) have achieved strong generative\nperformance, yet their inductive biases remain largely implicit. In this work,\nwe aim to build inductive biases into the training and sampling of diffusion\nmodels to better accommodate the target distribution of the data to model. We\nintroduce an anisotropic noise operator that shapes these biases by replacing\nthe isotropic forward covariance with a structured, frequency-diagonal\ncovariance. This operator unifies band-pass masks and power-law weightings,\nallowing us to emphasize or suppress designated frequency bands, while keeping\nthe forward process Gaussian. We refer to this as spectrally anisotropic\nGaussian diffusion (SAGD). In this work, we derive the score relation for\nanisotropic covariances and show that, under full support, the learned score\nconverges to the true data score as $t\\!\\to\\!0$, while anisotropy reshapes the\nprobability-flow path from noise to data. Empirically, we show the induced\nanisotropy outperforms standard diffusion across several vision datasets, and\nenables selective omission: learning while ignoring known corruptions confined\nto specific bands. Together, these results demonstrate that carefully designed\nanisotropic forward noise provides a simple, yet principled, handle to tailor\ninductive bias in DPMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8c31\u5404\u5411\u5f02\u6027\u9ad8\u65af\u6269\u6563(SAGD)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5404\u5411\u5f02\u6027\u566a\u58f0\u7b97\u5b50\u6765\u6784\u5efa\u6269\u6563\u6a21\u578b\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u7528\u7ed3\u6784\u5316\u7684\u9891\u7387\u5bf9\u89d2\u534f\u65b9\u5dee\u66ff\u4ee3\u5404\u5411\u540c\u6027\u524d\u5411\u534f\u65b9\u5dee\u3002", "motivation": "\u6269\u6563\u6982\u7387\u6a21\u578b(DPMs)\u867d\u7136\u53d6\u5f97\u4e86\u5f3a\u5927\u7684\u751f\u6210\u6027\u80fd\uff0c\u4f46\u5176\u5f52\u7eb3\u504f\u7f6e\u4ecd\u7136\u5f88\u5927\u7a0b\u5ea6\u4e0a\u662f\u9690\u5f0f\u7684\u3002\u672c\u6587\u65e8\u5728\u5c06\u5f52\u7eb3\u504f\u7f6e\u663e\u5f0f\u5730\u6784\u5efa\u5230\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u91c7\u6837\u8fc7\u7a0b\u4e2d\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u6570\u636e\u7684\u76ee\u6807\u5206\u5e03\u3002", "method": "\u5f15\u5165\u5404\u5411\u5f02\u6027\u566a\u58f0\u7b97\u5b50\uff0c\u5c06\u5404\u5411\u540c\u6027\u524d\u5411\u534f\u65b9\u5dee\u66ff\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u9891\u7387\u5bf9\u89d2\u534f\u65b9\u5dee\u3002\u8be5\u7b97\u5b50\u7edf\u4e00\u4e86\u5e26\u901a\u63a9\u7801\u548c\u5e42\u5f8b\u52a0\u6743\uff0c\u5141\u8bb8\u5f3a\u8c03\u6216\u6291\u5236\u6307\u5b9a\u7684\u9891\u5e26\uff0c\u540c\u65f6\u4fdd\u6301\u524d\u5411\u8fc7\u7a0b\u7684\u9ad8\u65af\u6027\u3002\u63a8\u5bfc\u4e86\u5404\u5411\u5f02\u6027\u534f\u65b9\u5dee\u7684\u5f97\u5206\u5173\u7cfb\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u89c9\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8bf1\u5bfc\u7684\u5404\u5411\u5f02\u6027\u6027\u80fd\u4f18\u4e8e\u6807\u51c6\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5b9e\u73b0\u4e86\u9009\u62e9\u6027\u7701\u7565\uff1a\u5728\u5ffd\u7565\u7279\u5b9a\u9891\u5e26\u4e2d\u5df2\u77e5\u7684\u635f\u574f\u7684\u540c\u65f6\u8fdb\u884c\u5b66\u4e60\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5404\u5411\u5f02\u6027\u524d\u5411\u566a\u58f0\u4e3a\u5728DPMs\u4e2d\u5b9a\u5236\u5f52\u7eb3\u504f\u7f6e\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u539f\u5219\u7684\u8c03\u63a7\u624b\u6bb5\u3002"}}
{"id": "2510.09905", "categories": ["cs.AI", "cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.09905", "abs": "https://arxiv.org/abs/2510.09905", "authors": ["Xi Fang", "Weijie Xu", "Yuchong Zhang", "Stephanie Eckman", "Scott Nickleach", "Chandan K. Reddy"], "title": "The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs", "comment": "12 pages 5 figures", "summary": "When an AI assistant remembers that Sarah is a single mother working two\njobs, does it interpret her stress differently than if she were a wealthy\nexecutive? As personalized AI systems increasingly incorporate long-term user\nmemory, understanding how this memory shapes emotional reasoning is critical.\nWe investigate how user memory affects emotional intelligence in large language\nmodels (LLMs) by evaluating 15 models on human validated emotional intelligence\ntests. We find that identical scenarios paired with different user profiles\nproduce systematically divergent emotional interpretations. Across validated\nuser independent emotional scenarios and diverse user profiles, systematic\nbiases emerged in several high-performing LLMs where advantaged profiles\nreceived more accurate emotional interpretations. Moreover, LLMs demonstrate\nsignificant disparities across demographic factors in emotion understanding and\nsupportive recommendations tasks, indicating that personalization mechanisms\ncan embed social hierarchies into models emotional reasoning. These results\nhighlight a key challenge for memory enhanced AI: systems designed for\npersonalization may inadvertently reinforce social inequalities.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53AI\u7cfb\u7edf\u6574\u5408\u7528\u6237\u957f\u671f\u8bb0\u5fc6\u65f6\uff0c\u4e0d\u540c\u7684\u7528\u6237\u6863\u6848\u4f1a\u5bfc\u81f4\u7cfb\u7edf\u6027\u7684\u60c5\u611f\u7406\u89e3\u504f\u5dee\uff0c\u4f18\u52bf\u7fa4\u4f53\u83b7\u5f97\u66f4\u51c6\u786e\u7684\u60c5\u611f\u89e3\u8bfb\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4AI\u4e2a\u6027\u5316\u673a\u5236\u5f3a\u5316\u793e\u4f1a\u4e0d\u5e73\u7b49\u3002", "motivation": "\u968f\u7740\u4e2a\u6027\u5316AI\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u6574\u5408\u957f\u671f\u7528\u6237\u8bb0\u5fc6\uff0c\u7406\u89e3\u8fd9\u79cd\u8bb0\u5fc6\u5982\u4f55\u5f71\u54cd\u60c5\u611f\u63a8\u7406\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u8981\u63a2\u7a76\u7528\u6237\u8bb0\u5fc6\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u60c5\u611f\u667a\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u8bc4\u4f3015\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4eba\u7c7b\u9a8c\u8bc1\u7684\u60c5\u611f\u667a\u80fd\u6d4b\u8bd5\u4e0a\u7684\u8868\u73b0\uff0c\u6bd4\u8f83\u76f8\u540c\u60c5\u5883\u4e0b\u4e0d\u540c\u7528\u6237\u6863\u6848\u7684\u60c5\u611f\u89e3\u8bfb\u5dee\u5f02\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u76f8\u540c\u7684\u573a\u666f\u642d\u914d\u4e0d\u540c\u7684\u7528\u6237\u6863\u6848\u4f1a\u4ea7\u751f\u7cfb\u7edf\u6027\u7684\u60c5\u611f\u7406\u89e3\u5206\u6b67\uff0c\u591a\u4e2a\u9ad8\u6027\u80fdLLM\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u4f18\u52bf\u7fa4\u4f53\u83b7\u5f97\u66f4\u51c6\u786e\u7684\u60c5\u611f\u89e3\u8bfb\uff0c\u4e14\u5728\u60c5\u611f\u7406\u89e3\u548c\u652f\u6301\u6027\u5efa\u8bae\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u5dee\u5f02\u3002", "conclusion": "\u8bb0\u5fc6\u589e\u5f3a\u578bAI\u9762\u4e34\u5173\u952e\u6311\u6218\uff1a\u4e3a\u4e2a\u6027\u5316\u8bbe\u8ba1\u7684\u7cfb\u7edf\u53ef\u80fd\u65e0\u610f\u4e2d\u5f3a\u5316\u793e\u4f1a\u4e0d\u5e73\u7b49\uff0c\u9700\u8981\u5173\u6ce8\u4e2a\u6027\u5316\u673a\u5236\u5982\u4f55\u5c06\u793e\u4f1a\u7b49\u7ea7\u5d4c\u5165\u6a21\u578b\u7684\u60c5\u611f\u63a8\u7406\u4e2d\u3002"}}
{"id": "2510.10870", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.10870", "abs": "https://arxiv.org/abs/2510.10870", "authors": ["Chenze Li", "Subhadeep Paul"], "title": "Transfer Learning with Distance Covariance for Random Forest: Error Bounds and an EHR Application", "comment": null, "summary": "Random forest is an important method for ML applications due to its broad\noutperformance over competing methods for structured tabular data. We propose a\nmethod for transfer learning in nonparametric regression using a centered\nrandom forest (CRF) with distance covariance-based feature weights, assuming\nthe unknown source and target regression functions are different for a few\nfeatures (sparsely different). Our method first obtains residuals from\npredicting the response in the target domain using a source domain-trained CRF.\nThen, we fit another CRF to the residuals, but with feature splitting\nprobabilities proportional to the sample distance covariance between the\nfeatures and the residuals in an independent sample. We derive an upper bound\non the mean square error rate of the procedure as a function of sample sizes\nand difference dimension, theoretically demonstrating transfer learning\nbenefits in random forests. In simulations, we show that the results obtained\nfor the CRFs also hold numerically for the standard random forest (SRF) method\nwith data-driven feature split selection. Beyond transfer learning, our results\nalso show the benefit of distance-covariance-based weights on the performance\nof RF in some situations. Our method shows significant gains in predicting the\nmortality of ICU patients in smaller-bed target hospitals using a large\nmulti-hospital dataset of electronic health records for 200,000 ICU patients.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e2d\u5fc3\u5316\u968f\u673a\u68ee\u6797\u548c\u8ddd\u79bb\u534f\u65b9\u5dee\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u975e\u53c2\u6570\u56de\u5f52\uff0c\u5728\u7a00\u758f\u5dee\u5f02\u5047\u8bbe\u4e0b\u5b9e\u73b0\u4ece\u6e90\u57df\u5230\u76ee\u6807\u57df\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "motivation": "\u968f\u673a\u68ee\u6797\u5728\u7ed3\u6784\u5316\u8868\u683c\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u6e90\u57df\u548c\u76ee\u6807\u57df\u56de\u5f52\u51fd\u6570\u4ec5\u5bf9\u5c11\u6570\u7279\u5f81\u5b58\u5728\u5dee\u5f02\u65f6\u7684\u8fc1\u79fb\u5b66\u4e60\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e2d\u5fc3\u5316\u968f\u673a\u68ee\u6797\uff0c\u9996\u5148\u7528\u6e90\u57df\u8bad\u7ec3\u7684CRF\u9884\u6d4b\u76ee\u6807\u57df\u54cd\u5e94\u5e76\u83b7\u53d6\u6b8b\u5dee\uff0c\u7136\u540e\u7528\u8ddd\u79bb\u534f\u65b9\u5dee\u52a0\u6743\u7684\u7279\u5f81\u5206\u88c2\u6982\u7387\u5bf9\u6b8b\u5dee\u62df\u5408\u7b2c\u4e8c\u4e2aCRF\u3002", "result": "\u7406\u8bba\u63a8\u5bfc\u4e86\u5747\u65b9\u8bef\u5dee\u7387\u7684\u4e0a\u754c\uff0c\u6570\u503c\u6a21\u62df\u663e\u793a\u6807\u51c6\u968f\u673a\u68ee\u6797\u4e5f\u80fd\u83b7\u5f97\u7c7b\u4f3c\u6548\u679c\u3002\u5728ICU\u60a3\u8005\u6b7b\u4ea1\u7387\u9884\u6d4b\u4e2d\uff0c\u4f7f\u7528\u5927\u578b\u591a\u533b\u9662\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u533b\u9662\u76ee\u6807\u57df\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u968f\u673a\u68ee\u6797\u4e2d\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u8ddd\u79bb\u534f\u65b9\u5dee\u6743\u91cd\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4e5f\u80fd\u63d0\u5347\u968f\u673a\u68ee\u6797\u6027\u80fd\uff0c\u5728\u533b\u7597\u6570\u636e\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2510.09943", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.09943", "abs": "https://arxiv.org/abs/2510.09943", "authors": ["Yutian Pang", "Andrew Kendall", "John-Paul Clarke"], "title": "Modeling the Impact of Communication and Human Uncertainties on Runway Capacity in Terminal Airspace", "comment": null, "summary": "We investigate the potential impact of communication and human performance\nuncertainties on runway operations. Specifically, we consider these impacts\nwithin the context of an arrival scenario with two converging flows: a\nstraight-in approach stream and a downwind stream merging into it. Both arrival\nstream are modeled using a modified Possion distribution that incorporate the\nseparation minima as well as the runway occupancy time. Various system level\nuncertainties are addressed in this process, including communication link- and\nhuman-related uncertainties. In this research, we first build a Monte\nCarlo-based discrete-time simulation, where aircraft arrivals are generated by\nmodified Poisson processes subject to minimum separation constraints,\nsimulating various traffic operations. The merging logic incorporates standard\nbank angle continuous turn-to-final, pilot response delays, and dynamic gap\navailability in real time. Then, we investigate an automated final approach\nvectoring model (i.e., Auto-ATC), in which inverse optimal control is used to\nlearn decision advisories from human expert records. By augmenting trajectories\nand incorporating the aforementioned uncertainties into the planning scenario,\nwe create a setup analogous to the discrete event simulation. For both studies,\nrunway capacity is measured by runway throughput, the fraction of downwind\narrivals that merge immediately without holding, and the average delay (i.e.,\nholding time/distance) experienced on the downwind leg. This research provides\na method for runway capacity estimation in merging scenarios, and demonstrates\nthat aeronautical communication link uncertainties significantly affect runway\ncapacity in current voice-based operations, whereas the impact can be mitigated\nin autonomous operational settings.", "AI": {"tldr": "\u7814\u7a76\u4e86\u901a\u4fe1\u548c\u4eba\u4e3a\u4e0d\u786e\u5b9a\u6027\u5bf9\u8dd1\u9053\u8fd0\u884c\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u548c\u81ea\u52a8ATC\u6a21\u578b\u5206\u6790\u6c47\u5408\u573a\u666f\u4e0b\u7684\u8dd1\u9053\u5bb9\u91cf\uff0c\u53d1\u73b0\u8bed\u97f3\u901a\u4fe1\u4e0d\u786e\u5b9a\u6027\u663e\u8457\u5f71\u54cd\u5bb9\u91cf\uff0c\u4f46\u5728\u81ea\u4e3b\u64cd\u4f5c\u4e2d\u53ef\u7f13\u89e3\u3002", "motivation": "\u7814\u7a76\u901a\u4fe1\u94fe\u8def\u548c\u4eba\u4e3a\u6027\u80fd\u4e0d\u786e\u5b9a\u6027\u5bf9\u8dd1\u9053\u8fd0\u884c\u5bb9\u91cf\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u4e24\u6761\u8fdb\u8fd1\u6d41\u6c47\u5408\u7684\u573a\u666f\u4e0b\uff0c\u8bc4\u4f30\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u56e0\u7d20\u5bf9\u8dd1\u9053\u541e\u5410\u91cf\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u79bb\u6563\u65f6\u95f4\u6a21\u62df\u751f\u6210\u53d7\u6700\u5c0f\u95f4\u9694\u7ea6\u675f\u7684\u98de\u673a\u5230\u8fbe\uff0c\u5efa\u7acb\u5305\u542b\u6807\u51c6\u8f6c\u5f2f\u89d2\u5ea6\u3001\u98de\u884c\u5458\u54cd\u5e94\u5ef6\u8fdf\u548c\u52a8\u6001\u95f4\u9694\u53ef\u7528\u6027\u7684\u6c47\u5408\u903b\u8f91\uff1b\u540c\u65f6\u5f00\u53d1\u57fa\u4e8e\u9006\u6700\u4f18\u63a7\u5236\u7684\u81ea\u52a8\u8fdb\u8fd1\u5f15\u5bfc\u6a21\u578b\uff0c\u4ece\u4e13\u5bb6\u8bb0\u5f55\u4e2d\u5b66\u4e60\u51b3\u7b56\u5efa\u8bae\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u822a\u7a7a\u901a\u4fe1\u94fe\u8def\u4e0d\u786e\u5b9a\u6027\u5728\u5f53\u524d\u8bed\u97f3\u64cd\u4f5c\u4e2d\u663e\u8457\u5f71\u54cd\u8dd1\u9053\u5bb9\u91cf\uff0c\u4f46\u5728\u81ea\u4e3b\u64cd\u4f5c\u73af\u5883\u4e2d\u8fd9\u79cd\u5f71\u54cd\u53ef\u4ee5\u5f97\u5230\u7f13\u89e3\u3002\u901a\u8fc7\u6a21\u62df\u83b7\u5f97\u4e86\u8dd1\u9053\u541e\u5410\u91cf\u3001\u7acb\u5373\u6c47\u5408\u6bd4\u4f8b\u548c\u5e73\u5747\u5ef6\u8bef\u7b49\u5bb9\u91cf\u6307\u6807\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u6c47\u5408\u573a\u666f\u4e0b\u8dd1\u9053\u5bb9\u91cf\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u901a\u4fe1\u4e0d\u786e\u5b9a\u6027\u5bf9\u8dd1\u9053\u8fd0\u884c\u7684\u91cd\u8981\u5f71\u54cd\uff0c\u5e76\u5c55\u793a\u4e86\u81ea\u4e3b\u64cd\u4f5c\u5728\u7f13\u89e3\u8fd9\u4e9b\u5f71\u54cd\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.09840", "categories": ["cs.CY", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.09840", "abs": "https://arxiv.org/abs/2510.09840", "authors": ["Jarrad Hope", "Peter Ludlow"], "title": "Farewell to Westphalia: Crypto Sovereignty and Post-Nation-State Governaance", "comment": null, "summary": "We argue that the principal application for blockchain technology will not be\nin the financial sector, but rather in maintaining decentralized human\ngovernance, from archives to transparent policies encoded in the blockchain in\nthe form of smart contracts.. Such decentralized, blockchain-grounded\ngovernance comes not a moment too soon, as nation states are dissolving before\nour eyes. Will blockchain-based communities replace the nation state? What are\nthe prospects and dangers of this development?", "AI": {"tldr": "\u533a\u5757\u94fe\u6280\u672f\u7684\u4e3b\u8981\u5e94\u7528\u5c06\u4e0d\u662f\u91d1\u878d\u9886\u57df\uff0c\u800c\u662f\u7528\u4e8e\u7ef4\u62a4\u53bb\u4e2d\u5fc3\u5316\u7684\u4eba\u7c7b\u6cbb\u7406\uff0c\u5305\u62ec\u6863\u6848\u7ba1\u7406\u548c\u4ee5\u667a\u80fd\u5408\u7ea6\u5f62\u5f0f\u7f16\u7801\u7684\u900f\u660e\u653f\u7b56\u3002", "motivation": "\u968f\u7740\u6c11\u65cf\u56fd\u5bb6\u6b63\u5728\u74e6\u89e3\uff0c\u53bb\u4e2d\u5fc3\u5316\u7684\u533a\u5757\u94fe\u6cbb\u7406\u663e\u5f97\u5c24\u4e3a\u8feb\u5207\uff0c\u63a2\u8ba8\u533a\u5757\u94fe\u793e\u533a\u662f\u5426\u80fd\u591f\u53d6\u4ee3\u6c11\u65cf\u56fd\u5bb6\u53ca\u5176\u524d\u666f\u4e0e\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u8bba\u8bc1\u533a\u5757\u94fe\u6280\u672f\u5728\u6cbb\u7406\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5206\u6790\u5176\u5728\u6863\u6848\u7ef4\u62a4\u548c\u653f\u7b56\u900f\u660e\u5ea6\u65b9\u9762\u7684\u5e94\u7528\u3002", "result": "\u63d0\u51fa\u533a\u5757\u94fe\u6280\u672f\u80fd\u591f\u652f\u6301\u53bb\u4e2d\u5fc3\u5316\u6cbb\u7406\u7ed3\u6784\uff0c\u4e3a\u6b63\u5728\u74e6\u89e3\u7684\u6c11\u65cf\u56fd\u5bb6\u63d0\u4f9b\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u533a\u5757\u94fe\u6280\u672f\u6709\u671b\u5728\u6cbb\u7406\u9886\u57df\u53d1\u6325\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u9700\u8981\u5ba1\u614e\u8bc4\u4f30\u5176\u53d6\u4ee3\u6c11\u65cf\u56fd\u5bb6\u7684\u524d\u666f\u548c\u6f5c\u5728\u5371\u9669\u3002"}}
{"id": "2510.10229", "categories": ["math.OC", "65R32, 68T05, 65M12"], "pdf": "https://arxiv.org/pdf/2510.10229", "abs": "https://arxiv.org/abs/2510.10229", "authors": ["Nina M. Gottschling", "David Iagaru", "Jakob Gawlikowski", "Ioannis Sgouralis"], "title": "Average Kernel Sizes -- Computable Sharp Accuracy Bounds for Inverse Problems", "comment": null, "summary": "The reconstruction of an unknown quantity from noisy measurements is a\nmathematical problem relevant in most applied sciences, for example, in medical\nimaging, radar inverse scattering, or astronomy. This underlying mathematical\nproblem is often an ill-posed (non-linear) reconstruction problem, referred to\nas an ill-posed inverse problem. To tackle such problems, there exist a myriad\nof methods to design approximate inverse maps, ranging from optimization-based\napproaches, such as compressed sensing, over Bayesian approaches, to\ndata-driven techniques such as deep learning. For all stable approximate\ninverse maps, there are accuracy limits that are strictly larger than zero for\nill-posed inverse problems, due to the accuracy-stability tradeoff [Gottschling\net al., SIAM Review, 67.1 (2025)] and [Colbrook et al., Proceedings of the\nNational Academy of Sciences, 119.12 (2022)]. The variety of methods that aim\nto solve such problems begs for a unifying approach to help scientists choose\nthe approximate inverse map that obtains this theoretical optimum. Up to now\nthere do not exist computable accuracy bounds to this optimum that are\napplicable to all inverse problems. We provide computable sharp accuracy bounds\nto the reconstruction error of solution methods to inverse problems. The bounds\nare method-independent and purely depend on the dataset of signals, the forward\nmodel of the inverse problem, and the noise model. To facilitate the use in\nscientific applications, we provide an algorithmic framework and an\naccompanying software library to compute these accuracy bounds. We demonstrate\nthe validity of the algorithms on two inverse problems from different domains:\nfluorescence localization microscopy and super-resolution of multi-spectral\nsatellite data. Computing the accuracy bounds for a problem before solving it,\nenables a fundamental shift towards optimizing datasets and forward models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u53ef\u8ba1\u7b97\u7684\u5c16\u9510\u7cbe\u5ea6\u754c\u9650\uff0c\u7528\u4e8e\u8bc4\u4f30\u9006\u95ee\u9898\u6c42\u89e3\u65b9\u6cd5\u7684\u91cd\u6784\u8bef\u5dee\u3002\u8fd9\u4e9b\u754c\u9650\u4e0e\u5177\u4f53\u65b9\u6cd5\u65e0\u5173\uff0c\u4ec5\u4f9d\u8d56\u4e8e\u4fe1\u53f7\u6570\u636e\u96c6\u3001\u524d\u5411\u6a21\u578b\u548c\u566a\u58f0\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u9006\u95ee\u9898\u7684\u65b9\u6cd5\u4f17\u591a\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u5e2e\u52a9\u79d1\u5b66\u5bb6\u9009\u62e9\u80fd\u8fbe\u5230\u7406\u8bba\u6700\u4f18\u7684\u8fd1\u4f3c\u9006\u6620\u5c04\u3002\u76ee\u524d\u4e0d\u5b58\u5728\u9002\u7528\u4e8e\u6240\u6709\u9006\u95ee\u9898\u7684\u53ef\u8ba1\u7b97\u7cbe\u5ea6\u754c\u9650\u3002", "method": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b97\u6cd5\u6846\u67b6\u548c\u914d\u5957\u8f6f\u4ef6\u5e93\u6765\u8ba1\u7b97\u8fd9\u4e9b\u7cbe\u5ea6\u754c\u9650\u3002\u754c\u9650\u4ec5\u4f9d\u8d56\u4e8e\u4fe1\u53f7\u6570\u636e\u96c6\u3001\u524d\u5411\u6a21\u578b\u548c\u566a\u58f0\u6a21\u578b\uff0c\u4e0e\u5177\u4f53\u6c42\u89e3\u65b9\u6cd5\u65e0\u5173\u3002", "result": "\u5728\u8367\u5149\u5b9a\u4f4d\u663e\u5fae\u955c\u548c\u591a\u5149\u8c31\u536b\u661f\u6570\u636e\u8d85\u5206\u8fa8\u7387\u4e24\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u9006\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5728\u89e3\u51b3\u95ee\u9898\u524d\u8ba1\u7b97\u7cbe\u5ea6\u754c\u9650\uff0c\u80fd\u591f\u5b9e\u73b0\u5411\u4f18\u5316\u6570\u636e\u96c6\u548c\u524d\u5411\u6a21\u578b\u7684\u57fa\u7840\u6027\u8f6c\u53d8\u3002"}}
{"id": "2510.09720", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09720", "abs": "https://arxiv.org/abs/2510.09720", "authors": ["Haoran Sun", "Zekun Zhang", "Shaoning Zeng"], "title": "Preference-Aware Memory Update for Long-Term LLM Agents", "comment": null, "summary": "One of the key factors influencing the reasoning capabilities of LLM-based\nagents is their ability to leverage long-term memory. Integrating long-term\nmemory mechanisms allows agents to make informed decisions grounded in\nhistorical interactions. While recent advances have significantly improved the\nstorage and retrieval components, by encoding memory into dense vectors for\nsimilarity search or organizing memory as structured knowledge graphs most\nexisting approaches fall short in memory updating. In particular, they lack\nmechanisms for dynamically refining preference memory representations in\nresponse to evolving user behaviors and contexts. To address this gap, we\npropose a Preference-Aware Memory Update Mechanism (PAMU) that enables dynamic\nand personalized memory refinement. By integrating sliding window averages (SW)\nwith exponential moving averages (EMA), PAMU constructs a fused\npreference-aware representation that captures both short-term fluctuations and\nlong-term user tendencies. We conduct experiments on five task scenarios of the\nLoCoMo dataset, and the results show that our mechanism can significantly\nimprove the output quality of LLM in five baselines, validating its\neffectiveness in long-term conversations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u504f\u597d\u611f\u77e5\u8bb0\u5fc6\u66f4\u65b0\u673a\u5236(PAMU)\uff0c\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u5e73\u5747\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7684\u878d\u5408\u6765\u52a8\u6001\u4f18\u5316LLM\u4ee3\u7406\u7684\u957f\u671f\u8bb0\u5fc6\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u5bf9\u8bdd\u4e2d\u7684\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u957f\u671f\u8bb0\u5fc6\u65b9\u6cd5\u5728\u5b58\u50a8\u548c\u68c0\u7d22\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\uff0c\u4f46\u7f3a\u4e4f\u52a8\u6001\u66f4\u65b0\u673a\u5236\u6765\u9002\u5e94\u7528\u6237\u884c\u4e3a\u548c\u4e0a\u4e0b\u6587\u7684\u6f14\u53d8\uff0c\u65e0\u6cd5\u6709\u6548\u4f18\u5316\u504f\u597d\u8bb0\u5fc6\u8868\u793a\u3002", "method": "\u63d0\u51faPAMU\u673a\u5236\uff0c\u6574\u5408\u6ed1\u52a8\u7a97\u53e3\u5e73\u5747(SW)\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747(EMA)\uff0c\u6784\u5efa\u878d\u5408\u7684\u504f\u597d\u611f\u77e5\u8868\u793a\uff0c\u6355\u6349\u77ed\u671f\u6ce2\u52a8\u548c\u957f\u671f\u7528\u6237\u503e\u5411\u3002", "result": "\u5728LoCoMo\u6570\u636e\u96c6\u7684\u4e94\u4e2a\u4efb\u52a1\u573a\u666f\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u673a\u5236\u80fd\u663e\u8457\u63d0\u5347\u4e94\u4e2a\u57fa\u7ebf\u6a21\u578b\u5728LLM\u8f93\u51fa\u8d28\u91cf\u65b9\u9762\u7684\u8868\u73b0\u3002", "conclusion": "PAMU\u673a\u5236\u5728\u957f\u671f\u5bf9\u8bdd\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u80fd\u591f\u52a8\u6001\u548c\u4e2a\u6027\u5316\u5730\u4f18\u5316\u8bb0\u5fc6\u8868\u793a\u3002"}}
{"id": "2510.09662", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.09662", "abs": "https://arxiv.org/abs/2510.09662", "authors": ["Ali Jaberi", "Amin Sadeghi", "Runze Zhang", "Zhaoyang Zhao", "Qiuyu Shi", "Robert Black", "Zoya Sadighi", "Jason Hattrick-Simpers"], "title": "Assessment of different loss functions for fitting equivalent circuit models to electrochemical impedance spectroscopy data", "comment": null, "summary": "Electrochemical impedance spectroscopy (EIS) data is typically modeled using\nan equivalent circuit model (ECM), with parameters obtained by minimizing a\nloss function via nonlinear least squares fitting. This paper introduces two\nnew loss functions, log-B and log-BW, derived from the Bode representation of\nEIS. Using a large dataset of generated EIS data, the performance of proposed\nloss functions was evaluated alongside existing ones in terms of R2 scores,\nchi-squared, computational efficiency, and the mean absolute percentage error\n(MAPE) between the predicted component values and the original values.\nStatistical comparisons revealed that the choice of loss function impacts\nconvergence, computational efficiency, quality of fit, and MAPE. Our analysis\nshowed that X2 loss function (squared sum of residuals with proportional\nweighting) achieved the highest performance across multiple quality of fit\nmetrics, making it the preferred choice when the quality of fit is the primary\ngoal. On the other hand, log-B offered a slightly lower quality of fit while\nbeing approximately 1.4 times faster and producing lower MAPE for most circuit\ncomponents, making log-B as a strong alternative. This is a critical factor for\nlarge-scale least squares fitting in data-driven applications, such as training\nmachine learning models on extensive datasets or iterations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8eBode\u56fe\u8868\u793a\u7684\u65b0\u635f\u5931\u51fd\u6570log-B\u548clog-BW\uff0c\u7528\u4e8e\u7535\u5316\u5b66\u963b\u6297\u8c31\u6570\u636e\u7684\u7b49\u6548\u7535\u8def\u6a21\u578b\u62df\u5408\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8bc4\u4f30\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7535\u5316\u5b66\u963b\u6297\u8c31\u6570\u636e\u4f7f\u7528\u7b49\u6548\u7535\u8def\u6a21\u578b\u8fdb\u884c\u62df\u5408\u65f6\uff0c\u901a\u5e38\u901a\u8fc7\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u6cd5\u6700\u5c0f\u5316\u635f\u5931\u51fd\u6570\u6765\u83b7\u53d6\u53c2\u6570\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u65b0\u7684\u635f\u5931\u51fd\u6570\u4ee5\u63d0\u9ad8\u62df\u5408\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u751f\u6210\u7684\u5927\u89c4\u6a21EIS\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u63d0\u51fa\u7684log-B\u548clog-BW\u635f\u5931\u51fd\u6570\u4e0e\u73b0\u6709\u635f\u5931\u51fd\u6570\u5728R2\u5206\u6570\u3001\u5361\u65b9\u503c\u3001\u8ba1\u7b97\u6548\u7387\u548cMAPE\u7b49\u65b9\u9762\u7684\u6027\u80fd\u3002", "result": "X2\u635f\u5931\u51fd\u6570\u5728\u591a\u4e2a\u62df\u5408\u8d28\u91cf\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800clog-B\u635f\u5931\u51fd\u6570\u867d\u7136\u62df\u5408\u8d28\u91cf\u7565\u4f4e\uff0c\u4f46\u8ba1\u7b97\u901f\u5ea6\u5feb\u7ea61.4\u500d\uff0c\u4e14\u5bf9\u5927\u591a\u6570\u7535\u8def\u5143\u4ef6\u4ea7\u751f\u66f4\u4f4e\u7684MAPE\u3002", "conclusion": "\u5f53\u62df\u5408\u8d28\u91cf\u662f\u9996\u8981\u76ee\u6807\u65f6\uff0cX2\u635f\u5931\u51fd\u6570\u662f\u6700\u4f73\u9009\u62e9\uff1blog-B\u635f\u5931\u51fd\u6570\u4f5c\u4e3a\u5f3a\u6709\u529b\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u5927\u89c4\u6a21\u6700\u5c0f\u4e8c\u4e58\u62df\u5408\u5e94\u7528\u4e2d\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2510.09970", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09970", "abs": "https://arxiv.org/abs/2510.09970", "authors": ["Olivia Peiyu Wang", "Tashvi Bansal", "Ryan Bai", "Emily M. Chui", "Leilani H. Gilpin"], "title": "Follow My Lead: Logical Fallacy Classification with Knowledge-Augmented LLMs", "comment": "Accepted as a poster at the Twelfth Annual Conference on Advances in\n  Cognitive Systems. 21 pages, 7 figures and 1 table", "summary": "Large Language Models (LLMs) suffer from critical reasoning gaps, including a\ntendency to hallucinate and poor accuracy in classifying logical fallacies.\nThis limitation stems from their default System 1 processing, which is fast and\nintuitive, whereas reliable reasoning requires the deliberate, effortful System\n2 approach (Kahneman, 2011; Li et al., 2025). Since full System 2 training is\noften prohibitively expensive, we explore a low-cost, instruction-based\nintervention to bridge this gap. Our methodology introduces a novel stepwise\ninstruction dataset that decomposes fallacy classification into a series of\natomic procedural steps (simple binary questions). We further augment this with\na final verification step where models consult a relational knowledge graph of\nrelated fallacies. This procedural, rule-based intervention yields a\nsignificant improvement in LLM logical fallacy classification. Crucially, the\napproach also provides enhanced transparency into the LLMs' decision-making,\nhighlighting a practical pathway for Neuro-symbolic architectures to address\nLLM reasoning deficits.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u57fa\u4e8e\u6307\u4ee4\u7684\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u903b\u8f91\u8c2c\u8bef\u5206\u7c7b\u5206\u89e3\u4e3a\u539f\u5b50\u7a0b\u5e8f\u6b65\u9aa4\uff0c\u5e76\u7ed3\u5408\u5173\u7cfb\u77e5\u8bc6\u56fe\u8c31\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u903b\u8f91\u8c2c\u8bef\u5206\u7c7b\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5173\u952e\u63a8\u7406\u7f3a\u9677\uff0c\u5305\u62ec\u4ea7\u751f\u5e7b\u89c9\u548c\u5728\u903b\u8f91\u8c2c\u8bef\u5206\u7c7b\u4e2d\u51c6\u786e\u6027\u5dee\u3002\u8fd9\u4e9b\u9650\u5236\u6e90\u4e8e\u5176\u9ed8\u8ba4\u7684\u5feb\u901f\u76f4\u89c9\u5f0f\u7cfb\u7edf1\u5904\u7406\uff0c\u800c\u53ef\u9760\u63a8\u7406\u9700\u8981\u6df1\u601d\u719f\u8651\u7684\u7cfb\u7edf2\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u65b0\u9896\u7684\u9010\u6b65\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u5c06\u8c2c\u8bef\u5206\u7c7b\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u539f\u5b50\u7a0b\u5e8f\u6b65\u9aa4\uff08\u7b80\u5355\u4e8c\u5143\u95ee\u9898\uff09\uff0c\u5e76\u901a\u8fc7\u54a8\u8be2\u76f8\u5173\u8c2c\u8bef\u7684\u5173\u7cfb\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u6700\u7ec8\u9a8c\u8bc1\u3002", "result": "\u8fd9\u79cd\u57fa\u4e8e\u7a0b\u5e8f\u7684\u89c4\u5219\u5e72\u9884\u663e\u8457\u63d0\u9ad8\u4e86LLM\u903b\u8f91\u8c2c\u8bef\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u5e76\u4e3aLLM\u51b3\u7b56\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u900f\u660e\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u89e3\u51b3LLM\u63a8\u7406\u7f3a\u9677\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u9014\u5f84\uff0c\u5c55\u793a\u4e86\u4f4e\u6210\u672c\u5e72\u9884\u5728\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.10981", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10981", "abs": "https://arxiv.org/abs/2510.10981", "authors": ["Tomoya Wakayama", "Taiji Suzuki"], "title": "In-Context Learning Is Provably Bayesian Inference: A Generalization Theory for Meta-Learning", "comment": null, "summary": "This paper develops a finite-sample statistical theory for in-context\nlearning (ICL), analyzed within a meta-learning framework that accommodates\nmixtures of diverse task types. We introduce a principled risk decomposition\nthat separates the total ICL risk into two orthogonal components: Bayes Gap and\nPosterior Variance. The Bayes Gap quantifies how well the trained model\napproximates the Bayes-optimal in-context predictor. For a uniform-attention\nTransformer, we derive a non-asymptotic upper bound on this gap, which\nexplicitly clarifies the dependence on the number of pretraining prompts and\ntheir context length. The Posterior Variance is a model-independent risk\nrepresenting the intrinsic task uncertainty. Our key finding is that this term\nis determined solely by the difficulty of the true underlying task, while the\nuncertainty arising from the task mixture vanishes exponentially fast with only\na few in-context examples. Together, these results provide a unified view of\nICL: the Transformer selects the optimal meta-algorithm during pretraining and\nrapidly converges to the optimal algorithm for the true task at test time.", "AI": {"tldr": "\u672c\u6587\u4e3a\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u5efa\u7acb\u4e86\u6709\u9650\u6837\u672c\u7edf\u8ba1\u7406\u8bba\uff0c\u5c06ICL\u98ce\u9669\u5206\u89e3\u4e3a\u8d1d\u53f6\u65af\u95f4\u9699\u548c\u540e\u9a8c\u65b9\u5dee\u4e24\u4e2a\u6b63\u4ea4\u5206\u91cf\uff0c\u63ed\u793a\u4e86Transformer\u5728\u9884\u8bad\u7ec3\u4e2d\u9009\u62e9\u6700\u4f18\u5143\u7b97\u6cd5\u5e76\u5728\u6d4b\u8bd5\u65f6\u5feb\u901f\u6536\u655b\u5230\u771f\u5b9e\u4efb\u52a1\u6700\u4f18\u7b97\u6cd5\u7684\u673a\u5236\u3002", "motivation": "\u73b0\u6709ICL\u7406\u8bba\u5206\u6790\u4e3b\u8981\u5173\u6ce8\u6e10\u8fd1\u6027\u80fd\uff0c\u7f3a\u4e4f\u6709\u9650\u6837\u672c\u7edf\u8ba1\u7406\u8bba\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acbICL\u7684\u6709\u9650\u6837\u672c\u7edf\u8ba1\u6846\u67b6\uff0c\u7406\u89e3Transformer\u5982\u4f55\u901a\u8fc7\u9884\u8bad\u7ec3\u5b66\u4e60\u5143\u7b97\u6cd5\u5e76\u5728\u6d4b\u8bd5\u65f6\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u3002", "method": "\u5728\u5143\u5b66\u4e60\u6846\u67b6\u4e0b\u5206\u6790ICL\uff0c\u5f15\u5165\u98ce\u9669\u5206\u89e3\u65b9\u6cd5\uff0c\u5c06\u603b\u98ce\u9669\u5206\u4e3a\u8d1d\u53f6\u65af\u95f4\u9699\u548c\u540e\u9a8c\u65b9\u5dee\u3002\u5bf9\u5747\u5300\u6ce8\u610f\u529bTransformer\u63a8\u5bfc\u975e\u6e10\u8fd1\u4e0a\u754c\uff0c\u5206\u6790\u9884\u8bad\u7ec3\u63d0\u793a\u6570\u91cf\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u540e\u9a8c\u65b9\u5dee\u4ec5\u7531\u771f\u5b9e\u4efb\u52a1\u96be\u5ea6\u51b3\u5b9a\uff0c\u4efb\u52a1\u6df7\u5408\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u6027\u968f\u5c11\u91cf\u4e0a\u4e0b\u6587\u793a\u4f8b\u5448\u6307\u6570\u7ea7\u5feb\u901f\u6d88\u5931\u3002Transformer\u5728\u9884\u8bad\u7ec3\u4e2d\u9009\u62e9\u6700\u4f18\u5143\u7b97\u6cd5\uff0c\u6d4b\u8bd5\u65f6\u5feb\u901f\u6536\u655b\u5230\u771f\u5b9e\u4efb\u52a1\u6700\u4f18\u7b97\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86ICL\u7684\u7edf\u4e00\u7406\u8bba\u89c6\u89d2\uff1aTransformer\u901a\u8fc7\u9884\u8bad\u7ec3\u5b66\u4e60\u9009\u62e9\u6700\u4f18\u5143\u7b97\u6cd5\uff0c\u5e76\u5728\u6d4b\u8bd5\u65f6\u4ec5\u9700\u5c11\u91cf\u4e0a\u4e0b\u6587\u793a\u4f8b\u5c31\u80fd\u5feb\u901f\u6536\u655b\u5230\u771f\u5b9e\u4efb\u52a1\u7684\u6700\u4f18\u7b97\u6cd5\u3002"}}
{"id": "2510.10202", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.10202", "abs": "https://arxiv.org/abs/2510.10202", "authors": ["Ayush Rai", "Shaoshuai Mou", "Brian D. O. Anderson"], "title": "Performance Index Shaping for Closed-loop Optimal Control", "comment": null, "summary": "The design of the performance index, also referred to as cost or reward\nshaping, is central to both optimal control and reinforcement learning, as it\ndirectly determines the behaviors, trade-offs, and objectives that the\nresulting control laws seek to achieve. A commonly used approach for this\ninference task in recent years is differentiable trajectory optimization, which\nallows gradients to be computed with respect to cost parameters by\ndifferentiating through an optimal control solver. However, this method often\nrequires repeated solving of the underlying optimal control problem at every\niteration, making the method computationally expensive. In this work, assuming\nknown dynamics, we propose a novel framework that analytically links the\nperformance index to the resulting closed-loop optimal control law, thereby\ntransforming a typically bi-level inverse problem into a tractable single-level\nformulation. Our approach is motivated by the question: given a closed-loop\ncontrol law that solves an infinite-horizon optimal control problem, how does\nthis law change when the performance index is modified with additional terms?\nThis formulation yields closed-form characterizations for broad classes of\nsystems and performance indices, which not only facilitate interpretation and\nstability analysis, but also provide insight into the robust stability and\ninput-to-state stable behavior of the resulting nonlinear closed-loop system.\nMoreover, this analytical perspective enables the generalization of our\napproach to diverse design objectives, yielding a unifying framework for\nperformance index shaping. Given specific design objectives, we propose a\nsystematic methodology to guide the shaping of the performance index and\nthereby design the resulting optimal control law.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6027\u80fd\u6307\u6807\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u6790\u65b9\u6cd5\u5c06\u6027\u80fd\u6307\u6807\u4e0e\u95ed\u73af\u6700\u4f18\u63a7\u5236\u5f8b\u8054\u7cfb\u8d77\u6765\uff0c\u5c06\u53cc\u5c42\u9006\u95ee\u9898\u8f6c\u5316\u4e3a\u5355\u5c42\u53ef\u5904\u7406\u5f62\u5f0f\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u53ef\u5fae\u5206\u8f68\u8ff9\u4f18\u5316\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002", "motivation": "\u4f20\u7edf\u53ef\u5fae\u5206\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u9700\u8981\u53cd\u590d\u6c42\u89e3\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u6027\u80fd\u6307\u6807\u8bbe\u8ba1\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u63a2\u7d22\u6027\u80fd\u6307\u6807\u53d8\u5316\u5982\u4f55\u5f71\u54cd\u95ed\u73af\u63a7\u5236\u5f8b\u3002", "method": "\u5047\u8bbe\u5df2\u77e5\u7cfb\u7edf\u52a8\u529b\u5b66\uff0c\u901a\u8fc7\u89e3\u6790\u65b9\u6cd5\u5efa\u7acb\u6027\u80fd\u6307\u6807\u4e0e\u95ed\u73af\u6700\u4f18\u63a7\u5236\u5f8b\u7684\u76f4\u63a5\u8054\u7cfb\uff0c\u5c06\u9006\u95ee\u9898\u8f6c\u5316\u4e3a\u5355\u5c42\u53ef\u5904\u7406\u5f62\u5f0f\uff0c\u5e76\u63d0\u4f9b\u95ed\u5f0f\u7279\u6027\u63cf\u8ff0\u3002", "result": "\u4e3a\u5e7f\u6cdb\u7cfb\u7edf\u7c7b\u522b\u548c\u6027\u80fd\u6307\u6807\u63d0\u4f9b\u4e86\u95ed\u5f0f\u7279\u6027\u63cf\u8ff0\uff0c\u4fbf\u4e8e\u89e3\u91ca\u548c\u7a33\u5b9a\u6027\u5206\u6790\uff0c\u5e76\u6d1e\u5bdf\u975e\u7ebf\u6027\u95ed\u73af\u7cfb\u7edf\u7684\u9c81\u68d2\u7a33\u5b9a\u6027\u548c\u8f93\u5165\u72b6\u6001\u7a33\u5b9a\u884c\u4e3a\u3002", "conclusion": "\u8be5\u89e3\u6790\u89c6\u89d2\u4f7f\u65b9\u6cd5\u80fd\u591f\u63a8\u5e7f\u5230\u591a\u6837\u5316\u8bbe\u8ba1\u76ee\u6807\uff0c\u4e3a\u6027\u80fd\u6307\u6807\u6574\u5b9a\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u7cfb\u7edf\u6027\u65b9\u6cd5\u6765\u6307\u5bfc\u6027\u80fd\u6307\u6807\u6574\u5b9a\u548c\u6700\u4f18\u63a7\u5236\u5f8b\u8bbe\u8ba1\u3002"}}
{"id": "2510.10176", "categories": ["cs.CY", "K.4.0"], "pdf": "https://arxiv.org/pdf/2510.10176", "abs": "https://arxiv.org/abs/2510.10176", "authors": ["Linda Rocco"], "title": "The Mechanical Yes-Man: Emancipatory AI Pedagogy in Higher Education", "comment": "7 pages, 2 figures. To be published in Concreta journal n. 26, 2025", "summary": "The proliferation of Large Language Models in higher education presents a\nfundamental challenge to traditional pedagogical frameworks. Drawing on Jacques\nRanci\\`ere's theory of intellectual emancipation, this paper examines how\ngenerative AI risks becoming a \"mechanical yes-man\" that reinforces passivity\nrather than fostering intellectual autonomy. Generative AI's statistical logic\nand lack of causal reasoning, combined with frictionless information access,\nthreatens to hollow out cognitive processes essential for genuine learning.\nThis creates a critical paradox: while generative AI systems are trained for\ncomplex reasoning, students increasingly use them to bypass the intellectual\nwork that builds such capabilities. The paper critiques both techno-optimistic\nand restrictive approaches to generative AI in education, proposing instead an\nemancipatory pedagogy grounded in verification, mastery, and co-inquiry. This\nframework positions generative AI as material for intellectual work rather than\na substitute for it, emphasising the cultivation of metacognitive awareness and\ncritical interrogation of AI outputs. It requires educators to engage directly\nwith these tools to guide students toward critical AI literacy, transforming\npedagogical authority from explication to critical interloping that models\nintellectual courage and collaborative inquiry.", "AI": {"tldr": "\u672c\u6587\u57fa\u4e8eRanci\u00e8re\u7684\u667a\u529b\u89e3\u653e\u7406\u8bba\uff0c\u6279\u5224\u751f\u6210\u5f0fAI\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\u53ef\u80fd\u6210\u4e3a\u5f3a\u5316\u5b66\u751f\u88ab\u52a8\u6027\u7684\"\u673a\u68b0\u5e94\u58f0\u866b\"\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u9a8c\u8bc1\u3001\u638c\u63e1\u548c\u5171\u540c\u63a2\u7a76\u7684\u89e3\u653e\u6027\u6559\u5b66\u6846\u67b6\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\u7684\u666e\u53ca\u5bf9\u4f20\u7edf\u6559\u5b66\u6846\u67b6\u6784\u6210\u6839\u672c\u6311\u6218\uff0c\u5176\u7edf\u8ba1\u903b\u8f91\u548c\u65e0\u6469\u64e6\u4fe1\u606f\u83b7\u53d6\u53ef\u80fd\u524a\u5f31\u5b66\u751f\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u5b66\u751f\u7ed5\u8fc7\u57f9\u517b\u63a8\u7406\u80fd\u529b\u6240\u9700\u7684\u667a\u529b\u5de5\u4f5c\u3002", "method": "\u91c7\u7528Jacques Ranci\u00e8re\u7684\u667a\u529b\u89e3\u653e\u7406\u8bba\u6846\u67b6\uff0c\u6279\u5224\u5206\u6790\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u4e2d\u7684\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u89e3\u653e\u6027\u6559\u5b66\u6cd5\uff0c\u5f3a\u8c03\u9a8c\u8bc1\u3001\u638c\u63e1\u548c\u5171\u540c\u63a2\u7a76\u3002", "result": "\u63ed\u793a\u4e86\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u4e2d\u5b58\u5728\u7684\u6096\u8bba\uff1a\u867d\u7136AI\u7cfb\u7edf\u672c\u8eab\u8bad\u7ec3\u7528\u4e8e\u590d\u6742\u63a8\u7406\uff0c\u4f46\u5b66\u751f\u5374\u7528\u5b83\u6765\u89c4\u907f\u57f9\u517b\u8fd9\u4e9b\u80fd\u529b\u7684\u667a\u529b\u5de5\u4f5c\u3002", "conclusion": "\u5e94\u5efa\u7acb\u89e3\u653e\u6027\u6559\u5b66\u6846\u67b6\uff0c\u5c06\u751f\u6210\u5f0fAI\u5b9a\u4f4d\u4e3a\u667a\u529b\u5de5\u4f5c\u7684\u6750\u6599\u800c\u975e\u66ff\u4ee3\u54c1\uff0c\u5f3a\u8c03\u5143\u8ba4\u77e5\u610f\u8bc6\u548c\u6279\u5224\u6027\u5ba1\u95eeAI\u8f93\u51fa\uff0c\u8981\u6c42\u6559\u80b2\u8005\u76f4\u63a5\u53c2\u4e0e\u4ee5\u57f9\u517b\u5b66\u751f\u6279\u5224\u6027AI\u7d20\u517b\u3002"}}
{"id": "2510.09722", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09722", "abs": "https://arxiv.org/abs/2510.09722", "authors": ["Fanwei Zhu", "Jinke Yu", "Zulong Chen", "Ying Zhou", "Junhao Ji", "Zhibo Yang", "Yuxue Zhang", "Haoyuan Hu", "Zhenghao Liu"], "title": "Layout-Aware Parsing Meets Efficient LLMs: A Unified, Scalable Framework for Resume Information Extraction and Evaluation", "comment": null, "summary": "Automated resume information extraction is critical for scaling talent\nacquisition, yet its real-world deployment faces three major challenges: the\nextreme heterogeneity of resume layouts and content, the high cost and latency\nof large language models (LLMs), and the lack of standardized datasets and\nevaluation tools. In this work, we present a layout-aware and\nefficiency-optimized framework for automated extraction and evaluation that\naddresses all three challenges. Our system combines a fine-tuned layout parser\nto normalize diverse document formats, an inference-efficient LLM extractor\nbased on parallel prompting and instruction tuning, and a robust two-stage\nautomated evaluation framework supported by new benchmark datasets. Extensive\nexperiments show that our framework significantly outperforms strong baselines\nin both accuracy and efficiency. In particular, we demonstrate that a\nfine-tuned compact 0.6B LLM achieves top-tier accuracy while significantly\nreducing inference latency and computational cost. The system is fully deployed\nin Alibaba's intelligent HR platform, supporting real-time applications across\nits business units.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9762\u5411\u7b80\u5386\u4fe1\u606f\u63d0\u53d6\u7684\u5e03\u5c40\u611f\u77e5\u548c\u6548\u7387\u4f18\u5316\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5e03\u5c40\u5f02\u6784\u6027\u3001LLM\u9ad8\u6210\u672c\u9ad8\u5ef6\u8fdf\u3001\u7f3a\u4e4f\u6807\u51c6\u5316\u6570\u636e\u96c6\u4e09\u5927\u6311\u6218\u3002", "motivation": "\u81ea\u52a8\u5316\u7b80\u5386\u4fe1\u606f\u63d0\u53d6\u5bf9\u89c4\u6a21\u5316\u4eba\u624d\u62db\u8058\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u7b80\u5386\u5e03\u5c40\u548c\u5185\u5bb9\u7684\u6781\u7aef\u5f02\u6784\u6027\u3001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6210\u672c\u548c\u5ef6\u8fdf\u3001\u7f3a\u4e4f\u6807\u51c6\u5316\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u7ed3\u5408\u4e86\u5fae\u8c03\u7684\u5e03\u5c40\u89e3\u6790\u5668\u6765\u89c4\u8303\u5316\u4e0d\u540c\u6587\u6863\u683c\u5f0f\u3001\u57fa\u4e8e\u5e76\u884c\u63d0\u793a\u548c\u6307\u4ee4\u8c03\u4f18\u7684\u9ad8\u6548LLM\u63d0\u53d6\u5668\u3001\u4ee5\u53ca\u7531\u65b0\u57fa\u51c6\u6570\u636e\u96c6\u652f\u6301\u7684\u4e24\u9636\u6bb5\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u3002\u7279\u522b\u662f\uff0c\u5fae\u8c03\u76840.6B\u7d27\u51d1LLM\u5b9e\u73b0\u4e86\u9876\u7ea7\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u548c\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5df2\u5728\u963f\u91cc\u5df4\u5df4\u667a\u80fdHR\u5e73\u53f0\u5168\u9762\u90e8\u7f72\uff0c\u652f\u6301\u5176\u4e1a\u52a1\u90e8\u95e8\u7684\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2510.09664", "categories": ["cs.LG", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.09664", "abs": "https://arxiv.org/abs/2510.09664", "authors": ["Changchang Sun", "Vickie Chen", "Yan Yan"], "title": "Semantic-Cohesive Knowledge Distillation for Deep Cross-modal Hashing", "comment": null, "summary": "Recently, deep supervised cross-modal hashing methods have achieve compelling\nsuccess by learning semantic information in a self-supervised way. However,\nthey still suffer from the key limitation that the multi-label semantic\nextraction process fail to explicitly interact with raw multimodal data, making\nthe learned representation-level semantic information not compatible with the\nheterogeneous multimodal data and hindering the performance of bridging\nmodality gap. To address this limitation, in this paper, we propose a novel\nsemantic cohesive knowledge distillation scheme for deep cross-modal hashing,\ndubbed as SODA. Specifically, the multi-label information is introduced as a\nnew textual modality and reformulated as a set of ground-truth label prompt,\ndepicting the semantics presented in the image like the text modality. Then, a\ncross-modal teacher network is devised to effectively distill cross-modal\nsemantic characteristics between image and label modalities and thus learn a\nwell-mapped Hamming space for image modality. In a sense, such Hamming space\ncan be regarded as a kind of prior knowledge to guide the learning of\ncross-modal student network and comprehensively preserve the semantic\nsimilarities between image and text modality. Extensive experiments on two\nbenchmark datasets demonstrate the superiority of our model over the\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSODA\u7684\u8bed\u4e49\u51dd\u805a\u77e5\u8bc6\u84b8\u998f\u65b9\u6848\uff0c\u901a\u8fc7\u5c06\u591a\u6807\u7b7e\u4fe1\u606f\u4f5c\u4e3a\u65b0\u7684\u6587\u672c\u6a21\u6001\uff0c\u6784\u5efa\u8de8\u6a21\u6001\u6559\u5e08\u7f51\u7edc\u6765\u5b66\u4e60\u56fe\u50cf\u6a21\u6001\u7684\u6c49\u660e\u7a7a\u95f4\uff0c\u6307\u5bfc\u5b66\u751f\u7f51\u7edc\u66f4\u597d\u5730\u4fdd\u7559\u56fe\u50cf\u548c\u6587\u672c\u6a21\u6001\u95f4\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u8de8\u6a21\u6001\u54c8\u5e0c\u65b9\u6cd5\u867d\u7136\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u8bed\u4e49\u4fe1\u606f\uff0c\u4f46\u591a\u6807\u7b7e\u8bed\u4e49\u63d0\u53d6\u8fc7\u7a0b\u672a\u80fd\u4e0e\u539f\u59cb\u591a\u6a21\u6001\u6570\u636e\u663e\u5f0f\u4ea4\u4e92\uff0c\u5bfc\u81f4\u5b66\u4e60\u5230\u7684\u8868\u793a\u7ea7\u8bed\u4e49\u4fe1\u606f\u4e0e\u5f02\u6784\u591a\u6a21\u6001\u6570\u636e\u4e0d\u517c\u5bb9\uff0c\u963b\u788d\u4e86\u6a21\u6001\u95f4\u9699\u7684\u5f25\u5408\u3002", "method": "\u5c06\u591a\u6807\u7b7e\u4fe1\u606f\u4f5c\u4e3a\u65b0\u7684\u6587\u672c\u6a21\u6001\uff0c\u6784\u5efa\u6807\u7b7e\u63d0\u793a\uff1b\u8bbe\u8ba1\u8de8\u6a21\u6001\u6559\u5e08\u7f51\u7edc\u5728\u56fe\u50cf\u548c\u6807\u7b7e\u6a21\u6001\u95f4\u84b8\u998f\u8bed\u4e49\u7279\u5f81\uff0c\u5b66\u4e60\u56fe\u50cf\u6a21\u6001\u7684\u6c49\u660e\u7a7a\u95f4\uff1b\u7528\u8be5\u6c49\u660e\u7a7a\u95f4\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\u6307\u5bfc\u5b66\u751f\u7f51\u7edc\u5b66\u4e60\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "SODA\u65b9\u6cd5\u901a\u8fc7\u8bed\u4e49\u51dd\u805a\u77e5\u8bc6\u84b8\u998f\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u54c8\u5e0c\u4e2d\u8bed\u4e49\u4fe1\u606f\u4e0e\u591a\u6a21\u6001\u6570\u636e\u4e0d\u517c\u5bb9\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2510.10002", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10002", "abs": "https://arxiv.org/abs/2510.10002", "authors": ["Pratik S. Sachdeva", "Tom van Nuenen"], "title": "Deliberative Dynamics and Value Alignment in LLM Debates", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in sensitive\neveryday contexts - offering personal advice, mental health support, and moral\nguidance - understanding their elicited values in navigating complex moral\nreasoning is essential. Most evaluations study this sociotechnical alignment\nthrough single-turn prompts, but it is unclear if these findings extend to\nmulti-turn settings where values emerge through dialogue, revision, and\nconsensus. We address this gap using LLM debate to examine deliberative\ndynamics and value alignment in multi-turn settings by prompting subsets of\nthree models (GPT-4.1, Claude 3.7 Sonnet, and Gemini 2.0 Flash) to collectively\nassign blame in 1,000 everyday dilemmas from Reddit's \"Am I the Asshole\"\ncommunity. We use both synchronous (parallel responses) and round-robin\n(sequential responses) formats to test order effects and verdict revision. Our\nfindings show striking behavioral differences. In the synchronous setting, GPT\nshowed strong inertia (0.6-3.1% revision rates) while Claude and Gemini were\nfar more flexible (28-41%). Value patterns also diverged: GPT emphasized\npersonal autonomy and direct communication, while Claude and Gemini prioritized\nempathetic dialogue. Certain values proved especially effective at driving\nverdict changes. We further find that deliberation format had a strong impact\non model behavior: GPT and Gemini stood out as highly conforming relative to\nClaude, with their verdict behavior strongly shaped by order effects. These\nresults show how deliberation format and model-specific behaviors shape moral\nreasoning in multi-turn interactions, underscoring that sociotechnical\nalignment depends on how systems structure dialogue as much as on their\noutputs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u591a\u8f6eLLM\u8fa9\u8bba\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9053\u5fb7\u63a8\u7406\u4e2d\u7684\u4ef7\u503c\u5bf9\u9f50\uff0c\u53d1\u73b0\u5728\u540c\u6b65\u548c\u8f6e\u8be2\u8fa9\u8bba\u683c\u5f0f\u4e0b\uff0cGPT\u3001Claude\u548cGemini\u5728\u9053\u5fb7\u5224\u65ad\u4fee\u8ba2\u3001\u4ef7\u503c\u4f18\u5148\u6027\u548c\u4ece\u4f17\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u968f\u7740LLM\u5728\u654f\u611f\u65e5\u5e38\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u9700\u8981\u7406\u89e3\u5176\u5728\u590d\u6742\u9053\u5fb7\u63a8\u7406\u4e2d\u7684\u4ef7\u503c\u5bf9\u9f50\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5355\u8f6e\u63d0\u793a\uff0c\u4f46\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u4ef7\u503c\u5f62\u6210\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a\u6a21\u578b\uff08GPT-4.1\u3001Claude 3.7 Sonnet\u3001Gemini 2.0 Flash\uff09\u5728Reddit\u7684\"Am I the Asshole\"\u793e\u533a\u76841000\u4e2a\u65e5\u5e38\u56f0\u5883\u4e2d\u8fdb\u884c\u96c6\u4f53\u5f52\u8d23\u8fa9\u8bba\uff0c\u91c7\u7528\u540c\u6b65\u548c\u8f6e\u8be2\u4e24\u79cd\u683c\u5f0f\u6d4b\u8bd5\u987a\u5e8f\u6548\u5e94\u548c\u5224\u65ad\u4fee\u8ba2\u3002", "result": "GPT\u8868\u73b0\u51fa\u5f3a\u60ef\u6027\uff08\u4fee\u8ba2\u73870.6-3.1%\uff09\uff0cClaude\u548cGemini\u66f4\u7075\u6d3b\uff0828-41%\uff09\u3002GPT\u5f3a\u8c03\u4e2a\u4eba\u81ea\u4e3b\u6027\u548c\u76f4\u63a5\u6c9f\u901a\uff0cClaude\u548cGemini\u4f18\u5148\u8003\u8651\u5171\u60c5\u5bf9\u8bdd\u3002\u8fa9\u8bba\u683c\u5f0f\u5bf9\u6a21\u578b\u884c\u4e3a\u6709\u5f3a\u70c8\u5f71\u54cd\uff0cGPT\u548cGemini\u8868\u73b0\u51fa\u9ad8\u4ece\u4f17\u6027\u3002", "conclusion": "\u591a\u8f6e\u4e92\u52a8\u4e2d\u7684\u9053\u5fb7\u63a8\u7406\u53d7\u5230\u8fa9\u8bba\u683c\u5f0f\u548c\u6a21\u578b\u7279\u5b9a\u884c\u4e3a\u7684\u5f3a\u70c8\u5f71\u54cd\uff0c\u8868\u660e\u793e\u4f1a\u6280\u672f\u5bf9\u9f50\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u7cfb\u7edf\u8f93\u51fa\uff0c\u8fd8\u53d6\u51b3\u4e8e\u5bf9\u8bdd\u7ed3\u6784\u65b9\u5f0f\u3002"}}
{"id": "2510.10988", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10988", "abs": "https://arxiv.org/abs/2510.10988", "authors": ["Yannis Montreuil", "Letian Yu", "Axel Carlier", "Lai Xing Ng", "Wei Tsang Ooi"], "title": "Adversarial Robustness in One-Stage Learning-to-Defer", "comment": null, "summary": "Learning-to-Defer (L2D) enables hybrid decision-making by routing inputs\neither to a predictor or to external experts. While promising, L2D is highly\nvulnerable to adversarial perturbations, which can not only flip predictions\nbut also manipulate deferral decisions. Prior robustness analyses focus solely\non two-stage settings, leaving open the end-to-end (one-stage) case where\npredictor and allocation are trained jointly. We introduce the first framework\nfor adversarial robustness in one-stage L2D, covering both classification and\nregression. Our approach formalizes attacks, proposes cost-sensitive\nadversarial surrogate losses, and establishes theoretical guarantees including\n$\\mathcal{H}$, $(\\mathcal{R }, \\mathcal{F})$, and Bayes consistency.\nExperiments on benchmark datasets confirm that our methods improve robustness\nagainst untargeted and targeted attacks while preserving clean performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7aef\u5230\u7aef\u5b66\u4e60\u5ef6\u8fdf\uff08L2D\uff09\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u6846\u67b6\uff0c\u6db5\u76d6\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\uff0c\u901a\u8fc7\u5b9a\u4e49\u653b\u51fb\u3001\u63d0\u51fa\u6210\u672c\u654f\u611f\u5bf9\u6297\u635f\u5931\u51fd\u6570\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u5b66\u4e60\u5ef6\u8fdf\uff08L2D\uff09\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u653b\u51fb\uff0c\u73b0\u6709\u9c81\u68d2\u6027\u5206\u6790\u4ec5\u9650\u4e8e\u4e24\u9636\u6bb5\u8bbe\u7f6e\uff0c\u800c\u7aef\u5230\u7aef\uff08\u5355\u9636\u6bb5\uff09\u60c5\u51b5\u5c1a\u672a\u88ab\u7814\u7a76\u3002", "method": "\u6784\u5efa\u4e86\u7aef\u5230\u7aefL2D\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u6846\u67b6\uff0c\u5305\u62ec\u5f62\u5f0f\u5316\u653b\u51fb\u5b9a\u4e49\u3001\u63d0\u51fa\u6210\u672c\u654f\u611f\u7684\u5bf9\u6297\u4ee3\u7406\u635f\u5931\u51fd\u6570\uff0c\u5e76\u5efa\u7acb\u4e86\u7406\u8bba\u4e00\u81f4\u6027\u4fdd\u8bc1\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6e05\u6d01\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u5bf9\u975e\u76ee\u6807\u653b\u51fb\u548c\u76ee\u6807\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7aef\u5230\u7aef\u5b66\u4e60\u5ef6\u8fdf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u5206\u6790\u6846\u67b6\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2510.10215", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.10215", "abs": "https://arxiv.org/abs/2510.10215", "authors": ["Pranav Gupta", "Ravi Banavar", "Anastasia Bizyaeva"], "title": "Bounds of Validity for Bifurcations of Equilibria in a Class of Networked Dynamical Systems", "comment": "This manuscript has been submitted to the 2026 American Control\n  Conference taking place in New Orleans, Louisiana, in May 2026", "summary": "Local bifurcation analysis plays a central role in understanding qualitative\ntransitions in networked nonlinear dynamical systems, including dynamic neural\nnetwork and opinion dynamics models. In this article we establish explicit\nbounds of validity for the classification of bifurcation diagrams in two\nclasses of continuous-time networked dynamical systems, analogous in structure\nto the Hopfield and the Firing Rate dynamic neural network models. Our approach\nleverages recent advances in computing the bounds for the validity of\nLyapunov-Schmidt reduction, a reduction method widely employed in nonlinear\nsystems analysis. Using these bounds we rigorously characterize neighborhoods\naround bifurcation points where predictions from reduced-order models remain\nreliable. We further demonstrate how these bounds can be applied to an\nillustrative family of nonlinear opinion dynamics on k-regular graphs, which\nemerges as a special case of the general framework. These results provide new\nanalytical tools for quantifying the robustness of bifurcation phenomena in\ndynamics over networked systems and highlight the interplay between network\nstructure and nonlinear dynamical behavior.", "AI": {"tldr": "\u672c\u6587\u5efa\u7acb\u4e86\u8fde\u7eed\u65f6\u95f4\u7f51\u7edc\u52a8\u529b\u7cfb\u7edf\u4e2d\u5206\u5c94\u56fe\u5206\u7c7b\u7684\u6709\u6548\u6027\u663e\u5f0f\u8fb9\u754c\uff0c\u91cd\u70b9\u5173\u6ce8\u7c7b\u4f3cHopfield\u548cFiring Rate\u6a21\u578b\u7684\u7f51\u7edc\u7ed3\u6784\uff0c\u5e76\u5e94\u7528\u4e8ek-\u6b63\u5219\u56fe\u4e0a\u7684\u975e\u7ebf\u6027\u610f\u89c1\u52a8\u529b\u5b66\u3002", "motivation": "\u7406\u89e3\u7f51\u7edc\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u4e2d\u7684\u5b9a\u6027\u8f6c\u53d8\uff0c\u5982\u52a8\u6001\u795e\u7ecf\u7f51\u7edc\u548c\u610f\u89c1\u52a8\u529b\u5b66\u6a21\u578b\u4e2d\u7684\u5206\u5c94\u73b0\u8c61\uff0c\u9700\u8981\u6709\u6548\u7684\u5c40\u90e8\u5206\u5c94\u5206\u6790\u5de5\u5177\u3002", "method": "\u5229\u7528Lyapunov-Schmidt\u7f29\u51cf\u6709\u6548\u6027\u8fb9\u754c\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5efa\u7acb\u5206\u5c94\u70b9\u9644\u8fd1\u9884\u6d4b\u53ef\u9760\u6027\u7684\u4e25\u683c\u90bb\u57df\u7279\u5f81\u3002", "result": "\u5f00\u53d1\u4e86\u91cf\u5316\u7f51\u7edc\u7cfb\u7edf\u4e2d\u5206\u5c94\u73b0\u8c61\u9c81\u68d2\u6027\u7684\u65b0\u5206\u6790\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u7f51\u7edc\u7ed3\u6784\u4e0e\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u884c\u4e3a\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "conclusion": "\u4e3a\u7f51\u7edc\u52a8\u529b\u7cfb\u7edf\u4e2d\u5206\u5c94\u56fe\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6709\u6548\u6027\u8fb9\u754c\uff0c\u589e\u5f3a\u4e86\u5206\u5c94\u9884\u6d4b\u5728\u590d\u6742\u7f51\u7edc\u7cfb\u7edf\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2510.10315", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.10315", "abs": "https://arxiv.org/abs/2510.10315", "authors": ["Nicolas Steinacker-Olsztyn", "Devashish Gosain", "Ha Dao"], "title": "Is Misinformation More Open? A Study of robots.txt Gatekeeping on the Web", "comment": "10 pages, 11 figures", "summary": "Large Language Models (LLMs) are increasingly relying on web crawling to stay\nup to date and accurately answer user queries. These crawlers are expected to\nhonor robots.txt files, which govern automated access. In this study, for the\nfirst time, we investigate whether reputable news websites and misinformation\nsites differ in how they configure these files, particularly in relation to AI\ncrawlers. Analyzing a curated dataset, we find a stark contrast: 60.0% of\nreputable sites disallow at least one AI crawler, compared to just 9.1% of\nmisinformation sites in their robots.txt files. Reputable sites forbid an\naverage of 15.5 AI user agents, while misinformation sites prohibit fewer than\none. We then measure active blocking behavior, where websites refuse to return\ncontent when HTTP requests include AI crawler user agents, and reveal that both\ncategories of websites utilize it. Notably, the behavior of reputable news\nwebsites in this regard aligns more closely with their declared robots.txt\ndirective than that of misinformation websites. Finally, our longitudinal\nanalysis reveals that this gap has widened over time, with AI-blocking by\nreputable sites rising from 23% in September 2023 to nearly 60% by May 2025.\nOur findings highlight a growing asymmetry in content accessibility that may\nshape the training data available to LLMs, raising essential questions for web\ntransparency, data ethics, and the future of AI training practices.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u4fe1\u8a89\u826f\u597d\u7684\u65b0\u95fb\u7f51\u7ad9\u4e0e\u865a\u5047\u4fe1\u606f\u7f51\u7ad9\u5728AI\u722c\u866b\u8bbf\u95ee\u63a7\u5236\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1a60%\u4fe1\u8a89\u7f51\u7ad9\u7981\u6b62AI\u722c\u866b\uff0c\u800c\u865a\u5047\u4fe1\u606f\u7f51\u7ad9\u4ec59.1%\u7981\u6b62\uff0c\u8fd9\u79cd\u5dee\u8ddd\u8fd8\u5728\u6269\u5927\u3002", "motivation": "\u8c03\u67e5\u4fe1\u8a89\u65b0\u95fb\u7f51\u7ad9\u548c\u865a\u5047\u4fe1\u606f\u7f51\u7ad9\u5728robots.txt\u6587\u4ef6\u4e2d\u5bf9AI\u722c\u866b\u7684\u914d\u7f6e\u5dee\u5f02\uff0c\u4ee5\u53ca\u8fd9\u79cd\u5dee\u5f02\u5982\u4f55\u5f71\u54cdLLMs\u7684\u8bad\u7ec3\u6570\u636e\u83b7\u53d6\u3002", "method": "\u5206\u6790\u7cbe\u9009\u6570\u636e\u96c6\u4e2d\u7684robots.txt\u6587\u4ef6\u914d\u7f6e\uff0c\u6d4b\u91cf\u7f51\u7ad9\u5bf9AI\u722c\u866b\u7684\u5b9e\u9645\u963b\u6b62\u884c\u4e3a\uff0c\u5e76\u8fdb\u884c\u7eb5\u5411\u65f6\u95f4\u5206\u6790\u3002", "result": "\u4fe1\u8a89\u7f51\u7ad9\u5e73\u5747\u7981\u6b6215.5\u4e2aAI\u7528\u6237\u4ee3\u7406\uff0c\u865a\u5047\u4fe1\u606f\u7f51\u7ad9\u7981\u6b62\u5c11\u4e8e1\u4e2a\uff1bAI\u963b\u6b62\u7387\u4ece2023\u5e749\u6708\u768423%\u4e0a\u5347\u52302025\u5e745\u6708\u7684\u8fd160%\u3002", "conclusion": "\u5185\u5bb9\u53ef\u8bbf\u95ee\u6027\u7684\u4e0d\u5bf9\u79f0\u6027\u53ef\u80fd\u5f71\u54cdLLMs\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5f15\u53d1\u5173\u4e8e\u7f51\u7edc\u900f\u660e\u5ea6\u3001\u6570\u636e\u4f26\u7406\u548cAI\u8bad\u7ec3\u5b9e\u8df5\u7684\u601d\u8003\u3002"}}
{"id": "2510.10399", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.10399", "abs": "https://arxiv.org/abs/2510.10399", "authors": ["Harshal D. Kaushik", "Roshni Anna Jacob", "Souma Chowdhury", "Jie Zhang"], "title": "Grid Restoration Under Uncertainty Considering Coupled Transportation-Power Networks", "comment": null, "summary": "A stochastic mixed-integer programming model is developed to address the\npower distribution system repair and restoration following failures caused by\nextreme events such as natural disasters. This model addresses the complex\nchallenge of efficiently assigning and dispatching repair crews to minimize\nboth downtime and the extent of outages, following the crew sequence\nconstraints. By incorporating a realistic transportation network, our model\naccounts for uncertainties in repair times, repair demand at damaged nodes, and\npotential transportation network failures. This ensures uncertainty awareness\nand mitigation, providing coordinated schedules for the repair crews, assigned\nbased on the severity of failures at each node. Our approach prioritizes\ncomponents in the distribution network based on their potential for power\nrestoration, while also considering the constraints imposed by the actual\ntransportation network. A case study using an 8500-bus system, coupled with a\nreal-world transportation network at the Dallas-Fort Worth area, demonstrating\nthe effectiveness of this approach, is discussed. The proposed methodology\nemphasizes rapid power restoration and efficient repair crew routing over an\nintegrated transportation and power network, demonstrating results that match\nor outperform conventional metaheuristic solvers for classical vehicle routing\nproblems in a deterministic instance.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u968f\u673a\u6df7\u5408\u6574\u6570\u89c4\u5212\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u6781\u7aef\u4e8b\u4ef6\u540e\u4fee\u590d\u548c\u6062\u590d\u914d\u7535\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u7ef4\u4fee\u4eba\u5458\u8c03\u5ea6\u6765\u6700\u5c0f\u5316\u505c\u7535\u65f6\u95f4\u548c\u8303\u56f4\u3002", "motivation": "\u89e3\u51b3\u81ea\u7136\u707e\u5bb3\u7b49\u6781\u7aef\u4e8b\u4ef6\u540e\u914d\u7535\u7cfb\u7edf\u4fee\u590d\u7684\u590d\u6742\u6311\u6218\uff0c\u8003\u8651\u7ef4\u4fee\u65f6\u95f4\u3001\u9700\u6c42\u548c\u4ea4\u901a\u7f51\u7edc\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u5feb\u901f\u7535\u529b\u6062\u590d\u3002", "method": "\u91c7\u7528\u968f\u673a\u6df7\u5408\u6574\u6570\u89c4\u5212\u65b9\u6cd5\uff0c\u7ed3\u5408\u73b0\u5b9e\u4ea4\u901a\u7f51\u7edc\uff0c\u8003\u8651\u7ef4\u4fee\u65f6\u95f4\u3001\u8282\u70b9\u7ef4\u4fee\u9700\u6c42\u548c\u4ea4\u901a\u7f51\u7edc\u6545\u969c\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u57fa\u4e8e\u6545\u969c\u4e25\u91cd\u7a0b\u5ea6\u534f\u8c03\u7ef4\u4fee\u4eba\u5458\u8c03\u5ea6\u3002", "result": "\u57288500\u8282\u70b9\u7cfb\u7edf\u548c\u8fbe\u62c9\u65af-\u6c83\u65af\u5821\u5730\u533a\u771f\u5b9e\u4ea4\u901a\u7f51\u7edc\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u786e\u5b9a\u6027\u5b9e\u4f8b\u4e2d\u8fbe\u5230\u6216\u4f18\u4e8e\u4f20\u7edf\u5143\u542f\u53d1\u5f0f\u6c42\u89e3\u5668\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u96c6\u6210\u4ea4\u901a\u548c\u7535\u529b\u7f51\u7edc\uff0c\u5f3a\u8c03\u5feb\u901f\u7535\u529b\u6062\u590d\u548c\u9ad8\u6548\u7ef4\u4fee\u4eba\u5458\u8def\u7531\uff0c\u4e3a\u6781\u7aef\u4e8b\u4ef6\u540e\u7684\u914d\u7535\u7cfb\u7edf\u4fee\u590d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.09733", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09733", "abs": "https://arxiv.org/abs/2510.09733", "authors": ["Yubo Sun", "Chunyi Peng", "Yukun Yan", "Shi Yu", "Zhenghao Liu", "Chi Chen", "Zhiyuan Liu", "Maosong Sun"], "title": "VisRAG 2.0: Evidence-Guided Multi-Image Reasoning in Visual Retrieval-Augmented Generation", "comment": null, "summary": "Visual retrieval-augmented generation (VRAG) augments vision-language models\n(VLMs) with external visual knowledge to ground reasoning and reduce\nhallucinations. Yet current VRAG systems often fail to reliably perceive and\nintegrate evidence across multiple images, leading to weak grounding and\nerroneous conclusions. In this paper, we propose EVisRAG, an end-to-end\nframework that learns to reason with evidence-guided multi-image to address\nthis issue. The model first observes retrieved images and records per-image\nevidence, then derives the final answer from the aggregated evidence. To train\nEVisRAG effectively, we introduce Reward-Scoped Group Relative Policy\nOptimization (RS-GRPO), which binds fine-grained rewards to scope-specific\ntokens to jointly optimize visual perception and reasoning abilities of VLMs.\nExperimental results on multiple visual question answering benchmarks\ndemonstrate that EVisRAG delivers substantial end-to-end gains over backbone\nVLM with 27\\% improvements on average. Further analysis shows that, powered by\nRS-GRPO, EVisRAG improves answer accuracy by precisely perceiving and\nlocalizing question-relevant evidence across multiple images and deriving the\nfinal answer from that evidence, much like a real detective.", "AI": {"tldr": "EVisRAG\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u89c6\u89c9\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8bc1\u636e\u5f15\u5bfc\u7684\u591a\u56fe\u50cf\u63a8\u7406\u6765\u89e3\u51b3\u5f53\u524dVRAG\u7cfb\u7edf\u5728\u591a\u56fe\u50cf\u611f\u77e5\u548c\u8bc1\u636e\u6574\u5408\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u5728\u591a\u56fe\u50cf\u611f\u77e5\u548c\u8bc1\u636e\u6574\u5408\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u63a8\u7406\u57fa\u7840\u8584\u5f31\u548c\u9519\u8bef\u7ed3\u8bba\u3002", "method": "\u63d0\u51faEVisRAG\u6846\u67b6\uff0c\u9996\u5148\u89c2\u5bdf\u68c0\u7d22\u5230\u7684\u56fe\u50cf\u5e76\u8bb0\u5f55\u6bcf\u5f20\u56fe\u50cf\u7684\u8bc1\u636e\uff0c\u7136\u540e\u4ece\u805a\u5408\u8bc1\u636e\u4e2d\u5f97\u51fa\u6700\u7ec8\u7b54\u6848\u3002\u4f7f\u7528Reward-Scoped Group Relative Policy Optimization (RS-GRPO)\u65b9\u6cd5\u8054\u5408\u4f18\u5316\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEVisRAG\u76f8\u6bd4\u9aa8\u5e72VLM\u5e73\u5747\u63d0\u5347\u4e8627%\u7684\u6027\u80fd\uff0c\u80fd\u591f\u7cbe\u786e\u611f\u77e5\u548c\u5b9a\u4f4d\u95ee\u9898\u76f8\u5173\u8bc1\u636e\uff0c\u5e76\u4ece\u8bc1\u636e\u4e2d\u63a8\u5bfc\u6700\u7ec8\u7b54\u6848\u3002", "conclusion": "EVisRAG\u901a\u8fc7RS-GRPO\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u56fe\u50cf\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u80fd\u591f\u50cf\u771f\u5b9e\u4fa6\u63a2\u4e00\u6837\u51c6\u786e\u611f\u77e5\u548c\u6574\u5408\u8bc1\u636e\u3002"}}
{"id": "2510.09665", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09665", "abs": "https://arxiv.org/abs/2510.09665", "authors": ["Yihua Cheng", "Yuhan Liu", "Jiayi Yao", "Yuwei An", "Xiaokun Chen", "Shaoting Feng", "Yuyang Huang", "Samuel Shen", "Kuntai Du", "Junchen Jiang"], "title": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference", "comment": null, "summary": "Today's LLM inference systems treat individual engines and queries\nindependently for simplicity, but this causes significant resource\ninefficiencies. While there are proposals to avoid redundant computation by\nreusing KV caches across queries and to increase GPU utilization by\ndisaggregating a single query to different engines, their promises cannot be\nrealized without efficiently offloading and communicating KV cache across LLM\ninference engines and queries.\n  We present LMCache, the first and so far the most efficient open-source KV\ncaching solution, which extracts and stores KV caches generated by modern LLM\nengines (vLLM and SGLang) and shares the KV caches across engines and queries.\nLMCache exposes KV caches in the LLM engine interface, effectively transforming\nLLM engines from individual token processors to a collection of engines with KV\ncache as the storage and communication medium. In particular, it supports both\ncache offloading (prefix reuse across queries) and prefill-decode\ndisaggregation (cross-engine cache transfer). LMCache's high performance and\nwide adoption stem from the following contributions: highly optimized KV cache\ndata movement with performance optimizations including batched data movement\noperations, compute and I/O pipelining; a modular KV cache connector component,\ndecoupling LMCache from the rapid evolution of inference engines; a first-class\ncontrol API, such as pinning, lookup, cleanup, movement, and compression, for\nflexible cache orchestration across GPU, CPU, storage, and network layers.\nEvaluation shows that combining LMCache with vLLM achieves up to 15x\nimprovement in throughput across diverse workloads. With a growing community,\nLMCache has seen dramatic growth in adoption by enterprise inference systems,\nwhich provides valuable lessons for future KV caching solutions. The source\ncode of LMCache is at: https://github.com/LMCache/LMCache.", "AI": {"tldr": "LMCache\u662f\u9996\u4e2a\u9ad8\u6548\u7684\u5f00\u6e90KV\u7f13\u5b58\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5728LLM\u63a8\u7406\u5f15\u64ce\u95f4\u5171\u4eabKV\u7f13\u5b58\u6765\u63d0\u5347\u8d44\u6e90\u5229\u7528\u7387\uff0c\u652f\u6301\u7f13\u5b58\u5378\u8f7d\u548c\u9884\u586b\u5145-\u89e3\u7801\u89e3\u8026\uff0c\u5b9e\u73b0\u9ad8\u8fbe15\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u7cfb\u7edf\u5c06\u5f15\u64ce\u548c\u67e5\u8be2\u72ec\u7acb\u5904\u7406\u5bfc\u81f4\u8d44\u6e90\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u89e3\u51b3\u8de8\u5f15\u64ce\u548c\u67e5\u8be2\u7684KV\u7f13\u5b58\u9ad8\u6548\u5378\u8f7d\u548c\u901a\u4fe1\u95ee\u9898\u3002", "method": "LMCache\u63d0\u53d6\u5e76\u5b58\u50a8\u73b0\u4ee3LLM\u5f15\u64ce\u751f\u6210\u7684KV\u7f13\u5b58\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u79fb\u52a8\uff08\u6279\u5904\u7406\u3001\u6d41\u6c34\u7ebf\uff09\u3001\u6a21\u5757\u5316\u8fde\u63a5\u5668\u548c\u63a7\u5236API\u5b9e\u73b0\u7f13\u5b58\u7f16\u6392\u3002", "result": "\u4e0evLLM\u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u5728\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8fbe15\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5e76\u5728\u4f01\u4e1a\u63a8\u7406\u7cfb\u7edf\u4e2d\u5f97\u5230\u5e7f\u6cdb\u91c7\u7528\u3002", "conclusion": "LMCache\u901a\u8fc7\u9ad8\u6548\u7684KV\u7f13\u5b58\u5171\u4eab\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u7cfb\u7edf\u7684\u8d44\u6e90\u5229\u7528\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u672a\u6765KV\u7f13\u5b58\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7ecf\u9a8c\u3002"}}
{"id": "2510.10008", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10008", "abs": "https://arxiv.org/abs/2510.10008", "authors": ["Meng Xi", "Sihan Lv", "Yechen Jin", "Guanjie Cheng", "Naibo Wang", "Ying Li", "Jianwei Yin"], "title": "RIPRAG: Hack a Black-box Retrieval-Augmented Generation Question-Answering System with Reinforcement Learning", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems based on Large Language Models\n(LLMs) have become a core technology for tasks such as question-answering (QA)\nand content generation. However, by injecting poisoned documents into the\ndatabase of RAG systems, attackers can manipulate LLMs to generate text that\naligns with their intended preferences. Existing research has primarily focused\non white-box attacks against simplified RAG architectures. In this paper, we\ninvestigate a more complex and realistic scenario: the attacker lacks knowledge\nof the RAG system's internal composition and implementation details, and the\nRAG system comprises components beyond a mere retriever. Specifically, we\npropose the RIPRAG attack framework, an end-to-end attack pipeline that treats\nthe target RAG system as a black box, where the only information accessible to\nthe attacker is whether the poisoning succeeds. Our method leverages\nReinforcement Learning (RL) to optimize the generation model for poisoned\ndocuments, ensuring that the generated poisoned document aligns with the target\nRAG system's preferences. Experimental results demonstrate that this method can\neffectively execute poisoning attacks against most complex RAG systems,\nachieving an attack success rate (ASR) improvement of up to 0.72 compared to\nbaseline methods. This highlights prevalent deficiencies in current defensive\nmethods and provides critical insights for LLM security research.", "AI": {"tldr": "\u63d0\u51fa\u4e86RIPRAG\u653b\u51fb\u6846\u67b6\uff0c\u4e00\u79cd\u9488\u5bf9\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u7684\u9ed1\u76d2\u6295\u6bd2\u653b\u51fb\u65b9\u6cd5\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6295\u6bd2\u6587\u6863\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u9488\u5bf9\u7b80\u5316RAG\u67b6\u6784\u7684\u767d\u76d2\u653b\u51fb\uff0c\u7f3a\u4e4f\u5bf9\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e0b\u9ed1\u76d2\u653b\u51fb\u7684\u7814\u7a76\u3002\u653b\u51fb\u8005\u901a\u5e38\u4e0d\u4e86\u89e3RAG\u7cfb\u7edf\u5185\u90e8\u7ec4\u6210\uff0c\u4e14\u7cfb\u7edf\u5305\u542b\u68c0\u7d22\u5668\u4e4b\u5916\u7684\u591a\u4e2a\u7ec4\u4ef6\u3002", "method": "\u63d0\u51faRIPRAG\u653b\u51fb\u6846\u67b6\uff0c\u5c06\u76ee\u6807RAG\u7cfb\u7edf\u89c6\u4e3a\u9ed1\u76d2\uff0c\u4ec5\u5229\u7528\u653b\u51fb\u662f\u5426\u6210\u529f\u7684\u4fe1\u606f\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6295\u6bd2\u6587\u6863\u751f\u6210\u6a21\u578b\uff0c\u786e\u4fdd\u751f\u6210\u7684\u6295\u6bd2\u6587\u6863\u7b26\u5408\u76ee\u6807RAG\u7cfb\u7edf\u7684\u504f\u597d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5bf9\u5927\u591a\u6570\u590d\u6742RAG\u7cfb\u7edf\u6267\u884c\u6295\u6bd2\u653b\u51fb\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u653b\u51fb\u6210\u529f\u7387\u63d0\u5347\u9ad8\u8fbe0.72\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u9632\u5fa1\u65b9\u6cd5\u7684\u666e\u904d\u7f3a\u9677\uff0c\u4e3aLLM\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2510.11147", "categories": ["stat.ML", "cs.LG", "68T07, 68T05, 68-04", "I.2.6; I.5.1; I.5.3"], "pdf": "https://arxiv.org/pdf/2510.11147", "abs": "https://arxiv.org/abs/2510.11147", "authors": ["Louis Berthier", "Ahmed Shokry", "Maxime Moreaud", "Guillaume Ramelet", "Eric Moulines"], "title": "torchsom: The Reference PyTorch Library for Self-Organizing Maps", "comment": "4 mains pages with 2 tables, 4 pages of references, 15 pages of\n  appendices with 13 figures and 3 tables", "summary": "This paper introduces torchsom, an open-source Python library that provides a\nreference implementation of the Self-Organizing Map (SOM) in PyTorch. This\npackage offers three main features: (i) dimensionality reduction, (ii)\nclustering, and (iii) friendly data visualization. It relies on a PyTorch\nbackend, enabling (i) fast and efficient training of SOMs through GPU\nacceleration, and (ii) easy and scalable integrations with PyTorch ecosystem.\nMoreover, torchsom follows the scikit-learn API for ease of use and\nextensibility. The library is released under the Apache 2.0 license with 90%\ntest coverage, and its source code and documentation are available at\nhttps://github.com/michelin/TorchSOM.", "AI": {"tldr": "torchsom\u662f\u4e00\u4e2a\u57fa\u4e8ePyTorch\u7684\u81ea\u7ec4\u7ec7\u6620\u5c04(SOM)\u5f00\u6e90Python\u5e93\uff0c\u63d0\u4f9b\u964d\u7ef4\u3001\u805a\u7c7b\u548c\u53ef\u89c6\u5316\u529f\u80fd\uff0c\u652f\u6301GPU\u52a0\u901f\u8bad\u7ec3\u5e76\u4e0ePyTorch\u751f\u6001\u7cfb\u7edf\u96c6\u6210\u3002", "motivation": "\u4e3aPyTorch\u7528\u6237\u63d0\u4f9b\u4e00\u4e2a\u9ad8\u6548\u3001\u6613\u7528\u7684\u81ea\u7ec4\u7ec7\u6620\u5c04\u5b9e\u73b0\uff0c\u5229\u7528GPU\u52a0\u901f\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u5e76\u4e0escikit-learn API\u517c\u5bb9\u4ee5\u4fbf\u4e8e\u4f7f\u7528\u548c\u6269\u5c55\u3002", "method": "\u57fa\u4e8ePyTorch\u540e\u7aef\u5b9e\u73b0SOM\u7b97\u6cd5\uff0c\u652f\u6301GPU\u52a0\u901f\u8bad\u7ec3\uff0c\u91c7\u7528scikit-learn\u98ce\u683c\u7684API\u8bbe\u8ba1\uff0c\u63d0\u4f9b\u964d\u7ef4\u3001\u805a\u7c7b\u548c\u53ef\u89c6\u5316\u4e09\u5927\u6838\u5fc3\u529f\u80fd\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u529f\u80fd\u5b8c\u6574\u7684torchsom\u5e93\uff0c\u5177\u670990%\u7684\u6d4b\u8bd5\u8986\u76d6\u7387\uff0c\u91c7\u7528Apache 2.0\u5f00\u6e90\u534f\u8bae\uff0c\u4ee3\u7801\u548c\u6587\u6863\u5df2\u5728GitHub\u4e0a\u516c\u5f00\u3002", "conclusion": "torchsom\u4e3aPyTorch\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u6613\u6269\u5c55\u7684\u81ea\u7ec4\u7ec7\u6620\u5c04\u5b9e\u73b0\uff0c\u652f\u6301GPU\u52a0\u901f\u548cscikit-learn\u517c\u5bb9API\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdbSOM\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2510.10289", "categories": ["eess.SY", "cs.SY", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.10289", "abs": "https://arxiv.org/abs/2510.10289", "authors": ["Ke Ma", "Andrey Vlasov", "Zeynep B. Simsek", "Jinshui Zhang", "Yiru Li", "Boshuo Wang", "David L. K. Murphy", "Jessica Y. Choi", "Maya E. Clinton", "Noreen Bukhari-Parlakturk", "Angel V. Peterchev", "Stephan M. Goetz"], "title": "Optimal monophasic, asymmetric electric field pulses for selective transcranial magnetic stimulation (TMS) with minimised power and coil heating", "comment": "31 pages, 8 figures", "summary": "Transcranial magnetic stimulation (TMS) with asymmetric electric field\npulses, such as monophasic, offers directional selectivity for neural\nactivation but requires excessive energy. Previous pulse shape optimisation has\nbeen limited to symmetric pulses or heavily constrained variations of\nconventional waveforms without achieving general optimality in energy\nefficiency or neural selectivity. We implemented an optimisation framework that\nincorporates neuron model activation constraints and flexible control of pulse\nasymmetry. The optimised electric field waveforms achieved up to 92 % and 88 %\nreduction in energy loss and thus coil heating respectively compared to\nconventional monophasic pulses and previously improved monophasic-equivalent\npulses. In the human experiments, OUR pulses showed similar motor thresholds to\nmonophasic pulses in both AP and PA directions with significantly lower energy\nloss, particularly in the AP direction. Moreover, there was a significant MEP\nlatency difference of (1.79 +/- 0.41) ms between AP and PA direction with OUR\npulses, which suggests directional selectivity. Our framework successfully\nidentified highly energy-efficient asymmetric pulses for\ndirectionally-selective neural engagement. These pulses can enable selective\nrapid-rate repetitive TMS protocols with reduced power consumption and coil\nheating, with potential benefits for precision and potency of neuro-modulation.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u4f18\u5316\u6846\u67b6\uff0c\u751f\u6210\u9ad8\u5ea6\u8282\u80fd\u7684\u4e0d\u5bf9\u79f0\u7535\u573a\u8109\u51b2\uff0c\u76f8\u6bd4\u4f20\u7edf\u5355\u76f8\u8109\u51b2\u51cf\u5c1192%\u80fd\u91cf\u635f\u5931\u548c88%\u7ebf\u5708\u53d1\u70ed\uff0c\u540c\u65f6\u4fdd\u6301\u65b9\u5411\u9009\u62e9\u6027\u795e\u7ecf\u6fc0\u6d3b\u3002", "motivation": "\u4f20\u7edf\u5355\u76f8TMS\u8109\u51b2\u867d\u7136\u5177\u6709\u65b9\u5411\u9009\u62e9\u6027\uff0c\u4f46\u80fd\u8017\u8fc7\u9ad8\uff1b\u73b0\u6709\u8109\u51b2\u5f62\u72b6\u4f18\u5316\u5c40\u9650\u4e8e\u5bf9\u79f0\u8109\u51b2\u6216\u53d7\u7ea6\u675f\u7684\u6ce2\u5f62\u53d8\u4f53\uff0c\u672a\u80fd\u5b9e\u73b0\u80fd\u91cf\u6548\u7387\u548c\u795e\u7ecf\u9009\u62e9\u6027\u7684\u5168\u5c40\u6700\u4f18\u3002", "method": "\u5b9e\u65bd\u5305\u542b\u795e\u7ecf\u5143\u6a21\u578b\u6fc0\u6d3b\u7ea6\u675f\u548c\u8109\u51b2\u4e0d\u5bf9\u79f0\u6027\u7075\u6d3b\u63a7\u5236\u7684\u4f18\u5316\u6846\u67b6\uff0c\u751f\u6210\u4f18\u5316\u7684\u7535\u573a\u6ce2\u5f62\u3002", "result": "\u4f18\u5316\u8109\u51b2\u76f8\u6bd4\u4f20\u7edf\u5355\u76f8\u8109\u51b2\u548c\u5148\u524d\u6539\u8fdb\u7684\u5355\u76f8\u7b49\u6548\u8109\u51b2\uff0c\u5206\u522b\u51cf\u5c1192%\u548c88%\u7684\u80fd\u91cf\u635f\u5931\u548c\u7ebf\u5708\u53d1\u70ed\uff1b\u4eba\u4f53\u5b9e\u9a8c\u4e2d\u663e\u793a\u76f8\u4f3c\u7684\u8fd0\u52a8\u9608\u503c\uff0c\u4f46\u80fd\u91cf\u635f\u5931\u663e\u8457\u964d\u4f4e\uff0cAP\u548cPA\u65b9\u5411\u95f4\u5b58\u5728\u663e\u8457MEP\u6f5c\u4f0f\u671f\u5dee\u5f02(1.79\u00b10.41ms)\uff0c\u8868\u660e\u65b9\u5411\u9009\u62e9\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u8bc6\u522b\u51fa\u9ad8\u5ea6\u8282\u80fd\u7684\u4e0d\u5bf9\u79f0\u8109\u51b2\uff0c\u53ef\u7528\u4e8e\u65b9\u5411\u9009\u62e9\u6027\u795e\u7ecf\u53c2\u4e0e\uff0c\u6709\u671b\u5b9e\u73b0\u9009\u62e9\u6027\u5feb\u901f\u7387\u91cd\u590dTMS\u534f\u8bae\uff0c\u964d\u4f4e\u529f\u8017\u548c\u7ebf\u5708\u53d1\u70ed\uff0c\u63d0\u5347\u795e\u7ecf\u8c03\u63a7\u7684\u7cbe\u5ea6\u548c\u6548\u529b\u3002"}}
{"id": "2510.10327", "categories": ["cs.CY", "cs.AI", "F.2.2, I.2.7"], "pdf": "https://arxiv.org/pdf/2510.10327", "abs": "https://arxiv.org/abs/2510.10327", "authors": ["Junhao Xu", "Hui Zeng"], "title": "Mapping the Urban Mobility Intelligence Frontier: A Scientometric Analysis of Data-Driven Pedestrian Trajectory Prediction and Simulation", "comment": "5 figures", "summary": "Understanding and predicting pedestrian dynamics has become essential for\nshaping safer, more responsive, and human-centered urban environments. This\nstudy conducts a comprehensive scientometric analysis of research on\ndata-driven pedestrian trajectory prediction and crowd simulation, mapping its\nintellectual evolution and interdisciplinary structure. Using bibliometric data\nfrom the Web of Science Core Collection, we employ SciExplorer and Bibliometrix\nto identify major trends, influential contributors, and emerging frontiers.\nResults reveal a strong convergence between artificial intelligence, urban\ninformatics, and crowd behavior modeling--driven by graph neural networks,\ntransformers, and generative models. Beyond technical advances, the field\nincreasingly informs urban mobility design, public safety planning, and digital\ntwin development for smart cities. However, challenges remain in ensuring\ninterpretability, inclusivity, and cross-domain transferability. By connecting\nmethodological trajectories with urban applications, this work highlights how\ndata-driven approaches can enrich urban governance and pave the way for\nadaptive, socially responsible mobility intelligence in future cities.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u79d1\u5b66\u8ba1\u91cf\u5206\u6790\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86\u6570\u636e\u9a71\u52a8\u7684\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u548c\u4eba\u7fa4\u6a21\u62df\u7814\u7a76\u9886\u57df\uff0c\u63ed\u793a\u4e86\u4eba\u5de5\u667a\u80fd\u3001\u57ce\u5e02\u4fe1\u606f\u5b66\u548c\u4eba\u7fa4\u884c\u4e3a\u5efa\u6a21\u7684\u878d\u5408\u8d8b\u52bf\uff0c\u4ee5\u53ca\u8be5\u9886\u57df\u5728\u57ce\u5e02\u79fb\u52a8\u8bbe\u8ba1\u3001\u516c\u5171\u5b89\u5168\u89c4\u5212\u548c\u6570\u5b57\u5b6a\u751f\u7b49\u5e94\u7528\u4e2d\u7684\u4ef7\u503c\u3002", "motivation": "\u7406\u89e3\u548c\u9884\u6d4b\u884c\u4eba\u52a8\u6001\u5bf9\u4e8e\u5851\u9020\u66f4\u5b89\u5168\u3001\u54cd\u5e94\u66f4\u7075\u654f\u3001\u4ee5\u4eba\u4e3a\u672c\u7684\u57ce\u5e02\u73af\u5883\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u548c\u53d1\u5c55\u8d8b\u52bf\u3002", "method": "\u4f7f\u7528Web of Science\u6838\u5fc3\u5408\u96c6\u7684\u6570\u636e\uff0c\u91c7\u7528SciExplorer\u548cBibliometrix\u5de5\u5177\u8fdb\u884c\u6587\u732e\u8ba1\u91cf\u5206\u6790\uff0c\u8bc6\u522b\u4e3b\u8981\u8d8b\u52bf\u3001\u6709\u5f71\u54cd\u529b\u7684\u8d21\u732e\u8005\u548c\u65b0\u5174\u524d\u6cbf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4eba\u5de5\u667a\u80fd\u3001\u57ce\u5e02\u4fe1\u606f\u5b66\u548c\u4eba\u7fa4\u884c\u4e3a\u5efa\u6a21\u4e4b\u95f4\u51fa\u73b0\u4e86\u5f3a\u70c8\u878d\u5408\uff0c\u4e3b\u8981\u7531\u56fe\u795e\u7ecf\u7f51\u7edc\u3001Transformer\u548c\u751f\u6210\u6a21\u578b\u9a71\u52a8\u3002\u8be5\u9886\u57df\u6b63\u8d8a\u6765\u8d8a\u591a\u5730\u4e3a\u57ce\u5e02\u79fb\u52a8\u8bbe\u8ba1\u3001\u516c\u5171\u5b89\u5168\u89c4\u5212\u548c\u667a\u6167\u57ce\u5e02\u6570\u5b57\u5b6a\u751f\u5f00\u53d1\u63d0\u4f9b\u652f\u6301\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u53ef\u4ee5\u4e30\u5bcc\u57ce\u5e02\u6cbb\u7406\uff0c\u4e3a\u672a\u6765\u57ce\u5e02\u4e2d\u9002\u5e94\u6027\u3001\u793e\u4f1a\u8d23\u4efb\u7684\u79fb\u52a8\u667a\u80fd\u94fa\u5e73\u9053\u8def\uff0c\u4f46\u5728\u786e\u4fdd\u53ef\u89e3\u91ca\u6027\u3001\u5305\u5bb9\u6027\u548c\u8de8\u9886\u57df\u53ef\u8f6c\u79fb\u6027\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002"}}
{"id": "2510.10535", "categories": ["math.OC", "cs.NA", "math.DS", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.10535", "abs": "https://arxiv.org/abs/2510.10535", "authors": ["M Gulliksson", "A Oleynik", "M Ogren", "R Bakhshandeh-Chamazkoti"], "title": "Linear Algebra Problems Solved by Using Damped Dynamical Systems on the Stiefel Manifold", "comment": null, "summary": "We develop a new method for solving minimization problems on the Stiefel\nManifold using damped dynamical systems. The constraints are satisfied in the\nlimit by an additional damped dynamical system. The method is illustrated by\nnumerical experiments and compared to a state-of-the-art conjugate gradient\nmethod.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728Stiefel\u6d41\u5f62\u4e0a\u6c42\u89e3\u6700\u5c0f\u5316\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u963b\u5c3c\u52a8\u529b\u7cfb\u7edf\u6765\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u7684\u5171\u8f6d\u68af\u5ea6\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3Stiefel\u6d41\u5f62\u4e0a\u7684\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5904\u7406\u6d41\u5f62\u7ea6\u675f\u7684\u6570\u503c\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u963b\u5c3c\u52a8\u529b\u7cfb\u7edf\u65b9\u6cd5\uff0c\u901a\u8fc7\u989d\u5916\u7684\u963b\u5c3c\u52a8\u529b\u7cfb\u7edf\u5728\u6781\u9650\u60c5\u51b5\u4e0b\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\u3002", "result": "\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u7684\u5171\u8f6d\u68af\u5ea6\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u63d0\u51fa\u7684\u963b\u5c3c\u52a8\u529b\u7cfb\u7edf\u65b9\u6cd5\u4e3aStiefel\u6d41\u5f62\u4e0a\u7684\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.09738", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09738", "abs": "https://arxiv.org/abs/2510.09738", "authors": ["Steve Han", "Gilberto Titericz Junior", "Tom Balough", "Wenfei Zhou"], "title": "Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement", "comment": "10 pages, 1 figure, 4 tables, under review as a conference paper at\n  ICLR 2026", "summary": "This research introduces the Judge's Verdict Benchmark, a novel two-step\nmethodology to evaluate Large Language Models (LLMs) as judges for response\naccuracy evaluation tasks. We assess how well 54 LLMs can replicate human\njudgment when scoring responses from RAG (Retrieval-Augmented Generation) or\nAgentic pipelines against ground truth answers. Our methodology progresses from\ntraditional correlation analysis to comprehensive Cohen's Kappa analysis that\nmeasures actual agreement patterns. The two-step approach includes: (1) a\ncorrelation test that filters judges with strong alignment, followed by (2) a\nhuman-likeness test using z-scores to identify two distinct judgment patterns:\nhuman-like judgment (|z| < 1) that mimics natural human variation, and\nsuper-consistent judgment (z > 1) that exceeds typical human-to-human agreement\nlevels. This methodology reveals that 27 out of 54 tested LLMs achieve Tier 1\nperformance: 23 models exhibit human-like patterns that preserve the nuances of\nhuman judgment, while 4 models demonstrate super-consistent behavior, a pattern\nthat could indicate either enhanced reliability or oversimplification of\ncomplex judgments. Testing 43 open-source models (1B-405B parameters) and 11\nclosed models (GPT, Gemini, Claude variants), we demonstrate that judge\nexcellence is not solely dependent on model size but on specific training\nstrategies. Our key contributions include: (1) establishing that correlation\nalone is insufficient for judge evaluation, (2) introducing a \"Turing Test for\njudges\" based on agreement patterns, and (3) providing a standardized benchmark\nfor classifying LLM judges into distinct performance tiers for different\nevaluation needs.", "AI": {"tldr": "\u63d0\u51fa\u4e86Judge's Verdict Benchmark\uff0c\u4e00\u79cd\u4e24\u6b65\u6cd5\u8bc4\u4f30LLM\u4f5c\u4e3a\u88c1\u5224\u7684\u80fd\u529b\uff0c\u6d4b\u8bd554\u4e2a\u6a21\u578b\u5728RAG\u548cAgentic\u4efb\u52a1\u4e2d\u590d\u5236\u4eba\u7c7b\u5224\u65ad\u7684\u80fd\u529b\u3002\u53d1\u73b027\u4e2a\u6a21\u578b\u8fbe\u5230Tier 1\u6027\u80fd\uff0c\u5176\u4e2d23\u4e2a\u5c55\u73b0\u7c7b\u4eba\u5224\u65ad\u6a21\u5f0f\uff0c4\u4e2a\u5c55\u73b0\u8d85\u4e00\u81f4\u884c\u4e3a\u3002", "motivation": "\u8bc4\u4f30LLM\u4f5c\u4e3a\u88c1\u5224\u5728\u54cd\u5e94\u51c6\u786e\u6027\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u80fd\u5426\u590d\u5236\u4eba\u7c7b\u5224\u65ad\uff0c\u56e0\u4e3a\u4ec5\u9760\u76f8\u5173\u6027\u5206\u6790\u4e0d\u8db3\u4ee5\u5168\u9762\u8bc4\u4f30\u88c1\u5224\u80fd\u529b\u3002", "method": "\u4e24\u6b65\u6cd5\uff1a\u5148\u8fdb\u884c\u76f8\u5173\u6027\u6d4b\u8bd5\u7b5b\u9009\u5bf9\u9f50\u5ea6\u9ad8\u7684\u88c1\u5224\uff0c\u7136\u540e\u4f7f\u7528z\u5206\u6570\u8fdb\u884c\u4eba\u7c7b\u76f8\u4f3c\u6027\u6d4b\u8bd5\uff0c\u8bc6\u522b\u7c7b\u4eba\u5224\u65ad\uff08|z|<1\uff09\u548c\u8d85\u4e00\u81f4\u5224\u65ad\uff08z>1\uff09\u4e24\u79cd\u6a21\u5f0f\u3002", "result": "54\u4e2a\u6a21\u578b\u4e2d27\u4e2a\u8fbe\u5230Tier 1\u6027\u80fd\uff1a23\u4e2a\u6a21\u578b\u5c55\u73b0\u7c7b\u4eba\u5224\u65ad\u6a21\u5f0f\uff0c4\u4e2a\u5c55\u73b0\u8d85\u4e00\u81f4\u884c\u4e3a\u3002\u6a21\u578b\u6027\u80fd\u4e0d\u5355\u7eaf\u4f9d\u8d56\u89c4\u6a21\uff0c\u800c\u662f\u7279\u5b9a\u8bad\u7ec3\u7b56\u7565\u3002", "conclusion": "\u76f8\u5173\u6027\u5206\u6790\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u88c1\u5224\u80fd\u529b\uff0c\u9700\u8981\u57fa\u4e8e\u4e00\u81f4\u6027\u6a21\u5f0f\u7684\"\u56fe\u7075\u6d4b\u8bd5\"\uff0c\u5e76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\u6765\u5c06LLM\u88c1\u5224\u5206\u7c7b\u5230\u4e0d\u540c\u6027\u80fd\u5c42\u7ea7\u3002"}}
{"id": "2510.09666", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09666", "abs": "https://arxiv.org/abs/2510.09666", "authors": ["Aditya Chakravarty"], "title": "Spatial Uncertainty Quantification in Wildfire Forecasting for Climate-Resilient Emergency Planning", "comment": null, "summary": "Climate change is intensifying wildfire risks globally, making reliable\nforecasting critical for adaptation strategies. While machine learning shows\npromise for wildfire prediction from Earth observation data, current approaches\nlack uncertainty quantification essential for risk-aware decision making. We\npresent the first systematic analysis of spatial uncertainty in wildfire spread\nforecasting using multimodal Earth observation inputs. We demonstrate that\npredictive uncertainty exhibits coherent spatial structure concentrated near\nfire perimeters. Our novel distance metric reveals high-uncertainty regions\nform consistent 20-60 meter buffer zones around predicted firelines - directly\napplicable for emergency planning. Feature attribution identifies vegetation\nhealth and fire activity as primary uncertainty drivers. This work enables more\nrobust wildfire management systems supporting communities adapting to\nincreasing fire risk under climate change.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u57fa\u4e8e\u591a\u6a21\u6001\u5730\u7403\u89c2\u6d4b\u6570\u636e\u7684\u91ce\u706b\u8513\u5ef6\u9884\u6d4b\u4e2d\u7684\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\uff0c\u53d1\u73b0\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u5728\u706b\u7ebf\u5468\u8fb9\u5f62\u621020-60\u7c73\u7684\u7f13\u51b2\u533a\uff0c\u8fd9\u5bf9\u5e94\u6025\u89c4\u5212\u5177\u6709\u76f4\u63a5\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u52a0\u5267\u4e86\u5168\u7403\u91ce\u706b\u98ce\u9669\uff0c\u9700\u8981\u53ef\u9760\u7684\u9884\u6d4b\u6765\u5236\u5b9a\u9002\u5e94\u7b56\u7565\u3002\u5f53\u524d\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u91ce\u706b\u9884\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u800c\u8fd9\u5bf9\u98ce\u9669\u611f\u77e5\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u5730\u7403\u89c2\u6d4b\u6570\u636e\u8fdb\u884c\u91ce\u706b\u8513\u5ef6\u9884\u6d4b\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\u6a21\u5f0f\uff0c\u5f00\u53d1\u4e86\u65b0\u7684\u8ddd\u79bb\u5ea6\u91cf\u65b9\u6cd5\u6765\u8bc6\u522b\u9ad8\u4e0d\u786e\u5b9a\u6027\u533a\u57df\u3002", "result": "\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u5448\u73b0\u8fde\u8d2f\u7684\u7a7a\u95f4\u7ed3\u6784\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u706b\u7ebf\u5468\u8fb9\uff0c\u5f62\u621020-60\u7c73\u7684\u7f13\u51b2\u533a\u3002\u7279\u5f81\u5f52\u56e0\u8bc6\u522b\u51fa\u690d\u88ab\u5065\u5eb7\u548c\u706b\u60c5\u6d3b\u52a8\u662f\u4e3b\u8981\u7684\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u56e0\u7d20\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u80fd\u591f\u652f\u6301\u66f4\u7a33\u5065\u7684\u91ce\u706b\u7ba1\u7406\u7cfb\u7edf\uff0c\u5e2e\u52a9\u793e\u533a\u9002\u5e94\u6c14\u5019\u53d8\u5316\u4e0b\u65e5\u76ca\u589e\u52a0\u7684\u706b\u707e\u98ce\u9669\uff0c\u4e3a\u5e94\u6025\u89c4\u5212\u63d0\u4f9b\u76f4\u63a5\u53ef\u7528\u7684\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\u3002"}}
{"id": "2510.10035", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10035", "abs": "https://arxiv.org/abs/2510.10035", "authors": ["Jusheng Zhang", "Kaitong Cai", "Qinglin Zeng", "Ningyuan Liu", "Stephen Fan", "Ziliang Chen", "Keze Wang"], "title": "Failure-Driven Workflow Refinement", "comment": null, "summary": "Optimizing LLM-based workflows is typically formulated as a global search,\nwhere candidate workflows are evaluated based on a scalar metric. This\nparadigm, however, suffers from a critical flaw: information collapse. By\nreducing rich, multi-step execution traces to simple success/failure signals,\nexisting methods are rendered blind to the underlying structure of failures,\nfundamentally preventing them from modeling the workflow's failure\ndistribution. We reconceptualize this challenge as a distributional problem. We\npropose a new paradigm where the optimization goal is not to maximize a scalar\nscore, but to directly minimize a workflow's Expected Failure Mass, i.e., the\nintegral of its failure probability density function defined over a\nhigh-dimensional Failure Signature Space (FSS). This distributional lens allows\nus to move from inefficient, zero-order optimization to a principled,\ngradient-like descent on the failure landscape itself. We introduce CE-Graph, a\nframework that operationalizes this paradigm through a novel, failure-driven\nrefinement process. CE-Graph approximates the failure distribution from a pool\nof counterexamples, identifies its densest regions as recurring failure modes,\nand applies targeted, operator-constrained graph edits via a Propose-and-Verify\nmechanism to greedily reduce the failure mass. On math, code, and QA\nbenchmarks, our CE-Graph achieves higher robustness at a significantly lower\ncost than strong baselines. This suggests that a system's reliability emerges\nnot from avoiding failures, but from systematically learning and reshaping the\ngeometric structure of its failure distributions.", "AI": {"tldr": "\u63d0\u51faCE-Graph\u6846\u67b6\uff0c\u5c06LLM\u5de5\u4f5c\u6d41\u4f18\u5316\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5206\u5e03\u95ee\u9898\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u671f\u671b\u5931\u8d25\u8d28\u91cf\u6765\u76f4\u63a5\u4f18\u5316\u5931\u8d25\u5206\u5e03\uff0c\u800c\u975e\u6700\u5927\u5316\u6807\u91cf\u5206\u6570", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u591a\u6b65\u9aa4\u6267\u884c\u8f68\u8ff9\u7b80\u5316\u4e3a\u6210\u529f/\u5931\u8d25\u4fe1\u53f7\uff0c\u5bfc\u81f4\u4fe1\u606f\u574d\u584c\uff0c\u65e0\u6cd5\u5efa\u6a21\u5de5\u4f5c\u6d41\u7684\u5931\u8d25\u5206\u5e03\u7ed3\u6784", "method": "\u5f15\u5165\u5931\u8d25\u7b7e\u540d\u7a7a\u95f4(FSS)\uff0c\u901a\u8fc7\u53cd\u4f8b\u6c60\u8fd1\u4f3c\u5931\u8d25\u5206\u5e03\uff0c\u8bc6\u522b\u5bc6\u96c6\u533a\u57df\u4f5c\u4e3a\u91cd\u590d\u5931\u8d25\u6a21\u5f0f\uff0c\u4f7f\u7528\u63d0\u8bae-\u9a8c\u8bc1\u673a\u5236\u8fdb\u884c\u9488\u5bf9\u6027\u56fe\u7f16\u8f91", "result": "\u5728\u6570\u5b66\u3001\u4ee3\u7801\u548cQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCE-Graph\u4ee5\u663e\u8457\u66f4\u4f4e\u7684\u6210\u672c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u9c81\u68d2\u6027", "conclusion": "\u7cfb\u7edf\u53ef\u9760\u6027\u6e90\u4e8e\u7cfb\u7edf\u6027\u5730\u5b66\u4e60\u548c\u91cd\u5851\u5176\u5931\u8d25\u5206\u5e03\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u800c\u975e\u907f\u514d\u5931\u8d25"}}
{"id": "2510.11169", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11169", "abs": "https://arxiv.org/abs/2510.11169", "authors": ["Hind Atbir", "Farah Cherfaoui", "Guillaume Metzler", "Emilie Morvant", "Paul Viallard"], "title": "PAC-Bayesian Bounds on Constrained f-Entropic Risk Measures", "comment": null, "summary": "PAC generalization bounds on the risk, when expressed in terms of the\nexpected loss, are often insufficient to capture imbalances between subgroups\nin the data. To overcome this limitation, we introduce a new family of risk\nmeasures, called constrained f-entropic risk measures, which enable finer\ncontrol over distributional shifts and subgroup imbalances via f-divergences,\nand include the Conditional Value at Risk (CVaR), a well-known risk measure. We\nderive both classical and disintegrated PAC-Bayesian generalization bounds for\nthis family of risks, providing the first disintegratedPAC-Bayesian guarantees\nbeyond standard risks. Building on this theory, we design a self-bounding\nalgorithm that minimizes our bounds directly, yielding models with guarantees\nat the subgroup level. Finally, we empirically demonstrate the usefulness of\nour approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u98ce\u9669\u5ea6\u91cf\u5bb6\u65cf\u2014\u2014\u7ea6\u675ff-\u71b5\u98ce\u9669\u5ea6\u91cf\uff0c\u7528\u4e8e\u89e3\u51b3\u4f20\u7edf\u98ce\u9669\u5ea6\u91cf\u5728\u6355\u6349\u6570\u636e\u5b50\u7ec4\u4e0d\u5e73\u8861\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u63a8\u5bfc\u4e86\u76f8\u5e94\u7684PAC-Bayesian\u6cdb\u5316\u8fb9\u754c\u3002", "motivation": "\u4f20\u7edf\u7684\u98ce\u9669\u5ea6\u91cf\u5728\u8868\u8fbe\u671f\u671b\u635f\u5931\u65f6\u5f80\u5f80\u65e0\u6cd5\u5145\u5206\u6355\u6349\u6570\u636e\u4e2d\u4e0d\u540c\u5b50\u7ec4\u4e4b\u95f4\u7684\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u5f15\u5165\u7ea6\u675ff-\u71b5\u98ce\u9669\u5ea6\u91cf\u5bb6\u65cf\uff0c\u901a\u8fc7f-\u6563\u5ea6\u5b9e\u73b0\u5bf9\u5206\u5e03\u504f\u79fb\u548c\u5b50\u7ec4\u4e0d\u5e73\u8861\u7684\u7cbe\u7ec6\u63a7\u5236\uff1b\u63a8\u5bfc\u4e86\u7ecf\u5178\u548c\u5206\u89e3\u7684PAC-Bayesian\u6cdb\u5316\u8fb9\u754c\uff1b\u8bbe\u8ba1\u4e86\u81ea\u7ea6\u675f\u7b97\u6cd5\u76f4\u63a5\u6700\u5c0f\u5316\u8fb9\u754c\u3002", "result": "\u4e3a\u7ea6\u675ff-\u71b5\u98ce\u9669\u5ea6\u91cf\u5bb6\u65cf\u63d0\u4f9b\u4e86\u9996\u4e2a\u8d85\u51fa\u6807\u51c6\u98ce\u9669\u7684\u5206\u89e3PAC-Bayesian\u4fdd\u8bc1\uff1b\u5f00\u53d1\u4e86\u5728\u5b50\u7ec4\u7ea7\u522b\u5177\u6709\u4fdd\u8bc1\u7684\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5b50\u7ec4\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.10313", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.10313", "abs": "https://arxiv.org/abs/2510.10313", "authors": ["Luiz Fernando M. Arruda", "Moises Ferber", "Diego Greff"], "title": "Low-cost Pyranometer-Based ANN Approach for MPPT in Solar PV Systems", "comment": null, "summary": "This article presents a study on the application of artificial neural\nnetworks (ANNs) for maximum power point tracking (MPPT) in photovoltaic (PV)\nsystems using low-cost pyranometer sensors. The proposed approach integrates\npyranometers, temperature sensors, and an ANN to estimate the duty cycle of a\nDC/DC converter, enabling the system to consistently operate at its maximum\npower point. The strategy was implemented in the local control of a Cuk\nconverter and experimentally validated against the conventional Perturb and\nObserve (P&O) method. Results demonstrate that the ANN-based technique,\nleveraging affordable sensor technology, achieves accurate MPPT performance\nwith reduced fluctuations, enhancing the responsiveness and efficiency of PV\ntracking systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc(ANN)\u7684\u5149\u4f0f\u7cfb\u7edf\u6700\u5927\u529f\u7387\u70b9\u8ddf\u8e2a(MPPT)\u65b9\u6cd5\uff0c\u4f7f\u7528\u4f4e\u6210\u672c\u8f90\u5c04\u8ba1\u4f20\u611f\u5668\uff0c\u901a\u8fc7ANN\u4f30\u8ba1DC/DC\u53d8\u6362\u5668\u7684\u5360\u7a7a\u6bd4\uff0c\u5b9e\u73b0\u7cfb\u7edf\u5728\u6700\u5927\u529f\u7387\u70b9\u7a33\u5b9a\u8fd0\u884c\u3002", "motivation": "\u4f20\u7edfMPPT\u65b9\u6cd5\u5982\u6270\u52a8\u89c2\u5bdf\u6cd5(P&O)\u5b58\u5728\u529f\u7387\u6ce2\u52a8\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7cbe\u786e\u3001\u54cd\u5e94\u66f4\u5feb\u7684\u8ddf\u8e2a\u6280\u672f\uff0c\u540c\u65f6\u5229\u7528\u4f4e\u6210\u672c\u4f20\u611f\u5668\u964d\u4f4e\u7cfb\u7edf\u6210\u672c\u3002", "method": "\u96c6\u6210\u8f90\u5c04\u8ba1\u3001\u6e29\u5ea6\u4f20\u611f\u5668\u548c\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff0c\u5728Cuk\u53d8\u6362\u5668\u7684\u672c\u5730\u63a7\u5236\u4e2d\u5b9e\u73b0\uff0c\u901a\u8fc7ANN\u4f30\u8ba1DC/DC\u53d8\u6362\u5668\u7684\u5360\u7a7a\u6bd4\u6765\u5b9e\u73b0MPPT\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u57fa\u4e8eANN\u7684\u6280\u672f\u6bd4\u4f20\u7edfP&O\u65b9\u6cd5\u5177\u6709\u66f4\u51c6\u786e\u7684MPPT\u6027\u80fd\uff0c\u529f\u7387\u6ce2\u52a8\u51cf\u5c11\uff0c\u7cfb\u7edf\u54cd\u5e94\u6027\u548c\u6548\u7387\u5f97\u5230\u63d0\u5347\u3002", "conclusion": "ANN\u7ed3\u5408\u4f4e\u6210\u672c\u4f20\u611f\u5668\u6280\u672f\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u5149\u4f0f\u7cfb\u7edf\u7684MPPT\u6027\u80fd\uff0c\u4e3a\u5149\u4f0f\u8ddf\u8e2a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10413", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.10413", "abs": "https://arxiv.org/abs/2510.10413", "authors": ["Saurabh Khanna"], "title": "Knowing Unknowns in an Age of Information Overload", "comment": null, "summary": "The technological revolution of the Internet has digitized the social,\neconomic, political, and cultural activities of billions of humans. While\nresearchers have been paying due attention to concerns of misinformation and\nbias, these obscure a much less researched and equally insidious problem - that\nof uncritically consuming incomplete information. The problem of incomplete\ninformation consumption stems from the very nature of explicitly ranked\ninformation on digital platforms, where our limited mental capacities leave us\nwith little choice but to consume the tip of a pre-ranked information iceberg.\nThis study makes two chief contributions. First, we leverage the context of\ninternet search to propose an innovative metric that quantifies information\ncompleteness. For a given search query, this refers to the extent of the\ninformation spectrum that is observed during web browsing. We then validate\nthis metric using 6.5 trillion search results extracted from daily search\ntrends across 48 nations for one year. Second, we find causal evidence that\nawareness of information completeness while browsing the Internet reduces\nresistance to factual information, hence paving the way towards an open-minded\nand tolerant mindset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5316\u4fe1\u606f\u5b8c\u6574\u6027\u7684\u65b0\u6307\u6807\uff0c\u57fa\u4e8e\u4e92\u8054\u7f51\u641c\u7d22\u80cc\u666f\uff0c\u5e76\u901a\u8fc7\u5206\u679048\u4e2a\u56fd\u5bb6\u4e00\u5e74\u5185\u76846.5\u4e07\u4ebf\u641c\u7d22\u7ed3\u679c\u9a8c\u8bc1\u8be5\u6307\u6807\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u610f\u8bc6\u5230\u4fe1\u606f\u5b8c\u6574\u6027\u53ef\u4ee5\u51cf\u5c11\u5bf9\u4e8b\u5b9e\u4fe1\u606f\u7684\u62b5\u5236\u3002", "motivation": "\u89e3\u51b3\u6570\u5b57\u5e73\u53f0\u4e2d\u4fe1\u606f\u6d88\u8d39\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u5373\u7528\u6237\u901a\u5e38\u53ea\u6d88\u8d39\u9884\u6392\u540d\u4fe1\u606f\u51b0\u5c71\u7684\u4e00\u89d2\uff0c\u8fd9\u79cd\u4e0d\u5b8c\u6574\u4fe1\u606f\u6d88\u8d39\u53ef\u80fd\u6bd4\u9519\u8bef\u4fe1\u606f\u548c\u504f\u89c1\u66f4\u5177\u6f5c\u5728\u5371\u5bb3\u3002", "method": "\u5229\u7528\u4e92\u8054\u7f51\u641c\u7d22\u80cc\u666f\u63d0\u51fa\u4fe1\u606f\u5b8c\u6574\u6027\u91cf\u5316\u6307\u6807\uff0c\u8be5\u6307\u6807\u8861\u91cf\u5728\u7f51\u9875\u6d4f\u89c8\u8fc7\u7a0b\u4e2d\u89c2\u5bdf\u5230\u7684\u4fe1\u606f\u8303\u56f4\u3002\u4f7f\u752848\u4e2a\u56fd\u5bb6\u4e00\u5e74\u51856.5\u4e07\u4ebf\u641c\u7d22\u7ed3\u679c\u7684\u65e5\u5e38\u641c\u7d22\u8d8b\u52bf\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u9a8c\u8bc1\u4e86\u4fe1\u606f\u5b8c\u6574\u6027\u6307\u6807\u7684\u6709\u6548\u6027\uff0c\u5e76\u53d1\u73b0\u610f\u8bc6\u5230\u4fe1\u606f\u5b8c\u6574\u6027\u53ef\u4ee5\u51cf\u5c11\u5bf9\u4e8b\u5b9e\u4fe1\u606f\u7684\u62b5\u5236\u3002", "conclusion": "\u610f\u8bc6\u5230\u4fe1\u606f\u5b8c\u6574\u6027\u6709\u52a9\u4e8e\u57f9\u517b\u5f00\u653e\u5305\u5bb9\u7684\u601d\u7ef4\u65b9\u5f0f\uff0c\u4e3a\u51cf\u5c11\u4fe1\u606f\u6d88\u8d39\u4e2d\u7684\u504f\u89c1\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.10557", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.10557", "abs": "https://arxiv.org/abs/2510.10557", "authors": ["Qinglan Xia", "Haotian Sun"], "title": "Optimal transport paths with capacity induced cost function", "comment": null, "summary": "This article generalizes the study of ramified optimal transport with\ncapacity constraint in transport multi-paths by generalizing the\n$\\mathbf{M}_{\\alpha}$ cost to $\\mathbf{M}_{\\alpha,c}$, which incorporates\ncapacity constraints into the cost function. Equipped with\n$\\mathbf{M}_{\\alpha,c}$ cost, we prove the existence of optimal transport path,\n$\\mathbf{M}_{\\alpha,c}$ related inequalities, decomposition of any general\ntransport paths, and occurrence of direct line segments in an optimal transport\npath.", "AI": {"tldr": "\u672c\u6587\u63a8\u5e7f\u4e86\u5e26\u6709\u5bb9\u91cf\u7ea6\u675f\u7684\u5206\u652f\u6700\u4f18\u4f20\u8f93\u7814\u7a76\uff0c\u5c06M_\u03b1\u6210\u672c\u63a8\u5e7f\u4e3a\u5305\u542b\u5bb9\u91cf\u7ea6\u675f\u7684M_\u03b1,c\u6210\u672c\uff0c\u8bc1\u660e\u4e86\u6700\u4f18\u4f20\u8f93\u8def\u5f84\u7684\u5b58\u5728\u6027\u3001\u76f8\u5173\u4e0d\u7b49\u5f0f\u3001\u4e00\u822c\u4f20\u8f93\u8def\u5f84\u7684\u5206\u89e3\u4ee5\u53ca\u6700\u4f18\u4f20\u8f93\u8def\u5f84\u4e2d\u76f4\u7ebf\u6bb5\u7684\u51fa\u73b0\u3002", "motivation": "\u5c06\u5206\u652f\u6700\u4f18\u4f20\u8f93\u7814\u7a76\u6269\u5c55\u5230\u5305\u542b\u5bb9\u91cf\u7ea6\u675f\u7684\u60c5\u51b5\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u6210\u672c\u51fd\u6570\u6765\u66f4\u51c6\u786e\u5730\u5efa\u6a21\u73b0\u5b9e\u4e2d\u7684\u4f20\u8f93\u95ee\u9898\u3002", "method": "\u5f15\u5165M_\u03b1,c\u6210\u672c\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u5728\u539f\u6709M_\u03b1\u6210\u672c\u57fa\u7840\u4e0a\u52a0\u5165\u4e86\u5bb9\u91cf\u7ea6\u675f\uff0c\u5e76\u57fa\u4e8e\u6b64\u6210\u672c\u51fd\u6570\u8fdb\u884c\u7406\u8bba\u5206\u6790\u3002", "result": "\u8bc1\u660e\u4e86\u6700\u4f18\u4f20\u8f93\u8def\u5f84\u7684\u5b58\u5728\u6027\uff0c\u5efa\u7acb\u4e86M_\u03b1,c\u76f8\u5173\u7684\u4e0d\u7b49\u5f0f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4e00\u822c\u4f20\u8f93\u8def\u5f84\u7684\u5206\u89e3\uff0c\u5e76\u8bc1\u660e\u4e86\u6700\u4f18\u4f20\u8f93\u8def\u5f84\u4e2d\u4f1a\u51fa\u73b0\u76f4\u7ebf\u6bb5\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u5bb9\u91cf\u7ea6\u675f\u7684\u6210\u672c\u51fd\u6570\uff0c\u6210\u529f\u6269\u5c55\u4e86\u5206\u652f\u6700\u4f18\u4f20\u8f93\u7406\u8bba\uff0c\u4e3a\u5904\u7406\u66f4\u590d\u6742\u7684\u5b9e\u9645\u4f20\u8f93\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2510.09770", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09770", "abs": "https://arxiv.org/abs/2510.09770", "authors": ["Adam Byerly", "Daniel Khashabi"], "title": "Gold Panning: Turning Positional Bias into Signal for Multi-Document LLM Reasoning", "comment": "20 pages, 6 figures", "summary": "Large language models exhibit a strong position bias in multi-document\ncontexts, systematically prioritizing information based on location rather than\nrelevance. While existing approaches treat this bias as noise to be mitigated,\nwe introduce Gold Panning Bandits, a framework that leverages position bias as\na diagnostic signal: by reordering documents and observing shifts in the\nmodel's responses, we can efficiently identify the most relevant content. We\nframe the problem of choosing reorderings as a bipartite matching problem.\nWhile an optimal assignment can be computed at each iteration with the\nHungarian algorithm in $O(N^3)$ time, we propose a greedy $O(N \\log N)$\nstrategy that achieves comparable performance by prioritizing the placement of\nthe most uncertain documents in the most informative positions. Our approach\nidentifies relevant documents using up to 65\\% fewer language model queries\nthan random permutation baselines on knowledge-intensive NLP tasks,\nsubstantially reducing computational cost without model retraining. This work\ndemonstrates that inherent LLM biases can be transformed from liabilities into\nassets for efficient, inference-time optimization.", "AI": {"tldr": "\u63d0\u51faGold Panning Bandits\u6846\u67b6\uff0c\u5229\u7528LLM\u7684\u4f4d\u7f6e\u504f\u89c1\u4f5c\u4e3a\u8bca\u65ad\u4fe1\u53f7\uff0c\u901a\u8fc7\u91cd\u65b0\u6392\u5217\u6587\u6863\u6765\u9ad8\u6548\u8bc6\u522b\u6700\u76f8\u5173\u5185\u5bb9\uff0c\u76f8\u6bd4\u968f\u673a\u6392\u5217\u57fa\u7ebf\u51cf\u5c1165%\u7684\u67e5\u8be2\u6b21\u6570\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6587\u6863\u73af\u5883\u4e2d\u5b58\u5728\u5f3a\u70c8\u7684\u4f4d\u7f6e\u504f\u89c1\uff0c\u4f20\u7edf\u65b9\u6cd5\u5c06\u5176\u89c6\u4e3a\u9700\u8981\u51cf\u8f7b\u7684\u566a\u58f0\uff0c\u4f46\u672c\u6587\u8ba4\u4e3a\u8fd9\u79cd\u504f\u89c1\u53ef\u4ee5\u4f5c\u4e3a\u8bca\u65ad\u4fe1\u53f7\u6765\u5229\u7528\u3002", "method": "\u5c06\u6587\u6863\u91cd\u65b0\u6392\u5e8f\u95ee\u9898\u6784\u5efa\u4e3a\u4e8c\u5206\u56fe\u5339\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u8d2a\u5fc3O(N log N)\u7b56\u7565\uff0c\u5c06\u6700\u4e0d\u786e\u5b9a\u7684\u6587\u6863\u653e\u5728\u6700\u5177\u4fe1\u606f\u91cf\u7684\u4f4d\u7f6e\uff0c\u76f8\u6bd4\u5308\u7259\u5229\u7b97\u6cd5\u7684O(N^3)\u65f6\u95f4\u66f4\u9ad8\u6548\u3002", "result": "\u5728\u77e5\u8bc6\u5bc6\u96c6\u578bNLP\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u968f\u673a\u6392\u5217\u57fa\u7ebf\u51cf\u5c11\u591a\u8fbe65%\u7684\u8bed\u8a00\u6a21\u578b\u67e5\u8be2\u6b21\u6570\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u4e14\u65e0\u9700\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8868\u660e\uff0c\u56fa\u6709\u7684LLM\u504f\u89c1\u53ef\u4ee5\u4ece\u8d1f\u62c5\u8f6c\u53d8\u4e3a\u8d44\u4ea7\uff0c\u7528\u4e8e\u5b9e\u73b0\u9ad8\u6548\u7684\u63a8\u7406\u65f6\u4f18\u5316\u3002"}}
{"id": "2510.09668", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.09668", "abs": "https://arxiv.org/abs/2510.09668", "authors": ["Maryam Abdollahi Shamami", "Babak Teimourpour", "Farshad Sharifi"], "title": "A Hybrid Computational Intelligence Framework with Metaheuristic Optimization for Drug-Drug Interaction Prediction", "comment": null, "summary": "Drug-drug interactions (DDIs) are a leading cause of preventable adverse\nevents, often complicating treatment and increasing healthcare costs. At the\nsame time, knowing which drugs do not interact is equally important, as such\nknowledge supports safer prescriptions and better patient outcomes. In this\nstudy, we propose an interpretable and efficient framework that blends modern\nmachine learning with domain knowledge to improve DDI prediction. Our approach\ncombines two complementary molecular embeddings - Mol2Vec, which captures\nfragment-level structural patterns, and SMILES-BERT, which learns contextual\nchemical features - together with a leakage-free, rule-based clinical score\n(RBScore) that injects pharmacological knowledge without relying on interaction\nlabels. A lightweight neural classifier is then optimized using a novel\nthree-stage metaheuristic strategy (RSmpl-ACO-PSO), which balances global\nexploration and local refinement for stable performance. Experiments on\nreal-world datasets demonstrate that the model achieves high predictive\naccuracy (ROC-AUC 0.911, PR-AUC 0.867 on DrugBank) and generalizes well to a\nclinically relevant Type 2 Diabetes Mellitus cohort. Beyond raw performance,\nstudies show how embedding fusion, RBScore, and the optimizer each contribute\nto precision and robustness. Together, these results highlight a practical\npathway for building reliable, interpretable, and computationally efficient\nmodels that can support safer drug therapies and clinical decision-making.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5b50\u5d4c\u5165\u548c\u4e34\u5e8a\u77e5\u8bc6\u8bc4\u5206\uff0c\u4f7f\u7528\u4e09\u9636\u6bb5\u5143\u542f\u53d1\u5f0f\u4f18\u5316\u7b56\u7565\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u826f\u597d\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u662f\u5bfc\u81f4\u53ef\u9884\u9632\u4e0d\u826f\u4e8b\u4ef6\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4e86\u89e3\u54ea\u4e9b\u836f\u7269\u4e0d\u76f8\u4e92\u4f5c\u7528\u540c\u6837\u91cd\u8981\uff0c\u8fd9\u6709\u52a9\u4e8e\u66f4\u5b89\u5168\u7684\u5904\u65b9\u548c\u66f4\u597d\u7684\u60a3\u8005\u9884\u540e\u3002", "method": "\u7ed3\u5408Mol2Vec\u548cSMILES-BERT\u4e24\u79cd\u5206\u5b50\u5d4c\u5165\uff0c\u4ee5\u53ca\u57fa\u4e8e\u89c4\u5219\u7684\u4e34\u5e8a\u8bc4\u5206\uff0c\u4f7f\u7528\u4e09\u9636\u6bb5\u5143\u542f\u53d1\u5f0f\u7b56\u7565\u4f18\u5316\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\u3002", "result": "\u5728DrugBank\u6570\u636e\u96c6\u4e0a\u83b7\u5f97ROC-AUC 0.911\u548cPR-AUC 0.867\u7684\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u57282\u578b\u7cd6\u5c3f\u75c5\u961f\u5217\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u4e34\u5e8a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\uff0c\u652f\u6301\u66f4\u5b89\u5168\u7684\u836f\u7269\u6cbb\u7597\u548c\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2510.10042", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10042", "abs": "https://arxiv.org/abs/2510.10042", "authors": ["Saleh Nikooroo", "Thomas Engel"], "title": "Belief Graphs with Reasoning Zones: Structure, Dynamics, and Epistemic Activation", "comment": null, "summary": "Belief systems are rarely globally consistent, yet effective reasoning often\npersists locally. We propose a novel graph-theoretic framework that cleanly\nseparates credibility--external, a priori trust in sources--from confidence--an\ninternal, emergent valuation induced by network structure. Beliefs are nodes in\na directed, signed, weighted graph whose edges encode support and\ncontradiction. Confidence is obtained by a contractive propagation process that\nmixes a stated prior with structure-aware influence and guarantees a unique,\nstable solution. Within this dynamics, we define reasoning zones:\nhigh-confidence, structurally balanced subgraphs on which classical inference\nis safe despite global contradictions. We provide a near-linear procedure that\nseeds zones by confidence, tests balance using a parity-based coloring, and\napplies a greedy, locality-preserving repair with Jaccard de-duplication to\nbuild a compact atlas. To model belief change, we introduce shock updates that\nlocally downscale support and elevate targeted contradictions while preserving\ncontractivity via a simple backtracking rule. Re-propagation yields localized\nreconfiguration-zones may shrink, split, or collapse--without destabilizing the\nentire graph. We outline an empirical protocol on synthetic signed graphs with\nplanted zones, reporting zone recovery, stability under shocks, and runtime.\nThe result is a principled foundation for contradiction-tolerant reasoning that\nactivates classical logic precisely where structure supports it.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u56fe\u8bba\u6846\u67b6\uff0c\u5c06\u53ef\u4fe1\u5ea6\u4e0e\u7f6e\u4fe1\u5ea6\u5206\u79bb\uff0c\u901a\u8fc7\u6536\u7f29\u4f20\u64ad\u8fc7\u7a0b\u8ba1\u7b97\u7f6e\u4fe1\u5ea6\uff0c\u5b9a\u4e49\u63a8\u7406\u533a\u57df\u4f5c\u4e3a\u9ad8\u7f6e\u4fe1\u5ea6\u3001\u7ed3\u6784\u5e73\u8861\u7684\u5b50\u56fe\uff0c\u5e76\u5f15\u5165\u51b2\u51fb\u66f4\u65b0\u6765\u5efa\u6a21\u4fe1\u5ff5\u53d8\u5316\u3002", "motivation": "\u4fe1\u5ff5\u7cfb\u7edf\u901a\u5e38\u5b58\u5728\u5168\u5c40\u4e0d\u4e00\u81f4\u6027\uff0c\u4f46\u5c40\u90e8\u63a8\u7406\u4ecd\u7136\u6709\u6548\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5bb9\u5fcd\u77db\u76fe\u3001\u5728\u7ed3\u6784\u652f\u6301\u7684\u5730\u65b9\u6fc0\u6d3b\u7ecf\u5178\u903b\u8f91\u7684\u63a8\u7406\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u6709\u5411\u3001\u5e26\u7b26\u53f7\u3001\u52a0\u6743\u7684\u56fe\u8868\u793a\u4fe1\u5ff5\uff0c\u8282\u70b9\u4e3a\u4fe1\u5ff5\uff0c\u8fb9\u7f16\u7801\u652f\u6301\u548c\u77db\u76fe\u5173\u7cfb\u3002\u901a\u8fc7\u6536\u7f29\u4f20\u64ad\u8fc7\u7a0b\u8ba1\u7b97\u7f6e\u4fe1\u5ea6\uff0c\u5b9a\u4e49\u63a8\u7406\u533a\u57df\u4e3a\u9ad8\u7f6e\u4fe1\u5ea6\u3001\u7ed3\u6784\u5e73\u8861\u7684\u5b50\u56fe\uff0c\u5e76\u63d0\u4f9b\u8fd1\u7ebf\u6027\u7b97\u6cd5\u6784\u5efa\u533a\u57df\u56fe\u8c31\u3002\u5f15\u5165\u51b2\u51fb\u66f4\u65b0\u6765\u5efa\u6a21\u4fe1\u5ff5\u53d8\u5316\u3002", "result": "\u5728\u5408\u6210\u5e26\u7b26\u53f7\u56fe\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u62a5\u544a\u4e86\u533a\u57df\u6062\u590d\u3001\u51b2\u51fb\u4e0b\u7684\u7a33\u5b9a\u6027\u4ee5\u53ca\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "\u4e3a\u5bb9\u5fcd\u77db\u76fe\u7684\u63a8\u7406\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u5728\u7ed3\u6784\u652f\u6301\u7684\u5730\u65b9\u7cbe\u786e\u6fc0\u6d3b\u7ecf\u5178\u903b\u8f91\u3002"}}
{"id": "2510.11546", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.11546", "abs": "https://arxiv.org/abs/2510.11546", "authors": ["Meixia Lin", "Meijiao Shi", "Yunhai Xiao", "Qian Zhang"], "title": "Efficient Group Lasso Regularized Rank Regression with Data-Driven Parameter Determination", "comment": "36 pages, 4 figures, 8 tables", "summary": "High-dimensional regression often suffers from heavy-tailed noise and\noutliers, which can severely undermine the reliability of least-squares based\nmethods. To improve robustness, we adopt a non-smooth Wilcoxon score based rank\nobjective and incorporate structured group sparsity regularization, a natural\ngeneralization of the lasso, yielding a group lasso regularized rank regression\nmethod. By extending the tuning-free parameter selection scheme originally\ndeveloped for the lasso, we introduce a data-driven, simulation-based tuning\nrule and further establish a finite-sample error bound for the resulting\nestimator. On the computational side, we develop a proximal augmented\nLagrangian method for solving the associated optimization problem, which\neliminates the singularity issues encountered in existing methods, thereby\nenabling efficient semismooth Newton updates for the subproblems. Extensive\nnumerical experiments demonstrate the robustness and effectiveness of our\nproposed estimator against alternatives, and showcase the scalability of the\nalgorithm across both simulated and real-data settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWilcoxon\u79e9\u5f97\u5206\u7684\u7ec4lasso\u6b63\u5219\u5316\u79e9\u56de\u5f52\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u9ad8\u7ef4\u56de\u5f52\u4e2d\u7684\u91cd\u5c3e\u566a\u58f0\u548c\u5f02\u5e38\u503c\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u4f18\u5316\u7b97\u6cd5\u3002", "motivation": "\u9ad8\u7ef4\u56de\u5f52\u7ecf\u5e38\u53d7\u5230\u91cd\u5c3e\u566a\u58f0\u548c\u5f02\u5e38\u503c\u7684\u5f71\u54cd\uff0c\u8fd9\u4f1a\u4e25\u91cd\u524a\u5f31\u57fa\u4e8e\u6700\u5c0f\u4e8c\u4e58\u6cd5\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u3002\u4e3a\u4e86\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u62b5\u6297\u5f02\u5e38\u503c\u7684\u7a33\u5065\u56de\u5f52\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u975e\u5149\u6ed1Wilcoxon\u79e9\u5f97\u5206\u76ee\u6807\u51fd\u6570\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u7ec4\u7a00\u758f\u6b63\u5219\u5316\uff08\u7ec4lasso\u7684\u63a8\u5e7f\uff09\uff0c\u63d0\u51fa\u4e86\u7ec4lasso\u6b63\u5219\u5316\u79e9\u56de\u5f52\u65b9\u6cd5\u3002\u5f00\u53d1\u4e86\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u8c03\u53c2\u89c4\u5219\u548c\u8fd1\u7aef\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u5efa\u7acb\u4e86\u6240\u5f97\u4f30\u8ba1\u91cf\u7684\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\uff0c\u6570\u503c\u5b9e\u9a8c\u8868\u660e\u6240\u63d0\u4f30\u8ba1\u91cf\u5728\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u65b9\u9762\u4f18\u4e8e\u66ff\u4ee3\u65b9\u6cd5\uff0c\u7b97\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u573a\u666f\u4e2d\u5747\u5c55\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u9ad8\u7ef4\u56de\u5f52\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u79e9\u56de\u5f52\u548c\u7ec4\u7a00\u758f\u6b63\u5219\u5316\u7684\u7ed3\u5408\uff0c\u4ee5\u53ca\u9ad8\u6548\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u4e3a\u5904\u7406\u91cd\u5c3e\u566a\u58f0\u548c\u5f02\u5e38\u503c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10411", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.10411", "abs": "https://arxiv.org/abs/2510.10411", "authors": ["Ilias Mitrai"], "title": "Discovering interpretable piecewise nonlinear model predictive control laws via symbolic decision trees", "comment": null, "summary": "In this paper, we propose symbolic decision trees as surrogate models for\napproximating model predictive control laws. The proposed approach learns\nsimultaneously the partition of the input domain (splitting logic) as well as\nlocal nonlinear expressions for predicting the control action leading to\ninterpretable piecewise nonlinear control laws. The local nonlinear expressions\nare determined by the learning problem and are modeled using a set of basis\nfunctions. The learning task is posed as a mixed integer optimization, which is\nsolved to global optimality with state-of-the-art global optimization solvers.\nWe apply the proposed approach to a case study regarding the control of an\nisothermal reactor. The results show that the proposed approach can learn the\ncontrol law accurately, leading to closed-loop performance comparable to that\nof a standard model predictive controller. Finally, comparison with existing\ninterpretable models shows that the symbolic trees achieve both lower\nprediction error and superior closed-loop performance.", "AI": {"tldr": "\u63d0\u51fa\u7b26\u53f7\u51b3\u7b56\u6811\u4f5c\u4e3a\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5f8b\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u6574\u6570\u4f18\u5316\u5b66\u4e60\u8f93\u5165\u57df\u5212\u5206\u548c\u5c40\u90e8\u975e\u7ebf\u6027\u8868\u8fbe\u5f0f\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u5206\u6bb5\u975e\u7ebf\u6027\u63a7\u5236\u5f8b\u3002", "motivation": "\u4e3a\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u540c\u65f6\u5b66\u4e60\u8f93\u5165\u57df\u5212\u5206\u548c\u5c40\u90e8\u975e\u7ebf\u6027\u63a7\u5236\u8868\u8fbe\u5f0f\uff0c\u63d0\u9ad8\u63a7\u5236\u5f8b\u7684\u900f\u660e\u5ea6\u548c\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u7b26\u53f7\u51b3\u7b56\u6811\u4f5c\u4e3a\u66ff\u4ee3\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u6574\u6570\u4f18\u5316\u95ee\u9898\u540c\u65f6\u5b66\u4e60\u8f93\u5165\u57df\u5212\u5206\uff08\u5206\u88c2\u903b\u8f91\uff09\u548c\u5c40\u90e8\u975e\u7ebf\u6027\u8868\u8fbe\u5f0f\uff0c\u4f7f\u7528\u57fa\u51fd\u6570\u5efa\u6a21\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u5e76\u7528\u5168\u5c40\u4f18\u5316\u6c42\u89e3\u5668\u6c42\u89e3\u3002", "result": "\u5728\u7b49\u6e29\u53cd\u5e94\u5668\u63a7\u5236\u6848\u4f8b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u5b66\u4e60\u63a7\u5236\u5f8b\uff0c\u95ed\u73af\u6027\u80fd\u4e0e\u6807\u51c6\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\u76f8\u5f53\uff0c\u4e14\u6bd4\u73b0\u6709\u53ef\u89e3\u91ca\u6a21\u578b\u5177\u6709\u66f4\u4f4e\u7684\u9884\u6d4b\u8bef\u5dee\u548c\u66f4\u597d\u7684\u95ed\u73af\u6027\u80fd\u3002", "conclusion": "\u7b26\u53f7\u51b3\u7b56\u6811\u65b9\u6cd5\u80fd\u6709\u6548\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5f8b\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u5206\u6bb5\u975e\u7ebf\u6027\u63a7\u5236\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u63a7\u5236\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u3002"}}
{"id": "2510.10520", "categories": ["cs.CY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10520", "abs": "https://arxiv.org/abs/2510.10520", "authors": ["Fuze Sun", "Paul Craig", "Lingyu Li", "Shixiangyue Meng", "Chuxi Nan"], "title": "AI-Agents for Culturally Diverse Online Higher Education Environments", "comment": null, "summary": "As the global reach of online higher education continues to grow,\nuniversities are increasingly accommodating students from diverse cultural\nbackgrounds \\parencite{tereshko2024culturally}. This can present a number of\nchallenges including linguistic barriers \\parencite{ullah2021linguistic},\ncultural differences in learning style \\parencite{omidvar2012cultural},\ncultural sensitivity in course design \\parencite{nguyen2022cultural} and\nperceived isolation when students feel their perspectives or experiences are\nnot reflected or valued in the learning environment\n\\parencite{hansen2022belonging}. Ensuring active engagement and reasonable\nlearning outcomes in such a environments requires distance educational systems\nthat are not only adaptive but also culturally resonant\n\\parencite{dalle2024cultural}. Both embodied and virtual AI-Agents have great\npotential in this regard as they can facilitate personalized learning and adapt\ntheir interactions and content delivery to align with students' cultural\ncontext. In addition Generative AI (GAI), such as, Large Language Models (LLMs)\ncan amplify the potential for these culturally aware AI agents to address\neducational challenges due to their advanced capacity for understanding and\ngenerating contextually relevant content \\parencite{wang2024large}. This\nchapter reviews existing research and suggests the usage of culturally aware\nAI-Agents, powered by GAI, to foster engagement and improve learning outcomes\nin culturally diverse online higher education environments.", "AI": {"tldr": "\u672c\u7ae0\u63a2\u8ba8\u4e86\u5728\u6587\u5316\u591a\u5143\u7684\u5728\u7ebf\u9ad8\u7b49\u6559\u80b2\u73af\u5883\u4e2d\u4f7f\u7528\u6587\u5316\u611f\u77e5AI\u4ee3\u7406\uff08\u7531\u751f\u6210\u5f0fAI\u9a71\u52a8\uff09\u6765\u4fc3\u8fdb\u53c2\u4e0e\u5ea6\u548c\u6539\u5584\u5b66\u4e60\u6210\u679c\u3002", "motivation": "\u968f\u7740\u5728\u7ebf\u9ad8\u7b49\u6559\u80b2\u7684\u5168\u7403\u5316\u53d1\u5c55\uff0c\u5927\u5b66\u9700\u8981\u5bb9\u7eb3\u6765\u81ea\u4e0d\u540c\u6587\u5316\u80cc\u666f\u7684\u5b66\u751f\uff0c\u8fd9\u5e26\u6765\u4e86\u8bed\u8a00\u969c\u788d\u3001\u5b66\u4e60\u98ce\u683c\u5dee\u5f02\u3001\u8bfe\u7a0b\u8bbe\u8ba1\u6587\u5316\u654f\u611f\u6027\u4ee5\u53ca\u5b66\u751f\u5b64\u7acb\u611f\u7b49\u6311\u6218\u3002", "method": "\u901a\u8fc7\u56de\u987e\u73b0\u6709\u7814\u7a76\uff0c\u63d0\u51fa\u4f7f\u7528\u7531\u751f\u6210\u5f0fAI\uff08\u5982\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u9a71\u52a8\u7684\u6587\u5316\u611f\u77e5AI\u4ee3\u7406\uff0c\u8fd9\u4e9b\u4ee3\u7406\u80fd\u591f\u4e2a\u6027\u5316\u5b66\u4e60\u5e76\u8c03\u6574\u4e92\u52a8\u65b9\u5f0f\u548c\u5185\u5bb9\u4f20\u9012\u4ee5\u7b26\u5408\u5b66\u751f\u7684\u6587\u5316\u80cc\u666f\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6587\u5316\u611f\u77e5AI\u4ee3\u7406\u6709\u6f5c\u529b\u901a\u8fc7\u63d0\u4f9b\u4e0e\u6587\u5316\u80cc\u666f\u76f8\u5173\u7684\u4e2a\u6027\u5316\u5b66\u4e60\u4f53\u9a8c\u6765\u89e3\u51b3\u5728\u7ebf\u6559\u80b2\u4e2d\u7684\u6587\u5316\u591a\u6837\u6027\u6311\u6218\u3002", "conclusion": "\u6587\u5316\u611f\u77e5AI\u4ee3\u7406\uff0c\u7279\u522b\u662f\u7531\u751f\u6210\u5f0fAI\u9a71\u52a8\u7684\u4ee3\u7406\uff0c\u80fd\u591f\u6709\u6548\u4fc3\u8fdb\u6587\u5316\u591a\u5143\u5728\u7ebf\u9ad8\u7b49\u6559\u80b2\u73af\u5883\u4e2d\u7684\u5b66\u751f\u53c2\u4e0e\u5ea6\u548c\u5b66\u4e60\u6210\u679c\u3002"}}
{"id": "2510.10599", "categories": ["math.OC", "68T07, 91G20"], "pdf": "https://arxiv.org/pdf/2510.10599", "abs": "https://arxiv.org/abs/2510.10599", "authors": ["Bahadur Yadav", "Sanjay Kumar Mohanty"], "title": "Hybrid Ridgelet Deep Neural Networks for Data-Driven Arbitrage Strategies", "comment": null, "summary": "In this study, we propose a novel model framework that integrates deep neural\nnetworks with the Ridgelet Transform. The Ridgelet Transform on Borel\nmeasurable functions is used for arbitrage detection on high-dimensional sparse\nstructures. This transform also enhances the expressive power of neural\nnetworks, enabling them to capture complex and high-dimensional market\nstructures. Theoretically, we determine profitable trading strategies by\noptimizing hybrid ridgelet deep neural networks. Further, we emphasize the role\nof activation functions in ensuring stability and adaptability under\nuncertainty. We use a high-performance computing cluster for the detection of\narbitrage across multiple assets, ensuring scalability, and processing\nlarge-scale financial data. Empirical results demonstrate strong profitability\nacross diverse scenarios involving up to 50 assets, with particularly robust\nperformance during periods of market volatility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548cRidgelet\u53d8\u6362\u7684\u65b0\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u7ef4\u7a00\u758f\u7ed3\u6784\u4e2d\u7684\u5957\u5229\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u6df7\u5408ridgelet\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6765\u786e\u5b9a\u76c8\u5229\u4ea4\u6613\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u9ad8\u7ef4\u5e02\u573a\u4e2d\u7684\u590d\u6742\u7ed3\u6784\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc\u8868\u8fbe\u80fd\u529b\u5e76\u5904\u7406\u9ad8\u7ef4\u7a00\u758f\u6570\u636e\u7684\u6846\u67b6\u6765\u8fdb\u884c\u6709\u6548\u7684\u5957\u5229\u68c0\u6d4b\u3002", "method": "\u4f7f\u7528Ridgelet\u53d8\u6362\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6784\u5efa\u6df7\u5408\u6a21\u578b\uff0c\u4f18\u5316\u6fc0\u6d3b\u51fd\u6570\u4ee5\u786e\u4fdd\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\uff0c\u5e76\u5229\u7528\u9ad8\u6027\u80fd\u8ba1\u7b97\u96c6\u7fa4\u5904\u7406\u5927\u89c4\u6a21\u91d1\u878d\u6570\u636e\u3002", "result": "\u5728\u6d89\u53ca\u591a\u8fbe50\u79cd\u8d44\u4ea7\u7684\u4e0d\u540c\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u52b2\u7684\u76c8\u5229\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5e02\u573a\u6ce2\u52a8\u671f\u95f4\u8868\u73b0\u5c24\u4e3a\u7a33\u5065\u3002", "conclusion": "\u8be5\u96c6\u6210\u6846\u67b6\u6210\u529f\u63d0\u5347\u4e86\u5728\u9ad8\u7ef4\u7a00\u758f\u5e02\u573a\u7ed3\u6784\u4e2d\u68c0\u6d4b\u5957\u5229\u673a\u4f1a\u7684\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u91d1\u878d\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.09771", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09771", "abs": "https://arxiv.org/abs/2510.09771", "authors": ["Rakib Hossan", "Shubhashis Roy Dipta"], "title": "PromptGuard at BLP-2025 Task 1: A Few-Shot Classification Framework Using Majority Voting and Keyword Similarity for Bengali Hate Speech Detection", "comment": null, "summary": "The BLP-2025 Task 1A requires Bengali hate speech classification into six\ncategories. Traditional supervised approaches need extensive labeled datasets\nthat are expensive for low-resource languages. We developed PromptGuard, a\nfew-shot framework combining chi-square statistical analysis for keyword\nextraction with adaptive majority voting for decision-making. We explore\nstatistical keyword selection versus random approaches and adaptive voting\nmechanisms that extend classification based on consensus quality. Chi-square\nkeywords provide consistent improvements across categories, while adaptive\nvoting benefits ambiguous cases requiring extended classification rounds.\nPromptGuard achieves a micro-F1 of 67.61, outperforming n-gram baselines\n(60.75) and random approaches (14.65). Ablation studies confirm\nchi-square-based keywords show the most consistent impact across all\ncategories.", "AI": {"tldr": "PromptGuard\u662f\u4e00\u4e2a\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u4ec7\u6068\u8a00\u8bba\u5206\u7c7b\u7684\u5c11\u6837\u672c\u6846\u67b6\uff0c\u7ed3\u5408\u5361\u65b9\u7edf\u8ba1\u5206\u6790\u548c\u81ea\u9002\u5e94\u591a\u6570\u6295\u7968\u673a\u5236\uff0c\u5728BLP-2025\u4efb\u52a11A\u4e2d\u53d6\u5f9767.61\u7684\u5faeF1\u5206\u6570\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u8fd9\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u6765\u8bf4\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u5c11\u6837\u672c\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u5361\u65b9\u7edf\u8ba1\u8fdb\u884c\u5173\u952e\u8bcd\u63d0\u53d6\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u591a\u6570\u6295\u7968\u673a\u5236\u8fdb\u884c\u51b3\u7b56\uff0c\u63a2\u7d22\u7edf\u8ba1\u5173\u952e\u8bcd\u9009\u62e9\u4e0e\u968f\u673a\u65b9\u6cd5\u7684\u5bf9\u6bd4\u3002", "result": "PromptGuard\u8fbe\u523067.61\u7684\u5faeF1\u5206\u6570\uff0c\u4f18\u4e8en-gram\u57fa\u7ebf(60.75)\u548c\u968f\u673a\u65b9\u6cd5(14.65)\uff0c\u5361\u65b9\u5173\u952e\u8bcd\u5728\u6240\u6709\u7c7b\u522b\u4e2d\u8868\u73b0\u6700\u4e00\u81f4\u3002", "conclusion": "\u5361\u65b9\u5173\u952e\u8bcd\u63d0\u53d6\u63d0\u4f9b\u8de8\u7c7b\u522b\u7684\u6301\u7eed\u6539\u8fdb\uff0c\u81ea\u9002\u5e94\u6295\u7968\u673a\u5236\u5bf9\u9700\u8981\u6269\u5c55\u5206\u7c7b\u8f6e\u6b21\u7684\u6a21\u7cca\u6848\u4f8b\u6709\u76ca\u3002"}}
{"id": "2510.09669", "categories": ["cs.LG", "cs.CY", "cs.SI", "physics.soc-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09669", "abs": "https://arxiv.org/abs/2510.09669", "authors": ["Jacopo Lenti", "Lorenzo Costantini", "Ariadna Fosch", "Anna Monticelli", "David Scala", "Marco Pangallo"], "title": "Population synthesis with geographic coordinates", "comment": null, "summary": "It is increasingly important to generate synthetic populations with explicit\ncoordinates rather than coarse geographic areas, yet no established methods\nexist to achieve this. One reason is that latitude and longitude differ from\nother continuous variables, exhibiting large empty spaces and highly uneven\ndensities. To address this, we propose a population synthesis algorithm that\nfirst maps spatial coordinates into a more regular latent space using\nNormalizing Flows (NF), and then combines them with other features in a\nVariational Autoencoder (VAE) to generate synthetic populations. This approach\nalso learns the joint distribution between spatial and non-spatial features,\nexploiting spatial autocorrelations. We demonstrate the method by generating\nsynthetic homes with the same statistical properties of real homes in 121\ndatasets, corresponding to diverse geographies. We further propose an\nevaluation framework that measures both spatial accuracy and practical utility,\nwhile ensuring privacy preservation. Our results show that the NF+VAE\narchitecture outperforms popular benchmarks, including copula-based methods and\nuniform allocation within geographic areas. The ability to generate geolocated\nsynthetic populations at fine spatial resolution opens the door to applications\nrequiring detailed geography, from household responses to floods, to epidemic\nspread, evacuation planning, and transport modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f52\u4e00\u5316\u6d41\u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u7a7a\u95f4\u4eba\u53e3\u5408\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u7cbe\u786e\u5750\u6807\u7684\u5408\u6210\u4eba\u53e3\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7a7a\u95f4\u5750\u6807\u7279\u6b8a\u5206\u5e03\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u751f\u6210\u5177\u6709\u660e\u786e\u5750\u6807\u7684\u5408\u6210\u4eba\u53e3\u6570\u636e\uff0c\u800c\u7a7a\u95f4\u5750\u6807\u5177\u6709\u5927\u8303\u56f4\u7a7a\u533a\u548c\u9ad8\u5ea6\u4e0d\u5747\u5300\u5bc6\u5ea6\u7684\u7279\u70b9\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u5904\u7406\u3002", "method": "\u4f7f\u7528\u5f52\u4e00\u5316\u6d41\u5c06\u7a7a\u95f4\u5750\u6807\u6620\u5c04\u5230\u66f4\u89c4\u5219\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u7136\u540e\u4e0e\u5176\u5b83\u7279\u5f81\u5728\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4e2d\u7ed3\u5408\u751f\u6210\u5408\u6210\u4eba\u53e3\uff0c\u540c\u65f6\u5b66\u4e60\u7a7a\u95f4\u548c\u975e\u7a7a\u95f4\u7279\u5f81\u7684\u8054\u5408\u5206\u5e03\u3002", "result": "\u5728121\u4e2a\u4e0d\u540c\u5730\u7406\u533a\u57df\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cNF+VAE\u67b6\u6784\u4f18\u4e8e\u6d41\u884c\u7684\u57fa\u51c6\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8ecopula\u7684\u65b9\u6cd5\u548c\u5730\u7406\u533a\u57df\u5185\u5747\u5300\u5206\u914d\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u7684\u5408\u6210\u5730\u7406\u4f4d\u7f6e\u4eba\u53e3\u6570\u636e\uff0c\u4e3a\u9700\u8981\u8be6\u7ec6\u5730\u7406\u4fe1\u606f\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u9053\u8def\uff0c\u5982\u6d2a\u6c34\u54cd\u5e94\u3001\u6d41\u884c\u75c5\u4f20\u64ad\u3001\u758f\u6563\u89c4\u5212\u548c\u4ea4\u901a\u5efa\u6a21\u7b49\u3002"}}
{"id": "2510.10047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10047", "abs": "https://arxiv.org/abs/2510.10047", "authors": ["Ruohao Li", "Hongjun Liu", "Leyi Zhao", "Zisu Li", "Jiawei Li", "Jiajun Jiang", "Linning Xu", "Chen Zhao", "Mingming Fan", "Chen Liang"], "title": "SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning", "comment": "14 pages, 7 figures", "summary": "Large language model (LLM) agents have shown remarkable reasoning abilities.\nHowever, existing multi-agent frameworks often rely on fixed roles or\ncentralized control, limiting scalability and adaptability in long-horizon\nreasoning. We introduce SwarmSys, a closed-loop framework for distributed\nmulti-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys\nemerges through iterative interactions among three specialized roles,\nExplorers, Workers, and Validators, that continuously cycle through\nexploration, exploitation, and validation. To enable scalable and adaptive\ncollaboration, we integrate adaptive agent and event profiles, embedding-based\nprobabilistic matching, and a pheromone-inspired reinforcement mechanism,\nsupporting dynamic task allocation and self-organizing convergence without\nglobal supervision. Across symbolic reasoning, research synthesis, and\nscientific programming tasks, SwarmSys consistently outperforms baselines,\nimproving both accuracy and reasoning stability. These findings highlight\nswarm-inspired coordination as a promising paradigm for scalable, robust, and\nadaptive multi-agent reasoning, suggesting that coordination scaling may rival\nmodel scaling in advancing LLM intelligence.", "AI": {"tldr": "SwarmSys\u662f\u4e00\u4e2a\u53d7\u7fa4\u4f53\u667a\u80fd\u542f\u53d1\u7684\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u63a2\u7d22\u8005\u3001\u5de5\u4f5c\u8005\u548c\u9a8c\u8bc1\u8005\u4e09\u4e2a\u89d2\u8272\u7684\u8fed\u4ee3\u4ea4\u4e92\u5b9e\u73b0\u534f\u8c03\uff0c\u65e0\u9700\u5168\u5c40\u76d1\u7763\u5373\u53ef\u5b9e\u73b0\u52a8\u6001\u4efb\u52a1\u5206\u914d\u548c\u81ea\u7ec4\u7ec7\u6536\u655b\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u6846\u67b6\u4f9d\u8d56\u56fa\u5b9a\u89d2\u8272\u6216\u96c6\u4e2d\u63a7\u5236\uff0c\u9650\u5236\u4e86\u957f\u65f6\u7a0b\u63a8\u7406\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u96c6\u6210\u81ea\u9002\u5e94\u667a\u80fd\u4f53\u548c\u4e8b\u4ef6\u6863\u6848\u3001\u57fa\u4e8e\u5d4c\u5165\u7684\u6982\u7387\u5339\u914d\u4ee5\u53ca\u4fe1\u606f\u7d20\u542f\u53d1\u7684\u5f3a\u5316\u673a\u5236\uff0c\u652f\u6301\u52a8\u6001\u4efb\u52a1\u5206\u914d\u3002", "result": "\u5728\u7b26\u53f7\u63a8\u7406\u3001\u7814\u7a76\u7efc\u5408\u548c\u79d1\u5b66\u7f16\u7a0b\u4efb\u52a1\u4e2d\uff0cSwarmSys\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u63a8\u7406\u7a33\u5b9a\u6027\u3002", "conclusion": "\u7fa4\u4f53\u542f\u53d1\u7684\u534f\u8c03\u662f\u63a8\u8fdbLLM\u667a\u80fd\u7684\u6709\u524d\u666f\u8303\u5f0f\uff0c\u534f\u8c03\u6269\u5c55\u53ef\u80fd\u4e0e\u6a21\u578b\u6269\u5c55\u540c\u7b49\u91cd\u8981\u3002"}}
{"id": "2510.10442", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.10442", "abs": "https://arxiv.org/abs/2510.10442", "authors": ["Pei Yu Chang", "Vishnu Renganathan", "Qadeer Ahmed"], "title": "Risk-Budgeted Control Framework for Balanced Performance and Safety in Autonomous Vehicles", "comment": null, "summary": "This paper presents a risk-budgeted monitor with a control framework that\ncertifies safety for autonomous driving. In this process, a sliding window is\nproposed to monitor for insufficient barrier residuals or nonzero tail risk,\nensuring system safety. When the safety margin deteriorates, it triggers\nswitching the safety constraint from a performance-based relaxed-control\nbarrier function (R-CBF) to a conservative conditional value at risk (CVaR-CBF)\nto address the safety concern. This switching is governed by two real-time\ntriggers: Feasibility-Triggered (FT) and Quality-Triggered (QT) conditions. In\nthe FT condition, if the R-CBF constraint becomes infeasible or yields a\nsuboptimal solution, the risk monitor triggers the use of the CVaR constraints\nfor the controller. In the QT condition, the risk monitor observes the safety\nmargin of the R-CBF solution at every step, regardless of feasibility. If it\nfalls below the safety margin, the safety filter switches to the CVaR-CBF\nconstraints.\n  The proposed framework is evaluated using a model predictive controller (MPC)\nfor autonomous driving in the presence of autonomous vehicle (AV) localization\nnoise and obstacle position uncertainties. Multiple AV-pedestrian interaction\nscenarios are considered, with 1,500 Monte Carlo runs conducted for all\nscenarios. In the most challenging setting with pedestrian detection\nuncertainty of 5 m, the proposed framework achieves a 94-96% success rate of\nnot colliding with the pedestrians over 300 trials while maintaining the lowest\nmean cross-track error (CTE = 3.2-3.6 m) to the reference path. The reduced CTE\nindicates faster trajectory recovery after obstacle avoidance, demonstrating a\nbalance between safety and performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u98ce\u9669\u9884\u7b97\u76d1\u63a7\u5668\u4e0e\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u76d1\u6d4b\u5b89\u5168\u88d5\u5ea6\uff0c\u5728\u5b89\u5168\u88d5\u5ea6\u6076\u5316\u65f6\u4ece\u6027\u80fd\u5bfc\u5411\u7684R-CBF\u5207\u6362\u5230\u4fdd\u5b88\u7684CVaR-CBF\u7ea6\u675f\uff0c\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u5728\u5b58\u5728\u5b9a\u4f4d\u566a\u58f0\u548c\u969c\u788d\u7269\u4f4d\u7f6e\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u5728\u4fdd\u8bc1\u5b89\u5168\u7684\u540c\u65f6\u7ef4\u6301\u6027\u80fd\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u76d1\u63a7\u5b89\u5168\u88d5\u5ea6\uff0c\u8bbe\u8ba1\u53ef\u884c\u6027\u89e6\u53d1(FT)\u548c\u8d28\u91cf\u89e6\u53d1(QT)\u4e24\u79cd\u5b9e\u65f6\u5207\u6362\u673a\u5236\uff0c\u5728R-CBF\u7ea6\u675f\u4e0d\u53ef\u884c\u6216\u5b89\u5168\u88d5\u5ea6\u4e0d\u8db3\u65f6\u5207\u6362\u5230CVaR-CBF\u7ea6\u675f\u3002", "result": "\u5728\u6700\u6311\u6218\u76845\u7c73\u884c\u4eba\u68c0\u6d4b\u4e0d\u786e\u5b9a\u6027\u573a\u666f\u4e0b\uff0c300\u6b21\u8bd5\u9a8c\u4e2d\u8fbe\u523094-96%\u7684\u65e0\u78b0\u649e\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u4f4e\u7684\u5e73\u5747\u6a2a\u5411\u8ddf\u8e2a\u8bef\u5dee(3.2-3.6\u7c73)\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5b89\u5168\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u5b9e\u73b0\u4e86\u826f\u597d\u5e73\u8861\uff0c\u80fd\u591f\u5728\u969c\u788d\u7269\u907f\u8ba9\u540e\u5feb\u901f\u6062\u590d\u8f68\u8ff9\uff0c\u6709\u6548\u5904\u7406\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2510.10588", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.10588", "abs": "https://arxiv.org/abs/2510.10588", "authors": ["Weina Jin", "Elise Li Zheng", "Ghassan Hamarneh"], "title": "Making Power Explicable in AI: Analyzing, Understanding, and Redirecting Power to Operationalize Ethics in AI Technical Practice", "comment": null, "summary": "The operationalization of ethics in the technical practices of artificial\nintelligence (AI) is facing significant challenges. To address the problem of\nineffective implementation of AI ethics, we present our diagnosis, analysis,\nand interventional recommendations from a unique perspective of the real-world\nimplementation of AI ethics through explainable AI (XAI) techniques. We first\ndescribe the phenomenon (i.e., the \"symptoms\") of ineffective implementation of\nAI ethics in explainable AI using four empirical cases. From the \"symptoms\", we\ndiagnose the root cause (i.e., the \"disease\") being the dysfunction and\nimbalance of power structures in the sociotechnical system of AI. The power\nstructures are dominated by unjust and unchecked power that does not represent\nthe benefits and interests of the public and the most impacted communities, and\ncannot be countervailed by ethical power. Based on the understanding of power\nmechanisms, we propose three interventional recommendations to tackle the root\ncause, including: 1) Making power explicable and checked, 2) Reframing the\nnarratives and assumptions of AI and AI ethics to check unjust power and\nreflect the values and benefits of the public, and 3) Uniting the efforts of\nethical and scientific conduct of AI to encode ethical values as technical\nstandards, norms, and methods, including conducting critical examinations and\nlimitation analyses of AI technical practices. We hope that our diagnosis and\ninterventional recommendations can be a useful input to the AI community and\ncivil society's ongoing discussion and implementation of ethics in AI for\nethical and responsible AI practice.", "AI": {"tldr": "\u672c\u6587\u4ece\u53ef\u89e3\u91caAI\u89d2\u5ea6\u5206\u6790AI\u4f26\u7406\u5b9e\u65bd\u5931\u6548\u95ee\u9898\uff0c\u8bca\u65ad\u5176\u6839\u6e90\u4e3aAI\u793e\u4f1a\u6280\u672f\u7cfb\u7edf\u4e2d\u6743\u529b\u7ed3\u6784\u5931\u8861\uff0c\u5e76\u63d0\u51fa\u4e09\u9879\u5e72\u9884\u5efa\u8bae\u6765\u5e94\u5bf9\u6743\u529b\u5931\u8861\u95ee\u9898\u3002", "motivation": "AI\u4f26\u7406\u5728\u6280\u672f\u5b9e\u8df5\u4e2d\u7684\u64cd\u4f5c\u5316\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3AI\u4f26\u7406\u5b9e\u65bd\u65e0\u6548\u7684\u95ee\u9898\u3002\u4f5c\u8005\u4ece\u53ef\u89e3\u91caAI\u5728\u73b0\u5b9e\u4e16\u754c\u5b9e\u65bd\u4f26\u7406\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u5206\u6790\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u9996\u5148\u901a\u8fc7\u56db\u4e2a\u5b9e\u8bc1\u6848\u4f8b\u63cf\u8ff0AI\u4f26\u7406\u5728\u53ef\u89e3\u91caAI\u4e2d\u5b9e\u65bd\u65e0\u6548\u7684\"\u75c7\u72b6\"\uff0c\u7136\u540e\u8bca\u65ad\u5176\u6839\u6e90\u4e3a\u6743\u529b\u7ed3\u6784\u5931\u8861\uff0c\u6700\u540e\u57fa\u4e8e\u5bf9\u6743\u529b\u673a\u5236\u7684\u7406\u89e3\u63d0\u51fa\u4e09\u9879\u5e72\u9884\u5efa\u8bae\u3002", "result": "\u8bca\u65ad\u51faAI\u4f26\u7406\u5b9e\u65bd\u65e0\u6548\u7684\u6839\u6e90\u5728\u4e8eAI\u793e\u4f1a\u6280\u672f\u7cfb\u7edf\u4e2d\u6743\u529b\u7ed3\u6784\u7684\u529f\u80fd\u5931\u8c03\u548c\u5931\u8861\uff0c\u8fd9\u4e9b\u6743\u529b\u7ed3\u6784\u7531\u4e0d\u516c\u6b63\u4e14\u4e0d\u53d7\u5236\u7ea6\u7684\u6743\u529b\u4e3b\u5bfc\uff0c\u65e0\u6cd5\u4ee3\u8868\u516c\u4f17\u548c\u53d7\u5f71\u54cd\u793e\u533a\u7684\u5229\u76ca\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e09\u9879\u5e72\u9884\u5efa\u8bae\uff1a1)\u4f7f\u6743\u529b\u53ef\u89e3\u91ca\u548c\u53d7\u5236\u7ea6\uff1b2)\u91cd\u6784AI\u548cAI\u4f26\u7406\u7684\u53d9\u4e8b\u548c\u5047\u8bbe\uff1b3)\u8054\u5408\u4f26\u7406\u548c\u79d1\u5b66\u52aa\u529b\u5c06\u4f26\u7406\u4ef7\u503c\u7f16\u7801\u4e3a\u6280\u672f\u6807\u51c6\u3002\u8fd9\u4e9b\u5efa\u8bae\u53ef\u4e3aAI\u793e\u533a\u548c\u516c\u6c11\u793e\u4f1a\u63d0\u4f9b\u6709\u7528\u53c2\u8003\u3002"}}
{"id": "2510.10622", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.10622", "abs": "https://arxiv.org/abs/2510.10622", "authors": ["Kaito Ohtani", "Hiroki Kawabe", "Kentaro Yaji", "Kikuo Fujita", "Vikrant Aute"], "title": "Homogenization-based optimization of wall thickness distribution for TPMS two-fluid heat exchangers", "comment": null, "summary": "Triply Periodic Minimal Surface (TPMS) structures are attracting growing\nattention as promising geometries for next-generation high-performance heat\nexchangers (HXs), due to their continuous flow paths and high\nsurface-area-to-volume ratio that enhance heat transfer performance. Among\nthese, graded TPMS structures with spatially varying thickness have emerged as\na potential means to further improve performance. This study proposes an\noptimization method of wall thickness distribution for TPMS HXs based on an\neffective porous media model, which allows accurate performance prediction\nwhile significantly reducing computational cost. The proposed method is applied\nto a gyroid two-fluid HX aiming to improve the thermal-hydraulic performance.\nFurthermore, full-scale numerical simulations of the optimized-thickness design\nshow a 12.2% improvement in the performance evaluation criterion (PEC) compared\nto the uniform-thickness design. The improvement is primarily attributed to the\noptimized non-uniform wall thickness, which directs more flow toward the core\nends and enhances velocity uniformity. As a result, heat transfer is enhanced\nat the core ends, leading to more effective use of the entire HX core and\nimproved overall thermal performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6709\u6548\u591a\u5b54\u4ecb\u8d28\u6a21\u578b\u7684TPMS\u6362\u70ed\u5668\u58c1\u539a\u5206\u5e03\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u975e\u5747\u5300\u58c1\u539a\u8bbe\u8ba1\uff0c\u4f7f\u6027\u80fd\u8bc4\u4ef7\u6807\u51c6(PEC)\u76f8\u6bd4\u5747\u5300\u58c1\u539a\u8bbe\u8ba1\u63d0\u9ad8\u4e8612.2%\u3002", "motivation": "\u68af\u5ea6TPMS\u7ed3\u6784\u5177\u6709\u7a7a\u95f4\u53d8\u5316\u7684\u58c1\u539a\uff0c\u6709\u671b\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6362\u70ed\u5668\u6027\u80fd\uff0c\u4f46\u9700\u8981\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u6765\u8bbe\u8ba1\u58c1\u539a\u5206\u5e03\u3002", "method": "\u57fa\u4e8e\u6709\u6548\u591a\u5b54\u4ecb\u8d28\u6a21\u578b\u5f00\u53d1\u58c1\u539a\u5206\u5e03\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5e94\u7528\u4e8egyroid\u53cc\u6d41\u4f53\u6362\u70ed\u5668\u4ee5\u6539\u5584\u70ed\u5de5\u6c34\u529b\u6027\u80fd\u3002", "result": "\u4f18\u5316\u58c1\u539a\u8bbe\u8ba1\u4f7fPEC\u63d0\u9ad8\u4e8612.2%\uff0c\u4e3b\u8981\u5f52\u56e0\u4e8e\u4f18\u5316\u7684\u975e\u5747\u5300\u58c1\u539a\u5c06\u66f4\u591a\u6d41\u4f53\u5bfc\u5411\u6838\u5fc3\u7aef\u90e8\u5e76\u589e\u5f3a\u901f\u5ea6\u5747\u5300\u6027\u3002", "conclusion": "\u4f18\u5316\u7684\u975e\u5747\u5300\u58c1\u539a\u8bbe\u8ba1\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u6574\u4e2a\u6362\u70ed\u5668\u6838\u5fc3\uff0c\u63d0\u9ad8\u6574\u4f53\u70ed\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728TPMS\u6362\u70ed\u5668\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.09790", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09790", "abs": "https://arxiv.org/abs/2510.09790", "authors": ["Michael Freenor", "Lauren Alvarez"], "title": "Steering Embedding Models with Geometric Rotation: Mapping Semantic Relationships Across Languages and Models", "comment": "9 pages, 3 Figure, 1 table, preprint", "summary": "Understanding how language and embedding models encode semantic relationships\nis fundamental to model interpretability and control. While early word\nembeddings exhibited intuitive vector arithmetic (''king'' - ''man'' +\n''woman'' = ''queen''), modern high-dimensional text representations lack\nstraightforward interpretable geometric properties. We introduce\nRotor-Invariant Shift Estimation (RISE), a geometric approach that represents\nsemantic transformations as consistent rotational operations in embedding\nspace, leveraging the manifold structure of modern language representations.\nRISE operations have the ability to operate across both languages and models\nwith high transfer of performance, suggesting the existence of analogous\ncross-lingual geometric structure. We evaluate RISE across three embedding\nmodels, three datasets, and seven morphologically diverse languages in five\nmajor language groups. Our results demonstrate that RISE consistently maps\ndiscourse-level semantic transformations with distinct grammatical features\n(e.g., negation and conditionality) across languages and models. This work\nprovides the first systematic demonstration that discourse-level semantic\ntransformations correspond to consistent geometric operations in multilingual\nembedding spaces, empirically supporting the Linear Representation Hypothesis\nat the sentence level.", "AI": {"tldr": "RISE\u65b9\u6cd5\u901a\u8fc7\u65cb\u8f6c\u64cd\u4f5c\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8868\u793a\u8bed\u4e49\u8f6c\u6362\uff0c\u63ed\u793a\u4e86\u591a\u8bed\u8a00\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8bdd\u8bed\u7ea7\u8bed\u4e49\u8f6c\u6362\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "motivation": "\u7406\u89e3\u8bed\u8a00\u548c\u5d4c\u5165\u6a21\u578b\u5982\u4f55\u7f16\u7801\u8bed\u4e49\u5173\u7cfb\u5bf9\u4e8e\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u63a7\u5236\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u4ee3\u9ad8\u7ef4\u6587\u672c\u8868\u793a\u7f3a\u4e4f\u76f4\u89c2\u7684\u51e0\u4f55\u89e3\u91ca\u6027\u3002", "method": "\u5f15\u5165Rotor-Invariant Shift Estimation (RISE)\uff0c\u5229\u7528\u73b0\u4ee3\u8bed\u8a00\u8868\u793a\u7684\u6d41\u5f62\u7ed3\u6784\uff0c\u5c06\u8bed\u4e49\u8f6c\u6362\u8868\u793a\u4e3a\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u4e00\u81f4\u65cb\u8f6c\u64cd\u4f5c\u3002", "result": "\u57283\u4e2a\u5d4c\u5165\u6a21\u578b\u30013\u4e2a\u6570\u636e\u96c6\u548c7\u79cd\u5f62\u6001\u591a\u6837\u7684\u8bed\u8a00\u4e0a\u8bc4\u4f30\uff0cRISE\u80fd\u4e00\u81f4\u5730\u6620\u5c04\u5177\u6709\u4e0d\u540c\u8bed\u6cd5\u7279\u5f81\u7684\u8bdd\u8bed\u7ea7\u8bed\u4e49\u8f6c\u6362\u3002", "conclusion": "\u9996\u6b21\u7cfb\u7edf\u8bc1\u660e\u8bdd\u8bed\u7ea7\u8bed\u4e49\u8f6c\u6362\u5bf9\u5e94\u4e8e\u591a\u8bed\u8a00\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u4e00\u81f4\u51e0\u4f55\u64cd\u4f5c\uff0c\u5728\u53e5\u5b50\u5c42\u9762\u5b9e\u8bc1\u652f\u6301\u7ebf\u6027\u8868\u793a\u5047\u8bbe\u3002"}}
{"id": "2510.09670", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.09670", "abs": "https://arxiv.org/abs/2510.09670", "authors": ["Xinlun Cheng", "Bingzhe Chen", "Joseph Choi", "Yen T. Nguyen", "Pradeep Seshadri", "Mayank Verma", "H. S. Udaykumar", "Stephen Baek"], "title": "A physics-aware deep learning model for shear band formation around collapsing pores in shocked reactive materials", "comment": null, "summary": "Modeling shock-to-detonation phenomena in energetic materials (EMs) requires\ncapturing complex physical processes such as strong shocks, rapid changes in\nmicrostructural morphology, and nonlinear dynamics of chemical reaction fronts.\nThese processes participate in energy localization at hotspots, which initiate\nchemical energy release leading to detonation. This study addresses the\nformation of hotspots in crystalline EMs subjected to weak-to-moderate shock\nloading, which, despite its critical relevance to the safe storage and handling\nof EMs, remains underexplored compared to the well-studied strong shock\nconditions. To overcome the computational challenges associated with direct\nnumerical simulations, we advance the Physics-Aware Recurrent Convolutional\nNeural Network (PARCv2), which has been shown to be capable of predicting\nstrong shock responses in EMs. We improved the architecture of PARCv2 to\nrapidly predict shear localizations and plastic heating, which play important\nroles in the weak-to-moderate shock regime. PARCv2 is benchmarked against two\nwidely used physics-informed models, namely, Fourier neural operator and neural\nordinary differential equation; we demonstrate its superior performance in\ncapturing the spatiotemporal dynamics of shear band formation. While all models\nexhibit certain failure modes, our findings underscore the importance of\ndomain-specific considerations in developing robust AI-accelerated simulation\ntools for reactive materials.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u6539\u8fdb\u7684PARCv2\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u5f31\u5230\u4e2d\u7b49\u51b2\u51fb\u8f7d\u8377\u4e0b\u542b\u80fd\u6750\u6599\u4e2d\u70ed\u70b9\u5f62\u6210\u7684\u526a\u5207\u5c40\u90e8\u5316\u548c\u5851\u6027\u52a0\u70ed\u8fc7\u7a0b\uff0c\u76f8\u6bd4\u4f20\u7edf\u7269\u7406\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u542b\u80fd\u6750\u6599\u5728\u5f31\u5230\u4e2d\u7b49\u51b2\u51fb\u8f7d\u8377\u4e0b\u7684\u70ed\u70b9\u5f62\u6210\u673a\u5236\u7814\u7a76\u4e0d\u8db3\uff0c\u8fd9\u5bf9\u5b89\u5168\u50a8\u5b58\u548c\u5904\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8ba1\u7b97\u6a21\u62df\u9762\u4e34\u6311\u6218\u3002", "method": "\u6539\u8fdb\u7269\u7406\u611f\u77e5\u5faa\u73af\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(PARCv2)\u67b6\u6784\uff0c\u4e13\u6ce8\u4e8e\u9884\u6d4b\u526a\u5207\u5c40\u90e8\u5316\u548c\u5851\u6027\u52a0\u70ed\uff0c\u5e76\u4e0e\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u548c\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u7b49\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "PARCv2\u5728\u6355\u6349\u526a\u5207\u5e26\u5f62\u6210\u7684\u65f6\u7a7a\u52a8\u6001\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5c3d\u7ba1\u6240\u6709\u6a21\u578b\u90fd\u5b58\u5728\u4e00\u5b9a\u7684\u5931\u6548\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u5f00\u53d1\u53cd\u5e94\u6027\u6750\u6599\u7684AI\u52a0\u901f\u6a21\u62df\u5de5\u5177\u65f6\uff0c\u9886\u57df\u7279\u5b9a\u8003\u8651\u56e0\u7d20\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.10069", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.10069", "abs": "https://arxiv.org/abs/2510.10069", "authors": ["Zeyu Ling", "Xiaodong Gu", "Jiangnan Tang", "Changqing Zou"], "title": "SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation", "comment": null, "summary": "We introduce SyncLipMAE, a self-supervised pretraining framework for\ntalking-face video that learns synchronization-aware and transferable facial\ndynamics from unlabeled audio-visual streams. Our approach couples masked\nvisual modeling with cross-modal contrastive alignment and employs three\nper-frame prompt tokens that explicitly encode the essential factors of a\ntalking-face frame - identity, vocal motion (speech-synchronized facial\ndynamics), and ambient motion (audio-agnostic movements such as blinks and head\npose). The contrastive objective uses time-aligned vocal-motion and audio\ntokens as positives and misaligned pairs as negatives, driving both modalities\ninto a shared embedding space and yielding token-level audio-visual stream\nsynchronization. After pretraining, the aligned audio tokens together with the\nvisual prompt tokens (identity, vocal motion, ambient motion) form a unified\ninterface for four disparate downstream settings: (i) audio-visual stream\nsynchronization; (ii) facial emotion and head/face action recognition; (iii)\nvisual speech recognition; and (iv) visual dubbing, for which we enable\nindistinguishable audio- or video-driven control within a single model. Across\nfour task families that require distinct capabilities, SyncLipMAE achieves\nstate-of-the-art results, underscoring the effectiveness of\nsynchronization-aware, factorized self-supervised pretraining.", "AI": {"tldr": "SyncLipMAE\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u7801\u89c6\u89c9\u5efa\u6a21\u548c\u8de8\u6a21\u6001\u5bf9\u6bd4\u5bf9\u9f50\uff0c\u4ece\u65e0\u6807\u7b7e\u97f3\u89c6\u9891\u6d41\u4e2d\u5b66\u4e60\u540c\u6b65\u611f\u77e5\u548c\u53ef\u8fc1\u79fb\u7684\u9762\u90e8\u52a8\u6001\u3002", "motivation": "\u89e3\u51b3\u8bf4\u8bdd\u4eba\u8138\u89c6\u9891\u4e2d\u9700\u8981\u5b66\u4e60\u540c\u6b65\u611f\u77e5\u7684\u9762\u90e8\u52a8\u6001\u8868\u793a\uff0c\u5e76\u5b9e\u73b0\u8de8\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u7684\u7edf\u4e00\u63a5\u53e3\u3002", "method": "\u7ed3\u5408\u63a9\u7801\u89c6\u89c9\u5efa\u6a21\u4e0e\u8de8\u6a21\u6001\u5bf9\u6bd4\u5bf9\u9f50\uff0c\u4f7f\u7528\u4e09\u4e2a\u9010\u5e27\u63d0\u793a\u4ee4\u724c\u7f16\u7801\u8eab\u4efd\u3001\u8bed\u97f3\u540c\u6b65\u9762\u90e8\u52a8\u6001\u548c\u97f3\u9891\u65e0\u5173\u8fd0\u52a8\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5c06\u4e24\u79cd\u6a21\u6001\u6620\u5c04\u5230\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u3002", "result": "\u5728\u56db\u4e2a\u9700\u8981\u4e0d\u540c\u80fd\u529b\u7684\u4efb\u52a1\u5bb6\u65cf\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5305\u62ec\u97f3\u89c6\u9891\u6d41\u540c\u6b65\u3001\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u3001\u89c6\u89c9\u8bed\u97f3\u8bc6\u522b\u548c\u89c6\u89c9\u914d\u97f3\u3002", "conclusion": "\u540c\u6b65\u611f\u77e5\u3001\u5206\u89e3\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5728\u8bf4\u8bdd\u4eba\u8138\u89c6\u9891\u5206\u6790\u4e2d\u975e\u5e38\u6709\u6548\uff0c\u80fd\u591f\u4e3a\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u7edf\u4e00\u7684\u63a7\u5236\u63a5\u53e3\u3002"}}
{"id": "2510.09676", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09676", "abs": "https://arxiv.org/abs/2510.09676", "authors": ["Shayan Mohajer Hamidi", "En-Hui Yang", "Ben Liang"], "title": "Coupled Data and Measurement Space Dynamics for Enhanced Diffusion Posterior Sampling", "comment": null, "summary": "Inverse problems, where the goal is to recover an unknown signal from noisy\nor incomplete measurements, are central to applications in medical imaging,\nremote sensing, and computational biology. Diffusion models have recently\nemerged as powerful priors for solving such problems. However, existing methods\neither rely on projection-based techniques that enforce measurement consistency\nthrough heuristic updates, or they approximate the likelihood $p(\\boldsymbol{y}\n\\mid \\boldsymbol{x})$, often resulting in artifacts and instability under\ncomplex or high-noise conditions. To address these limitations, we propose a\nnovel framework called \\emph{coupled data and measurement space diffusion\nposterior sampling} (C-DPS), which eliminates the need for constraint tuning or\nlikelihood approximation. C-DPS introduces a forward stochastic process in the\nmeasurement space $\\{\\boldsymbol{y}_t\\}$, evolving in parallel with the\ndata-space diffusion $\\{\\boldsymbol{x}_t\\}$, which enables the derivation of a\nclosed-form posterior $p(\\boldsymbol{x}_{t-1} \\mid \\boldsymbol{x}_t,\n\\boldsymbol{y}_{t-1})$. This coupling allows for accurate and recursive\nsampling based on a well-defined posterior distribution. Empirical results\ndemonstrate that C-DPS consistently outperforms existing baselines, both\nqualitatively and quantitatively, across multiple inverse problem benchmarks.", "AI": {"tldr": "\u63d0\u51faC-DPS\u6846\u67b6\uff0c\u901a\u8fc7\u8026\u5408\u6570\u636e\u548c\u6d4b\u91cf\u7a7a\u95f4\u7684\u6269\u6563\u8fc7\u7a0b\uff0c\u65e0\u9700\u7ea6\u675f\u8c03\u4f18\u6216\u4f3c\u7136\u8fd1\u4f3c\uff0c\u5728\u591a\u4e2a\u9006\u95ee\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u57fa\u4e8e\u6295\u5f71\u7684\u6280\u672f\u6216\u8fd1\u4f3c\u4f3c\u7136\u51fd\u6570\uff0c\u5bfc\u81f4\u4f2a\u5f71\u548c\u4e0d\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u6216\u9ad8\u566a\u58f0\u6761\u4ef6\u4e0b\u3002", "method": "\u5f15\u5165\u6d4b\u91cf\u7a7a\u95f4\u7684\u6b63\u5411\u968f\u673a\u8fc7\u7a0b\uff0c\u4e0e\u6570\u636e\u7a7a\u95f4\u6269\u6563\u5e76\u884c\u6f14\u5316\uff0c\u63a8\u5bfc\u51fa\u5c01\u95ed\u5f62\u5f0f\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u5b9e\u73b0\u51c6\u786e\u9012\u5f52\u91c7\u6837\u3002", "result": "C-DPS\u5728\u591a\u4e2a\u9006\u95ee\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "C-DPS\u6846\u67b6\u901a\u8fc7\u8026\u5408\u6269\u6563\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5b9a\u4e14\u51c6\u786e\u7684\u9006\u95ee\u9898\u6c42\u89e3\u65b9\u6cd5\uff0c\u65e0\u9700\u8c03\u4f18\u7ea6\u675f\u6216\u8fd1\u4f3c\u4f3c\u7136\u3002"}}
{"id": "2510.10450", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.10450", "abs": "https://arxiv.org/abs/2510.10450", "authors": ["P Sangeerth", "David Smith Sundarsingh", "Bhabani Shankar Dey", "Pushpak Jagtap"], "title": "Controller for Incremental Input-to-State Practical Stabilization of Partially Unknown systems with Invariance Guarantees", "comment": "2 figures,9 pages", "summary": "Incremental stability is a property of dynamical systems that ensures the\nconvergence of trajectories with respect to each other rather than a fixed\nequilibrium point or a fixed trajectory. In this paper, we introduce a related\nstability notion called incremental input-to-state practical stability\n({\\delta}-ISpS), ensuring safety guarantees. We also present a feedback\nlinearization based control design scheme that renders a partially unknown\nsystem incrementally input-to-state practically stable and safe with formal\nguarantees. To deal with the unknown dynamics, we utilize Gaussian process\nregression to approximate the model. Finally, we implement the controller\nsynthesized by the proposed scheme on a manipulator example", "AI": {"tldr": "\u63d0\u51fa\u589e\u91cf\u8f93\u5165\u5230\u72b6\u6001\u5b9e\u7528\u7a33\u5b9a\u6027\u6982\u5ff5\uff0c\u5e76\u57fa\u4e8e\u53cd\u9988\u7ebf\u6027\u5316\u8bbe\u8ba1\u63a7\u5236\u5668\uff0c\u786e\u4fdd\u90e8\u5206\u672a\u77e5\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u589e\u91cf\u7a33\u5b9a\u6027\u786e\u4fdd\u8f68\u8ff9\u76f8\u4e92\u6536\u655b\u800c\u975e\u56fa\u5b9a\u5e73\u8861\u70b9\uff0c\u4f46\u9700\u8981\u5904\u7406\u672a\u77e5\u52a8\u6001\u5e76\u4fdd\u8bc1\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u8fd1\u4f3c\u672a\u77e5\u52a8\u6001\uff0c\u57fa\u4e8e\u53cd\u9988\u7ebf\u6027\u5316\u8bbe\u8ba1\u63a7\u5236\u5668\u3002", "result": "\u5b9e\u73b0\u4e86\u589e\u91cf\u8f93\u5165\u5230\u72b6\u6001\u5b9e\u7528\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u673a\u68b0\u81c2\u4e0a\u9a8c\u8bc1\u4e86\u63a7\u5236\u5668\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u672a\u77e5\u52a8\u6001\uff0c\u786e\u4fdd\u7cfb\u7edf\u5b89\u5168\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u5177\u6709\u5f62\u5f0f\u5316\u4fdd\u8bc1\u3002"}}
{"id": "2510.10732", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.10732", "abs": "https://arxiv.org/abs/2510.10732", "authors": ["Tamara Paris", "Shalaleh Rismani"], "title": "When Openness Fails: Lessons from System Safety for Assessing Openness in AI", "comment": "Accepted to Symposium on Model Accountability, Sustainability and\n  Healthcare (SMASH) 2025", "summary": "Most frameworks for assessing the openness of AI systems use narrow criteria\nsuch as availability of data, model, code, documentation, and licensing terms.\nHowever, to evaluate whether the intended effects of openness - such as\ndemocratization and autonomy - are realized, we need a more holistic approach\nthat considers the context of release: who will reuse the system, for what\npurposes, and under what conditions. To this end, we adapt five lessons from\nsystem safety that offer guidance on how openness can be evaluated at the\nsystem level.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u5f00\u653e\u6807\u51c6\uff08\u6570\u636e\u3001\u6a21\u578b\u3001\u4ee3\u7801\u7b49\uff09\uff0c\u91c7\u7528\u66f4\u5168\u9762\u7684\u7cfb\u7edf\u7ea7\u65b9\u6cd5\u6765\u8bc4\u4f30AI\u7cfb\u7edf\u7684\u5f00\u653e\u6027\uff0c\u501f\u9274\u7cfb\u7edf\u5b89\u5168\u9886\u57df\u7684\u4e94\u4e2a\u7ecf\u9a8c\u3002", "motivation": "\u4f20\u7edfAI\u7cfb\u7edf\u5f00\u653e\u6027\u8bc4\u4f30\u6846\u67b6\u4ec5\u5173\u6ce8\u6570\u636e\u3001\u6a21\u578b\u3001\u4ee3\u7801\u7b49\u72ed\u4e49\u6807\u51c6\uff0c\u65e0\u6cd5\u8bc4\u4f30\u5f00\u653e\u6027\u7684\u5b9e\u9645\u6548\u679c\uff08\u5982\u6c11\u4e3b\u5316\u548c\u81ea\u4e3b\u6027\uff09\u3002\u9700\u8981\u66f4\u5168\u9762\u7684\u65b9\u6cd5\u8003\u8651\u53d1\u5e03\u80cc\u666f\uff1a\u8c01\u91cd\u7528\u7cfb\u7edf\u3001\u4e3a\u4f55\u76ee\u7684\u3001\u5728\u4ec0\u4e48\u6761\u4ef6\u4e0b\u3002", "method": "\u501f\u9274\u7cfb\u7edf\u5b89\u5168\u9886\u57df\u7684\u4e94\u4e2a\u7ecf\u9a8c\u6559\u8bad\uff0c\u4e3a\u7cfb\u7edf\u7ea7\u5f00\u653e\u6027\u8bc4\u4f30\u63d0\u4f9b\u6307\u5bfc\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u7cfb\u7edf\u7ea7\u5f00\u653e\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u8bc4\u4f30\u5f00\u653e\u6027\u662f\u5426\u5b9e\u73b0\u5176\u9884\u671f\u6548\u679c\u3002", "conclusion": "\u8bc4\u4f30AI\u7cfb\u7edf\u5f00\u653e\u6027\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u72ed\u4e49\u6807\u51c6\uff0c\u91c7\u7528\u8003\u8651\u53d1\u5e03\u80cc\u666f\u7684\u7cfb\u7edf\u7ea7\u65b9\u6cd5\uff0c\u501f\u9274\u7cfb\u7edf\u5b89\u5168\u7ecf\u9a8c\u53ef\u4ee5\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u6307\u5bfc\u3002"}}
{"id": "2510.10690", "categories": ["math.OC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10690", "abs": "https://arxiv.org/abs/2510.10690", "authors": ["Abdurakhmon Sadiev", "Peter Richt\u00e1rik", "Ilyas Fatkhullin"], "title": "Second-order Optimization under Heavy-Tailed Noise: Hessian Clipping and Sample Complexity Limits", "comment": "Accepted for publication at NeurIPS 2025", "summary": "Heavy-tailed noise is pervasive in modern machine learning applications,\narising from data heterogeneity, outliers, and non-stationary stochastic\nenvironments. While second-order methods can significantly accelerate\nconvergence in light-tailed or bounded-noise settings, such algorithms are\noften brittle and lack guarantees under heavy-tailed noise -- precisely the\nregimes where robustness is most critical. In this work, we take a first step\ntoward a theoretical understanding of second-order optimization under\nheavy-tailed noise. We consider a setting where stochastic gradients and\nHessians have only bounded $p$-th moments, for some $p\\in (1,2]$, and establish\ntight lower bounds on the sample complexity of any second-order method. We then\ndevelop a variant of normalized stochastic gradient descent that leverages\nsecond-order information and provably matches these lower bounds. To address\nthe instability caused by large deviations, we introduce a novel algorithm\nbased on gradient and Hessian clipping, and prove high-probability upper bounds\nthat nearly match the fundamental limits. Our results provide the first\ncomprehensive sample complexity characterization for second-order optimization\nunder heavy-tailed noise. This positions Hessian clipping as a robust and\ntheoretically sound strategy for second-order algorithm design in heavy-tailed\nregimes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u91cd\u5c3e\u566a\u58f0\u4e0b\u7684\u4e8c\u9636\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u68af\u5ea6\u548cHessian\u88c1\u526a\u7684\u65b0\u7b97\u6cd5\uff0c\u5e76\u5efa\u7acb\u4e86\u5339\u914d\u4e0b\u754c\u7684\u6837\u672c\u590d\u6742\u5ea6\u7406\u8bba\u3002", "motivation": "\u91cd\u5c3e\u566a\u58f0\u5728\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4f46\u73b0\u6709\u7684\u4e8c\u9636\u65b9\u6cd5\u5728\u91cd\u5c3e\u566a\u58f0\u4e0b\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u548c\u9c81\u68d2\u6027\uff0c\u8fd9\u6b63\u662f\u6700\u9700\u8981\u9c81\u68d2\u6027\u7684\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1a1) \u5229\u7528\u4e8c\u9636\u4fe1\u606f\u7684\u5f52\u4e00\u5316\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u53d8\u4f53\uff1b2) \u57fa\u4e8e\u68af\u5ea6\u548cHessian\u88c1\u526a\u7684\u65b0\u7b97\u6cd5\uff0c\u4ee5\u5904\u7406\u5927\u504f\u5dee\u5f15\u8d77\u7684\u4e0d\u7a33\u5b9a\u6027\u3002", "result": "\u5efa\u7acb\u4e86\u91cd\u5c3e\u566a\u58f0\u4e0b\u4e8c\u9636\u4f18\u5316\u7684\u7d27\u4e0b\u754c\uff0c\u5e76\u8bc1\u660e\u6240\u63d0\u7b97\u6cd5\u80fd\u591f\u5339\u914d\u8fd9\u4e9b\u4e0b\u754c\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6982\u7387\u4e0a\u754c\u3002", "conclusion": "Hessian\u88c1\u526a\u662f\u91cd\u5c3e\u566a\u58f0\u4e0b\u4e8c\u9636\u7b97\u6cd5\u8bbe\u8ba1\u7684\u9c81\u68d2\u4e14\u7406\u8bba\u5b8c\u5907\u7684\u7b56\u7565\uff0c\u9996\u6b21\u5168\u9762\u523b\u753b\u4e86\u91cd\u5c3e\u566a\u58f0\u4e0b\u4e8c\u9636\u4f18\u5316\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002"}}
{"id": "2510.09849", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09849", "abs": "https://arxiv.org/abs/2510.09849", "authors": ["Ruizhe Zhu"], "title": "Text Prompt Injection of Vision Language Models", "comment": null, "summary": "The widespread application of large vision language models has significantly\nraised safety concerns. In this project, we investigate text prompt injection,\na simple yet effective method to mislead these models. We developed an\nalgorithm for this type of attack and demonstrated its effectiveness and\nefficiency through experiments. Compared to other attack methods, our approach\nis particularly effective for large models without high demand for\ncomputational resources.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4f4e\u3002", "motivation": "\u968f\u7740\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5b89\u5168\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u7814\u7a76\u6709\u6548\u7684\u653b\u51fb\u65b9\u6cd5\u6765\u63ed\u793a\u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6587\u672c\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7b97\u6cd5\uff0c\u901a\u8fc7\u8bef\u5bfc\u6a21\u578b\u6765\u5b9e\u73b0\u653b\u51fb\u76ee\u7684\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5bf9\u5927\u578b\u6a21\u578b\u7279\u522b\u6709\u6548\uff0c\u4e14\u76f8\u6bd4\u5176\u4ed6\u653b\u51fb\u65b9\u6cd5\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u66f4\u4f4e\u3002", "conclusion": "\u6587\u672c\u63d0\u793a\u6ce8\u5165\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u653b\u51fb\u65b9\u5f0f\uff0c\u80fd\u591f\u6709\u6548\u8bef\u5bfc\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u6b64\u7c7b\u6a21\u578b\u7684\u5b89\u5168\u8106\u5f31\u6027\u3002"}}
{"id": "2510.10074", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10074", "abs": "https://arxiv.org/abs/2510.10074", "authors": ["Jiayi Mao", "Liqun Li", "Yanjie Gao", "Zegang Peng", "Shilin He", "Chaoyun Zhang", "Si Qin", "Samia Khalid", "Qingwei Lin", "Saravan Rajmohan", "Sitaram Lanka", "Dongmei Zhang"], "title": "Agentic Troubleshooting Guide Automation for Incident Management", "comment": null, "summary": "Effective incident management in large-scale IT systems relies on\ntroubleshooting guides (TSGs), but their manual execution is slow and\nerror-prone. While recent advances in LLMs offer promise for automating\nincident management tasks, existing LLM-based solutions lack specialized\nsupport for several key challenges, including managing TSG quality issues,\ninterpreting complex control flow, handling data-intensive queries, and\nexploiting execution parallelism. We first conducted an empirical study on 92\nreal-world TSGs, and, guided by our findings, we present StepFly, a novel\nend-to-end agentic framework for troubleshooting guide automation. Our approach\nfeatures a three-stage workflow: the first stage provides a comprehensive guide\ntogether with a tool, TSG Mentor, to assist SREs in improving TSG quality; the\nsecond stage performs offline preprocessing using LLMs to extract structured\nexecution DAGs from unstructured TSGs and to create dedicated Query Preparation\nPlugins (QPPs); and the third stage executes online using a DAG-guided\nscheduler-executor framework with a memory system to guarantee correct workflow\nand support parallel execution of independent steps. Our empirical evaluation\non a collection of real-world TSGs and incidents demonstrates that StepFly\nachieves a ~94% success rate on GPT-4.1, outperforming baselines with less time\nand token consumption. Furthermore, it achieves a remarkable execution time\nreduction of 32.9% to 70.4% for parallelizable TSGs.", "AI": {"tldr": "StepFly\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u5316\u6545\u969c\u6392\u9664\u6307\u5357\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u5de5\u4f5c\u6d41\u89e3\u51b3TSG\u8d28\u91cf\u95ee\u9898\u3001\u590d\u6742\u63a7\u5236\u6d41\u89e3\u6790\u3001\u6570\u636e\u5bc6\u96c6\u578b\u67e5\u8be2\u548c\u6267\u884c\u5e76\u884c\u5316\u7b49\u5173\u952e\u6311\u6218\uff0c\u5728\u771f\u5b9eTSG\u4e0a\u5b9e\u73b0\u7ea694%\u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u51cf\u5c11\u6267\u884c\u65f6\u95f4\u3002", "motivation": "\u5927\u89c4\u6a21IT\u7cfb\u7edf\u4e2d\u7684\u6545\u969c\u7ba1\u7406\u4f9d\u8d56\u6545\u969c\u6392\u9664\u6307\u5357(TSG)\uff0c\u4f46\u624b\u52a8\u6267\u884c\u7f13\u6162\u4e14\u6613\u51fa\u9519\u3002\u73b0\u6709\u57fa\u4e8eLLM\u7684\u89e3\u51b3\u65b9\u6848\u7f3a\u4e4f\u5bf9TSG\u8d28\u91cf\u95ee\u9898\u3001\u590d\u6742\u63a7\u5236\u6d41\u89e3\u6790\u3001\u6570\u636e\u5bc6\u96c6\u578b\u67e5\u8be2\u548c\u6267\u884c\u5e76\u884c\u5316\u7b49\u5173\u952e\u6311\u6218\u7684\u4e13\u4e1a\u652f\u6301\u3002", "method": "StepFly\u91c7\u7528\u4e09\u9636\u6bb5\u5de5\u4f5c\u6d41\uff1a1) TSG Mentor\u5de5\u5177\u5e2e\u52a9SRE\u6539\u8fdbTSG\u8d28\u91cf\uff1b2) \u79bb\u7ebf\u9884\u5904\u7406\u4f7f\u7528LLM\u4ece\u975e\u7ed3\u6784\u5316TSG\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6267\u884cDAG\u5e76\u521b\u5efa\u4e13\u7528\u67e5\u8be2\u51c6\u5907\u63d2\u4ef6(QPPs)\uff1b3) \u5728\u7ebf\u6267\u884c\u4f7f\u7528DAG\u5f15\u5bfc\u7684\u8c03\u5ea6\u5668-\u6267\u884c\u5668\u6846\u67b6\uff0c\u652f\u6301\u5e76\u884c\u6267\u884c\u72ec\u7acb\u6b65\u9aa4\u3002", "result": "\u5728\u771f\u5b9eTSG\u548c\u4e8b\u4ef6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cStepFly\u5728GPT-4.1\u4e0a\u8fbe\u5230\u7ea694%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u51cf\u5c11\u65f6\u95f4\u548ctoken\u6d88\u8017\u3002\u5bf9\u4e8e\u53ef\u5e76\u884c\u5316\u7684TSG\uff0c\u6267\u884c\u65f6\u95f4\u51cf\u5c11\u4e8632.9%\u523070.4%\u3002", "conclusion": "StepFly\u901a\u8fc7\u5176\u521b\u65b0\u7684\u4e09\u9636\u6bb5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86TSG\u81ea\u52a8\u5316\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6545\u969c\u6392\u9664\u7684\u6548\u7387\u548c\u6210\u529f\u7387\uff0c\u4e3a\u5927\u89c4\u6a21IT\u7cfb\u7edf\u7684\u81ea\u52a8\u5316\u6545\u969c\u7ba1\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.09705", "categories": ["cs.LG", "cs.CY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09705", "abs": "https://arxiv.org/abs/2510.09705", "authors": ["Sudip Khadka", "L. S. Paudel"], "title": "A Multi-Component Reward Function with Policy Gradient for Automated Feature Selection with Dynamic Regularization and Bias Mitigation", "comment": null, "summary": "Static feature exclusion strategies often fail to prevent bias when hidden\ndependencies influence the model predictions. To address this issue, we explore\na reinforcement learning (RL) framework that integrates bias mitigation and\nautomated feature selection within a single learning process. Unlike\ntraditional heuristic-driven filter or wrapper approaches, our RL agent\nadaptively selects features using a reward signal that explicitly integrates\npredictive performance with fairness considerations. This dynamic formulation\nallows the model to balance generalization, accuracy, and equity throughout the\ntraining process, rather than rely exclusively on pre-processing adjustments or\npost hoc correction mechanisms. In this paper, we describe the construction of\na multi-component reward function, the specification of the agents action space\nover feature subsets, and the integration of this system with ensemble\nlearning. We aim to provide a flexible and generalizable way to select features\nin environments where predictors are correlated and biases can inadvertently\nre-emerge.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7279\u5f81\u9009\u62e9\u6846\u67b6\uff0c\u5c06\u504f\u5dee\u7f13\u89e3\u548c\u7279\u5f81\u9009\u62e9\u6574\u5408\u5230\u5355\u4e00\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u52a8\u6001\u5e73\u8861\u9884\u6d4b\u6027\u80fd\u548c\u516c\u5e73\u6027\u6765\u9632\u6b62\u504f\u89c1\u91cd\u73b0\u3002", "motivation": "\u9759\u6001\u7279\u5f81\u6392\u9664\u7b56\u7565\u5f80\u5f80\u65e0\u6cd5\u9632\u6b62\u504f\u89c1\uff0c\u56e0\u4e3a\u9690\u85cf\u7684\u4f9d\u8d56\u5173\u7cfb\u4f1a\u5f71\u54cd\u6a21\u578b\u9884\u6d4b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u5e73\u8861\u51c6\u786e\u6027\u3001\u6cdb\u5316\u6027\u548c\u516c\u5e73\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6784\u5efa\u591a\u7ec4\u4ef6\u5956\u52b1\u51fd\u6570\uff0c\u5b9a\u4e49\u7279\u5f81\u5b50\u96c6\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5e76\u4e0e\u96c6\u6210\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u8ba9\u667a\u80fd\u4f53\u81ea\u9002\u5e94\u5730\u9009\u62e9\u7279\u5f81\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u52a8\u6001\u5e73\u8861\u9884\u6d4b\u6027\u80fd\u548c\u516c\u5e73\u6027\u8003\u8651\uff0c\u5728\u5b58\u5728\u9884\u6d4b\u53d8\u91cf\u76f8\u5173\u6027\u7684\u73af\u5883\u4e2d\u63d0\u4f9b\u7075\u6d3b\u4e14\u53ef\u63a8\u5e7f\u7684\u7279\u5f81\u9009\u62e9\u65b9\u5f0f\u3002", "conclusion": "\u63d0\u51fa\u7684RL\u6846\u67b6\u4e3a\u5b58\u5728\u76f8\u5173\u9884\u6d4b\u53d8\u91cf\u548c\u504f\u89c1\u53ef\u80fd\u65e0\u610f\u91cd\u73b0\u7684\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u53ef\u63a8\u5e7f\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u3002"}}
{"id": "2510.10552", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.10552", "abs": "https://arxiv.org/abs/2510.10552", "authors": ["Rafael R. Yumul", "Enalyn T. Domingo"], "title": "Transforming Tarlac State University (TSU) Gymnasium to a Nearly Zero-Energy Building through Integration of a Solar Photovoltaic (PV) System", "comment": null, "summary": "The study is anchored to the principles of Nearly-Zero Energy Building\n(NZEB). It aimed to transform the Tarlac State University Gymnasium into a\nfacility with energy-efficient equipment to contribute to reducing carbon\nfootprints by integrating a solar PV system as its renewable energy source. The\nresearchers found out that the electrical infrastructure of the Gym was\noutdated, and the lighting was not energy efficient, and there were too few\nconvenience or power outlets. There was also insufficient cooling equipment to\nmaintain a comfortable temperature. Analysis shows that the payback period is\nwithin the average range, making it a cost-effective investment for the\nUniversity. Aside from the cost of the PV System, adherence to engineering\ndesign standards will mean additional costs to replace the metal halides with\nLED high bay lamps, installation of additional air conditioning units, and\nprovision of additional convenience outlets. These additional costs should be\nconsidered when evaluating the feasibility of the project. It is recommended\nthat the integrity of the existing roof system of the Gymnasium be considered.\nThe total cost of putting up the whole electrical system, including new\nlighting, cooling, and convenience loads, must be calculated to determine the\ntotal cost of implementing the whole NZEB project. Other factors in the\neconomic evaluation may be considered to determine a more stringent result.", "AI": {"tldr": "\u5c06Tarlac\u5dde\u7acb\u5927\u5b66\u4f53\u80b2\u9986\u6539\u9020\u6210\u8fd1\u96f6\u80fd\u8017\u5efa\u7b51\uff0c\u901a\u8fc7\u96c6\u6210\u592a\u9633\u80fd\u5149\u4f0f\u7cfb\u7edf\u5b9e\u73b0\u80fd\u6e90\u6548\u7387\uff0c\u4f46\u9700\u8981\u8003\u8651\u7535\u6c14\u7cfb\u7edf\u5347\u7ea7\u7684\u989d\u5916\u6210\u672c\u3002", "motivation": "\u57fa\u4e8e\u8fd1\u96f6\u80fd\u8017\u5efa\u7b51\u539f\u5219\uff0c\u65e8\u5728\u901a\u8fc7\u5c06\u4f53\u80b2\u9986\u6539\u9020\u4e3a\u4f7f\u7528\u8282\u80fd\u8bbe\u5907\u548c\u53ef\u518d\u751f\u80fd\u6e90\u6765\u51cf\u5c11\u78b3\u8db3\u8ff9\u3002", "method": "\u5206\u6790\u73b0\u6709\u7535\u6c14\u57fa\u7840\u8bbe\u65bd\uff0c\u96c6\u6210\u592a\u9633\u80fd\u5149\u4f0f\u7cfb\u7edf\uff0c\u66ff\u6362\u91d1\u5c5e\u5364\u5316\u7269\u706f\u4e3aLED\u9ad8\u68da\u706f\uff0c\u589e\u52a0\u7a7a\u8c03\u8bbe\u5907\u548c\u4fbf\u5229\u63d2\u5ea7\u3002", "result": "\u6295\u8d44\u56de\u6536\u671f\u5728\u5e73\u5747\u8303\u56f4\u5185\uff0c\u4f46\u9700\u8981\u989d\u5916\u6210\u672c\u6765\u5347\u7ea7\u7535\u6c14\u7cfb\u7edf\u4ee5\u6ee1\u8db3\u5de5\u7a0b\u6807\u51c6\u3002", "conclusion": "\u9879\u76ee\u5177\u6709\u6210\u672c\u6548\u76ca\uff0c\u4f46\u5fc5\u987b\u8003\u8651\u5c4b\u9876\u7cfb\u7edf\u5b8c\u6574\u6027\u548c\u6574\u4f53\u7535\u6c14\u7cfb\u7edf\u5347\u7ea7\u7684\u603b\u6210\u672c\uff0c\u8fdb\u884c\u66f4\u4e25\u683c\u7684\u7ecf\u6d4e\u8bc4\u4f30\u3002"}}
{"id": "2510.11064", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.11064", "abs": "https://arxiv.org/abs/2510.11064", "authors": ["Isabella Gra\u00dfl", "Benedikt Fein", "Gordon Fraser"], "title": "Detecting Gender Stereotypes in Scratch Programming Tutorials", "comment": "Koli Calling 2025", "summary": "Gender stereotypes in introductory programming courses often go unnoticed,\nyet they can negatively influence young learners' interest and learning,\nparticularly under-represented groups such as girls. Popular tutorials on\nblock-based programming with Scratch may unintentionally reinforce biases\nthrough character choices, narrative framing, or activity types. Educators\ncurrently lack support in identifying and addressing such bias. With large\nlanguage models~(LLMs) increasingly used to generate teaching materials, this\nproblem is potentially exacerbated by LLMs trained on biased datasets. However,\nLLMs also offer an opportunity to address this issue. In this paper, we explore\nthe use of LLMs for automatically identifying gender-stereotypical elements in\nScratch tutorials, thus offering feedback on how to improve teaching content.\nWe develop a framework for assessing gender bias considering characters,\ncontent, instructions, and programming concepts. Analogous to how code analysis\ntools provide feedback on code in terms of code smells, we operationalise this\nframework using an automated tool chain that identifies *gender stereotype\nsmells*. Evaluation on 73 popular Scratch tutorials from leading educational\nplatforms demonstrates that stereotype smells are common in practice. LLMs are\nnot effective at detecting them, but our gender bias evaluation framework can\nguide LLMs in generating tutorials with fewer stereotype smells.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\u6765\u8bc6\u522bScratch\u7f16\u7a0b\u6559\u7a0b\u4e2d\u7684\u6027\u522b\u523b\u677f\u5370\u8c61\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u4e86\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6765\u68c0\u6d4b\u548c\u6539\u8fdb\u6559\u5b66\u5185\u5bb9\u7684\u6027\u522b\u504f\u89c1\u3002", "motivation": "\u7f16\u7a0b\u5165\u95e8\u8bfe\u7a0b\u4e2d\u7684\u6027\u522b\u523b\u677f\u5370\u8c61\u5f80\u5f80\u88ab\u5ffd\u89c6\uff0c\u4f46\u4f1a\u5bf9\u5e74\u8f7b\u5b66\u4e60\u8005\u7279\u522b\u662f\u5973\u751f\u7684\u5174\u8da3\u548c\u5b66\u4e60\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u5f53\u524d\u6559\u80b2\u5de5\u4f5c\u8005\u7f3a\u4e4f\u8bc6\u522b\u548c\u89e3\u51b3\u8fd9\u79cd\u504f\u89c1\u7684\u65b9\u6cd5\uff0c\u800cLLMs\u751f\u6210\u6559\u5b66\u6750\u6599\u53ef\u80fd\u52a0\u5267\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6027\u522b\u504f\u89c1\u7684\u6846\u67b6\uff0c\u8003\u8651\u89d2\u8272\u3001\u5185\u5bb9\u3001\u6307\u4ee4\u548c\u7f16\u7a0b\u6982\u5ff5\uff0c\u5e76\u521b\u5efa\u4e00\u4e2a\u81ea\u52a8\u5316\u5de5\u5177\u94fe\u6765\u8bc6\u522b\"\u6027\u522b\u523b\u677f\u5370\u8c61\u6c14\u5473\"\u3002\u572873\u4e2a\u6d41\u884cScratch\u6559\u7a0b\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u5b9e\u8df5\u4e2d\u7684\u523b\u677f\u5370\u8c61\u6c14\u5473\u5f88\u5e38\u89c1\u3002LLMs\u5728\u68c0\u6d4b\u8fd9\u4e9b\u6c14\u5473\u65b9\u9762\u6548\u679c\u4e0d\u4f73\uff0c\u4f46\u6027\u522b\u504f\u89c1\u8bc4\u4f30\u6846\u67b6\u53ef\u4ee5\u6307\u5bfcLLMs\u751f\u6210\u523b\u677f\u5370\u8c61\u6c14\u5473\u66f4\u5c11\u7684\u6559\u7a0b\u3002", "conclusion": "\u867d\u7136LLMs\u672c\u8eab\u4e0d\u80fd\u6709\u6548\u68c0\u6d4b\u6027\u522b\u523b\u677f\u5370\u8c61\uff0c\u4f46\u901a\u8fc7\u9002\u5f53\u7684\u6846\u67b6\u6307\u5bfc\uff0c\u5b83\u4eec\u53ef\u4ee5\u5e2e\u52a9\u751f\u6210\u66f4\u5c11\u504f\u89c1\u7684\u6559\u5b66\u5185\u5bb9\uff0c\u4e3a\u89e3\u51b3\u7f16\u7a0b\u6559\u80b2\u4e2d\u7684\u6027\u522b\u504f\u89c1\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.10697", "categories": ["math.OC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10697", "abs": "https://arxiv.org/abs/2510.10697", "authors": ["Nicholas Pischke"], "title": "Mean-square and linear convergence of a stochastic proximal point algorithm in metric spaces of nonpositive curvature", "comment": "24 pages", "summary": "We define a stochastic variant of the proximal point algorithm in the general\nsetting of nonlinear (separable) Hadamard spaces for approximating zeros of the\nmean of a stochastically perturbed monotone vector field and prove its\nconvergence under a suitable strong monotonicity assumption, together with a\nprobabilistic independence assumption and a separability assumption on the\ntangent spaces. As a particular case, our results transfer previous work by P.\nBianchi on that method in Hilbert spaces for the first time to Hadamard\nmanifolds. Moreover, our convergence proof is fully effective and allows for\nthe construction of explicit rates of convergence for the iteration towards the\n(unique) solution both in mean and almost surely. These rates are moreover\nhighly uniform, being independent of most data surrounding the iteration, space\nor distribution. In that generality, these rates are novel already in the\ncontext of Hilbert spaces. Linear nonasymptotic guarantees under additional\nsecond-moment conditions on the Yosida approximates and special cases of\nstochastic convex minimization are discussed.", "AI": {"tldr": "\u63d0\u51fa\u4e86Hadamard\u7a7a\u95f4\u4e2d\u968f\u673a\u8fd1\u7aef\u70b9\u7b97\u6cd5\u7684\u6536\u655b\u6027\u5206\u6790\uff0c\u9996\u6b21\u5c06Hilbert\u7a7a\u95f4\u4e2d\u7684\u76f8\u5173\u5de5\u4f5c\u6269\u5c55\u5230Hadamard\u6d41\u5f62\uff0c\u5e76\u6784\u9020\u4e86\u663e\u5f0f\u6536\u655b\u901f\u7387\u3002", "motivation": "\u5c06\u968f\u673a\u8fd1\u7aef\u70b9\u7b97\u6cd5\u4eceHilbert\u7a7a\u95f4\u63a8\u5e7f\u5230\u975e\u7ebf\u6027Hadamard\u7a7a\u95f4\uff0c\u89e3\u51b3\u968f\u673a\u6270\u52a8\u5355\u8c03\u5411\u91cf\u573a\u7684\u96f6\u70b9\u903c\u8fd1\u95ee\u9898\u3002", "method": "\u5728\u53ef\u5206\u79bbHadamard\u7a7a\u95f4\u4e2d\u5b9a\u4e49\u968f\u673a\u8fd1\u7aef\u70b9\u7b97\u6cd5\uff0c\u5229\u7528\u5f3a\u5355\u8c03\u6027\u5047\u8bbe\u3001\u6982\u7387\u72ec\u7acb\u6027\u5047\u8bbe\u548c\u5207\u7a7a\u95f4\u53ef\u5206\u79bb\u6027\u5047\u8bbe\u8bc1\u660e\u6536\u655b\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u7b97\u6cd5\u5728\u5747\u503c\u548c\u51e0\u4e4e\u5fc5\u7136\u610f\u4e49\u4e0b\u7684\u6536\u655b\u6027\uff0c\u6784\u9020\u4e86\u9ad8\u5ea6\u5747\u5300\u7684\u663e\u5f0f\u6536\u655b\u901f\u7387\uff0c\u8fd9\u4e9b\u901f\u7387\u5728Hilbert\u7a7a\u95f4\u80cc\u666f\u4e0b\u4e5f\u662f\u65b0\u9896\u7684\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u5c06\u968f\u673a\u8fd1\u7aef\u70b9\u7b97\u6cd5\u6269\u5c55\u5230Hadamard\u6d41\u5f62\uff0c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6536\u655b\u8bc1\u660e\u548c\u663e\u5f0f\u6536\u655b\u901f\u7387\uff0c\u5728\u9644\u52a0\u4e8c\u9636\u77e9\u6761\u4ef6\u4e0b\u8fd8\u8ba8\u8bba\u4e86\u7ebf\u6027\u975e\u6e10\u8fd1\u4fdd\u8bc1\u3002"}}
{"id": "2510.09854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09854", "abs": "https://arxiv.org/abs/2510.09854", "authors": ["Kaiwen Shi", "Zheyuan Zhang", "Zhengqing Yuan", "Keerthiram Murugesan", "Vincent Galass", "Chuxu Zhang", "Yanfang Ye"], "title": "NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering", "comment": null, "summary": "Diet plays a central role in human health, and Nutrition Question Answering\n(QA) offers a promising path toward personalized dietary guidance and the\nprevention of diet-related chronic diseases. However, existing methods face two\nfundamental challenges: the limited reasoning capacity of single-agent systems\nand the complexity of designing effective multi-agent architectures, as well as\ncontextual overload that hinders accurate decision-making. We introduce\nNutritional-Graph Router (NG-Router), a novel framework that formulates\nnutritional QA as a supervised, knowledge-graph-guided multi-agent\ncollaboration problem. NG-Router integrates agent nodes into heterogeneous\nknowledge graphs and employs a graph neural network to learn task-aware routing\ndistributions over agents, leveraging soft supervision derived from empirical\nagent performance. To further address contextual overload, we propose a\ngradient-based subgraph retrieval mechanism that identifies salient evidence\nduring training, thereby enhancing multi-hop and relational reasoning.\nExtensive experiments across multiple benchmarks and backbone models\ndemonstrate that NG-Router consistently outperforms both single-agent and\nensemble baselines, offering a principled approach to domain-aware multi-agent\nreasoning for complex nutritional health tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86NG-Router\u6846\u67b6\uff0c\u5c06\u8425\u517b\u95ee\u7b54\u5efa\u6a21\u4e3a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u4efb\u52a1\u611f\u77e5\u7684\u8def\u7531\u5206\u5e03\uff0c\u5e76\u4f7f\u7528\u68af\u5ea6\u5b50\u56fe\u68c0\u7d22\u89e3\u51b3\u4e0a\u4e0b\u6587\u8fc7\u8f7d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u5355\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\u6709\u9650\u548c\u591a\u667a\u80fd\u4f53\u67b6\u6784\u8bbe\u8ba1\u590d\u6742\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u4e0a\u4e0b\u6587\u8fc7\u8f7d\u963b\u788d\u51c6\u786e\u51b3\u7b56\u7684\u6311\u6218\u3002", "method": "\u5c06\u667a\u80fd\u4f53\u8282\u70b9\u96c6\u6210\u5230\u5f02\u8d28\u77e5\u8bc6\u56fe\u8c31\u4e2d\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u667a\u80fd\u4f53\u8def\u7531\u5206\u5e03\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u5b50\u56fe\u68c0\u7d22\u673a\u5236\u8bc6\u522b\u5173\u952e\u8bc1\u636e\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u9aa8\u5e72\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNG-Router\u59cb\u7ec8\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u548c\u96c6\u6210\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "NG-Router\u4e3a\u590d\u6742\u8425\u517b\u5065\u5eb7\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u9886\u57df\u611f\u77e5\u591a\u667a\u80fd\u4f53\u63a8\u7406\u65b9\u6cd5\u3002"}}
{"id": "2510.09684", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.09684", "abs": "https://arxiv.org/abs/2510.09684", "authors": ["Chris Engh", "P. M. Aronow"], "title": "Using LLMs to Directly Guess Conditional Expectations Can Improve Efficiency in Causal Estimation", "comment": null, "summary": "We propose a simple yet effective use of LLM-powered AI tools to improve\ncausal estimation. In double machine learning, the accuracy of causal estimates\nof the effect of a treatment on an outcome in the presence of a\nhigh-dimensional confounder depends on the performance of estimators of\nconditional expectation functions. We show that predictions made by generative\nmodels trained on historical data can be used to improve the performance of\nthese estimators relative to approaches that solely rely on adjusting for\nembeddings extracted from these models. We argue that the historical knowledge\nand reasoning capacities associated with these generative models can help\novercome curse-of-dimensionality problems in causal inference problems. We\nconsider a case study using a small dataset of online jewelry auctions, and\ndemonstrate that inclusion of LLM-generated guesses as predictors can improve\nefficiency in estimation.", "AI": {"tldr": "\u4f7f\u7528LLM\u751f\u6210\u7684\u9884\u6d4b\u4f5c\u4e3a\u8f85\u52a9\u53d8\u91cf\u53ef\u4ee5\u6539\u8fdb\u53cc\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u56e0\u679c\u4f30\u8ba1\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u6df7\u6742\u56e0\u7d20\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u53cc\u673a\u5668\u5b66\u4e60\u4e2d\u56e0\u679c\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u4f9d\u8d56\u4e8e\u6761\u4ef6\u671f\u671b\u51fd\u6570\u7684\u4f30\u8ba1\u6027\u80fd\uff0c\u800c\u9ad8\u7ef4\u6df7\u6742\u56e0\u7d20\u4f1a\u5bfc\u81f4\u7ef4\u5ea6\u8bc5\u5492\u95ee\u9898\u3002LLM\u7684\u5386\u53f2\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u53ef\u80fd\u5e2e\u52a9\u514b\u670d\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5728\u53cc\u673a\u5668\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u5c06LLM\u57fa\u4e8e\u5386\u53f2\u6570\u636e\u8bad\u7ec3\u7684\u751f\u6210\u6a21\u578b\u6240\u505a\u7684\u9884\u6d4b\u4f5c\u4e3a\u9884\u6d4b\u53d8\u91cf\uff0c\u4e0e\u4ec5\u8c03\u6574\u6a21\u578b\u5d4c\u5165\u7684\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5728\u5728\u7ebf\u73e0\u5b9d\u62cd\u5356\u7684\u5c0f\u578b\u6570\u636e\u96c6\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u5305\u542bLLM\u751f\u6210\u7684\u731c\u6d4b\u4f5c\u4e3a\u9884\u6d4b\u53d8\u91cf\u53ef\u4ee5\u63d0\u9ad8\u4f30\u8ba1\u6548\u7387\u3002", "conclusion": "LLM\u9a71\u52a8\u7684AI\u5de5\u5177\u53ef\u4ee5\u7b80\u5355\u6709\u6548\u5730\u6539\u8fdb\u56e0\u679c\u4f30\u8ba1\uff0c\u5176\u751f\u6210\u9884\u6d4b\u7684\u80fd\u529b\u4f18\u4e8e\u4ec5\u4f7f\u7528\u5d4c\u5165\u8c03\u6574\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.10117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10117", "abs": "https://arxiv.org/abs/2510.10117", "authors": ["Yunxiang Mo", "Tianshi Zheng", "Qing Zong", "Jiayu Liu", "Baixuan Xu", "Yauwai Yim", "Chunkit Chan", "Jiaxin Bai", "Yangqiu Song"], "title": "DixitWorld: Evaluating Multimodal Abductive Reasoning in Vision-Language Models with Multi-Agent Dixit Gameplay", "comment": "EMNLP 2025 Wordplay (Spotlight)", "summary": "Multimodal abductive reasoning--the generation and selection of explanatory\nhypotheses from partial observations--is a cornerstone of intelligence. Current\nevaluations of this ability in vision-language models (VLMs) are largely\nconfined to static, single-agent tasks. Inspired by Dixit, we introduce\nDixitWorld, a comprehensive evaluation suite designed to deconstruct this\nchallenge. DIXITWORLD features two core components: DixitArena, a dynamic,\nmulti-agent environment that evaluates both hypothesis generation (a\n\"storyteller\" crafting cryptic clues) and hypothesis selection (\"listeners\"\nchoosing the target image from decoys) under imperfect information; and\nDixitBench, a static QA benchmark that isolates the listener's task for\nefficient, controlled evaluation. Results from DixitArena reveal distinct,\nrole-dependent behaviors: smaller open-source models often excel as creative\nstorytellers, producing imaginative yet less discriminative clues, whereas\nlarger proprietary models demonstrate superior overall performance,\nparticularly as listeners. Performance on DixitBench strongly correlates with\nlistener results in DixitArena, validating it as a reliable proxy for\nhypothesis selection. Our findings reveal a key trade-off between generative\ncreativity and discriminative understanding in multimodal abductive reasoning,\na central challenge for developing more balanced and capable vision-language\nagents.", "AI": {"tldr": "\u63d0\u51fa\u4e86DixitWorld\u8bc4\u4f30\u5957\u4ef6\uff0c\u5305\u542bDixitArena\u591a\u667a\u80fd\u4f53\u73af\u5883\u548cDixitBench\u9759\u6001\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u6eaf\u56e0\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u751f\u6210\u521b\u9020\u6027\u548c\u5224\u522b\u7406\u89e3\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "motivation": "\u5f53\u524d\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u591a\u6a21\u6001\u6eaf\u56e0\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\u4e3b\u8981\u5c40\u9650\u4e8e\u9759\u6001\u3001\u5355\u667a\u80fd\u4f53\u4efb\u52a1\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u89e3\u6784\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u8bbe\u8ba1DixitWorld\u8bc4\u4f30\u5957\u4ef6\uff0c\u5305\u542b\uff1a1) DixitArena - \u52a8\u6001\u591a\u667a\u80fd\u4f53\u73af\u5883\uff0c\u8bc4\u4f30\u5047\u8bbe\u751f\u6210\uff08\u8bb2\u6545\u4e8b\u8005\uff09\u548c\u5047\u8bbe\u9009\u62e9\uff08\u542c\u4f17\uff09\uff1b2) DixitBench - \u9759\u6001\u95ee\u7b54\u57fa\u51c6\uff0c\u9694\u79bb\u542c\u4f17\u4efb\u52a1\u8fdb\u884c\u63a7\u5236\u8bc4\u4f30\u3002", "result": "\u8f83\u5c0f\u5f00\u6e90\u6a21\u578b\u4f5c\u4e3a\u8bb2\u6545\u4e8b\u8005\u8868\u73b0\u66f4\u597d\uff0c\u4ea7\u751f\u66f4\u5177\u60f3\u8c61\u529b\u4f46\u533a\u5206\u5ea6\u8f83\u4f4e\u7684\u7ebf\u7d22\uff1b\u8f83\u5927\u4e13\u6709\u6a21\u578b\u6574\u4f53\u8868\u73b0\u66f4\u4f18\uff0c\u7279\u522b\u662f\u5728\u4f5c\u4e3a\u542c\u4f17\u65f6\u3002DixitBench\u7ed3\u679c\u4e0eDixitArena\u542c\u4f17\u8868\u73b0\u5f3a\u76f8\u5173\u3002", "conclusion": "\u591a\u6a21\u6001\u6eaf\u56e0\u63a8\u7406\u4e2d\u5b58\u5728\u751f\u6210\u521b\u9020\u6027\u548c\u5224\u522b\u7406\u89e3\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\uff0c\u8fd9\u662f\u5f00\u53d1\u66f4\u5e73\u8861\u3001\u80fd\u529b\u66f4\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\u7684\u6838\u5fc3\u6311\u6218\u3002"}}
{"id": "2510.09775", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09775", "abs": "https://arxiv.org/abs/2510.09775", "authors": ["Alex Hiles", "Bashar I. Ahmad"], "title": "A Generic Machine Learning Framework for Radio Frequency Fingerprinting", "comment": null, "summary": "Fingerprinting Radio Frequency (RF) emitters typically involves finding\nunique emitter characteristics that are featured in their transmitted signals.\nThese fingerprints are nuanced but sufficiently detailed, motivating the\npursuit of methods that can successfully extract them. The most granular\ndownstream task is known as Specific Emitter Identification (SEI), which\nrequires a well informed RF fingerprinting (RFF) approach for it to be\nsuccessful. RFF and SEI have a long history, with numerous application areas in\ndefence and civilian contexts such as signal intelligence, electronic\nsurveillance, physical-layer authentication of wireless communication devices,\nto name a few. RFF methods also support many other downstream tasks such as\nEmitter Data Association (EDA) and RF Emitter Clustering (RFEC) and are\napplicable to a range of transmission types. In recent years, data-driven\napproaches have become popular in the RFF domain due to their ability to\nautomatically learn intricate fingerprints from raw data. These methods\ngenerally deliver superior performance when compared to traditional techniques.\nThe more traditional approaches are often labour-intensive, inflexible and only\napplicable to a particular emitter type or transmission scheme. Therefore, we\nconsider data-driven Machine Learning (ML)-enabled RFF. In particular, we\npropose a generic framework for ML-enabled RFF which is inclusive of several\npopular downstream tasks such as SEI, EDA and RFEC. Each task is formulated as\na RF fingerprint-dependent task. A variety of use cases using real RF datasets\nare presented here to demonstrate the framework for a range of tasks and\napplication areas, such as spaceborne surveillance, signal intelligence and\ncountering drones.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u5c04\u9891\u6307\u7eb9\u8bc6\u522b\u6846\u67b6\uff0c\u652f\u6301\u7279\u5b9a\u53d1\u5c04\u5668\u8bc6\u522b\u3001\u53d1\u5c04\u5668\u6570\u636e\u5173\u8054\u548c\u5c04\u9891\u53d1\u5c04\u5668\u805a\u7c7b\u7b49\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u7684\u5c04\u9891\u6307\u7eb9\u8bc6\u522b\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u3001\u7075\u6d3b\u6027\u5dee\u4e14\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u53d1\u5c04\u5668\u7c7b\u578b\uff0c\u800c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u80fd\u81ea\u52a8\u4ece\u539f\u59cb\u6570\u636e\u4e2d\u5b66\u4e60\u590d\u6742\u6307\u7eb9\uff0c\u6027\u80fd\u66f4\u4f18\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u673a\u5668\u5b66\u4e60\u5c04\u9891\u6307\u7eb9\u8bc6\u522b\u6846\u67b6\uff0c\u5c06\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\uff08SEI\u3001EDA\u3001RFEC\uff09\u7edf\u4e00\u4e3a\u5c04\u9891\u6307\u7eb9\u4f9d\u8d56\u4efb\u52a1\uff0c\u5e76\u4f7f\u7528\u771f\u5b9e\u5c04\u9891\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u8be5\u6846\u67b6\u5728\u591a\u79cd\u5e94\u7528\u573a\u666f\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5305\u62ec\u7a7a\u95f4\u76d1\u89c6\u3001\u4fe1\u53f7\u60c5\u62a5\u548c\u53cd\u65e0\u4eba\u673a\u7b49\uff0c\u5c55\u793a\u4e86\u5176\u901a\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u673a\u5668\u5b66\u4e60\u5c04\u9891\u6307\u7eb9\u8bc6\u522b\u6846\u67b6\u4e3a\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u56fd\u9632\u548c\u6c11\u7528\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.10651", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.10651", "abs": "https://arxiv.org/abs/2510.10651", "authors": ["Mohammad Hassan", "Mads R. Almassalkhi"], "title": "Aggregate Modeling of Air-Conditioner Loads Under Packet-based Control with Both On and Off Grid Access Requests", "comment": null, "summary": "Coordination of distributed energy resources (DERs) can engender flexibility\nnecessary to improve grid reliability. Packetized Energy Management (PEM) is a\nmethod for coordinating DERs, such as thermostatically controlled loads (TCLs)\nand electric vehicles, within customer quality-of-service (QoS) limits. In PEM,\na DER uses local information to offer flexibility by sending a request to the\nDER coordinator to turn-ON or turn-OFF. Much work has focused on modeling and\nanalyzing aggregations of DERs under PEM with fixed packet durations and only\nturn-ON requests. Different recent efforts to enable variable packet lengths\nhave shown an increase in available flexibility and ramping capability, but\nhave not been modeled in aggregate, which limits systematic analyses. To\naddress this issue, this paper presents a new aggregate bin-based (macro) model\nof PEM loads that incorporates both turn-ON and turn-OFF request features,\nenabling the model to accurately characterize the capability of the fleet of\nDERs to track a power reference signal, population temperature dynamics,\naggregate request rates, and variable packet lengths. Simulation-based\nvalidation is performed against an agent-based (micro) model to evaluate\nrobustness and quantify model accuracy. Finally, the distribution of variable\npacket lengths from macro-model simulations are applied to inform past work on\nPEM with randomized packet lengths", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5206\u7bb1\u7684\u805a\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u6790\u5177\u6709\u53ef\u53d8\u6570\u636e\u5305\u957f\u5ea6\u548c\u5f00\u5173\u8bf7\u6c42\u529f\u80fd\u7684\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\u534f\u8c03\u7cfb\u7edf\uff0c\u6539\u8fdb\u4e86\u5bf9\u7cfb\u7edf\u7075\u6d3b\u6027\u548c\u529f\u7387\u8ddf\u8e2a\u80fd\u529b\u7684\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\u534f\u8c03\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u56fa\u5b9a\u6570\u636e\u5305\u957f\u5ea6\u548c\u4ec5\u5f00\u542f\u8bf7\u6c42\uff0c\u65e0\u6cd5\u51c6\u786e\u5efa\u6a21\u53ef\u53d8\u6570\u636e\u5305\u957f\u5ea6\u5e26\u6765\u7684\u7075\u6d3b\u6027\u63d0\u5347\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u5206\u6790\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u5206\u7bb1\u7684\u805a\u5408\u6a21\u578b\uff0c\u6574\u5408\u4e86\u5f00\u542f\u548c\u5173\u95ed\u8bf7\u6c42\u529f\u80fd\uff0c\u80fd\u591f\u51c6\u786e\u8868\u5f81DER\u96c6\u7fa4\u7684\u529f\u7387\u8ddf\u8e2a\u80fd\u529b\u3001\u6e29\u5ea6\u52a8\u6001\u3001\u805a\u5408\u8bf7\u6c42\u7387\u548c\u53ef\u53d8\u6570\u636e\u5305\u957f\u5ea6\u3002", "result": "\u901a\u8fc7\u4e0e\u57fa\u4e8e\u4ee3\u7406\u7684\u5fae\u89c2\u6a21\u578b\u8fdb\u884c\u4eff\u771f\u9a8c\u8bc1\uff0c\u8bc4\u4f30\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u5e76\u5c06\u53ef\u53d8\u6570\u636e\u5305\u957f\u5ea6\u5206\u5e03\u5e94\u7528\u4e8e\u5148\u524d\u7684\u5de5\u4f5c\u4e2d\u3002", "conclusion": "\u8be5\u805a\u5408\u6a21\u578b\u4e3a\u7cfb\u7edf\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u5de5\u5177\uff0c\u80fd\u591f\u66f4\u597d\u5730\u8868\u5f81\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\u534f\u8c03\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2510.11556", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2510.11556", "abs": "https://arxiv.org/abs/2510.11556", "authors": ["Javed Ali Khan", "Muhammad Yaqoob", "Mamoona Tasadduq", "Hafsa Shareef Dar", "Aitezaz Ahsan"], "title": "Personalized and Constructive Feedback for Computer Science Students Using the Large Language Model (LLM)", "comment": null, "summary": "The evolving pedagogy paradigms are leading toward educational\ntransformations. One fundamental aspect of effective learning is relevant,\nimmediate, and constructive feedback to students. Providing constructive\nfeedback to large cohorts in academia is an ongoing challenge. Therefore,\nacademics are moving towards automated assessment to provide immediate\nfeedback. However, current approaches are often limited in scope, offering\nsimplistic responses that do not provide students with personalized feedback to\nguide them toward improvements. This paper addresses this limitation by\ninvestigating the performance of Large Language Models (LLMs) in processing\nstudents assessments with predefined rubrics and marking criteria to generate\npersonalized feedback for in-depth learning. We aim to leverage the power of\nexisting LLMs for Marking Assessments, Tracking, and Evaluation (LLM-MATE) with\npersonalized feedback to enhance students learning. To evaluate the performance\nof LLM-MATE, we consider the Software Architecture (SA) module as a case study.\nThe LLM-MATE approach can help module leaders overcome assessment challenges\nwith large cohorts. Also, it helps students improve their learning by obtaining\npersonalized feedback in a timely manner. Additionally, the proposed approach\nwill facilitate the establishment of ground truth for automating the generation\nof students assessment feedback using the ChatGPT API, thereby reducing the\noverhead associated with large cohort assessments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faLLM-MATE\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u9884\u5b9a\u4e49\u8bc4\u5206\u6807\u51c6\u5904\u7406\u5b66\u751f\u8bc4\u4f30\uff0c\u751f\u6210\u4e2a\u6027\u5316\u53cd\u9988\u4ee5\u89e3\u51b3\u5927\u89c4\u6a21\u73ed\u7ea7\u8bc4\u4f30\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u53cd\u9988\u8fc7\u4e8e\u7b80\u5355\uff0c\u65e0\u6cd5\u63d0\u4f9b\u4e2a\u6027\u5316\u6307\u5bfc\u3002\u5b66\u672f\u754c\u9700\u8981\u4e3a\u5927\u89c4\u6a21\u73ed\u7ea7\u63d0\u4f9b\u53ca\u65f6\u3001\u76f8\u5173\u7684\u5efa\u8bbe\u6027\u53cd\u9988\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u5b66\u751f\u8bc4\u4f30\uff0c\u7ed3\u5408\u9884\u5b9a\u4e49\u7684\u8bc4\u5206\u6807\u51c6\u548c\u6807\u8bb0\u51c6\u5219\uff0c\u751f\u6210\u4e2a\u6027\u5316\u53cd\u9988\u3002\u4ee5\u8f6f\u4ef6\u67b6\u6784\u6a21\u5757\u4e3a\u6848\u4f8b\u7814\u7a76\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "LLM-MATE\u65b9\u6cd5\u80fd\u5e2e\u52a9\u6559\u5e08\u5e94\u5bf9\u5927\u89c4\u6a21\u73ed\u7ea7\u8bc4\u4f30\u6311\u6218\uff0c\u8ba9\u5b66\u751f\u53ca\u65f6\u83b7\u5f97\u4e2a\u6027\u5316\u53cd\u9988\u6539\u8fdb\u5b66\u4e60\uff0c\u540c\u65f6\u901a\u8fc7ChatGPT API\u5efa\u7acb\u8bc4\u4f30\u53cd\u9988\u7684\u81ea\u52a8\u5316\u751f\u6210\u57fa\u7840\u3002", "conclusion": "LLM-MATE\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u73ed\u7ea7\u4e2a\u6027\u5316\u53cd\u9988\u7684\u96be\u9898\uff0c\u63d0\u5347\u4e86\u5b66\u751f\u5b66\u4e60\u6548\u679c\uff0c\u51cf\u8f7b\u4e86\u6559\u5e08\u8bc4\u4f30\u8d1f\u62c5\u3002"}}
{"id": "2510.10832", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.10832", "abs": "https://arxiv.org/abs/2510.10832", "authors": ["Baptiste Rabecq", "Thomas Lee", "Andy Sun"], "title": "Dynamic Line Ratings in AC Optimal Power Flow: Transient Temperature, Decomposition, and Large-scale Evaluation", "comment": "10 pages, 4 figures", "summary": "As power grids experience increasing renewable penetration and rapid load\ngrowth from AI data centers and electrification, alleviating line congestion\nbecomes critical to unlocking additional grid capacity. This work investigates\nDynamic Line Rating (DLR), a congestion mitigation method that adjusts power\nline current limits in response to meteorological conditions. Unlike\ntraditional approaches that impose predefined time-varying limits, we propose a\nnovel optimization framework that embeds the transient-state heat equation\ngoverning conductor temperature dynamics, enabling direct constraints on\nconductor temperature rather than simplified steady-state approximations. We\nderive a closed-form solution to the heat equation, enabling a\nfinite-dimensional reformulation of the dynamics. We then leverage a\ndistributed decomposition method, a bi-level Alternating Direction Method of\nMultipliers (ADMM) algorithm with provable convergence, aided by regularity\nproperties of the heat equation solution. These modeling and algorithmic\ninnovations allow us to conduct the first large-scale evaluation of DLR using\nmulti-period AC optimal power flow. Numerical experiments on the 2000-bus Texas\ngrid demonstrate that DLR allows significant reduction in generation cost in\ncongested systems over Static Line Rating (SLR) and Ambient Adjusted Ratings\n(AAR). The transient temperature formulation provides additional grid\nflexibility and headroom benefits with minimal computational overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77ac\u6001\u70ed\u65b9\u7a0b\u7684\u52a8\u6001\u7ebf\u8def\u8bc4\u7ea7\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u7ea6\u675f\u5bfc\u4f53\u6e29\u5ea6\u800c\u975e\u7a33\u6001\u8fd1\u4f3c\uff0c\u663e\u8457\u63d0\u5347\u7535\u7f51\u5bb9\u91cf\u548c\u964d\u4f4e\u53d1\u7535\u6210\u672c\u3002", "motivation": "\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\u6e17\u900f\u7387\u63d0\u9ad8\u548cAI\u6570\u636e\u4e2d\u5fc3\u3001\u7535\u6c14\u5316\u5e26\u6765\u7684\u8d1f\u8377\u5feb\u901f\u589e\u957f\uff0c\u7f13\u89e3\u7ebf\u8def\u62e5\u585e\u5bf9\u4e8e\u91ca\u653e\u989d\u5916\u7535\u7f51\u5bb9\u91cf\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86\u5d4c\u5165\u5bfc\u4f53\u6e29\u5ea6\u52a8\u6001\u77ac\u6001\u70ed\u65b9\u7a0b\u7684\u4f18\u5316\u6846\u67b6\uff0c\u63a8\u5bfc\u70ed\u65b9\u7a0b\u7684\u95ed\u5f0f\u89e3\u5b9e\u73b0\u6709\u9650\u7ef4\u91cd\u6784\uff0c\u91c7\u7528\u53cc\u5c42ADMM\u5206\u5e03\u5f0f\u5206\u89e3\u7b97\u6cd5\u3002", "result": "\u57282000\u603b\u7ebf\u5fb7\u514b\u8428\u65af\u7535\u7f51\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u9759\u6001\u7ebf\u8def\u8bc4\u7ea7\u548c\u73af\u5883\u8c03\u6574\u8bc4\u7ea7\uff0cDLR\u663e\u8457\u964d\u4f4e\u4e86\u62e5\u585e\u7cfb\u7edf\u7684\u53d1\u7535\u6210\u672c\u3002", "conclusion": "\u77ac\u6001\u6e29\u5ea6\u516c\u5f0f\u4ee5\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u7535\u7f51\u7075\u6d3b\u6027\u548c\u5bb9\u91cf\u4f59\u91cf\u4f18\u52bf\u3002"}}
{"id": "2510.09869", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09869", "abs": "https://arxiv.org/abs/2510.09869", "authors": ["Sil Hamilton", "Matthew Wilkens", "Andrew Piper"], "title": "NarraBench: A Comprehensive Framework for Narrative Benchmarking", "comment": null, "summary": "We present NarraBench, a theory-informed taxonomy of narrative-understanding\ntasks, as well as an associated survey of 78 existing benchmarks in the area.\nWe find significant need for new evaluations covering aspects of narrative\nunderstanding that are either overlooked in current work or are poorly aligned\nwith existing metrics. Specifically, we estimate that only 27% of narrative\ntasks are well captured by existing benchmarks, and we note that some areas --\nincluding narrative events, style, perspective, and revelation -- are nearly\nabsent from current evaluations. We also note the need for increased\ndevelopment of benchmarks capable of assessing constitutively subjective and\nperspectival aspects of narrative, that is, aspects for which there is\ngenerally no single correct answer. Our taxonomy, survey, and methodology are\nof value to NLP researchers seeking to test LLM narrative understanding.", "AI": {"tldr": "NarraBench\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7406\u8bba\u7684\u6545\u4e8b\u7406\u89e3\u4efb\u52a1\u5206\u7c7b\u6cd5\uff0c\u5e76\u8c03\u67e5\u4e86\u8be5\u9886\u57df\u768478\u4e2a\u73b0\u6709\u57fa\u51c6\uff0c\u53d1\u73b0\u73b0\u6709\u8bc4\u4f30\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u7684\u6545\u4e8b\u7406\u89e3\u8bc4\u4f30\u8986\u76d6\u4e0d\u8db3\uff0c\u53ea\u670927%\u7684\u4efb\u52a1\u88ab\u73b0\u6709\u57fa\u51c6\u5145\u5206\u6355\u6349\uff0c\u8bb8\u591a\u91cd\u8981\u65b9\u9762\u5982\u53d9\u4e8b\u4e8b\u4ef6\u3001\u98ce\u683c\u3001\u89c6\u89d2\u548c\u542f\u793a\u7b49\u51e0\u4e4e\u7f3a\u5931\u3002", "method": "\u5f00\u53d1\u4e86\u7406\u8bba\u6307\u5bfc\u7684\u6545\u4e8b\u7406\u89e3\u4efb\u52a1\u5206\u7c7b\u6cd5\uff0c\u5e76\u5bf978\u4e2a\u73b0\u6709\u57fa\u51c6\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8c03\u67e5\u548c\u5206\u6790\u3002", "result": "\u53d1\u73b0\u73b0\u6709\u57fa\u51c6\u4ec5\u8986\u76d627%\u7684\u53d9\u4e8b\u4efb\u52a1\uff0c\u8bb8\u591a\u5173\u952e\u65b9\u9762\u8bc4\u4f30\u7f3a\u5931\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u4e3b\u89c2\u6027\u548c\u89c6\u89d2\u6027\u65b9\u9762\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u6765\u8986\u76d6\u88ab\u5ffd\u89c6\u7684\u53d9\u4e8b\u7406\u89e3\u65b9\u9762\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u4e3b\u89c2\u6027\u548c\u89c6\u89d2\u6027\u7684\u65b9\u9762\uff0c\u8fd9\u5bf9\u6d4b\u8bd5LLM\u7684\u6545\u4e8b\u7406\u89e3\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.09685", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NA", "math.NA", "A.1; I.2; I.4"], "pdf": "https://arxiv.org/pdf/2510.09685", "abs": "https://arxiv.org/abs/2510.09685", "authors": ["Yongshuai Liu", "Lianfang Wang", "Kuilin Qin", "Qinghua Zhang", "Faqiang Wang", "Li Cui", "Jun Liu", "Yuping Duan", "Tieyong Zeng"], "title": "Deep Neural Networks Inspired by Differential Equations", "comment": "35 Pages, 3 figures", "summary": "Deep learning has become a pivotal technology in fields such as computer\nvision, scientific computing, and dynamical systems, significantly advancing\nthese disciplines. However, neural Networks persistently face challenges\nrelated to theoretical understanding, interpretability, and generalization. To\naddress these issues, researchers are increasingly adopting a differential\nequations perspective to propose a unified theoretical framework and systematic\ndesign methodologies for neural networks. In this paper, we provide an\nextensive review of deep neural network architectures and dynamic modeling\nmethods inspired by differential equations. We specifically examine deep neural\nnetwork models and deterministic dynamical network constructs based on ordinary\ndifferential equations (ODEs), as well as regularization techniques and\nstochastic dynamical network models informed by stochastic differential\nequations (SDEs). We present numerical comparisons of these models to\nillustrate their characteristics and performance. Finally, we explore promising\nresearch directions in integrating differential equations with deep learning to\noffer new insights for developing intelligent computational methods that boast\nenhanced interpretability and generalization capabilities.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5fae\u5206\u65b9\u7a0b\u89c6\u89d2\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u52a8\u6001\u5efa\u6a21\u65b9\u6cd5\uff0c\u5305\u62ecODE\u542f\u53d1\u7684\u6df1\u5ea6\u7f51\u7edc\u6a21\u578b\u548c\u786e\u5b9a\u6027\u52a8\u6001\u7f51\u7edc\u6784\u5efa\uff0c\u4ee5\u53caSDE\u542f\u53d1\u7684\u6b63\u5219\u5316\u6280\u672f\u548c\u968f\u673a\u52a8\u6001\u7f51\u7edc\u6a21\u578b\uff0c\u65e8\u5728\u4e3a\u5f00\u53d1\u5177\u6709\u66f4\u597d\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u667a\u80fd\u8ba1\u7b97\u65b9\u6cd5\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u795e\u7ecf\u7f51\u7edc\u5728\u7406\u8bba\u7406\u89e3\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u5fae\u5206\u65b9\u7a0b\u7684\u89c6\u89d2\u4e3a\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u548c\u7cfb\u7edf\u5316\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7cfb\u7edf\u56de\u987e\u57fa\u4e8eODE\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u548c\u786e\u5b9a\u6027\u52a8\u6001\u7f51\u7edc\u6784\u5efa\uff0c\u4ee5\u53ca\u57fa\u4e8eSDE\u7684\u6b63\u5219\u5316\u6280\u672f\u548c\u968f\u673a\u52a8\u6001\u7f51\u7edc\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u6570\u503c\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6bd4\u8f83\u5c55\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u7279\u6027\u548c\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5fae\u5206\u65b9\u7a0b\u89c6\u89d2\u5728\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5c06\u5fae\u5206\u65b9\u7a0b\u4e0e\u6df1\u5ea6\u5b66\u4e60\u76f8\u7ed3\u5408\u662f\u672a\u6765\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\uff0c\u6709\u671b\u5f00\u53d1\u51fa\u5177\u6709\u66f4\u5f3a\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u667a\u80fd\u8ba1\u7b97\u65b9\u6cd5\u3002"}}
{"id": "2510.10135", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10135", "abs": "https://arxiv.org/abs/2510.10135", "authors": ["Zhongsheng Wang", "Ming Lin", "Zhedong Lin", "Yaser Shakib", "Qian Liu", "Jiamou Liu"], "title": "CharCom: Composable Identity Control for Multi-Character Story Illustration", "comment": "Accepted by ACM MMAsia 2025", "summary": "Ensuring character identity consistency across varying prompts remains a\nfundamental limitation in diffusion-based text-to-image generation. We propose\nCharCom, a modular and parameter-efficient framework that achieves\ncharacter-consistent story illustration through composable LoRA adapters,\nenabling efficient per-character customization without retraining the base\nmodel. Built on a frozen diffusion backbone, CharCom dynamically composes\nadapters at inference using prompt-aware control. Experiments on multi-scene\nnarratives demonstrate that CharCom significantly enhances character fidelity,\nsemantic alignment, and temporal coherence. It remains robust in crowded scenes\nand enables scalable multi-character generation with minimal overhead, making\nit well-suited for real-world applications such as story illustration and\nanimation.", "AI": {"tldr": "CharCom\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u7ec4\u5408\u7684LoRA\u9002\u914d\u5668\u5b9e\u73b0\u89d2\u8272\u4e00\u81f4\u7684\u6545\u4e8b\u63d2\u56fe\u751f\u6210\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u57fa\u7840\u6a21\u578b", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u89d2\u8272\u8eab\u4efd\u5728\u4e0d\u540c\u63d0\u793a\u4e0b\u4fdd\u6301\u4e00\u81f4\u6027\u7684\u57fa\u672c\u9650\u5236", "method": "\u57fa\u4e8e\u51bb\u7ed3\u6269\u6563\u9aa8\u5e72\u7f51\u7edc\uff0c\u4f7f\u7528\u53ef\u7ec4\u5408\u7684LoRA\u9002\u914d\u5668\uff0c\u901a\u8fc7\u63d0\u793a\u611f\u77e5\u63a7\u5236\u5728\u63a8\u7406\u65f6\u52a8\u6001\u7ec4\u5408\u9002\u914d\u5668", "result": "\u5728\u591a\u573a\u666f\u53d9\u4e8b\u5b9e\u9a8c\u4e2d\u663e\u8457\u63d0\u5347\u89d2\u8272\u4fdd\u771f\u5ea6\u3001\u8bed\u4e49\u5bf9\u9f50\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5728\u62e5\u6324\u573a\u666f\u4e2d\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u591a\u89d2\u8272\u751f\u6210", "conclusion": "CharCom\u9002\u7528\u4e8e\u6545\u4e8b\u63d2\u56fe\u548c\u52a8\u753b\u7b49\u5b9e\u9645\u5e94\u7528\uff0c\u5177\u6709\u6700\u5c0f\u5f00\u9500\u7684\u53ef\u6269\u5c55\u591a\u89d2\u8272\u751f\u6210\u80fd\u529b"}}
{"id": "2510.09776", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09776", "abs": "https://arxiv.org/abs/2510.09776", "authors": ["Yufa Zhou", "Yixiao Wang", "Surbhi Goel", "Anru R. Zhang"], "title": "Why Do Transformers Fail to Forecast Time Series In-Context?", "comment": "Code: https://github.com/MasterZhou1/ICL-Time-Series", "summary": "Time series forecasting (TSF) remains a challenging and largely unsolved\nproblem in machine learning, despite significant recent efforts leveraging\nLarge Language Models (LLMs), which predominantly rely on Transformer\narchitectures. Empirical evidence consistently shows that even powerful\nTransformers often fail to outperform much simpler models, e.g., linear models,\non TSF tasks; however, a rigorous theoretical understanding of this phenomenon\nremains limited. In this paper, we provide a theoretical analysis of\nTransformers' limitations for TSF through the lens of In-Context Learning (ICL)\ntheory. Specifically, under AR($p$) data, we establish that: (1) Linear\nSelf-Attention (LSA) models $\\textit{cannot}$ achieve lower expected MSE than\nclassical linear models for in-context forecasting; (2) as the context length\napproaches to infinity, LSA asymptotically recovers the optimal linear\npredictor; and (3) under Chain-of-Thought (CoT) style inference, predictions\ncollapse to the mean exponentially. We empirically validate these findings\nthrough carefully designed experiments. Our theory not only sheds light on\nseveral previously underexplored phenomena but also offers practical insights\nfor designing more effective forecasting architectures. We hope our work\nencourages the broader research community to revisit the fundamental\ntheoretical limitations of TSF and to critically evaluate the direct\napplication of increasingly sophisticated architectures without deeper\nscrutiny.", "AI": {"tldr": "\u672c\u6587\u4ece\u7406\u8bba\u89d2\u5ea6\u5206\u6790\u4e86Transformer\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u8bc1\u660e\u7ebf\u6027\u81ea\u6ce8\u610f\u529b\u6a21\u578b\u65e0\u6cd5\u8d85\u8d8a\u7ecf\u5178\u7ebf\u6027\u6a21\u578b\uff0c\u5e76\u63ed\u793a\u4e86\u5728\u601d\u7ef4\u94fe\u63a8\u7406\u4e0b\u9884\u6d4b\u4f1a\u6307\u6570\u7ea7\u574d\u7f29\u5230\u5747\u503c\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5b9e\u8bc1\u663e\u793aTransformer\u5f80\u5f80\u65e0\u6cd5\u8d85\u8d8a\u7b80\u5355\u7ebf\u6027\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u4ece\u7406\u8bba\u5c42\u9762\u6df1\u5165\u7406\u89e3\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u7406\u8bba\u6846\u67b6\uff0c\u5728AR(p)\u6570\u636e\u5047\u8bbe\u4e0b\uff0c\u5bf9\u7ebf\u6027\u81ea\u6ce8\u610f\u529b\u6a21\u578b\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5b9e\u9a8c\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1a(1)\u7ebf\u6027\u81ea\u6ce8\u610f\u529b\u6a21\u578b\u65e0\u6cd5\u83b7\u5f97\u6bd4\u7ecf\u5178\u7ebf\u6027\u6a21\u578b\u66f4\u4f4e\u7684\u671f\u671bMSE\uff1b(2)\u5f53\u4e0a\u4e0b\u6587\u957f\u5ea6\u8d8b\u8fd1\u65e0\u7a77\u65f6\uff0cLSA\u6e10\u8fd1\u6062\u590d\u6700\u4f18\u7ebf\u6027\u9884\u6d4b\u5668\uff1b(3)\u5728\u601d\u7ef4\u94fe\u63a8\u7406\u4e0b\u9884\u6d4b\u6307\u6570\u7ea7\u574d\u7f29\u5230\u5747\u503c\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86Transformer\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6839\u672c\u7406\u8bba\u5c40\u9650\uff0c\u547c\u5401\u7814\u7a76\u793e\u533a\u91cd\u65b0\u5ba1\u89c6\u590d\u6742\u67b6\u6784\u7684\u76f4\u63a5\u5e94\u7528\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u9884\u6d4b\u67b6\u6784\u63d0\u4f9b\u5b9e\u8df5\u6d1e\u89c1\u3002"}}
{"id": "2510.10820", "categories": ["eess.SY", "cs.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.10820", "abs": "https://arxiv.org/abs/2510.10820", "authors": ["Maarten van der Hulst", "Rodrigo A. Gonz\u00e1lez", "Koen Classens", "Paul Tacx", "Nick Dirkx", "Jeroen van de Wijdeven", "Tom Oomen"], "title": "Structured identification of multivariable modal systems", "comment": "20 pages, 12 figures", "summary": "Physically interpretable models are essential for next-generation industrial\nsystems, as these representations enable effective control, support design\nvalidation, and provide a foundation for monitoring strategies. The aim of this\npaper is to develop a system identification framework for estimating modal\nmodels of complex multivariable mechanical systems from frequency response\ndata. To achieve this, a two-step structured identification algorithm is\npresented, where an additive model is first estimated using a refined\ninstrumental variable method and subsequently projected onto a modal form. The\ndeveloped identification method provides accurate, physically-relevant,\nminimal-order models, for both generally-damped and proportionally damped modal\nsystems. The effectiveness of the proposed method is demonstrated through\nexperimental validation on a prototype wafer-stage system, which features a\nlarge number of spatially distributed actuators and sensors and exhibits\ncomplex flexible dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4ece\u9891\u54cd\u6570\u636e\u4f30\u8ba1\u591a\u53d8\u91cf\u673a\u68b0\u7cfb\u7edf\u6a21\u6001\u6a21\u578b\u7684\u7cfb\u7edf\u8fa8\u8bc6\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u6b65\u7ed3\u6784\u5316\u7b97\u6cd5\u83b7\u5f97\u7269\u7406\u53ef\u89e3\u91ca\u7684\u6700\u5c0f\u9636\u6a21\u578b", "motivation": "\u7269\u7406\u53ef\u89e3\u91ca\u6a21\u578b\u5bf9\u4e8e\u4e0b\u4e00\u4ee3\u5de5\u4e1a\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u652f\u6301\u6709\u6548\u63a7\u5236\u3001\u8bbe\u8ba1\u9a8c\u8bc1\u548c\u76d1\u63a7\u7b56\u7565", "method": "\u91c7\u7528\u4e24\u6b65\u7ed3\u6784\u5316\u8fa8\u8bc6\u7b97\u6cd5\uff1a\u9996\u5148\u4f7f\u7528\u6539\u8fdb\u7684\u8f85\u52a9\u53d8\u91cf\u6cd5\u4f30\u8ba1\u52a0\u6027\u6a21\u578b\uff0c\u7136\u540e\u5c06\u5176\u6295\u5f71\u5230\u6a21\u6001\u5f62\u5f0f\u4e0a", "result": "\u8be5\u65b9\u6cd5\u4e3a\u4e00\u822c\u963b\u5c3c\u548c\u6bd4\u4f8b\u963b\u5c3c\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u51c6\u786e\u3001\u7269\u7406\u76f8\u5173\u7684\u6700\u5c0f\u9636\u6a21\u578b\uff0c\u5e76\u5728\u6676\u5706\u53f0\u539f\u578b\u7cfb\u7edf\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1", "conclusion": "\u6240\u63d0\u51fa\u7684\u8fa8\u8bc6\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5177\u6709\u5927\u91cf\u7a7a\u95f4\u5206\u5e03\u6267\u884c\u5668\u548c\u4f20\u611f\u5668\u7684\u590d\u6742\u67d4\u6027\u52a8\u529b\u5b66\u7cfb\u7edf"}}
{"id": "2507.14306", "categories": ["cs.AI", "cs.CY", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.14306", "abs": "https://arxiv.org/abs/2507.14306", "authors": ["Samarth P", "Vyoman Jain", "Shiva Golugula", "Motamarri Sai Sathvik"], "title": "Manimator: Transforming Research Papers into Visual Explanations", "comment": null, "summary": "Understanding complex scientific and mathematical concepts, particularly\nthose presented in dense research papers, poses a significant challenge for\nlearners. Dynamic visualizations can greatly enhance comprehension, but\ncreating them manually is time-consuming and requires specialized knowledge and\nskills. We introduce manimator, an open-source system that leverages Large\nLanguage Models to transform research papers and natural language prompts into\nexplanatory animations using the Manim engine. Manimator employs a pipeline\nwhere an LLM interprets the input text or research paper PDF to generate a\nstructured scene description outlining key concepts, mathematical formulas, and\nvisual elements and another LLM translates this description into executable\nManim Python code. We discuss its potential as an educational tool for rapidly\ncreating engaging visual explanations for complex STEM topics, democratizing\nthe creation of high-quality educational content.", "AI": {"tldr": "Manimator\u662f\u4e00\u4e2a\u5f00\u6e90\u7cfb\u7edf\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u7814\u7a76\u8bba\u6587\u548c\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8f6c\u6362\u4e3a\u4f7f\u7528Manim\u5f15\u64ce\u7684\u89e3\u91ca\u6027\u52a8\u753b\u3002", "motivation": "\u7406\u89e3\u590d\u6742\u79d1\u5b66\u548c\u6570\u5b66\u6982\u5ff5\uff0c\u7279\u522b\u662f\u5bc6\u96c6\u7814\u7a76\u8bba\u6587\u4e2d\u7684\u5185\u5bb9\uff0c\u5bf9\u5b66\u4e60\u8005\u6784\u6210\u91cd\u5927\u6311\u6218\u3002\u52a8\u6001\u53ef\u89c6\u5316\u53ef\u4ee5\u5927\u5927\u589e\u5f3a\u7406\u89e3\uff0c\u4f46\u624b\u52a8\u521b\u5efa\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u91c7\u7528\u6d41\u6c34\u7ebf\u65b9\u6cd5\uff1a\u4e00\u4e2aLLM\u89e3\u91ca\u8f93\u5165\u6587\u672c\u6216\u7814\u7a76\u8bba\u6587PDF\uff0c\u751f\u6210\u7ed3\u6784\u5316\u573a\u666f\u63cf\u8ff0\uff1b\u53e6\u4e00\u4e2aLLM\u5c06\u6b64\u63cf\u8ff0\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684Manim Python\u4ee3\u7801\u3002", "result": "\u5f00\u53d1\u4e86\u80fd\u591f\u81ea\u52a8\u751f\u6210\u89e3\u91ca\u6027\u52a8\u753b\u7684\u7cfb\u7edf\uff0c\u4e3a\u590d\u6742STEM\u4e3b\u9898\u5feb\u901f\u521b\u5efa\u5f15\u4eba\u5165\u80dc\u7684\u89c6\u89c9\u89e3\u91ca\u3002", "conclusion": "Manimator\u6709\u6f5c\u529b\u4f5c\u4e3a\u6559\u80b2\u5de5\u5177\uff0c\u6c11\u4e3b\u5316\u9ad8\u8d28\u91cf\u6559\u80b2\u5185\u5bb9\u7684\u521b\u5efa\uff0c\u4f7f\u590d\u6742\u6982\u5ff5\u66f4\u6613\u4e8e\u7406\u89e3\u3002"}}
{"id": "2510.10856", "categories": ["math.OC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10856", "abs": "https://arxiv.org/abs/2510.10856", "authors": ["Dirk Lauinger", "Luc Cot\u00e9", "Andy Sun"], "title": "Storage Participation in Electricity Markets: Arbitrage and Ancillary Services", "comment": null, "summary": "Electricity storage is used for intertemporal price arbitrage and for\nancillary services that balance unforeseen supply and demand fluctuations via\nfrequency regulation. We present an optimization model that computes bids for\nboth arbitrage and frequency regulation and ensures that storage operators can\nhonor their market commitments at all times for all fluctuation signals in an\nuncertainty set inspired by market rules. This requirement, initially expressed\nby an infinite number of nonconvex functional constraints, is shown to be\nequivalent to a finite number of deterministic constraints. The resulting\nformulation is a mixed-integer bilinear program that admits mixed-integer\nlinear relaxations and restrictions. Empirical tests on European electricity\nmarkets show a negligible optimality gap between the relaxation and the\nrestriction. The model can account for intraday trading and, with a solution\ntime of under 5 seconds, may serve as a building block for more complex trading\nstrategies. Such strategies become necessary as battery capacity exceeds the\ndemand for ancillary services. In a backtest from 1 July 2020 through 30 June\n2024 joint market participation more than doubles profits and almost halves\nenergy storage output compared to arbitrage alone.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f18\u5316\u6a21\u578b\uff0c\u7528\u4e8e\u7535\u529b\u5b58\u50a8\u540c\u65f6\u53c2\u4e0e\u5957\u5229\u548c\u9891\u7387\u8c03\u8282\u5e02\u573a\uff0c\u901a\u8fc7\u6df7\u5408\u6574\u6570\u53cc\u7ebf\u6027\u89c4\u5212\u786e\u4fdd\u5728\u6240\u6709\u5e02\u573a\u6ce2\u52a8\u60c5\u51b5\u4e0b\u90fd\u80fd\u5c65\u884c\u627f\u8bfa\u3002", "motivation": "\u968f\u7740\u7535\u6c60\u5bb9\u91cf\u8d85\u8fc7\u8f85\u52a9\u670d\u52a1\u9700\u6c42\uff0c\u9700\u8981\u66f4\u590d\u6742\u7684\u4ea4\u6613\u7b56\u7565\u6765\u6700\u5927\u5316\u5229\u6da6\u3002\u7535\u529b\u5b58\u50a8\u540c\u65f6\u53c2\u4e0e\u5957\u5229\u548c\u9891\u7387\u8c03\u8282\u5e02\u573a\u5b58\u5728\u534f\u8c03\u6311\u6218\u3002", "method": "\u5f00\u53d1\u6df7\u5408\u6574\u6570\u53cc\u7ebf\u6027\u89c4\u5212\u6a21\u578b\uff0c\u5c06\u65e0\u9650\u6570\u91cf\u7684\u975e\u51f8\u51fd\u6570\u7ea6\u675f\u8f6c\u5316\u4e3a\u6709\u9650\u6570\u91cf\u7684\u786e\u5b9a\u6027\u7ea6\u675f\uff0c\u652f\u6301\u6df7\u5408\u6574\u6570\u7ebf\u6027\u677e\u5f1b\u548c\u9650\u5236\u3002", "result": "\u5728\u6b27\u6d32\u7535\u529b\u5e02\u573a\u7684\u5b9e\u8bc1\u6d4b\u8bd5\u663e\u793a\u677e\u5f1b\u548c\u9650\u5236\u4e4b\u95f4\u7684\u6700\u4f18\u6027\u5dee\u8ddd\u53ef\u5ffd\u7565\uff0c\u6c42\u89e3\u65f6\u95f4\u5c0f\u4e8e5\u79d2\u30022020-2024\u5e74\u56de\u6d4b\u663e\u793a\u8054\u5408\u5e02\u573a\u53c2\u4e0e\u4f7f\u5229\u6da6\u7ffb\u500d\u4ee5\u4e0a\uff0c\u50a8\u80fd\u8f93\u51fa\u51cf\u534a\u3002", "conclusion": "\u8be5\u6a21\u578b\u53ef\u4f5c\u4e3a\u590d\u6742\u4ea4\u6613\u7b56\u7565\u7684\u57fa\u7840\u6a21\u5757\uff0c\u6709\u6548\u534f\u8c03\u5957\u5229\u548c\u9891\u7387\u8c03\u8282\u5e02\u573a\u53c2\u4e0e\uff0c\u663e\u8457\u63d0\u9ad8\u50a8\u80fd\u7cfb\u7edf\u7ecf\u6d4e\u6548\u76ca\u3002"}}
{"id": "2510.09871", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09871", "abs": "https://arxiv.org/abs/2510.09871", "authors": ["Nafiseh Nikeghbal", "Amir Hossein Kargaran", "Jana Diesner"], "title": "CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs", "comment": "EMNLP 2025 (Oral)", "summary": "Improvements in model construction, including fortified safety guardrails,\nallow Large language models (LLMs) to increasingly pass standard safety checks.\nHowever, LLMs sometimes slip into revealing harmful behavior, such as\nexpressing racist viewpoints, during conversations. To analyze this\nsystematically, we introduce CoBia, a suite of lightweight adversarial attacks\nthat allow us to refine the scope of conditions under which LLMs depart from\nnormative or ethical behavior in conversations. CoBia creates a constructed\nconversation where the model utters a biased claim about a social group. We\nthen evaluate whether the model can recover from the fabricated bias claim and\nreject biased follow-up questions. We evaluate 11 open-source as well as\nproprietary LLMs for their outputs related to six socio-demographic categories\nthat are relevant to individual safety and fair treatment, i.e., gender, race,\nreligion, nationality, sex orientation, and others. Our evaluation is based on\nestablished LLM-based bias metrics, and we compare the results against human\njudgments to scope out the LLMs' reliability and alignment. The results suggest\nthat purposefully constructed conversations reliably reveal bias amplification\nand that LLMs often fail to reject biased follow-up questions during dialogue.\nThis form of stress-testing highlights deeply embedded biases that can be\nsurfaced through interaction. Code and artifacts are available at\nhttps://github.com/nafisenik/CoBia.", "AI": {"tldr": "CoBia\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5bf9\u6297\u653b\u51fb\u5957\u4ef6\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b\u504f\u89c1\u58f0\u660e\u7684\u5bf9\u8bdd\u6765\u6d4b\u8bd5LLMs\u662f\u5426\u80fd\u4ece\u504f\u89c1\u4e2d\u6062\u590d\u5e76\u62d2\u7edd\u540e\u7eed\u504f\u89c1\u95ee\u9898\uff0c\u63ed\u793a\u4e86LLMs\u5728\u5bf9\u8bdd\u4e2d\u5b58\u5728\u7684\u504f\u89c1\u653e\u5927\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1LLMs\u52a0\u5f3a\u4e86\u5b89\u5168\u62a4\u680f\uff0c\u4f46\u5728\u5bf9\u8bdd\u4e2d\u4ecd\u4f1a\u8868\u73b0\u51fa\u6709\u5bb3\u884c\u4e3a\uff0c\u5982\u79cd\u65cf\u4e3b\u4e49\u89c2\u70b9\u3002\u9700\u8981\u7cfb\u7edf\u6027\u5730\u5206\u6790LLMs\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\u4f1a\u504f\u79bb\u89c4\u8303\u6216\u4f26\u7406\u884c\u4e3a\u3002", "method": "CoBia\u521b\u5efa\u5305\u542b\u504f\u89c1\u58f0\u660e\u7684\u6784\u9020\u5bf9\u8bdd\uff0c\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u80fd\u4ece\u865a\u6784\u7684\u504f\u89c1\u58f0\u660e\u4e2d\u6062\u590d\u5e76\u62d2\u7edd\u504f\u89c1\u7684\u540e\u7eed\u95ee\u9898\u3002\u8bc4\u4f30\u4e8611\u4e2a\u5f00\u6e90\u548c\u4e13\u6709LLMs\u57286\u4e2a\u793e\u4f1a\u4eba\u53e3\u7c7b\u522b\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6709\u76ee\u7684\u6784\u5efa\u7684\u5bf9\u8bdd\u53ef\u9760\u5730\u63ed\u793a\u4e86\u504f\u89c1\u653e\u5927\uff0cLLMs\u7ecf\u5e38\u5728\u5bf9\u8bdd\u4e2d\u65e0\u6cd5\u62d2\u7edd\u504f\u89c1\u7684\u540e\u7eed\u95ee\u9898\u3002\u8fd9\u79cd\u538b\u529b\u6d4b\u8bd5\u7a81\u663e\u4e86\u901a\u8fc7\u4e92\u52a8\u53ef\u4ee5\u6d6e\u73b0\u7684\u6df1\u5c42\u5d4c\u5165\u504f\u89c1\u3002", "conclusion": "CoBia\u65b9\u6cd5\u6709\u6548\u63ed\u793a\u4e86LLMs\u4e2d\u6df1\u5c42\u7684\u504f\u89c1\u95ee\u9898\uff0c\u8868\u660e\u5f53\u524d\u7684\u5b89\u5168\u673a\u5236\u5728\u5bf9\u8bdd\u60c5\u5883\u4e0b\u4ecd\u5b58\u5728\u6f0f\u6d1e\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u504f\u89c1\u68c0\u6d4b\u548c\u7f13\u89e3\u7b56\u7565\u3002"}}
{"id": "2510.09687", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09687", "abs": "https://arxiv.org/abs/2510.09687", "authors": ["Stanis\u0142aw Pawlak"], "title": "On the Occurence of Critical Learning Periods in Neural Networks", "comment": "8 pages, 8 figures", "summary": "This study delves into the plasticity of neural networks, offering empirical\nsupport for the notion that critical learning periods and warm-starting\nperformance loss can be avoided through simple adjustments to learning\nhyperparameters. The critical learning phenomenon emerges when training is\ninitiated with deficit data. Subsequently, after numerous deficit epochs, the\nnetwork's plasticity wanes, impeding its capacity to achieve parity in accuracy\nwith models trained from scratch, even when extensive clean data training\nfollows deficit epochs. Building upon seminal research introducing critical\nlearning periods, we replicate key findings and broaden the experimental scope\nof the main experiment from the original work. In addition, we consider a\nwarm-starting approach and show that it can be seen as a form of deficit\npretraining. In particular, we demonstrate that these problems can be averted\nby employing a cyclic learning rate schedule. Our findings not only impact\nneural network training practices but also establish a vital link between\ncritical learning periods and ongoing research on warm-starting neural network\ntraining.", "AI": {"tldr": "\u901a\u8fc7\u8c03\u6574\u5b66\u4e60\u7387\u8d85\u53c2\u6570\u53ef\u4ee5\u907f\u514d\u795e\u7ecf\u7f51\u7edc\u7684\u5173\u952e\u5b66\u4e60\u671f\u548c\u70ed\u542f\u52a8\u6027\u80fd\u635f\u5931\u95ee\u9898", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u5851\u6027\uff0c\u9a8c\u8bc1\u5173\u952e\u5b66\u4e60\u671f\u73b0\u8c61\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u907f\u514d\u56e0\u6570\u636e\u4e0d\u8db3\u8bad\u7ec3\u5bfc\u81f4\u7684\u6027\u80fd\u635f\u5931", "method": "\u590d\u5236\u5173\u952e\u7814\u7a76\u7ed3\u679c\uff0c\u6269\u5c55\u5b9e\u9a8c\u8303\u56f4\uff0c\u8003\u8651\u70ed\u542f\u52a8\u65b9\u6cd5\u4f5c\u4e3a\u6570\u636e\u4e0d\u8db3\u9884\u8bad\u7ec3\u7684\u4e00\u79cd\u5f62\u5f0f\uff0c\u4f7f\u7528\u5faa\u73af\u5b66\u4e60\u7387\u8c03\u5ea6", "result": "\u8bc1\u660e\u5faa\u73af\u5b66\u4e60\u7387\u8c03\u5ea6\u53ef\u4ee5\u907f\u514d\u5173\u952e\u5b66\u4e60\u671f\u548c\u70ed\u542f\u52a8\u6027\u80fd\u635f\u5931\u95ee\u9898", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e0d\u4ec5\u5f71\u54cd\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u5b9e\u8df5\uff0c\u8fd8\u5728\u5173\u952e\u5b66\u4e60\u671f\u4e0e\u70ed\u542f\u52a8\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7814\u7a76\u4e4b\u95f4\u5efa\u7acb\u4e86\u91cd\u8981\u8054\u7cfb"}}
{"id": "2510.10168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10168", "abs": "https://arxiv.org/abs/2510.10168", "authors": ["Chengqian Gao", "Haonan Li", "Taylor W. Killian", "Jianshu She", "Renxi Wang", "Liqun Ma", "Zhoujun Cheng", "Shibo Hao", "Zhiqiang Xu"], "title": "Concise Reasoning in the Lens of Lagrangian Optimization", "comment": null, "summary": "Concise reasoning in large language models seeks to generate only essential\nintermediate steps needed to arrive at a final answer, thereby alleviating\nissues of overthinking. Most proposed approaches hinge on carefully\nhand-crafted heuristics, struggling to balance concision with performance,\noften failing to adapt across domains and model scales. In this work, we\naddress these challenges by introducing a principled and pragmatic strategy,\nperformance-aware length updating (PALU). As a principled algorithm, PALU\nformulates concise reasoning as a constrained optimization problem, minimizing\nresponse length subject to a performance constraint, and then applies\nLagrangian optimization to convert it into a tractable unconstrained problem.\nAs a pragmatic solution, PALU streamlines complicated update rules through\nthree approximations: (i) estimating performance with off-policy rollouts, (ii)\ntruncating the Lagrange multiplier to two extremes, and (iii) replacing\ngradient-based updates with quantile-driven length adjustments. PALU reduces\noutput length by 65% while improving accuracy by 15% when applied to\nDeepSeek-Distill-Qwen-1.5B, averaged over five benchmarks, outperforming a\nrange of alternative methods. Furthermore, PALU is demonstrated to adapt across\nboth domain (logic, STEM and math) and model scale (1.5B, 7B, 14B) entrenching\nthe algorithm as a practical and effective concise reasoning approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86PALU\u65b9\u6cd5\uff0c\u901a\u8fc7\u6027\u80fd\u611f\u77e5\u7684\u957f\u5ea6\u66f4\u65b0\u6765\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7b80\u6d01\u63a8\u7406\u95ee\u9898\uff0c\u5728\u51cf\u5c1165%\u8f93\u51fa\u957f\u5ea6\u7684\u540c\u65f6\u63d0\u9ad815%\u51c6\u786e\u7387", "motivation": "\u73b0\u6709\u7b80\u6d01\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u96be\u4ee5\u5e73\u8861\u7b80\u6d01\u6027\u4e0e\u6027\u80fd\uff0c\u4e14\u65e0\u6cd5\u8de8\u9886\u57df\u548c\u6a21\u578b\u89c4\u6a21\u9002\u914d", "method": "\u5c06\u7b80\u6d01\u63a8\u7406\u5efa\u6a21\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u62c9\u683c\u6717\u65e5\u4f18\u5316\u8f6c\u5316\u4e3a\u65e0\u7ea6\u675f\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u8fd1\u4f3c\u7b80\u5316\u66f4\u65b0\u89c4\u5219\uff1a\u79bb\u7b56\u7565\u6027\u80fd\u4f30\u8ba1\u3001\u4e8c\u5143\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u3001\u5206\u4f4d\u6570\u9a71\u52a8\u7684\u957f\u5ea6\u8c03\u6574", "result": "\u5728DeepSeek-Distill-Qwen-1.5B\u4e0a\uff0c\u5e73\u5747\u51cf\u5c1165%\u8f93\u51fa\u957f\u5ea6\u540c\u65f6\u63d0\u9ad815%\u51c6\u786e\u7387\uff0c\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5", "conclusion": "PALU\u662f\u4e00\u79cd\u5b9e\u7528\u6709\u6548\u7684\u7b80\u6d01\u63a8\u7406\u65b9\u6cd5\uff0c\u80fd\u591f\u8de8\u9886\u57df\uff08\u903b\u8f91\u3001STEM\u3001\u6570\u5b66\uff09\u548c\u6a21\u578b\u89c4\u6a21\uff081.5B\u30017B\u300114B\uff09\u81ea\u9002\u5e94"}}
{"id": "2510.09783", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09783", "abs": "https://arxiv.org/abs/2510.09783", "authors": ["Dang Nguyen", "Sunil Gupta", "Kien Do", "Thin Nguyen", "Taylor Braund", "Alexis Whitton", "Svetha Venkatesh"], "title": "Large Language Models for Imbalanced Classification: Diversity makes the difference", "comment": null, "summary": "Oversampling is one of the most widely used approaches for addressing\nimbalanced classification. The core idea is to generate additional minority\nsamples to rebalance the dataset. Most existing methods, such as SMOTE, require\nconverting categorical variables into numerical vectors, which often leads to\ninformation loss. Recently, large language model (LLM)-based methods have been\nintroduced to overcome this limitation. However, current LLM-based approaches\ntypically generate minority samples with limited diversity, reducing robustness\nand generalizability in downstream classification tasks. To address this gap,\nwe propose a novel LLM-based oversampling method designed to enhance diversity.\nFirst, we introduce a sampling strategy that conditions synthetic sample\ngeneration on both minority labels and features. Second, we develop a new\npermutation strategy for fine-tuning pre-trained LLMs. Third, we fine-tune the\nLLM not only on minority samples but also on interpolated samples to further\nenrich variability. Extensive experiments on 10 tabular datasets demonstrate\nthat our method significantly outperforms eight SOTA baselines. The generated\nsynthetic samples are both realistic and diverse. Moreover, we provide\ntheoretical analysis through an entropy-based perspective, proving that our\nmethod encourages diversity in the generated samples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u8fc7\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u6761\u4ef6\u751f\u6210\u3001\u6392\u5217\u5fae\u8c03\u548c\u63d2\u503c\u6837\u672c\u8bad\u7ec3\u6765\u589e\u5f3a\u5408\u6210\u6837\u672c\u7684\u591a\u6837\u6027\uff0c\u572810\u4e2a\u8868\u683c\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e8\u4e2a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u8fc7\u91c7\u6837\u65b9\u6cd5\u5982SMOTE\u9700\u8981\u5c06\u5206\u7c7b\u53d8\u91cf\u8f6c\u6362\u4e3a\u6570\u503c\u5411\u91cf\uff0c\u5bfc\u81f4\u4fe1\u606f\u635f\u5931\u3002\u73b0\u6709LLM\u65b9\u6cd5\u751f\u6210\u7684\u5c11\u6570\u7c7b\u6837\u672c\u591a\u6837\u6027\u6709\u9650\uff0c\u5f71\u54cd\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002", "method": "1) \u57fa\u4e8e\u5c11\u6570\u7c7b\u6807\u7b7e\u548c\u7279\u5f81\u7684\u6761\u4ef6\u5408\u6210\u6837\u672c\u751f\u6210\u7b56\u7565\uff1b2) \u65b0\u7684\u9884\u8bad\u7ec3LLM\u6392\u5217\u5fae\u8c03\u7b56\u7565\uff1b3) \u5728\u5c11\u6570\u7c7b\u6837\u672c\u548c\u63d2\u503c\u6837\u672c\u4e0a\u5fae\u8c03LLM\u4ee5\u589e\u5f3a\u53d8\u5f02\u6027\u3002", "result": "\u572810\u4e2a\u8868\u683c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e8\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u5408\u6210\u6837\u672c\u65e2\u771f\u5b9e\u53c8\u591a\u6837\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u71b5\u57fa\u7406\u8bba\u5206\u6790\u8bc1\u660e\u80fd\u591f\u4fc3\u8fdb\u751f\u6210\u6837\u672c\u7684\u591a\u6837\u6027\uff0c\u4e3a\u4e0d\u5e73\u8861\u5206\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10892", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.10892", "abs": "https://arxiv.org/abs/2510.10892", "authors": ["Bukunmi Gabriel Odunlami", "Marcos Netto"], "title": "Observability and parameter estimation of a generic model for aggregated distributed energy resources", "comment": null, "summary": "We propose a novel framework for estimating the parameters of an aggregated\ndistributed energy resources (der_a) model. First, we introduce a rigorous\nmethod to determine whether all model parameters are estimable. When they are\nnot, our approach identifies the subset of parameters that can be estimated.\nThe proposed framework offers new insights into the number and specific\nparameters that can be reliably estimated based on commonly available\nmeasurements. It also highlights the limitations of calibrating such models.\nSecond, we introduce a Kalman filtering method to calibrate the der_a model.\nSince we account for nonlinear effects such as saturation and deadbands, we\ndevelop a specific mechanism to handle smoothing functions within the Kalman\nfilter. Specifically, we consider the extended and the unscented Kalman filter.\nWe demonstrate the effectiveness of the proposed framework on a modified IEEE\n34-node distribution feeder with inverter-based resources. Our findings align\nwith the North American Electric Reliability Corporation's parameterization\nguideline and underscore the importance of model calibration in accurately\ncapturing the collective dynamics of distributed energy resources installed on\ndistribution systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u4f30\u8ba1\u805a\u5408\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\u6a21\u578b\u53c2\u6570\u7684\u65b0\u6846\u67b6\uff0c\u5305\u62ec\u53c2\u6570\u53ef\u4f30\u8ba1\u6027\u5206\u6790\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\u6821\u51c6\u65b9\u6cd5\u3002", "motivation": "\u9700\u8981\u89e3\u51b3\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\u6a21\u578b\u53c2\u6570\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u6a21\u578b\u53c2\u6570\u65e0\u6cd5\u5168\u90e8\u4f30\u8ba1\u65f6\uff0c\u9700\u8981\u786e\u5b9a\u53ef\u4f30\u8ba1\u7684\u53c2\u6570\u5b50\u96c6\uff0c\u5e76\u5904\u7406\u975e\u7ebf\u6027\u6548\u5e94\u5982\u9971\u548c\u548c\u6b7b\u533a\u3002", "method": "\u9996\u5148\u5f15\u5165\u4e25\u683c\u65b9\u6cd5\u5224\u65ad\u6240\u6709\u6a21\u578b\u53c2\u6570\u662f\u5426\u53ef\u4f30\u8ba1\uff0c\u8bc6\u522b\u53ef\u4f30\u8ba1\u53c2\u6570\u5b50\u96c6\uff1b\u7136\u540e\u5f00\u53d1\u5361\u5c14\u66fc\u6ee4\u6ce2\u65b9\u6cd5\uff08\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\uff09\u6765\u6821\u51c6\u6a21\u578b\uff0c\u7279\u522b\u5904\u7406\u5e73\u6ed1\u51fd\u6570\u3002", "result": "\u5728\u6539\u8fdb\u7684IEEE 34\u8282\u70b9\u914d\u7535\u9988\u7ebf\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0c\u7ed3\u679c\u7b26\u5408\u5317\u7f8e\u7535\u529b\u53ef\u9760\u6027\u516c\u53f8\u7684\u53c2\u6570\u5316\u6307\u5357\uff0c\u51c6\u786e\u6355\u6349\u4e86\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\u7684\u96c6\u4f53\u52a8\u6001\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u805a\u5408\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\u6a21\u578b\u53c2\u6570\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u6a21\u578b\u6821\u51c6\u5728\u51c6\u786e\u6355\u6349\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\u52a8\u6001\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.10860", "categories": ["math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2510.10860", "abs": "https://arxiv.org/abs/2510.10860", "authors": ["Antonios Zitridis"], "title": "Martingale Optimal Transport and Martingale Schr\u00f6dinger Bridges for Calibration of Stochastic Volatility Models", "comment": null, "summary": "Motivated by recent developments in the calibration of stochastic volatility\nmodels (SVMs for short), we study continuous-time formulations of martingale\noptimal transport and martingale Schr\\\"odinger bridge problems. We establish\nduality formulas and also provide alternative proofs, via different techniques,\nof duality results previously established in the mathematical finance\nliterature. Applications include calibration of SVMs to SPX options, as well as\njoint calibration to both SPX and VIX options.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u8fde\u7eed\u65f6\u95f4\u4e0b\u7684\u9785\u6700\u4f18\u8f93\u8fd0\u548c\u9785\u859b\u5b9a\u8c14\u6865\u95ee\u9898\uff0c\u5efa\u7acb\u5bf9\u5076\u516c\u5f0f\uff0c\u5e76\u5e94\u7528\u4e8e\u968f\u673a\u6ce2\u52a8\u7387\u6a21\u578b\u7684\u6821\u51c6\u3002", "motivation": "\u53d7\u968f\u673a\u6ce2\u52a8\u7387\u6a21\u578b\u6821\u51c6\u6700\u65b0\u53d1\u5c55\u7684\u542f\u53d1\uff0c\u7814\u7a76\u8fde\u7eed\u65f6\u95f4\u4e0b\u7684\u9785\u6700\u4f18\u8f93\u8fd0\u548c\u9785\u859b\u5b9a\u8c14\u6865\u95ee\u9898\u3002", "method": "\u5efa\u7acb\u5bf9\u5076\u516c\u5f0f\uff0c\u5e76\u901a\u8fc7\u4e0d\u540c\u6280\u672f\u63d0\u4f9b\u6570\u5b66\u91d1\u878d\u6587\u732e\u4e2d\u5df2\u6709\u5bf9\u5076\u7ed3\u679c\u7684\u66ff\u4ee3\u8bc1\u660e\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u8fde\u7eed\u65f6\u95f4\u9785\u6700\u4f18\u8f93\u8fd0\u548c\u9785\u859b\u5b9a\u8c14\u6865\u95ee\u9898\u7684\u5bf9\u5076\u7406\u8bba\u3002", "conclusion": "\u8be5\u7406\u8bba\u53ef\u5e94\u7528\u4e8eSPX\u671f\u6743\u7684\u968f\u673a\u6ce2\u52a8\u7387\u6a21\u578b\u6821\u51c6\uff0c\u4ee5\u53caSPX\u548cVIX\u671f\u6743\u7684\u8054\u5408\u6821\u51c6\u3002"}}
{"id": "2510.09882", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09882", "abs": "https://arxiv.org/abs/2510.09882", "authors": ["Vishal Anand", "Milad Alshomary", "Kathleen McKeown"], "title": "iBERT: Interpretable Style Embeddings via Sense Decomposition", "comment": null, "summary": "We present iBERT (interpretable-BERT), an encoder to produce inherently\ninterpretable and controllable embeddings - designed to modularize and expose\nthe discriminative cues present in language, such as stylistic and semantic\nstructure. Each input token is represented as a sparse, non-negative mixture\nover k context-independent sense vectors, which can be pooled into sentence\nembeddings or used directly at the token level. This enables modular control\nover representation, before any decoding or downstream use.\n  To demonstrate our model's interpretability, we evaluate it on a suite of\nstyle-focused tasks. On the STEL benchmark, it improves style representation\neffectiveness by ~8 points over SBERT-style baselines, while maintaining\ncompetitive performance on authorship verification. Because each embedding is a\nstructured composition of interpretable senses, we highlight how specific style\nattributes - such as emoji use, formality, or misspelling can be assigned to\nspecific sense vectors. While our experiments center on style, iBERT is not\nlimited to stylistic modeling. Its structural modularity is designed to\ninterpretably decompose whichever discriminative signals are present in the\ndata - enabling generalization even when supervision blends stylistic and\nsemantic factors.", "AI": {"tldr": "iBERT\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u7684BERT\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u7a00\u758f\u975e\u8d1f\u6df7\u5408\u7684\u4e0a\u4e0b\u6587\u65e0\u5173\u610f\u4e49\u5411\u91cf\u6765\u8868\u793a\u8f93\u5165\u6807\u8bb0\uff0c\u4f7f\u5d4c\u5165\u5177\u6709\u5185\u5728\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\u7684\u95ee\u9898\uff0c\u8bbe\u8ba1\u80fd\u591f\u6a21\u5757\u5316\u5c55\u793a\u8bed\u8a00\u4e2d\u5224\u522b\u6027\u7ebf\u7d22\uff08\u5982\u98ce\u683c\u548c\u8bed\u4e49\u7ed3\u6784\uff09\u7684\u5d4c\u5165\u8868\u793a\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u975e\u8d1f\u6df7\u5408\u7684k\u4e2a\u4e0a\u4e0b\u6587\u65e0\u5173\u610f\u4e49\u5411\u91cf\u8868\u793a\u6bcf\u4e2a\u8f93\u5165\u6807\u8bb0\uff0c\u8fd9\u4e9b\u5411\u91cf\u53ef\u4ee5\u6c60\u5316\u4e3a\u53e5\u5b50\u5d4c\u5165\u6216\u76f4\u63a5\u5728\u6807\u8bb0\u7ea7\u522b\u4f7f\u7528\uff0c\u5b9e\u73b0\u8868\u793a\u7684\u6a21\u5757\u5316\u63a7\u5236\u3002", "result": "\u5728STEL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u98ce\u683c\u8868\u793a\u6548\u679c\u6bd4SBERT\u57fa\u7ebf\u63d0\u9ad8\u7ea68\u4e2a\u70b9\uff0c\u5728\u4f5c\u8005\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u5e76\u80fd\u5c06\u7279\u5b9a\u98ce\u683c\u5c5e\u6027\uff08\u5982\u8868\u60c5\u7b26\u53f7\u4f7f\u7528\u3001\u6b63\u5f0f\u6027\u3001\u62fc\u5199\u9519\u8bef\uff09\u5206\u914d\u7ed9\u7279\u5b9a\u610f\u4e49\u5411\u91cf\u3002", "conclusion": "iBERT\u901a\u8fc7\u7ed3\u6784\u5316\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u80fd\u591f\u53ef\u89e3\u91ca\u5730\u5206\u89e3\u6570\u636e\u4e2d\u5b58\u5728\u7684\u4efb\u4f55\u5224\u522b\u6027\u4fe1\u53f7\uff0c\u5373\u4f7f\u76d1\u7763\u6df7\u5408\u4e86\u98ce\u683c\u548c\u8bed\u4e49\u56e0\u7d20\u4e5f\u80fd\u5b9e\u73b0\u6cdb\u5316\u3002"}}
{"id": "2510.09691", "categories": ["cs.LG", "cs.AI", "68T07, 68M14", "I.2.6; I.2.11; K.6.5"], "pdf": "https://arxiv.org/pdf/2510.09691", "abs": "https://arxiv.org/abs/2510.09691", "authors": ["Tejash Varsani"], "title": "Evaluation of Differential Privacy Mechanisms on Federated Learning", "comment": "Supervised by Prof. Dr.-Ing. habil. Alois C. Knoll; Advisor:\n  Nagacharan Teja Tangirala, M.Sc", "summary": "Federated learning is distributed model training across several clients\nwithout disclosing raw data. Despite advancements in data privacy, risks still\nremain. Differential Privacy (DP) is a technique to protect sensitive data by\nadding noise to model updates, usually controlled by a fixed privacy budget.\nHowever, this approach can introduce excessive noise, particularly when the\nmodel converges, which compromises performance. To address this problem,\nadaptive privacy budgets have been investigated as a potential solution. This\nwork implements DP methods using Laplace and Gaussian mechanisms with an\nadaptive privacy budget, extending the SelecEval simulator. We introduce an\nadaptive clipping approach in the Gaussian mechanism, ensuring that gradients\nof the model are dynamically updated rather than using a fixed sensitivity. We\nconduct extensive experiments with various privacy budgets, IID and non-IID\ndatasets, and different numbers of selected clients per round. While our\nexperiments were limited to 200 training rounds, the results suggest that\nadaptive privacy budgets and adaptive clipping can help maintain model accuracy\nwhile preserving privacy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u81ea\u9002\u5e94\u9690\u79c1\u9884\u7b97\u548c\u81ea\u9002\u5e94\u88c1\u526a\u65b9\u6cd5\uff0c\u901a\u8fc7\u62c9\u666e\u62c9\u65af\u548c\u9ad8\u65af\u673a\u5236\u5b9e\u73b0\u5dee\u5206\u9690\u79c1\uff0c\u65e8\u5728\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u7ef4\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u867d\u7136\u4fdd\u62a4\u4e86\u539f\u59cb\u6570\u636e\u9690\u79c1\uff0c\u4f46\u56fa\u5b9a\u9690\u79c1\u9884\u7b97\u7684\u5dee\u5206\u9690\u79c1\u65b9\u6cd5\u5728\u6a21\u578b\u6536\u655b\u65f6\u4f1a\u5f15\u5165\u8fc7\u591a\u566a\u58f0\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002\u56e0\u6b64\u9700\u8981\u7814\u7a76\u81ea\u9002\u5e94\u9690\u79c1\u9884\u7b97\u6765\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u62c9\u666e\u62c9\u65af\u548c\u9ad8\u65af\u673a\u5236\u5b9e\u73b0\u5dee\u5206\u9690\u79c1\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u88c1\u526a\u65b9\u6cd5\u52a8\u6001\u66f4\u65b0\u68af\u5ea6\u800c\u975e\u4f7f\u7528\u56fa\u5b9a\u654f\u611f\u5ea6\uff0c\u6269\u5c55SelecEval\u6a21\u62df\u5668\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728200\u8f6e\u8bad\u7ec3\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u81ea\u9002\u5e94\u9690\u79c1\u9884\u7b97\u548c\u81ea\u9002\u5e94\u88c1\u526a\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5e2e\u52a9\u7ef4\u6301\u6a21\u578b\u51c6\u786e\u7387\u3002", "conclusion": "\u81ea\u9002\u5e94\u9690\u79c1\u9884\u7b97\u548c\u81ea\u9002\u5e94\u88c1\u526a\u662f\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.10193", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10193", "abs": "https://arxiv.org/abs/2510.10193", "authors": ["Qingni Wang", "Yue Fan", "Xin Eric Wang"], "title": "SAFER: Risk-Constrained Sample-then-Filter in Large Language Models", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in risk-sensitive\napplications such as real-world open-ended question answering (QA), ensuring\nthe trustworthiness of their outputs has become critical. Existing selective\nconformal prediction (SCP) methods provide statistical guarantees by\nconstructing prediction sets with a constrained miscoverage rate for correct\nanswers. However, prior works unrealistically assume that admissible answers\nfor all instances can be obtained via finite sampling, even for open-ended QA\nscenarios that lack a fixed and finite solution space. To address this, we\nintroduce a two-stage risk control framework comprising abstention-aware\nsampling and conformalized filtering (SAFER). Firstly, on a held-out\ncalibration set, SAFER calibrates a sampling budget within the maximum sampling\ncap, using the Clopper-Pearson exact method at a user-desired risk level (i.e.,\nthe maximum allowable miscoverage rate of the sampling sets). If the risk level\ncannot be satisfied within the cap, we abstain; otherwise, the calibrated\nsampling budget becomes the minimum requirements at test time. Then, we employ\ncalibration instances where correct answers are attainable under the calibrated\nbudget and apply the conformal risk control method to determine a statistically\nvalid uncertainty threshold, which filters unreliable distractors from the\ncandidate set for each test data point. In this stage, SAFER introduces an\nadditional risk level to guide the calculation of the threshold, thereby\ncontrolling the risk of correct answers being excluded. Furthermore, we show\nthat SAFER is compatible with various task-specific admission criteria and\ncalibration-test split ratios, highlighting its robustness and high data\nefficiency.", "AI": {"tldr": "\u63d0\u51faSAFER\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u98ce\u9669\u63a7\u5236\u89e3\u51b3\u5f00\u653e\u57df\u95ee\u7b54\u4e2d\u65e0\u6cd5\u83b7\u5f97\u6240\u6709\u53ef\u80fd\u7b54\u6848\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u7edf\u8ba1\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709\u9009\u62e9\u6027\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u5047\u8bbe\u6240\u6709\u5b9e\u4f8b\u7684\u53ef\u63a5\u53d7\u7b54\u6848\u90fd\u80fd\u901a\u8fc7\u6709\u9650\u91c7\u6837\u83b7\u5f97\uff0c\u8fd9\u5728\u5f00\u653e\u57df\u95ee\u7b54\u4e2d\u4e0d\u73b0\u5b9e\uff0c\u9700\u8981\u65b0\u7684\u98ce\u9669\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u57fa\u4e8eClopper-Pearson\u65b9\u6cd5\u6821\u51c6\u91c7\u6837\u9884\u7b97\uff1b2) \u5e94\u7528\u5171\u5f62\u98ce\u9669\u63a7\u5236\u65b9\u6cd5\u786e\u5b9a\u4e0d\u786e\u5b9a\u6027\u9608\u503c\uff0c\u8fc7\u6ee4\u4e0d\u53ef\u9760\u5019\u9009\u7b54\u6848\u3002", "result": "SAFER\u6846\u67b6\u80fd\u591f\u63a7\u5236\u6b63\u786e\u7b54\u6848\u88ab\u6392\u9664\u7684\u98ce\u9669\uff0c\u517c\u5bb9\u5404\u79cd\u4efb\u52a1\u7279\u5b9a\u7684\u51c6\u5165\u6807\u51c6\u548c\u6821\u51c6-\u6d4b\u8bd5\u5206\u5272\u6bd4\u4f8b\u3002", "conclusion": "SAFER\u4e3a\u5f00\u653e\u57df\u95ee\u7b54\u63d0\u4f9b\u4e86\u7edf\u8ba1\u4e0a\u6709\u6548\u7684\u98ce\u9669\u63a7\u5236\u65b9\u6cd5\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u9ad8\u6570\u636e\u6548\u7387\u3002"}}
{"id": "2510.09796", "categories": ["cs.LG", "cs.NA", "math.NA", "math.OC", "stat.ML", "47A52, 47J30, 65J22, 65K10, 68T01, 68T07, 68W15, 94A08"], "pdf": "https://arxiv.org/pdf/2510.09796", "abs": "https://arxiv.org/abs/2510.09796", "authors": ["Xiaoyu Wang", "Alexandra Valavanis", "Azhir Mahmood", "Andreas Mang", "Martin Benning", "Audrey Repetti"], "title": "A Unified Framework for Lifted Training and Inversion Approaches", "comment": null, "summary": "The training of deep neural networks predominantly relies on a combination of\ngradient-based optimisation and back-propagation for the computation of the\ngradient. While incredibly successful, this approach faces challenges such as\nvanishing or exploding gradients, difficulties with non-smooth activations, and\nan inherently sequential structure that limits parallelisation. Lifted training\nmethods offer an alternative by reformulating the nested optimisation problem\ninto a higher-dimensional, constrained optimisation problem where the\nconstraints are no longer enforced directly but penalised with penalty terms.\nThis chapter introduces a unified framework that encapsulates various lifted\ntraining strategies, including the Method of Auxiliary Coordinates, Fenchel\nLifted Networks, and Lifted Bregman Training, and demonstrates how diverse\narchitectures, such as Multi-Layer Perceptrons, Residual Neural Networks, and\nProximal Neural Networks fit within this structure. By leveraging tools from\nconvex optimisation, particularly Bregman distances, the framework facilitates\ndistributed optimisation, accommodates non-differentiable proximal activations,\nand can improve the conditioning of the training landscape. We discuss the\nimplementation of these methods using block-coordinate descent strategies,\nincluding deterministic implementations enhanced by accelerated and adaptive\noptimisation techniques, as well as implicit stochastic gradient methods.\nFurthermore, we explore the application of this framework to inverse problems,\ndetailing methodologies for both the training of specialised networks (e.g.,\nunrolled architectures) and the stable inversion of pre-trained networks.\nNumerical results on standard imaging tasks validate the effectiveness and\nstability of the lifted Bregman approach compared to conventional training,\nparticularly for architectures employing proximal activations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u63d0\u5347\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6df1\u5ea6\u7f51\u7edc\u8bad\u7ec3\u91cd\u65b0\u8868\u8ff0\u4e3a\u9ad8\u7ef4\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u68af\u5ea6\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u6d88\u5931/\u7206\u70b8\u3001\u975e\u5149\u6ed1\u6fc0\u6d3b\u51fd\u6570\u548c\u5e76\u884c\u5316\u9650\u5236\u7b49\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u68af\u5ea6\u7684\u8bad\u7ec3\u65b9\u6cd5\u9762\u4e34\u68af\u5ea6\u6d88\u5931/\u7206\u70b8\u3001\u975e\u5149\u6ed1\u6fc0\u6d3b\u51fd\u6570\u5904\u7406\u56f0\u96be\u4ee5\u53ca\u987a\u5e8f\u7ed3\u6784\u9650\u5236\u5e76\u884c\u5316\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5bfb\u627e\u66ff\u4ee3\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u63d0\u5347\u8bad\u7ec3\u7b56\u7565\uff0c\u5c06\u5d4c\u5957\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u9ad8\u7ef4\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528Bregman\u8ddd\u79bb\u7b49\u51f8\u4f18\u5316\u5de5\u5177\uff0c\u5b9e\u73b0\u5206\u5e03\u5f0f\u4f18\u5316\u5e76\u9002\u5e94\u975e\u5149\u6ed1\u8fd1\u7aef\u6fc0\u6d3b\u51fd\u6570\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u663e\u793a\uff0c\u63d0\u5347Bregman\u65b9\u6cd5\u5728\u6807\u51c6\u6210\u50cf\u4efb\u52a1\u4e2d\u6bd4\u4f20\u7edf\u8bad\u7ec3\u66f4\u6709\u6548\u548c\u7a33\u5b9a\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f7f\u7528\u8fd1\u7aef\u6fc0\u6d3b\u51fd\u6570\u7684\u67b6\u6784\u3002", "conclusion": "\u63d0\u5347\u8bad\u7ec3\u6846\u67b6\u4e3a\u6df1\u5ea6\u7f51\u7edc\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u6539\u5584\u8bad\u7ec3\u6761\u4ef6\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u4f18\u5316\uff0c\u5e76\u6709\u6548\u5904\u7406\u975e\u5149\u6ed1\u6fc0\u6d3b\u51fd\u6570\u3002"}}
{"id": "2510.10914", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.10914", "abs": "https://arxiv.org/abs/2510.10914", "authors": ["Jiajie Qiu", "Dakota Thompson", "Kamal Youcef-Toumi", "Amro M. Farid"], "title": "Optimal Multi-Modal Transportation and Electric Power Flow: The Value of Coordinated Dynamic Operation", "comment": "31 pages, 9 figures", "summary": "The electrification of transportation represents a critical challenge in the\nglobal transition toward net-zero emissions, as the sector often accounts for\nmore than one-quarter of national energy consumption. Achieving this\ntransformation requires not only widespread adoption of electric vehicles (EVs)\nbut also their seamless integration into interdependent infrastructure\nsystems-specifically, the transportation-electricity nexus (TEN). This paper\ndevelops an optimal multi-modal transportation and electric power flow (OMTEPF)\nmodel to evaluate the benefits of coordinated, dynamic system operation.\nBuilding on recent advances in hetero-functional graph theory, the framework\nenables joint optimization of five key operational decisions in intelligent TEN\nmanagement: vehicle dispatch, route choice, charging station queuing,\ncoordinated charging, and vehicle-to-grid stabilization. The mesoscopic,\ndynamic model explicitly represents individual EVs and their state-of-charge\ntrajectories, thereby extending beyond the prevailing literature's focus on\nstatic, macroscopic traffic assignment. It further captures the full scope of\nthe TEN as a system-of-systems, incorporating five distinct charging\nmodalities: private residential, private commercial, wired public commercial,\ninductive public, and discharging. On the power system side, an IV-ACOPF\nformulation ensures globally optimal solutions to the electrical subproblems.\nComparative analysis demonstrates the substantial value of coordinated TEN\noperation relative to the status quo of siloed, uncoordinated infrastructure\nmanagement. This work provides both a novel methodological contribution and\nactionable insights for the co-design and operation of next-generation\nsustainable mobility-energy systems.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u4ea4\u901a\u548c\u7535\u529b\u6d41\u4f18\u5316\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u4ea4\u901a-\u7535\u529b\u8026\u5408\u7cfb\u7edf\u7684\u534f\u8c03\u8fd0\u884c\u6548\u76ca\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8f66\u8f86\u8c03\u5ea6\u3001\u8def\u5f84\u9009\u62e9\u3001\u5145\u7535\u7ad9\u6392\u961f\u7b49\u5173\u952e\u51b3\u7b56\uff0c\u5b9e\u73b0\u53ef\u6301\u7eed\u4ea4\u901a\u80fd\u6e90\u7cfb\u7edf\u7684\u534f\u540c\u8bbe\u8ba1\u3002", "motivation": "\u4ea4\u901a\u7535\u6c14\u5316\u662f\u5b9e\u73b0\u51c0\u96f6\u6392\u653e\u7684\u5173\u952e\u6311\u6218\uff0c\u4ea4\u901a\u90e8\u95e8\u901a\u5e38\u5360\u56fd\u5bb6\u80fd\u6e90\u6d88\u8017\u7684\u56db\u5206\u4e4b\u4e00\u4ee5\u4e0a\u3002\u9700\u8981\u5c06\u7535\u52a8\u6c7d\u8f66\u65e0\u7f1d\u96c6\u6210\u5230\u4ea4\u901a-\u7535\u529b\u8026\u5408\u7cfb\u7edf\u4e2d\uff0c\u800c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u9759\u6001\u5b8f\u89c2\u4ea4\u901a\u5206\u914d\uff0c\u7f3a\u4e4f\u5bf9\u7cfb\u7edf\u534f\u8c03\u8fd0\u884c\u7684\u52a8\u6001\u5206\u6790\u3002", "method": "\u57fa\u4e8e\u5f02\u8d28\u51fd\u6570\u56fe\u7406\u8bba\u5f00\u53d1\u4e86\u591a\u6a21\u6001\u4ea4\u901a\u548c\u7535\u529b\u6d41\u4f18\u5316\u6a21\u578b\uff0c\u8054\u5408\u4f18\u5316\u8f66\u8f86\u8c03\u5ea6\u3001\u8def\u5f84\u9009\u62e9\u3001\u5145\u7535\u7ad9\u6392\u961f\u3001\u534f\u8c03\u5145\u7535\u548c\u8f66\u7f51\u7a33\u5b9a\u4e94\u4e2a\u5173\u952e\u51b3\u7b56\u3002\u6a21\u578b\u91c7\u7528\u4ecb\u89c2\u52a8\u6001\u65b9\u6cd5\uff0c\u663e\u5f0f\u8868\u793a\u5355\u4e2a\u7535\u52a8\u6c7d\u8f66\u53ca\u5176\u5145\u7535\u72b6\u6001\u8f68\u8ff9\uff0c\u5e76\u5305\u542b\u4e94\u79cd\u5145\u7535\u6a21\u5f0f\u3002", "result": "\u6bd4\u8f83\u5206\u6790\u8868\u660e\uff0c\u76f8\u5bf9\u4e8e\u5f53\u524d\u5404\u81ea\u4e3a\u653f\u7684\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\uff0c\u534f\u8c03\u7684\u4ea4\u901a-\u7535\u529b\u8026\u5408\u7cfb\u7edf\u8fd0\u884c\u5177\u6709\u663e\u8457\u4ef7\u503c\u3002\u6a21\u578b\u80fd\u591f\u4e3a\u4e0b\u4e00\u4ee3\u53ef\u6301\u7eed\u4ea4\u901a\u80fd\u6e90\u7cfb\u7edf\u7684\u534f\u540c\u8bbe\u8ba1\u548c\u8fd0\u884c\u63d0\u4f9b\u53ef\u884c\u89c1\u89e3\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4ea4\u901a-\u7535\u529b\u8026\u5408\u7cfb\u7edf\u7684\u534f\u8c03\u8fd0\u884c\u63d0\u4f9b\u4e86\u65b0\u9896\u7684\u65b9\u6cd5\u8bba\u8d21\u732e\u548c\u53ef\u884c\u7684\u5b9e\u8df5\u89c1\u89e3\uff0c\u652f\u6301\u4e0b\u4e00\u4ee3\u53ef\u6301\u7eed\u4ea4\u901a\u80fd\u6e90\u7cfb\u7edf\u7684\u534f\u540c\u8bbe\u8ba1\u548c\u8fd0\u884c\u3002"}}
{"id": "2510.10891", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.10891", "abs": "https://arxiv.org/abs/2510.10891", "authors": ["Jinxin Xiong", "Yanting Huang", "Yingxiao Wang", "Linxin Yang", "Jianghua Wu", "Shunbo Lei", "Akang Wang"], "title": "Successive Fixing for Large-Scale SCUC Using First-Order Methods", "comment": null, "summary": "Security-Constrained Unit Commitment is a fundamental optimization problem in\npower systems operations. The primary computational bottleneck arises from the\nneed to solve large-scale Linear Programming (LP) relaxations within\nbranch-and-cut. Conventional simplex and barrier methods become computationally\nprohibitive at this scale due to their reliance on expensive matrix\nfactorizations. While matrix-free first-order methods present a promising\nalternative, their tendency to converge to non-vertex solutions renders them\nincompatible with standard branch-and-cut procedures. To bridge this gap, we\npropose a successive fixing framework that leverages a customized\nGPU-accelerated first-order LP solver to guide a logic-driven variable-fixing\nstrategy. Each iteration produces a reduced Mixed-Integer Linear Programming\n(MILP) problem, which is subsequently tightened via presolving. This iterative\ncycle of relaxation, fixing, and presolving progressively reduces problem\ncomplexity, producing a highly tractable final MILP model. When evaluated on\npublic benchmarks exceeding 13,000 buses, our approach achieves a tenfold\nspeedup over state-of-the-art methods without compromising solution quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u52a0\u901f\u4e00\u9636LP\u6c42\u89e3\u5668\u7684\u8fde\u7eed\u56fa\u5b9a\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21\u5b89\u5168\u7ea6\u675f\u673a\u7ec4\u7ec4\u5408\u95ee\u9898\uff0c\u5728\u8d85\u8fc713,000\u8282\u70b9\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8610\u500d\u52a0\u901f\u3002", "motivation": "\u4f20\u7edf\u5355\u7eaf\u5f62\u6cd5\u548c\u5185\u70b9\u6cd5\u5728\u5927\u89c4\u6a21\u5b89\u5168\u7ea6\u675f\u673a\u7ec4\u7ec4\u5408\u95ee\u9898\u4e2d\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u800c\u4e00\u9636\u65b9\u6cd5\u867d\u7136\u8ba1\u7b97\u6548\u7387\u9ad8\u4f46\u4f1a\u6536\u655b\u5230\u975e\u9876\u70b9\u89e3\uff0c\u4e0e\u6807\u51c6\u5206\u652f\u5b9a\u754c\u8fc7\u7a0b\u4e0d\u517c\u5bb9\u3002", "method": "\u91c7\u7528\u8fde\u7eed\u56fa\u5b9a\u6846\u67b6\uff0c\u5229\u7528\u5b9a\u5236\u5316\u7684GPU\u52a0\u901f\u4e00\u9636LP\u6c42\u89e3\u5668\u6307\u5bfc\u903b\u8f91\u9a71\u52a8\u7684\u53d8\u91cf\u56fa\u5b9a\u7b56\u7565\uff0c\u901a\u8fc7\u8fed\u4ee3\u7684\u677e\u5f1b\u3001\u56fa\u5b9a\u548c\u9884\u6c42\u89e3\u8fc7\u7a0b\u9010\u6b65\u964d\u4f4e\u95ee\u9898\u590d\u6742\u5ea6\u3002", "result": "\u5728\u8d85\u8fc713,000\u8282\u70b9\u7684\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5b9e\u73b0\u4e8610\u500d\u52a0\u901f\uff0c\u4e14\u4e0d\u635f\u5931\u89e3\u7684\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u4e00\u9636LP\u6c42\u89e3\u5668\u96c6\u6210\u5230\u5206\u652f\u5b9a\u754c\u6846\u67b6\u4e2d\uff0c\u4e3a\u5927\u89c4\u6a21\u5b89\u5168\u7ea6\u675f\u673a\u7ec4\u7ec4\u5408\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.09883", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09883", "abs": "https://arxiv.org/abs/2510.09883", "authors": ["Hossein Entezari Zarch", "Lei Gao", "Chaoyi Jiang", "Murali Annavarm"], "title": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context Reasoning", "comment": null, "summary": "Large reasoning models (LRMs) achieve state-of-the-art performance on\nchallenging benchmarks by generating long chains of intermediate steps, but\ntheir inference cost is dominated by decoding, where each new token must attend\nto the entire growing sequence. Existing sparse attention methods reduce\ncomputation by pruning the key-value (KV) cache, yet they suffer from severe\naccuracy degradation on reasoning tasks due to cumulative selection errors and\nthe dynamic importance of tokens over long derivations. We present\n\\textbf{DELTA}, a training-free sparse attention mechanism that achieves\ncomputational efficiency without sacrificing model accuracy. DELTA partitions\ntransformer layers into three groups: initial layers that use full attention, a\nsmall set of \\emph{selection layers} that identify salient tokens via\naggregated head-level attention scores, and subsequent \\emph{sparse-attention\nlayers} that attend only to the selected subset. This design preserves the full\nKV cache in GPU memory for accuracy, while avoiding expensive full-attention\ncomputation over many layers. On reasoning benchmarks such as AIME and\nGPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while\nreducing the number of attended tokens by up to $5\\times$ and delivering\n$1.5\\times$ end-to-end speedup. Our results show that selective reuse of\nintermediate attention maps offers a robust path toward efficient long-context\nreasoning.", "AI": {"tldr": "DELTA\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5c06Transformer\u5c42\u5206\u4e3a\u4e09\u7ec4\uff08\u5168\u6ce8\u610f\u529b\u5c42\u3001\u9009\u62e9\u5c42\u3001\u7a00\u758f\u6ce8\u610f\u529b\u5c42\uff09\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u5728AIME\u548cGPQA-Diamond\u7b49\u63a8\u7406\u57fa\u51c6\u4e0a\u8fbe\u52305\u500d\u4ee4\u724c\u51cf\u5c11\u548c1.5\u500d\u7aef\u5230\u7aef\u52a0\u901f\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u9700\u8981\u5173\u6ce8\u6574\u4e2a\u4e0d\u65ad\u589e\u957f\u7684\u5e8f\u5217\uff0c\u5bfc\u81f4\u9ad8\u6602\u7684\u8ba1\u7b97\u6210\u672c\u3002\u73b0\u6709\u7684\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u901a\u8fc7\u526a\u679dKV\u7f13\u5b58\u6765\u51cf\u5c11\u8ba1\u7b97\uff0c\u4f46\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7531\u4e8e\u7d2f\u79ef\u9009\u62e9\u9519\u8bef\u548c\u4ee4\u724c\u52a8\u6001\u91cd\u8981\u6027\u53d8\u5316\u800c\u906d\u53d7\u4e25\u91cd\u7684\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u5c06Transformer\u5c42\u5206\u4e3a\u4e09\u7ec4\uff1a\u521d\u59cb\u5c42\u4f7f\u7528\u5168\u6ce8\u610f\u529b\uff0c\u5c11\u91cf\u9009\u62e9\u5c42\u901a\u8fc7\u805a\u5408\u5934\u7ea7\u6ce8\u610f\u529b\u5206\u6570\u8bc6\u522b\u91cd\u8981\u4ee4\u724c\uff0c\u540e\u7eed\u7a00\u758f\u6ce8\u610f\u529b\u5c42\u4ec5\u5173\u6ce8\u9009\u5b9a\u7684\u4ee4\u724c\u5b50\u96c6\u3002\u8fd9\u79cd\u8bbe\u8ba1\u5728GPU\u5185\u5b58\u4e2d\u4fdd\u7559\u5b8c\u6574\u7684KV\u7f13\u5b58\u4ee5\u4fdd\u8bc1\u7cbe\u5ea6\uff0c\u540c\u65f6\u907f\u514d\u5728\u8bb8\u591a\u5c42\u4e0a\u8fdb\u884c\u6602\u8d35\u7684\u5168\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "result": "\u5728AIME\u548cGPQA-Diamond\u7b49\u63a8\u7406\u57fa\u51c6\u4e0a\uff0cDELTA\u5728\u7cbe\u5ea6\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u5168\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u540c\u65f6\u5c06\u5173\u6ce8\u7684\u4ee4\u724c\u6570\u91cf\u51cf\u5c11\u9ad8\u8fbe5\u500d\uff0c\u5e76\u5b9e\u73b01.5\u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\u3002", "conclusion": "\u9009\u62e9\u6027\u91cd\u7528\u4e2d\u95f4\u6ce8\u610f\u529b\u56fe\u4e3a\u5b9e\u73b0\u9ad8\u6548\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u7a33\u5065\u7684\u8def\u5f84\u3002"}}
{"id": "2510.09693", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.09693", "abs": "https://arxiv.org/abs/2510.09693", "authors": ["Jiakang Chen"], "title": "Neural PDE Solvers with Physics Constraints: A Comparative Study of PINNs, DRM, and WANs", "comment": "50 pages, 13 figures", "summary": "Partial differential equations (PDEs) underpin models across science and\nengineering, yet analytical solutions are atypical and classical mesh-based\nsolvers can be costly in high dimensions. This dissertation presents a unified\ncomparison of three mesh-free neural PDE solvers, physics-informed neural\nnetworks (PINNs), the deep Ritz method (DRM), and weak adversarial networks\n(WANs), on Poisson problems (up to 5D) and the time-independent Schr\\\"odinger\nequation in 1D/2D (infinite well and harmonic oscillator), and extends the\nstudy to a laser-driven case of Schr\\\"odinger's equation via the\nKramers-Henneberger (KH) transformation.\n  Under a common protocol, all methods achieve low $L_2$ errors\n($10^{-6}$-$10^{-9}$) when paired with forced boundary conditions (FBCs),\nforced nodes (FNs), and orthogonality regularization (OG). Across tasks, PINNs\nare the most reliable for accuracy and recovery of excited spectra; DRM offers\nthe best accuracy-runtime trade-off on stationary problems; WAN is more\nsensitive but competitive when weak-form constraints and FN/OG are used\neffectively. Sensitivity analyses show that FBC removes boundary-loss tuning,\nnetwork width matters more than depth for single-network solvers, and most\ngains occur within 5000-10,000 epochs. The same toolkit solves the KH case,\nindicating transfer beyond canonical benchmarks.\n  We provide practical guidelines for method selection and outline the\nfollowing extensions: time-dependent formulations for DRM and WAN, adaptive\nresidual-driven sampling, parallel multi-state training, and neural domain\ndecomposition. These results support physics-guided neural solvers as credible,\nscalable tools for solving complex PDEs.", "AI": {"tldr": "\u672c\u6587\u5bf9\u4e09\u79cd\u65e0\u7f51\u683c\u795e\u7ecfPDE\u6c42\u89e3\u5668\uff08PINNs\u3001DRM\u3001WANs\uff09\u5728\u6cca\u677e\u95ee\u9898\u548c\u859b\u5b9a\u8c14\u65b9\u7a0b\u4e0a\u8fdb\u884c\u4e86\u7edf\u4e00\u6bd4\u8f83\uff0c\u5c55\u793a\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u9ad8\u8fbe5D\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u6269\u5c55\u5230\u6fc0\u5149\u9a71\u52a8\u60c5\u51b5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7f51\u683c\u7684PDE\u6c42\u89e3\u5668\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u6210\u672c\u9ad8\u6602\uff0c\u800c\u89e3\u6790\u89e3\u901a\u5e38\u96be\u4ee5\u83b7\u5f97\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u65e0\u7f51\u683c\u795e\u7ecf\u6c42\u89e3\u5668\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u534f\u8bae\u6bd4\u8f83\u4e09\u79cd\u795e\u7ecfPDE\u6c42\u89e3\u5668\uff1aPINNs\u3001DRM\u548cWANs\uff0c\u4f7f\u7528\u5f3a\u5236\u8fb9\u754c\u6761\u4ef6\u3001\u5f3a\u5236\u8282\u70b9\u548c\u6b63\u4ea4\u6b63\u5219\u5316\uff0c\u5728\u6cca\u677e\u95ee\u9898\uff08\u6700\u9ad85D\uff09\u548c\u859b\u5b9a\u8c14\u65b9\u7a0b\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u6240\u6709\u65b9\u6cd5\u5728\u9002\u5f53\u914d\u7f6e\u4e0b\u90fd\u80fd\u8fbe\u523010^-6\u523010^-9\u7684\u4f4eL2\u8bef\u5dee\u3002PINNs\u5728\u7cbe\u5ea6\u548c\u6fc0\u53d1\u8c31\u6062\u590d\u65b9\u9762\u6700\u53ef\u9760\uff1bDRM\u5728\u7a33\u6001\u95ee\u9898\u4e0a\u63d0\u4f9b\u6700\u4f73\u7cbe\u5ea6-\u8fd0\u884c\u65f6\u6743\u8861\uff1bWAN\u5728\u6709\u6548\u4f7f\u7528\u5f31\u5f62\u5f0f\u7ea6\u675f\u65f6\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u7269\u7406\u5f15\u5bfc\u7684\u795e\u7ecf\u6c42\u89e3\u5668\u662f\u89e3\u51b3\u590d\u6742PDE\u7684\u53ef\u4fe1\u3001\u53ef\u6269\u5c55\u5de5\u5177\uff0c\u4e3a\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff0c\u5e76\u63d0\u51fa\u4e86\u65f6\u95f4\u76f8\u5173\u516c\u5f0f\u3001\u81ea\u9002\u5e94\u91c7\u6837\u7b49\u6269\u5c55\u65b9\u5411\u3002"}}
{"id": "2510.10197", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10197", "abs": "https://arxiv.org/abs/2510.10197", "authors": ["Siyuan Lu", "Zechuan Wang", "Hongxuan Zhang", "Qintong Wu", "Leilei Gan", "Chenyi Zhuang", "Jinjie Gu", "Tao Lin"], "title": "Don't Just Fine-tune the Agent, Tune the Environment", "comment": null, "summary": "Large Language Model (LLM) agents show great promise for complex, multi-turn\ntool-use tasks, but their development is often hampered by the extreme scarcity\nof high-quality training data. Supervised fine-tuning (SFT) on synthetic data\nleads to overfitting, whereas standard reinforcement learning (RL) struggles\nwith a critical cold-start problem and training instability. To address these\nchallenges, we introduce $\\textbf{Environment Tuning}$, a novel training\nparadigm that enables agents to learn complex behaviors directly from problem\ninstances without relying on pre-collected expert trajectories.\n$\\textbf{Environment Tuning}$ orchestrates this learning process through a\nstructured curriculum, actionable environment augmentation that provides\ncorrective feedback, and fine-grained progress rewards to ensure stable and\nefficient exploration. Using only 400 problem instances from Berkeley\nFunction-Calling Leaderboard (BFCL) benchmark, our method not only achieves\ncompetitive in-distribution performance against strong baselines but also\ndemonstrates superior out-of-distribution generalization, overcoming the\nperformance collapse common to SFT-based approaches. Our work presents a\nparadigm shift from supervised fine-tuning on static trajectories to dynamic,\nenvironment-based exploration, paving the way for training more robust and\ndata-efficient agents.", "AI": {"tldr": "\u63d0\u51faEnvironment Tuning\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bfe\u7a0b\u3001\u73af\u5883\u589e\u5f3a\u548c\u7ec6\u7c92\u5ea6\u5956\u52b1\uff0c\u4f7fLLM\u667a\u80fd\u4f53\u76f4\u63a5\u4ece\u95ee\u9898\u5b9e\u4f8b\u5b66\u4e60\u590d\u6742\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\uff0c\u65e0\u9700\u4e13\u5bb6\u8f68\u8ff9\u6570\u636e\u3002", "motivation": "\u89e3\u51b3LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4ee5\u53caSFT\u65b9\u6cd5\u5bb9\u6613\u8fc7\u62df\u5408\u3001\u6807\u51c6RL\u65b9\u6cd5\u5b58\u5728\u51b7\u542f\u52a8\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u6311\u6218\u3002", "method": "Environment Tuning\u8303\u5f0f\uff1a1\uff09\u7ed3\u6784\u5316\u8bfe\u7a0b\u7f16\u6392\u5b66\u4e60\u8fc7\u7a0b\uff1b2\uff09\u53ef\u64cd\u4f5c\u7684\u73af\u5883\u589e\u5f3a\u63d0\u4f9b\u7ea0\u6b63\u53cd\u9988\uff1b3\uff09\u7ec6\u7c92\u5ea6\u8fdb\u5ea6\u5956\u52b1\u786e\u4fdd\u7a33\u5b9a\u9ad8\u6548\u63a2\u7d22\u3002\u4ec5\u4f7f\u7528400\u4e2aBFCL\u57fa\u51c6\u95ee\u9898\u5b9e\u4f8b\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u5206\u5e03\u5185\u6027\u80fd\u4e0e\u5f3a\u57fa\u7ebf\u7ade\u4e89\uff0c\u5728\u5206\u5e03\u5916\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u514b\u670d\u4e86SFT\u65b9\u6cd5\u5e38\u89c1\u7684\u6027\u80fd\u5d29\u6e83\u95ee\u9898\u3002", "conclusion": "\u5b9e\u73b0\u4e86\u4ece\u57fa\u4e8e\u9759\u6001\u8f68\u8ff9\u7684\u76d1\u7763\u5fae\u8c03\u5230\u52a8\u6001\u73af\u5883\u63a2\u7d22\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3a\u8bad\u7ec3\u66f4\u9c81\u68d2\u548c\u6570\u636e\u9ad8\u6548\u7684\u667a\u80fd\u4f53\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.09827", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09827", "abs": "https://arxiv.org/abs/2510.09827", "authors": ["Michael Crawshaw", "Chirag Modi", "Mingrui Liu", "Robert M. Gower"], "title": "An Exploration of Non-Euclidean Gradient Descent: Muon and its Many Variants", "comment": null, "summary": "To define a steepest descent method over a neural network, we need to choose\na norm for each layer, a way to aggregate these norms across layers, and\nwhether to use normalization. We systematically explore different alternatives\nfor aggregating norms across layers, both formalizing existing combinations of\nAdam and the recently proposed Muon as a type of non-Euclidean gradient\ndescent, and deriving new variants of the Muon optimizer. Through a\ncomprehensive experimental evaluation of the optimizers within our framework,\nwe find that Muon is sensitive to the choice of learning rate, whereas a new\nvariant we call MuonMax is significantly more robust. We then show how to\ncombine any non-Euclidean gradient method with model based momentum (known as\nMomo). The new Momo variants of Muon are significantly more robust to\nhyperparameter tuning, and often achieve a better validation score. Thus for\nnew tasks, where the optimal hyperparameters are not known, we advocate for\nusing Momo in combination with MuonMax to save on costly hyperparameter tuning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u63a2\u7d22\u4e86\u795e\u7ecf\u7f51\u7edc\u4e2d\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\u7684\u89c4\u8303\u805a\u5408\u65b9\u5f0f\uff0c\u63d0\u51fa\u4e86\u65b0\u7684MuonMax\u4f18\u5316\u5668\uff0c\u5e76\u5c06\u5176\u4e0e\u6a21\u578b\u52a8\u91cf(Momo)\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u5b9a\u4e49\u6700\u901f\u4e0b\u964d\u65b9\u6cd5\uff0c\u9700\u8981\u4e3a\u6bcf\u5c42\u9009\u62e9\u89c4\u8303\u3001\u8de8\u5c42\u805a\u5408\u89c4\u8303\u7684\u65b9\u5f0f\u4ee5\u53ca\u662f\u5426\u4f7f\u7528\u5f52\u4e00\u5316\u3002\u73b0\u6709\u65b9\u6cd5\u5982Adam\u548cMuon\u5728\u4e0d\u540c\u89c4\u8303\u7ec4\u5408\u4e0b\u7684\u8868\u73b0\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u5c06Adam\u548cMuon\u5f62\u5f0f\u5316\u4e3a\u975e\u6b27\u51e0\u91cc\u5f97\u68af\u5ea6\u4e0b\u964d\u7c7b\u578b\uff0c\u63a8\u5bfcMuon\u4f18\u5316\u5668\u7684\u65b0\u53d8\u4f53MuonMax\uff0c\u5e76\u5c06\u5176\u4e0e\u6a21\u578b\u52a8\u91cf(Momo)\u65b9\u6cd5\u7ed3\u5408\u3002", "result": "Muon\u5bf9\u5b66\u4e60\u7387\u9009\u62e9\u654f\u611f\uff0c\u800cMuonMax\u663e\u8457\u66f4\u9c81\u68d2\u3002\u4e0eMomo\u7ed3\u5408\u7684Muon\u53d8\u4f53\u5bf9\u8d85\u53c2\u6570\u8c03\u4f18\u66f4\u9c81\u68d2\uff0c\u901a\u5e38\u83b7\u5f97\u66f4\u597d\u7684\u9a8c\u8bc1\u5206\u6570\u3002", "conclusion": "\u5bf9\u4e8e\u65b0\u4efb\u52a1\uff0c\u5f53\u6700\u4f18\u8d85\u53c2\u6570\u672a\u77e5\u65f6\uff0c\u5efa\u8bae\u4f7f\u7528Momo\u4e0eMuonMax\u7ec4\u5408\uff0c\u4ee5\u8282\u7701\u6602\u8d35\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u6210\u672c\u3002"}}
{"id": "2510.11089", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11089", "abs": "https://arxiv.org/abs/2510.11089", "authors": ["Fabio Marco Monetti", "Adam Lundstr\u00f6m", "Colin de Kwant", "Magnus Gyllenskepp", "Antonio Maffei"], "title": "Establishing assembly-oriented modular product architectures through Design for Assembly enhanced Modular Function Deployment", "comment": null, "summary": "Modular product design has become a strategic enabler for companies seeking\nto balance product variety, operational efficiency, and market responsiveness,\nmaking the alignment between modular architecture and manufacturing\nconsiderations increasingly critical. Modular Function Deployment (MFD) is a\nwidely adopted method for defining modular product architectures, yet it lacks\nsystematic support for assembly considerations during early concept and\nsystem-level development. This limitation increases the risk of delayed\nproduction ramp-up and lifecycle inefficiencies. This paper proposes a set of\nenhancements to MFD that integrate Design for Assembly (DFA) logic into\narchitectural synthesis. The extended method introduces structured heuristics,\nassembly-oriented module drivers, a coded interface taxonomy, and quantitative\nmetrics for assessing assembly feasibility and automation readiness. These\nadditions preserve compatibility with standard MFD workflows while enriching\ndecision-making with traceable, production-informed reasoning. An illustrative\ncase study involving a handheld leaf blower demonstrates the method's usability\nand effectiveness. The redesigned architecture shows reduced assembly effort,\nsimplified interfaces, and increased automation potential. By supporting\nearly-stage evaluation of architectural alternatives through an assembly lens,\nthe method enables faster transition to efficient volume production and\nprovides a foundation for continuous improvement throughout the product\nlifecycle.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5bf9\u6a21\u5757\u5316\u529f\u80fd\u90e8\u7f72(MFD)\u65b9\u6cd5\u7684\u589e\u5f3a\uff0c\u5c06\u88c5\u914d\u8bbe\u8ba1(DFA)\u903b\u8f91\u96c6\u6210\u5230\u67b6\u6784\u5408\u6210\u4e2d\uff0c\u4ee5\u89e3\u51b3MFD\u5728\u65e9\u671f\u6982\u5ff5\u548c\u7cfb\u7edf\u7ea7\u5f00\u53d1\u4e2d\u7f3a\u4e4f\u88c5\u914d\u8003\u8651\u7684\u95ee\u9898\u3002", "motivation": "\u6a21\u5757\u5316\u4ea7\u54c1\u8bbe\u8ba1\u9700\u8981\u5728\u4ea7\u54c1\u591a\u6837\u6027\u548c\u8fd0\u8425\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4f46\u73b0\u6709\u7684MFD\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u88c5\u914d\u8003\u8651\u7684\u7cfb\u7edf\u652f\u6301\uff0c\u8fd9\u589e\u52a0\u4e86\u751f\u4ea7\u722c\u5761\u5ef6\u8fdf\u548c\u751f\u547d\u5468\u671f\u6548\u7387\u4f4e\u4e0b\u7684\u98ce\u9669\u3002", "method": "\u6269\u5c55\u7684MFD\u65b9\u6cd5\u5f15\u5165\u4e86\u7ed3\u6784\u5316\u542f\u53d1\u5f0f\u3001\u9762\u5411\u88c5\u914d\u7684\u6a21\u5757\u9a71\u52a8\u56e0\u7d20\u3001\u7f16\u7801\u63a5\u53e3\u5206\u7c7b\u6cd5\u548c\u5b9a\u91cf\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u88c5\u914d\u53ef\u884c\u6027\u548c\u81ea\u52a8\u5316\u51c6\u5907\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6807\u51c6MFD\u5de5\u4f5c\u6d41\u7a0b\u7684\u517c\u5bb9\u6027\u3002", "result": "\u901a\u8fc7\u624b\u6301\u5439\u53f6\u673a\u7684\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u91cd\u65b0\u8bbe\u8ba1\u7684\u67b6\u6784\u51cf\u5c11\u4e86\u88c5\u914d\u5de5\u4f5c\u91cf\uff0c\u7b80\u5316\u4e86\u63a5\u53e3\uff0c\u5e76\u63d0\u9ad8\u4e86\u81ea\u52a8\u5316\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u88c5\u914d\u89c6\u89d2\u652f\u6301\u65e9\u671f\u9636\u6bb5\u5bf9\u67b6\u6784\u66ff\u4ee3\u65b9\u6848\u7684\u8bc4\u4f30\uff0c\u80fd\u591f\u66f4\u5feb\u5730\u8fc7\u6e21\u5230\u9ad8\u6548\u7684\u6279\u91cf\u751f\u4ea7\uff0c\u5e76\u4e3a\u4ea7\u54c1\u751f\u547d\u5468\u671f\u4e2d\u7684\u6301\u7eed\u6539\u8fdb\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2510.09752", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.09752", "abs": "https://arxiv.org/abs/2510.09752", "authors": ["Sai Krishna Reddy Mudhiganti", "Juanyan Wang", "Ruo Yang", "Manali Sharma"], "title": "Patentformer: A demonstration of AI-assisted automated patent drafting", "comment": null, "summary": "Patent drafting presents significant challenges due to its reliance on the\nextensive experience and specialized expertise of patent attorneys, who must\npossess both legal acumen and technical understanding of an invention to craft\npatent applications in a formal legal writing style. This paper presents a\ndemonstration of Patentformer, an AI-powered automated patent drafting platform\ndesigned to support patent attorneys by rapidly producing high-quality patent\napplications adhering to legal writing standards.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Patentformer\uff0c\u4e00\u4e2aAI\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u4e13\u5229\u8d77\u8349\u5e73\u53f0\uff0c\u65e8\u5728\u5e2e\u52a9\u4e13\u5229\u5f8b\u5e08\u5feb\u901f\u751f\u6210\u7b26\u5408\u6cd5\u5f8b\u5199\u4f5c\u6807\u51c6\u7684\u9ad8\u8d28\u91cf\u4e13\u5229\u7533\u8bf7\u3002", "motivation": "\u4e13\u5229\u8d77\u8349\u4e25\u91cd\u4f9d\u8d56\u4e13\u5229\u5f8b\u5e08\u7684\u4e30\u5bcc\u7ecf\u9a8c\u548c\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4ed6\u4eec\u9700\u8981\u540c\u65f6\u5177\u5907\u6cd5\u5f8b\u654f\u9510\u5ea6\u548c\u5bf9\u53d1\u660e\u7684\u6280\u672f\u7406\u89e3\uff0c\u4ee5\u6b63\u5f0f\u7684\u6cd5\u5f8b\u5199\u4f5c\u98ce\u683c\u64b0\u5199\u4e13\u5229\u7533\u8bf7\u3002", "method": "\u5f00\u53d1\u4e86Patentformer\u5e73\u53f0\uff0c\u8fd9\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u4e13\u5229\u8d77\u8349\u7cfb\u7edf\u3002", "result": "\u8be5\u5e73\u53f0\u80fd\u591f\u5feb\u901f\u751f\u6210\u7b26\u5408\u6cd5\u5f8b\u5199\u4f5c\u6807\u51c6\u7684\u9ad8\u8d28\u91cf\u4e13\u5229\u7533\u8bf7\u3002", "conclusion": "Patentformer\u5c55\u793a\u4e86AI\u6280\u672f\u5728\u4e13\u5229\u8d77\u8349\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u4e13\u5229\u5f8b\u5e08\u7684\u5de5\u4f5c\u3002"}}
{"id": "2510.10945", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.10945", "abs": "https://arxiv.org/abs/2510.10945", "authors": ["Haishan Ye", "Xiangyu Chang", "Xi Chen"], "title": "A Unified Zeroth-Order Optimization Framework via Oblivious Randomized Sketching", "comment": null, "summary": "We propose a new framework for analyzing zeroth-order optimization (ZOO) from\nthe perspective of \\emph{oblivious randomized sketching}.In this framework,\ncommonly used gradient estimators in ZOO-such as finite difference (FD) and\nrandom finite difference (RFD)-are unified through a general sketch-based\nformulation. By introducing the concept of oblivious randomized sketching, we\nshow that properly chosen sketch matrices can significantly reduce the high\nvariance of RFD estimates and enable \\emph{high-probability} convergence\nguarantees of ZOO, which are rarely available in existing RFD analyses.\n  \\noindent We instantiate the framework on convex quadratic objectives and\nderive a query complexity of $\\tilde{\\mathcal{O}}(\\mathrm{tr}(A)/L \\cdot\nL/\\mu\\log\\frac{1}{\\epsilon})$ to achieve a $\\epsilon$-suboptimal solution,\nwhere $A$ is the Hessian, $L$ is the largest eigenvalue of $A$, and $\\mu$\ndenotes the strong convexity parameter. This complexity can be substantially\nsmaller than the standard query complexity of ${\\cO}(d\\cdot L/\\mu\n\\log\\frac{1}{\\epsilon})$ that is linearly dependent on problem dimensionality,\nespecially when $A$ has rapidly decaying eigenvalues. These advantages\nnaturally extend to more general settings, including strongly convex and\nHessian-aware optimization.\n  \\noindent Overall, this work offers a novel sketch-based perspective on ZOO\nthat explains why and when RFD-type methods can achieve \\emph{weakly\ndimension-independent} convergence in general smooth problems, providing both\ntheoretical foundations and practical implications for ZOO.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u968f\u673a\u8349\u56fe\u7684\u96f6\u9636\u4f18\u5316\u65b0\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u6709\u9650\u5dee\u5206\u548c\u968f\u673a\u6709\u9650\u5dee\u5206\u7b49\u68af\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u5408\u9002\u7684\u8349\u56fe\u77e9\u9635\u663e\u8457\u964d\u4f4e\u65b9\u5dee\uff0c\u5b9e\u73b0\u9ad8\u6982\u7387\u6536\u655b\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u4e2d\u968f\u673a\u6709\u9650\u5dee\u5206\u68af\u5ea6\u4f30\u8ba1\u5b58\u5728\u9ad8\u65b9\u5dee\u95ee\u9898\uff0c\u7f3a\u4e4f\u9ad8\u6982\u7387\u6536\u655b\u4fdd\u8bc1\uff0c\u4e14\u6807\u51c6\u65b9\u6cd5\u67e5\u8be2\u590d\u6742\u5ea6\u4e0e\u7ef4\u5ea6\u7ebf\u6027\u76f8\u5173\uff0c\u9650\u5236\u4e86\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165\u968f\u673a\u8349\u56fe\u6982\u5ff5\uff0c\u5c06\u68af\u5ea6\u4f30\u8ba1\u7edf\u4e00\u4e3a\u57fa\u4e8e\u8349\u56fe\u7684\u901a\u7528\u516c\u5f0f\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5408\u9002\u7684\u8349\u56fe\u77e9\u9635\u6765\u964d\u4f4e\u68af\u5ea6\u4f30\u8ba1\u65b9\u5dee\u3002", "result": "\u5728\u51f8\u4e8c\u6b21\u76ee\u6807\u4e0a\u5b9e\u73b0\u4e86\u67e5\u8be2\u590d\u6742\u5ea6\u4e3a\u00d5(tr(A)/L\u00b7L/\u03bclog(1/\u03b5))\uff0c\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u65b9\u6cd5\u00d5(d\u00b7L/\u03bclog(1/\u03b5))\uff0c\u7279\u522b\u5f53Hessian\u77e9\u9635\u7279\u5f81\u503c\u5feb\u901f\u8870\u51cf\u65f6\u4f18\u52bf\u66f4\u660e\u663e\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u7406\u89e3\u96f6\u9636\u4f18\u5316\u4e2d\u968f\u673a\u6709\u9650\u5dee\u5206\u65b9\u6cd5\u4e3a\u4f55\u53ca\u4f55\u65f6\u80fd\u5b9e\u73b0\u5f31\u7ef4\u5ea6\u65e0\u5173\u6536\u655b\u7684\u65b0\u89c6\u89d2\uff0c\u4e3a\u7406\u8bba\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2510.09885", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09885", "abs": "https://arxiv.org/abs/2510.09885", "authors": ["Xu Pan", "Ely Hahami", "Jingxuan Fan", "Ziqian Xie", "Haim Sompolinsky"], "title": "Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs", "comment": null, "summary": "Despite autoregressive large language models (arLLMs) being the current\ndominant paradigm in language modeling, they resist knowledge injection via\nfine-tuning due to inherent shortcomings such as the \"reversal curse\" -- the\nchallenge of answering questions that reverse the original information order in\nthe training sample. Masked diffusion large language models (dLLMs) are rapidly\nemerging as a powerful alternative to the arLLM paradigm, with evidence of\nbetter data efficiency and free of the \"reversal curse\" in pre-training.\nHowever, it is unknown whether these advantages extend to the post-training\nphase, i.e. whether pre-trained dLLMs can easily acquire new knowledge through\nfine-tuning. On three diverse datasets, we fine-tune arLLMs and dLLMs,\nevaluating them with forward and backward style Question Answering (QA) to\nprobe knowledge generalization and the reversal curse. Our results confirm that\narLLMs critically rely on extensive data augmentation via paraphrases for QA\ngeneralization, and paraphrases are only effective when their information order\nmatches the QA style. Conversely, dLLMs achieve high accuracies on both forward\nand backward QAs without paraphrases; adding paraphrases yields only marginal\ngains. Lastly, inspired by the dLLM's performance, we introduce a novel masked\nfine-tuning paradigm for knowledge injection into pre-trained arLLMs. This\nproposed method successfully and drastically improves the data efficiency of\narLLM fine-tuning, effectively closing the performance gap with dLLMs.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.09694", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09694", "abs": "https://arxiv.org/abs/2510.09694", "authors": ["Xiaodan Li", "Mengjie Wu", "Yao Zhu", "Yunna Lv", "YueFeng Chen", "Cen Chen", "Jianmei Guo", "Hui Xue"], "title": "Kelp: A Streaming Safeguard for Large Models via Latent Dynamics-Guided Risk Detection", "comment": null, "summary": "Large models (LMs) are powerful content generators, yet their open-ended\nnature can also introduce potential risks, such as generating harmful or biased\ncontent. Existing guardrails mostly perform post-hoc detection that may expose\nunsafe content before it is caught, and the latency constraints further push\nthem toward lightweight models, limiting detection accuracy. In this work, we\npropose Kelp, a novel plug-in framework that enables streaming risk detection\nwithin the LM generation pipeline. Kelp leverages intermediate LM hidden states\nthrough a Streaming Latent Dynamics Head (SLD), which models the temporal\nevolution of risk across the generated sequence for more accurate real-time\nrisk detection. To ensure reliable streaming moderation in real applications,\nwe introduce an Anchored Temporal Consistency (ATC) loss to enforce monotonic\nharm predictions by embedding a benign-then-harmful temporal prior. Besides,\nfor a rigorous evaluation of streaming guardrails, we also present\nStreamGuardBench-a model-grounded benchmark featuring on-the-fly responses from\neach protected model, reflecting real-world streaming scenarios in both text\nand vision-language tasks. Across diverse models and datasets, Kelp\nconsistently outperforms state-of-the-art post-hoc guardrails and prior plug-in\nprobes (15.61% higher average F1), while using only 20M parameters and adding\nless than 0.5 ms of per-token latency.", "AI": {"tldr": "Kelp\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u6d41\u5f0f\u98ce\u9669\u68c0\u6d4b\u63d2\u4ef6\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u5206\u6790\u9690\u85cf\u72b6\u6001\u6765\u68c0\u6d4b\u6709\u5bb3\u5185\u5bb9\uff0c\u76f8\u6bd4\u540e\u5904\u7406\u68c0\u6d4b\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u51c6\u786e\u6027\u548c\u66f4\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u9632\u62a4\u63aa\u65bd\u591a\u4e3a\u540e\u5904\u7406\u68c0\u6d4b\uff0c\u53ef\u80fd\u5728\u6355\u83b7\u524d\u66b4\u9732\u4e0d\u5b89\u5168\u5185\u5bb9\uff0c\u4e14\u5ef6\u8fdf\u9650\u5236\u8feb\u4f7f\u5176\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u9650\u5236\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faKelp\u6846\u67b6\uff0c\u5229\u7528Streaming Latent Dynamics Head (SLD)\u5efa\u6a21\u98ce\u9669\u5728\u751f\u6210\u5e8f\u5217\u4e2d\u7684\u65f6\u95f4\u6f14\u5316\uff0c\u5e76\u5f15\u5165Anchored Temporal Consistency (ATC)\u635f\u5931\u6765\u5f3a\u5236\u5355\u8c03\u7684\u5371\u5bb3\u9884\u6d4b\u3002", "result": "\u5728\u591a\u6837\u5316\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\uff0cKelp\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u540e\u5904\u7406\u9632\u62a4\u63aa\u65bd\u548c\u5148\u524d\u7684\u63d2\u4ef6\u63a2\u9488\uff08\u5e73\u5747F1\u63d0\u9ad815.61%\uff09\uff0c\u4ec5\u4f7f\u752820M\u53c2\u6570\u4e14\u6bcftoken\u5ef6\u8fdf\u589e\u52a0\u5c0f\u4e8e0.5\u6beb\u79d2\u3002", "conclusion": "Kelp\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u51c6\u786e\u7684\u6d41\u5f0f\u5185\u5bb9\u98ce\u9669\u68c0\u6d4b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u65f6\u9632\u62a4\u80fd\u529b\u3002"}}
{"id": "2510.10205", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10205", "abs": "https://arxiv.org/abs/2510.10205", "authors": ["Manjiang Yu", "Hongji Li", "Priyanka Singh", "Xue Li", "Di Wang", "Lijie Hu"], "title": "PIXEL: Adaptive Steering Via Position-wise Injection with eXact Estimated Levels under Subspace Calibration", "comment": "18 pages,3 figures", "summary": "Reliable behavior control is central to deploying large language models\n(LLMs) on the web. Activation steering offers a tuning-free route to align\nattributes (e.g., truthfulness) that ensure trustworthy generation. Prevailing\napproaches rely on coarse heuristics and lack a principled account of where to\nsteer and how strongly to intervene. To this end, we propose Position-wise\nInjection with eXact Estimated Levels (PIXEL), a position-wise activation\nsteering framework that, in contrast to prior work, learns a property-aligned\nsubspace from dual views (tail-averaged and end-token) and selects intervention\nstrength via a constrained geometric objective with a closed-form solution,\nthereby adapting to token-level sensitivity without global hyperparameter\ntuning. PIXEL further performs sample-level orthogonal residual calibration to\nrefine the global attribute direction and employs a lightweight\nposition-scanning routine to identify receptive injection sites. We\nadditionally provide representation-level guarantees for the\nminimal-intervention rule, supporting reliable alignment. Across diverse models\nand evaluation paradigms, PIXEL consistently improves attribute alignment while\npreserving model general capabilities, offering a practical and principled\nmethod for LLMs' controllable generation. Our code is available at\nhttps://github.com/V1centNevwake/PIXEL-Adaptive-Steering", "AI": {"tldr": "PIXEL\u662f\u4e00\u4e2a\u4f4d\u7f6e\u611f\u77e5\u7684\u6fc0\u6d3b\u5f15\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u89c6\u89d2\u5b66\u4e60\u5c5e\u6027\u5bf9\u9f50\u5b50\u7a7a\u95f4\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u5e72\u9884\u5f3a\u5ea6\uff0c\u65e0\u9700\u5168\u5c40\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u5b9e\u73b0LLM\u7684\u53ef\u63a7\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\u4f9d\u8d56\u7c97\u7c92\u5ea6\u542f\u53d1\u5f0f\uff0c\u7f3a\u4e4f\u5bf9\u5f15\u5bfc\u4f4d\u7f6e\u548c\u5e72\u9884\u5f3a\u5ea6\u7684\u539f\u5219\u6027\u8003\u8651\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u884c\u4e3a\u63a7\u5236\u65b9\u6cd5\u6765\u90e8\u7f72LLM\u3002", "method": "\u4f7f\u7528\u53cc\u89c6\u89d2\uff08\u5c3e\u5e73\u5747\u548c\u672b\u7aef\u6807\u8bb0\uff09\u5b66\u4e60\u5c5e\u6027\u5bf9\u9f50\u5b50\u7a7a\u95f4\uff0c\u901a\u8fc7\u7ea6\u675f\u51e0\u4f55\u76ee\u6807\u9009\u62e9\u5e72\u9884\u5f3a\u5ea6\uff0c\u8fdb\u884c\u6837\u672c\u7ea7\u6b63\u4ea4\u6b8b\u5dee\u6821\u51c6\u548c\u8f7b\u91cf\u7ea7\u4f4d\u7f6e\u626b\u63cf\u3002", "result": "\u5728\u4e0d\u540c\u6a21\u578b\u548c\u8bc4\u4f30\u8303\u5f0f\u4e0b\uff0cPIXEL\u6301\u7eed\u6539\u5584\u5c5e\u6027\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u901a\u7528\u80fd\u529b\u3002", "conclusion": "PIXEL\u4e3aLLM\u7684\u53ef\u63a7\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u539f\u5219\u6027\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u8868\u793a\u7ea7\u7684\u6700\u5c0f\u5e72\u9884\u4fdd\u8bc1\u3002"}}
{"id": "2510.09877", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09877", "abs": "https://arxiv.org/abs/2510.09877", "authors": ["Kangping Hu", "Stephen Mussmann"], "title": "Myopic Bayesian Decision Theory for Batch Active Learning with Partial Batch Label Sampling", "comment": null, "summary": "Over the past couple of decades, many active learning acquisition functions\nhave been proposed, leaving practitioners with an unclear choice of which to\nuse. Bayesian Decision Theory (BDT) offers a universal principle to guide\ndecision-making. In this work, we derive BDT for (Bayesian) active learning in\nthe myopic framework, where we imagine we only have one more point to label.\nThis derivation leads to effective algorithms such as Expected Error Reduction\n(EER), Expected Predictive Information Gain (EPIG), and other algorithms that\nappear in the literature. Furthermore, we show that BAIT (active learning based\non V-optimal experimental design) can be derived from BDT and asymptotic\napproximations. A key challenge of such methods is the difficult scaling to\nlarge batch sizes, leading to either computational challenges (BatchBALD) or\ndramatic performance drops (top-$B$ selection). Here, using a particular\nformulation of the decision process, we derive Partial Batch Label Sampling\n(ParBaLS) for the EPIG algorithm. We show experimentally for several datasets\nthat ParBaLS EPIG gives superior performance for a fixed budget and Bayesian\nLogistic Regression on Neural Embeddings. Our code is available at\nhttps://github.com/ADDAPT-ML/ParBaLS.", "AI": {"tldr": "\u672c\u6587\u57fa\u4e8e\u8d1d\u53f6\u65af\u51b3\u7b56\u7406\u8bba\u63a8\u5bfc\u4e86\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\uff0c\u63d0\u51fa\u4e86ParBaLS EPIG\u65b9\u6cd5\u6765\u89e3\u51b3\u5927\u6279\u91cf\u91c7\u6837\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4e3b\u52a8\u5b66\u4e60\u83b7\u53d6\u51fd\u6570\u4f17\u591a\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6307\u5bfc\uff0c\u4e14\u5728\u5927\u6279\u91cf\u91c7\u6837\u65f6\u9762\u4e34\u8ba1\u7b97\u6311\u6218\u6216\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u8d1d\u53f6\u65af\u51b3\u7b56\u7406\u8bba\u63a8\u5bfc\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\uff0c\u63d0\u51fa\u4e86Partial Batch Label Sampling (ParBaLS)\u65b9\u6cd5\uff0c\u7279\u522b\u9488\u5bf9EPIG\u7b97\u6cd5\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660eParBaLS EPIG\u5728\u56fa\u5b9a\u9884\u7b97\u548c\u8d1d\u53f6\u65af\u903b\u8f91\u56de\u5f52\u795e\u7ecf\u7f51\u7edc\u5d4c\u5165\u4e0a\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8d1d\u53f6\u65af\u51b3\u7b56\u7406\u8bba\u4e3a\u4e3b\u52a8\u5b66\u4e60\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0cParBaLS EPIG\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u5927\u6279\u91cf\u91c7\u6837\u95ee\u9898\u5e76\u83b7\u5f97\u66f4\u597d\u6027\u80fd\u3002"}}
{"id": "2510.11181", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11181", "abs": "https://arxiv.org/abs/2510.11181", "authors": ["Tamme Emunds", "Paul Brunzema", "Sebastian Trimpe", "Nils Nie\u00dfen"], "title": "Utilizing Bayesian Optimization for Timetable-Independent Railway Junction Performance Determination", "comment": null, "summary": "The efficiency of railway infrastructure is significantly influenced by the\nmix of trains that utilize it, as different service types have competing\noperational requirements. While freight services might require extended service\ntimes, passenger services demand more predictable schedules. Traditional\nmethods for addressing long-term traffic assignment problems often rely on\nfixed-value capacity limitations, determined based on specific assumptions\nabout traffic composition. This paper introduces a methodology for determining\ntimetable-independent capacity within the traffic rate assignment problem,\nenabling the calculation of junction capacities under dynamic traffic\ndistributions. We solve the underlying non-linear constrained optimization\nproblem maximizing the traffic throughput using Bayesian optimization (BO).\nThis setting combines a known objective function with expensive- to-compute\ncapacity constraints, motivating an adaption of standard BO problems, where\nobjective functions are usually unknown. We tailor the acquisition process in\nBO to this specific setting and increase performance by incorporating prior\nknowledge about the shape of the constraint functions into the Gaussian process\nsurrogate model. Our derived approaches are benchmarked on a railway junction\nnear Paris, significantly outperforming fixed traffic composition models and\nhighlighting the benefits of dynamic capacity allocation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u52a8\u6001\u4ea4\u901a\u5206\u914d\u65b9\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u94c1\u8def\u67a2\u7ebd\u7684\u4e0e\u65f6\u523b\u8868\u65e0\u5173\u7684\u5bb9\u91cf\uff0c\u663e\u8457\u4f18\u4e8e\u56fa\u5b9a\u4ea4\u901a\u7ec4\u6210\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u94c1\u8def\u5bb9\u91cf\u8ba1\u7b97\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u4ea4\u901a\u7ec4\u6210\u5047\u8bbe\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u670d\u52a1\u7c7b\u578b\uff08\u8d27\u8fd0\u4e0e\u5ba2\u8fd0\uff09\u7684\u7ade\u4e89\u6027\u8fd0\u8425\u9700\u6c42\uff0c\u9700\u8981\u52a8\u6001\u5bb9\u91cf\u5206\u914d\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u89e3\u51b3\u975e\u7ebf\u6027\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u6700\u5927\u5316\u4ea4\u901a\u541e\u5410\u91cf\uff0c\u901a\u8fc7\u8c03\u6574\u91c7\u96c6\u8fc7\u7a0b\u5e76\u5c06\u7ea6\u675f\u51fd\u6570\u5f62\u72b6\u7684\u5148\u9a8c\u77e5\u8bc6\u878d\u5165\u9ad8\u65af\u8fc7\u7a0b\u4ee3\u7406\u6a21\u578b\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "result": "\u5728\u5df4\u9ece\u9644\u8fd1\u94c1\u8def\u67a2\u7ebd\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u56fa\u5b9a\u4ea4\u901a\u7ec4\u6210\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u52a8\u6001\u5bb9\u91cf\u5206\u914d\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8ba1\u7b97\u52a8\u6001\u4ea4\u901a\u5206\u5e03\u4e0b\u7684\u94c1\u8def\u67a2\u7ebd\u5bb9\u91cf\uff0c\u4e3a\u957f\u671f\u4ea4\u901a\u5206\u914d\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10953", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.10953", "abs": "https://arxiv.org/abs/2510.10953", "authors": ["Qing Zhu", "Xian Yu", "Yu-Li Huang"], "title": "Distributionally Robust Optimization for Chemotherapy Scheduling under Asymmetric and Multi-Modal Uncertainty", "comment": null, "summary": "We consider a real-world chemotherapy scheduling template design problem,\nwhere we cluster patient types into groups and find a representative time-slot\nduration for each group to accommodate all patient types assigned to that\ngroup, aiming to minimize the total expected idle time and overtime. From Mayo\nClinic's real data, most patients' treatment durations are asymmetric (e.g.,\nshorter/longer durations tend to have a longer right/left tail). Motivated by\nthis observation, we consider a distributionally robust optimization (DRO)\nmodel under an asymmetric and multi-modal ambiguity set, where the distribution\nof the random treatment duration is modeled as a mixture of distributions from\ndifferent patient types. The ambiguity set captures uncertainty in both the\nmode probabilities, modeled via a variation-distance-based set, and the\ndistributions within each mode, characterized by moment information such as the\nempirical mean, variance, and semivariance. We reformulate the DRO model as a\nsemi-infinite program, which cannot be solved by off-the-shelf solvers. To\novercome this, we derive a closed-form expression for the worst-case expected\ncost and establish lower and upper bounds that are positively related to the\nvariability of patient types assigned to each group, based on which we develop\nexact algorithms and highly efficient clustering-based heuristics. The lower\nand upper bounds on the worst-case cost imply that the optimal cost tends to\ndecrease if we group patient types with similar treatment times. Through\nnumerical experiments based on both synthetic datasets and Mayo Clinic's real\ndata, we illustrate the effectiveness and efficiency of the proposed exact\nalgorithms and heuristics and showcase the benefits of incorporating asymmetric\ninformation into the DRO formulation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5316\u7597\u8c03\u5ea6\u6a21\u677f\u8bbe\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u60a3\u8005\u7c7b\u578b\u805a\u7c7b\u5206\u7ec4\u5e76\u4e3a\u6bcf\u7ec4\u5206\u914d\u4ee3\u8868\u6027\u65f6\u9699\u65f6\u957f\uff0c\u4ee5\u6700\u5c0f\u5316\u603b\u671f\u671b\u7a7a\u95f2\u65f6\u95f4\u548c\u52a0\u73ed\u65f6\u95f4\u3002\u91c7\u7528\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u65b9\u6cd5\u5904\u7406\u4e0d\u5bf9\u79f0\u591a\u6a21\u6001\u6cbb\u7597\u65f6\u957f\u5206\u5e03\uff0c\u5f00\u53d1\u4e86\u7cbe\u786e\u7b97\u6cd5\u548c\u9ad8\u6548\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "motivation": "\u57fa\u4e8e\u6885\u5965\u8bca\u6240\u771f\u5b9e\u6570\u636e\uff0c\u53d1\u73b0\u60a3\u8005\u6cbb\u7597\u65f6\u957f\u5448\u73b0\u4e0d\u5bf9\u79f0\u5206\u5e03\u7279\u5f81\uff0c\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u5904\u7406\u8fd9\u79cd\u4e0d\u5bf9\u79f0\u6027\u548c\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5316\u7597\u8c03\u5ea6\u6548\u7387\u3002", "method": "\u4f7f\u7528\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u6a21\u578b\uff0c\u6784\u5efa\u5305\u542b\u591a\u6a21\u6001\u548c\u4e0d\u5bf9\u79f0\u7279\u5f81\u7684\u6a21\u7cca\u96c6\uff0c\u8003\u8651\u6a21\u5f0f\u6982\u7387\u548c\u6bcf\u4e2a\u6a21\u5f0f\u5185\u5206\u5e03\u7684\u77e9\u4fe1\u606f\uff08\u5747\u503c\u3001\u65b9\u5dee\u3001\u534a\u65b9\u5dee\uff09\u3002\u5f00\u53d1\u4e86\u7cbe\u786e\u7b97\u6cd5\u548c\u57fa\u4e8e\u805a\u7c7b\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u548c\u6885\u5965\u8bca\u6240\u771f\u5b9e\u6570\u636e\u7684\u6570\u503c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b97\u6cd5\u7684\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u5c55\u793a\u4e86\u5728DRO\u6a21\u578b\u4e2d\u7eb3\u5165\u4e0d\u5bf9\u79f0\u4fe1\u606f\u7684\u4f18\u52bf\u3002", "conclusion": "\u5c06\u6cbb\u7597\u65f6\u95f4\u76f8\u4f3c\u7684\u60a3\u8005\u7c7b\u578b\u5206\u7ec4\u53ef\u4ee5\u964d\u4f4e\u6700\u4f18\u6210\u672c\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5316\u7597\u8c03\u5ea6\u4e2d\u7684\u4e0d\u5bf9\u79f0\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2510.09887", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09887", "abs": "https://arxiv.org/abs/2510.09887", "authors": ["Yijin Ni", "Peng Qi"], "title": "Abductive Preference Learning", "comment": null, "summary": "Frontier large language models such as GPT-5 and Claude Sonnet remain prone\nto overconfidence even after alignment through Reinforcement Learning with\nHuman Feedback (RLHF) and Direct Preference Optimization (DPO). For instance,\nthey tend to offer the same conservative answer \"No\" to both questions \"Can I\neat the [food / potato chips] that has been left out overnight?\" despite the\nlatter requiring no refridgeration for safe consumption. We find that this\nfailure is potentially attributed to a limitation of existing preference\nlearning: it emphasizes selecting the correct response for a given prompt,\nwhile neglecting counterfactual prompts that should alter the response.\n  To address this limitation, we propose abductive preference learning, a\nfine-tuning paradigm that reverses the conventional conditioning by learning\npreferences over prompts given a response. To validate this idea, we construct\nan abductive dataset derived from the HaluEval QA benchmark with 1,001 entries,\nimplementing abductive DPO and its variant DPOP. Experiments reveal\ncomplementary strengths: standard methods improve response selection, abductive\nmethods improve prompt discrimination, while a multitask objective unifies\nboth. On the abductive dataset, multitask DPOP boosts accuracy from $90.0\\%$ to\n$99.5\\%$ in response selection and $54.7\\%$ to $85.0\\%$ in prompt\ndiscrimination, with qualitative evidence highlighting improved sensitivity to\nprompt differences. Finally, evaluation on AlpacaEval shows multitask DPOP\nimproves win rate (from $5.26\\%$ to $6.17\\%$), confirming that abductive\npreference learning preserves the benefits of conventional preference\noptimization while addressing the overlooked challenge of counterfactual\nprompts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cd\u7ece\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u57fa\u4e8e\u56de\u7b54\u6765\u533a\u5206\u63d0\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u504f\u597d\u5b66\u4e60\u5ffd\u89c6\u53cd\u4e8b\u5b9e\u63d0\u793a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5373\u4f7f\u7ecf\u8fc7RLHF\u548cDPO\u5bf9\u9f50\u540e\u4ecd\u7136\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u65e0\u6cd5\u6b63\u786e\u5904\u7406\u53cd\u4e8b\u5b9e\u63d0\u793a\uff0c\u8fd9\u6e90\u4e8e\u4f20\u7edf\u504f\u597d\u5b66\u4e60\u53ea\u5173\u6ce8\u7ed9\u5b9a\u63d0\u793a\u9009\u62e9\u6b63\u786e\u56de\u7b54\uff0c\u800c\u5ffd\u89c6\u4e86\u5e94\u8be5\u6539\u53d8\u56de\u7b54\u7684\u53cd\u4e8b\u5b9e\u63d0\u793a\u3002", "method": "\u63d0\u51fa\u4e86\u53cd\u7ece\u504f\u597d\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u6784\u5efa\u53cd\u7ece\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u53cd\u7eceDPO\u53ca\u5176\u53d8\u4f53DPOP\uff0c\u5c06\u4f20\u7edf\u6761\u4ef6\u53cd\u8f6c\uff0c\u5b66\u4e60\u57fa\u4e8e\u56de\u7b54\u7684\u63d0\u793a\u504f\u597d\u3002", "result": "\u5728\u53cd\u7ece\u6570\u636e\u96c6\u4e0a\uff0c\u591a\u4efb\u52a1DPOP\u5c06\u56de\u7b54\u9009\u62e9\u51c6\u786e\u7387\u4ece90.0%\u63d0\u5347\u523099.5%\uff0c\u63d0\u793a\u533a\u5206\u51c6\u786e\u7387\u4ece54.7%\u63d0\u5347\u523085.0%\u3002\u5728AlpacaEval\u4e0a\uff0c\u80dc\u7387\u4ece5.26%\u63d0\u5347\u52306.17%\u3002", "conclusion": "\u53cd\u7ece\u504f\u597d\u5b66\u4e60\u5728\u4fdd\u6301\u4f20\u7edf\u504f\u597d\u4f18\u5316\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u53cd\u4e8b\u5b9e\u63d0\u793a\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u63d0\u793a\u5dee\u5f02\u7684\u654f\u611f\u6027\u3002"}}
{"id": "2510.09696", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09696", "abs": "https://arxiv.org/abs/2510.09696", "authors": ["Lorenzo Nikiforos", "Charalampos Antoniadis", "Luciano Prono", "Fabio Pareschi", "Riccardo Rovatti", "Gianluca Setti"], "title": "Vanishing Contributions: A Unified Approach to Smoothly Transition Neural Models into Compressed Form", "comment": "Code available at https://github.com/foros15/vanishing-contributions", "summary": "The increasing scale of deep neural networks has led to a growing need for\ncompression techniques such as pruning, quantization, and low-rank\ndecomposition. While these methods are very effective in reducing memory,\ncomputation and energy consumption, they often introduce severe accuracy\ndegradation when applied directly. We introduce Vanishing Contributions (VCON),\na general approach for smoothly transitioning neural models into compressed\nform. Rather than replacing the original network directly with its compressed\nversion, VCON executes the two in parallel during fine-tuning. The contribution\nof the original (uncompressed) model is progressively reduced, while that of\nthe compressed model is gradually increased. This smooth transition allows the\nnetwork to adapt over time, improving stability and mitigating accuracy\ndegradation. We evaluate VCON across computer vision and natural language\nprocessing benchmarks, in combination with multiple compression strategies.\nAcross all scenarios, VCON leads to consistent improvements: typical gains\nexceed 3%, while some configuration exhibits accuracy boosts of 20%. VCON thus\nprovides a generalizable method that can be applied to the existing compression\ntechniques, with evidence of consistent gains across multiple benchmarks.", "AI": {"tldr": "VCON\u662f\u4e00\u79cd\u5e73\u6ed1\u8fc7\u6e21\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5fae\u8c03\u671f\u95f4\u5e76\u884c\u8fd0\u884c\u539f\u59cb\u6a21\u578b\u548c\u538b\u7f29\u6a21\u578b\uff0c\u9010\u6b65\u51cf\u5c11\u539f\u59cb\u6a21\u578b\u7684\u8d21\u732e\u5e76\u589e\u52a0\u538b\u7f29\u6a21\u578b\u7684\u8d21\u732e\uff0c\u4ece\u800c\u7f13\u89e3\u538b\u7f29\u5bfc\u81f4\u7684\u7cbe\u5ea6\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u89c4\u6a21\u6269\u5927\uff0c\u538b\u7f29\u6280\u672f\u9700\u6c42\u589e\u957f\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u538b\u7f29\u65b9\u6cd5\u5f80\u5f80\u5bfc\u81f4\u4e25\u91cd\u7684\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "VCON\u65b9\u6cd5\u5728\u5fae\u8c03\u671f\u95f4\u5e76\u884c\u6267\u884c\u539f\u59cb\u6a21\u578b\u548c\u538b\u7f29\u6a21\u578b\uff0c\u9010\u6b65\u51cf\u5c11\u539f\u59cb\u6a21\u578b\u7684\u8d21\u732e\u6743\u91cd\uff0c\u540c\u65f6\u589e\u52a0\u538b\u7f29\u6a21\u578b\u7684\u8d21\u732e\u6743\u91cd\uff0c\u5b9e\u73b0\u5e73\u6ed1\u8fc7\u6e21\u3002", "result": "\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVCON\u7ed3\u5408\u591a\u79cd\u538b\u7f29\u7b56\u7565\u5747\u5e26\u6765\u4e00\u81f4\u6539\u8fdb\uff1a\u5178\u578b\u589e\u76ca\u8d85\u8fc73%\uff0c\u67d0\u4e9b\u914d\u7f6e\u7cbe\u5ea6\u63d0\u5347\u8fbe20%\u3002", "conclusion": "VCON\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6cdb\u5316\u7684\u65b9\u6cd5\uff0c\u53ef\u5e94\u7528\u4e8e\u73b0\u6709\u538b\u7f29\u6280\u672f\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u663e\u793a\u51fa\u6301\u7eed\u589e\u76ca\u3002"}}
{"id": "2510.10207", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10207", "abs": "https://arxiv.org/abs/2510.10207", "authors": ["Yujian Zhang", "Keyu Chen", "Zhifeng Shen", "Ruizhi Qiao", "Xing Sun"], "title": "Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by Hybrid Reasoning", "comment": null, "summary": "Although Long Reasoning Models (LRMs) have achieved superior performance on\nvarious reasoning scenarios, they often suffer from increased computational\ncosts and inference latency caused by overthinking. To address these\nlimitations, we propose Adaptive Dual Reasoner, which supports two reasoning\nmodes: fast thinking and slow thinking. ADR dynamically alternates between\nthese modes based on the contextual complexity during reasoning. ADR is trained\nin two stages: (1) A cold-start stage using supervised fine-tuning (SFT) to\nequip the model with the ability to integrate both fast and slow reasoning\nmodes, in which we construct a hybrid reasoning dataset through a dedicated\npipeline to provide large-scale supervision. (2) A reinforcement learning stage\nfor optimizing reasoning effort, where we introduce Entropy-guided Hybrid\nPolicy Optimization EHPO, an RL training framework employing an entropy-guided\ndynamic rollout strategy for branching at high-entropy units and a\ndifficulty-aware penalty to balance fast and slow reasoning. Across challenging\nmathematical reasoning benchmarks, ADR achieves an effective balance between\nreasoning performance and efficiency among state-of-the-art approaches.\nSpecifically, ADR yields a performance gain of up to 6.1%, while reducing the\nreasoning output length by 49.5% to 59.3%.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u53cc\u63a8\u7406\u5668(ADR)\uff0c\u901a\u8fc7\u5feb\u901f\u601d\u8003\u548c\u6162\u901f\u601d\u8003\u4e24\u79cd\u63a8\u7406\u6a21\u5f0f\u7684\u52a8\u6001\u5207\u6362\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u957f\u63a8\u7406\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u589e\u52a0\u548c\u63a8\u7406\u5ef6\u8fdf\u4e0a\u5347\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5e73\u8861\u63a8\u7406\u6027\u80fd\u4e0e\u6548\u7387\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1) \u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u6784\u5efa\u6df7\u5408\u63a8\u7406\u6570\u636e\u96c6\uff1b2) \u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u5f15\u5165\u71b5\u5f15\u5bfc\u6df7\u5408\u7b56\u7565\u4f18\u5316(EHPO)\uff0c\u5728\u9ad8\u71b5\u5355\u5143\u8fdb\u884c\u5206\u652f\uff0c\u5e76\u4f7f\u7528\u96be\u5ea6\u611f\u77e5\u60e9\u7f5a\u6765\u5e73\u8861\u5feb\u6162\u63a8\u7406\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cADR\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5c06\u63a8\u7406\u8f93\u51fa\u957f\u5ea6\u51cf\u5c1149.5%-59.3%\uff0c\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe6.1%\u3002", "conclusion": "ADR\u80fd\u591f\u6709\u6548\u5e73\u8861\u63a8\u7406\u6027\u80fd\u4e0e\u6548\u7387\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347\u63a8\u7406\u6548\u679c\uff0c\u4e3a\u957f\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2510.09888", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09888", "abs": "https://arxiv.org/abs/2510.09888", "authors": ["Yunlong Feng", "Qiang Wu"], "title": "Understanding Robust Machine Learning for Nonparametric Regression with Heavy-Tailed Noise", "comment": null, "summary": "We investigate robust nonparametric regression in the presence of\nheavy-tailed noise, where the hypothesis class may contain unbounded functions\nand robustness is ensured via a robust loss function $\\ell_\\sigma$. Using Huber\nregression as a close-up example within Tikhonov-regularized risk minimization\nin reproducing kernel Hilbert spaces (RKHS), we address two central challenges:\n(i) the breakdown of standard concentration tools under weak moment\nassumptions, and (ii) the analytical difficulties introduced by unbounded\nhypothesis spaces. Our first message is conceptual: conventional\ngeneralization-error bounds for robust losses do not faithfully capture\nout-of-sample performance. We argue that learnability should instead be\nquantified through prediction error, namely the $L_2$-distance to the truth\n$f^\\star$, which is $\\sigma$-independent and directly reflects the target of\nrobust estimation. To make this workable under unboundedness, we introduce a\n\\emph{probabilistic effective hypothesis space} that confines the estimator\nwith high probability and enables a meaningful bias--variance decomposition\nunder weak $(1+\\epsilon)$-moment conditions. Technically, we establish new\ncomparison theorems linking the excess robust risk to the $L_2$ prediction\nerror up to a residual of order $\\mathcal{O}(\\sigma^{-2\\epsilon})$, clarifying\nthe robustness--bias trade-off induced by the scale parameter $\\sigma$.\nBuilding on this, we derive explicit finite-sample error bounds and convergence\nrates for Huber regression in RKHS that hold without uniform boundedness and\nunder heavy-tailed noise. Our study delivers principled tuning rules, extends\nbeyond Huber to other robust losses, and highlights prediction error, not\nexcess generalization risk, as the fundamental lens for analyzing robust\nlearning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u91cd\u5c3e\u566a\u58f0\u4e0b\u7684\u7a33\u5065\u975e\u53c2\u6570\u56de\u5f52\uff0c\u4f7f\u7528Huber\u56de\u5f52\u4f5c\u4e3a\u793a\u4f8b\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u9884\u6d4b\u8bef\u5dee\u800c\u975e\u6cdb\u5316\u8bef\u5dee\u7684\u5b66\u4e60\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5e76\u5728\u65e0\u754c\u5047\u8bbe\u7a7a\u95f4\u4e0b\u5efa\u7acb\u4e86\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\u3002", "motivation": "\u4f20\u7edf\u7a33\u5065\u635f\u5931\u7684\u6cdb\u5316\u8bef\u5dee\u754c\u4e0d\u80fd\u51c6\u786e\u53cd\u6620\u6837\u672c\u5916\u6027\u80fd\uff0c\u4e14\u65e0\u754c\u5047\u8bbe\u7a7a\u95f4\u548c\u5f31\u77e9\u5047\u8bbe\u4e0b\u6807\u51c6\u6d53\u5ea6\u5de5\u5177\u5931\u6548\uff0c\u9700\u8981\u65b0\u7684\u5206\u6790\u6846\u67b6\u3002", "method": "\u4f7f\u7528Tikhonov\u6b63\u5219\u5316\u98ce\u9669\u6700\u5c0f\u5316\u5728\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\uff0c\u5f15\u5165\u6982\u7387\u6709\u6548\u5047\u8bbe\u7a7a\u95f4\uff0c\u5efa\u7acb\u6bd4\u8f83\u5b9a\u7406\u8fde\u63a5\u8d85\u989d\u7a33\u5065\u98ce\u9669\u4e0eL2\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u5728\u91cd\u5c3e\u566a\u58f0\u4e0b\u63a8\u5bfc\u8bef\u5dee\u754c\u3002", "result": "\u5efa\u7acb\u4e86\u65b0\u7684\u6bd4\u8f83\u5b9a\u7406\uff0c\u5c06\u8d85\u989d\u7a33\u5065\u98ce\u9669\u4e0eL2\u9884\u6d4b\u8bef\u5dee\u8054\u7cfb\u8d77\u6765\uff0c\u6b8b\u5dee\u4e3aO(\u03c3^{-2\u03b5})\uff0c\u9610\u660e\u4e86\u5c3a\u5ea6\u53c2\u6570\u03c3\u5f15\u8d77\u7684\u7a33\u5065\u6027-\u504f\u5dee\u6743\u8861\uff0c\u5e76\u63a8\u5bfc\u4e86Huber\u56de\u5f52\u7684\u663e\u5f0f\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\u548c\u6536\u655b\u7387\u3002", "conclusion": "\u9884\u6d4b\u8bef\u5dee\u800c\u975e\u8d85\u989d\u6cdb\u5316\u98ce\u9669\u662f\u5206\u6790\u7a33\u5065\u5b66\u4e60\u7684\u57fa\u672c\u89c6\u89d2\uff0c\u7814\u7a76\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7684\u8c03\u53c2\u89c4\u5219\uff0c\u5e76\u9002\u7528\u4e8eHuber\u4ee5\u5916\u7684\u5176\u4ed6\u7a33\u5065\u635f\u5931\u51fd\u6570\u3002"}}
{"id": "2510.11286", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11286", "abs": "https://arxiv.org/abs/2510.11286", "authors": ["Jack Jackman", "David Ryan", "Arun Narayanan", "Pedro Nardelli", "Indrakshi Dey"], "title": "Edge-to-Cloud Computations-as-a-Service in Software-Defined Energy Networks for Smart Grids", "comment": null, "summary": "Modern power grids face an acute mismatch between where data is generated and\nwhere it can be processed: protection relays, EV (Electric Vehicle) charging,\nand distributed renewables demand millisecond analytics at the edge, while\nenergy-hungry workloads often sit in distant clouds leading to missed real-time\ndeadlines and wasted power. We address this by proposing, to our knowledge, the\nfirst-ever SDEN (Software Defined Energy Network) for CaaS\n(Computations-as-a-Service) that unifies edge, fog, and cloud compute with 5G\nURLLC (Ultra-Reliable Low-Latency Communications), SDN (Software Defined\nNetworking), and NFV (Network Functions Virtualization) to co-optimize energy,\nlatency, and reliability end-to-end. Our contributions are threefold: (i) a\njoint task offloading formulation that couples computation placement with\nnetwork capacity under explicit URLLC constraints; (ii) a feasibility\npreserving, lightweight greedy heuristic that scales while closely tracking\noptimal energy and latency trade-offs; and (iii) a tiered AI (Artificial\nIntelligence) pipeline-reactive at the edge, predictive in the fog, strategic\nin the cloud-featuring privacy-preserving, federated GNNs (Graph Neural\nNetworks) for fault detection and microgrid coordination. Unlike prior\nedge-only or cloud-only schemes, SDEN turns fragmented grid compute into a\nsingle, programmable substrate that delivers dependable, energy-aware, real\ntime analytics establishing a first-ever, software defined path to practical,\ngrid-scale CaaS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u8f6f\u4ef6\u5b9a\u4e49\u80fd\u6e90\u7f51\u7edc(SDEN)\uff0c\u901a\u8fc7\u7edf\u4e00\u8fb9\u7f18\u3001\u96fe\u548c\u4e91\u8ba1\u7b97\uff0c\u7ed3\u54085G URLLC\u3001SDN\u548cNFV\u6280\u672f\uff0c\u7aef\u5230\u7aef\u534f\u540c\u4f18\u5316\u80fd\u6e90\u3001\u5ef6\u8fdf\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u7535\u7f51\u63d0\u4f9b\u53ef\u9760\u3001\u80fd\u6e90\u611f\u77e5\u7684\u5b9e\u65f6\u5206\u6790\u670d\u52a1\u3002", "motivation": "\u73b0\u4ee3\u7535\u7f51\u9762\u4e34\u6570\u636e\u751f\u6210\u4f4d\u7f6e\u4e0e\u5904\u7406\u4f4d\u7f6e\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff1a\u4fdd\u62a4\u7ee7\u7535\u5668\u3001\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u548c\u5206\u5e03\u5f0f\u53ef\u518d\u751f\u80fd\u6e90\u9700\u8981\u5728\u8fb9\u7f18\u8fdb\u884c\u6beb\u79d2\u7ea7\u5206\u6790\uff0c\u800c\u80fd\u8017\u5927\u7684\u5de5\u4f5c\u8d1f\u8f7d\u5f80\u5f80\u4f4d\u4e8e\u9065\u8fdc\u7684\u4e91\u7aef\uff0c\u5bfc\u81f4\u9519\u8fc7\u5b9e\u65f6\u622a\u6b62\u671f\u9650\u548c\u6d6a\u8d39\u7535\u529b\u3002", "method": "\u63d0\u51fa\u4e86SDEN\u6846\u67b6\uff0c\u5305\u62ec\uff1a(i)\u8054\u5408\u4efb\u52a1\u5378\u8f7d\u516c\u5f0f\uff0c\u5728\u660e\u786eURLLC\u7ea6\u675f\u4e0b\u8026\u5408\u8ba1\u7b97\u653e\u7f6e\u4e0e\u7f51\u7edc\u5bb9\u91cf\uff1b(ii)\u4fdd\u6301\u53ef\u884c\u6027\u7684\u8f7b\u91cf\u7ea7\u8d2a\u5a6a\u542f\u53d1\u5f0f\u7b97\u6cd5\uff1b(iii)\u5206\u5c42AI\u7ba1\u9053-\u8fb9\u7f18\u53cd\u5e94\u5f0f\u3001\u96fe\u9884\u6d4b\u5f0f\u3001\u4e91\u6218\u7565\u5f0f\uff0c\u91c7\u7528\u9690\u79c1\u4fdd\u62a4\u7684\u8054\u90a6\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u6545\u969c\u68c0\u6d4b\u548c\u5fae\u7535\u7f51\u534f\u8c03\u3002", "result": "SDEN\u5c06\u788e\u7247\u5316\u7684\u7535\u7f51\u8ba1\u7b97\u8f6c\u53d8\u4e3a\u5355\u4e00\u53ef\u7f16\u7a0b\u5e73\u53f0\uff0c\u63d0\u4f9b\u53ef\u9760\u3001\u80fd\u6e90\u611f\u77e5\u7684\u5b9e\u65f6\u5206\u6790\uff0c\u5efa\u7acb\u4e86\u9996\u4e2a\u5b9e\u7528\u7684\u7535\u7f51\u7ea7\u8ba1\u7b97\u5373\u670d\u52a1(CaaS)\u7684\u8f6f\u4ef6\u5b9a\u4e49\u8def\u5f84\u3002", "conclusion": "\u4e0e\u4e4b\u524d\u4ec5\u8fb9\u7f18\u6216\u4ec5\u4e91\u7aef\u7684\u65b9\u6848\u4e0d\u540c\uff0cSDEN\u901a\u8fc7\u7edf\u4e00\u8ba1\u7b97\u67b6\u6784\u548c\u5206\u5c42AI\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7535\u7f51\u5b9e\u65f6\u5206\u6790\u7684\u5ef6\u8fdf\u548c\u80fd\u6e90\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u7535\u7f51\u8ba1\u7b97\u670d\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8f6f\u4ef6\u5b9a\u4e49\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10338", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.10338", "abs": "https://arxiv.org/abs/2510.10338", "authors": ["Balagopal Unnikrishnan", "Ariel Guerra Adames", "Amin Adibi", "Sameer Peesapati", "Rafal Kocielnik", "Shira Fischer", "Hillary Clinton Kasimbazi", "Rodrigo Gameiro", "Alina Peluso", "Chrystinne Oliveira Fernandes", "Maximin Lange", "Lovedeep Gondara", "Leo Anthony Celi"], "title": "Beyond Ethics: How Inclusive Innovation Drives Economic Returns in Medical AI", "comment": null, "summary": "While ethical arguments for fairness in healthcare AI are well-established,\nthe economic and strategic value of inclusive design remains underexplored.\nThis perspective introduces the ``inclusive innovation dividend'' -- the\ncounterintuitive principle that solutions engineered for diverse, constrained\nuse cases generate superior economic returns in broader markets. Drawing from\nassistive technologies that evolved into billion-dollar mainstream industries,\nwe demonstrate how inclusive healthcare AI development creates business value\nbeyond compliance requirements. We identify four mechanisms through which\ninclusive innovation drives returns: (1) market expansion via geographic\nscalability and trust acceleration; (2) risk mitigation through reduced\nremediation costs and litigation exposure; (3) performance dividends from\nsuperior generalization and reduced technical debt, and (4) competitive\nadvantages in talent acquisition and clinical adoption. We present the\nHealthcare AI Inclusive Innovation Framework (HAIIF), a practical scoring\nsystem that enables organizations to evaluate AI investments based on their\npotential to capture these benefits. HAIIF provides structured guidance for\nresource allocation, transforming fairness and inclusivity from regulatory\ncheckboxes into sources of strategic differentiation. Our findings suggest that\norganizations investing incrementally in inclusive design can achieve expanded\nmarket reach and sustained competitive advantages, while those treating these\nconsiderations as overhead face compounding disadvantages as network effects\nand data advantages accrue to early movers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\"\u5305\u5bb9\u6027\u521b\u65b0\u7ea2\u5229\"\u6982\u5ff5\uff0c\u8ba4\u4e3a\u4e3a\u591a\u6837\u5316\u3001\u53d7\u9650\u4f7f\u7528\u573a\u666f\u8bbe\u8ba1\u7684\u533b\u7597AI\u89e3\u51b3\u65b9\u6848\u80fd\u5728\u66f4\u5e7f\u6cdb\u5e02\u573a\u4ea7\u751f\u66f4\u4f18\u7ecf\u6d4e\u56de\u62a5\uff0c\u5e76\u5f00\u53d1\u4e86HAIIF\u8bc4\u5206\u7cfb\u7edf\u6765\u8bc4\u4f30AI\u6295\u8d44\u7684\u5305\u5bb9\u6027\u4ef7\u503c\u3002", "motivation": "\u867d\u7136\u533b\u7597AI\u516c\u5e73\u6027\u7684\u4f26\u7406\u8bba\u8bc1\u5df2\u5f88\u6210\u719f\uff0c\u4f46\u5305\u5bb9\u6027\u8bbe\u8ba1\u7684\u7ecf\u6d4e\u548c\u6218\u7565\u4ef7\u503c\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u5305\u5bb9\u6027\u521b\u65b0\u5982\u4f55\u521b\u9020\u8d85\u8d8a\u5408\u89c4\u8981\u6c42\u7684\u5546\u4e1a\u4ef7\u503c\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4ece\u8f85\u52a9\u6280\u672f\u6f14\u53d8\u4e3a\u4e3b\u6d41\u4ea7\u4e1a\u7684\u6848\u4f8b\uff0c\u8bc6\u522b\u5305\u5bb9\u6027\u521b\u65b0\u9a71\u52a8\u56de\u62a5\u7684\u56db\u4e2a\u673a\u5236\uff0c\u5e76\u5f00\u53d1\u4e86\u533b\u7597AI\u5305\u5bb9\u6027\u521b\u65b0\u6846\u67b6(HAIIF)\u4f5c\u4e3a\u5b9e\u8df5\u8bc4\u5206\u7cfb\u7edf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5305\u5bb9\u6027\u8bbe\u8ba1\u80fd\u901a\u8fc7\u5e02\u573a\u6269\u5f20\u3001\u98ce\u9669\u7f13\u89e3\u3001\u6027\u80fd\u7ea2\u5229\u548c\u7ade\u4e89\u4f18\u52bf\u56db\u4e2a\u673a\u5236\u521b\u9020\u663e\u8457\u7ecf\u6d4e\u4ef7\u503c\u3002HAIIF\u6846\u67b6\u4e3a\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u8bc4\u4f30AI\u6295\u8d44\u5305\u5bb9\u6027\u6f5c\u529b\u7684\u7ed3\u6784\u5316\u5de5\u5177\u3002", "conclusion": "\u6e10\u8fdb\u6295\u8d44\u5305\u5bb9\u6027\u8bbe\u8ba1\u7684\u7ec4\u7ec7\u80fd\u5b9e\u73b0\u5e02\u573a\u6269\u5f20\u548c\u6301\u7eed\u7ade\u4e89\u4f18\u52bf\uff0c\u800c\u5c06\u5305\u5bb9\u6027\u89c6\u4e3a\u6210\u672c\u7684\u7ec4\u7ec7\u5c06\u9762\u4e34\u7f51\u7edc\u6548\u5e94\u548c\u6570\u636e\u4f18\u52bf\u79ef\u7d2f\u5e26\u6765\u7684\u590d\u5408\u52a3\u52bf\u3002"}}
{"id": "2510.10966", "categories": ["math.OC", "cs.DM"], "pdf": "https://arxiv.org/pdf/2510.10966", "abs": "https://arxiv.org/abs/2510.10966", "authors": ["Santanu S. Dey", "Fr\u00e9d\u00e9ric Meunier", "Diego Moran Ramirez"], "title": "Geoffrion's theorem beyond finiteness and rationality", "comment": null, "summary": "Geoffrion's theorem is a fundamental result from mathematical programming\nassessing the quality of Lagrangian relaxation, a standard technique to get\nbounds for integer programs. An often implicit condition is that the set of\nfeasible solutions is finite or described by rational linear constraints.\nHowever, we show through concrete examples that the conclusion of Geoffrion's\ntheorem does not necessarily hold when this condition is dropped. We then\nprovide sufficient conditions ensuring the validity of the result even when the\nfeasible set is not finite and cannot be described using finitely-many linear\nconstraints.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5177\u4f53\u53cd\u4f8b\u8bc1\u660e\uff0c\u5f53\u53ef\u884c\u89e3\u96c6\u4e0d\u662f\u6709\u9650\u96c6\u6216\u4e0d\u80fd\u7528\u6709\u9650\u4e2a\u7ebf\u6027\u7ea6\u675f\u63cf\u8ff0\u65f6\uff0cGeoffrion\u5b9a\u7406\u7684\u7ed3\u8bba\u4e0d\u4e00\u5b9a\u6210\u7acb\uff0c\u5e76\u63d0\u4f9b\u4e86\u786e\u4fdd\u7ed3\u679c\u6709\u6548\u6027\u7684\u5145\u5206\u6761\u4ef6\u3002", "motivation": "Geoffrion\u5b9a\u7406\u662f\u6570\u5b66\u89c4\u5212\u4e2d\u8bc4\u4f30\u62c9\u683c\u6717\u65e5\u677e\u5f1b\u8d28\u91cf\u7684\u57fa\u672c\u7ed3\u679c\uff0c\u4f46\u901a\u5e38\u9690\u542b\u5047\u8bbe\u53ef\u884c\u89e3\u96c6\u662f\u6709\u9650\u7684\u6216\u7531\u6709\u7406\u7ebf\u6027\u7ea6\u675f\u63cf\u8ff0\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u5f53\u8fd9\u4e00\u6761\u4ef6\u4e0d\u6ee1\u8db3\u65f6\u5b9a\u7406\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u6784\u9020\u5177\u4f53\u53cd\u4f8b\u5c55\u793a\u5f53\u53ef\u884c\u89e3\u96c6\u4e0d\u662f\u6709\u9650\u96c6\u6216\u4e0d\u80fd\u7528\u6709\u9650\u7ebf\u6027\u7ea6\u675f\u63cf\u8ff0\u65f6\uff0cGeoffrion\u5b9a\u7406\u53ef\u80fd\u5931\u6548\uff0c\u7136\u540e\u63d0\u4f9b\u786e\u4fdd\u7ed3\u679c\u6210\u7acb\u7684\u5145\u5206\u6761\u4ef6\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u7f3a\u4e4f\u6709\u9650\u6027\u6216\u6709\u9650\u7ebf\u6027\u7ea6\u675f\u63cf\u8ff0\u7684\u6761\u4ef6\u4e0b\uff0cGeoffrion\u5b9a\u7406\u7684\u7ed3\u8bba\u4e0d\u4e00\u5b9a\u6210\u7acb\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u7279\u5b9a\u5145\u5206\u6761\u4ef6\u6765\u4fdd\u8bc1\u5176\u6709\u6548\u6027\u3002", "conclusion": "Geoffrion\u5b9a\u7406\u7684\u9002\u7528\u6027\u4f9d\u8d56\u4e8e\u53ef\u884c\u89e3\u96c6\u7684\u6027\u8d28\uff0c\u5f53\u4e0d\u6ee1\u8db3\u4f20\u7edf\u5047\u8bbe\u65f6\u9700\u8981\u989d\u5916\u7684\u5145\u5206\u6761\u4ef6\u6765\u786e\u4fdd\u7ed3\u679c\u7684\u6b63\u786e\u6027\u3002"}}
{"id": "2510.09893", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09893", "abs": "https://arxiv.org/abs/2510.09893", "authors": ["Guanming Chen", "Lingzhi Shen", "Xiaohao Cai", "Imran Razzak", "Shoaib Jameel"], "title": "HIPPD: Brain-Inspired Hierarchical Information Processing for Personality Detection", "comment": null, "summary": "Personality detection from text aims to infer an individual's personality\ntraits based on linguistic patterns. However, existing machine learning\napproaches often struggle to capture contextual information spanning multiple\nposts and tend to fall short in extracting representative and robust features\nin semantically sparse environments. This paper presents HIPPD, a\nbrain-inspired framework for personality detection that emulates the\nhierarchical information processing of the human brain. HIPPD utilises a large\nlanguage model to simulate the cerebral cortex, enabling global semantic\nreasoning and deep feature abstraction. A dynamic memory module, modelled after\nthe prefrontal cortex, performs adaptive gating and selective retention of\ncritical features, with all adjustments driven by dopaminergic prediction error\nfeedback. Subsequently, a set of specialised lightweight models, emulating the\nbasal ganglia, are dynamically routed via a strict winner-takes-all mechanism\nto capture the personality-related patterns they are most proficient at\nrecognising. Extensive experiments on the Kaggle and Pandora datasets\ndemonstrate that HIPPD consistently outperforms state-of-the-art baselines.", "AI": {"tldr": "HIPPD\u662f\u4e00\u4e2a\u53d7\u5927\u8111\u542f\u53d1\u7684\u6027\u683c\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u8111\u7684\u5206\u5c42\u4fe1\u606f\u5904\u7406\u673a\u5236\uff0c\u5728\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u7840\u4e0a\u7ed3\u5408\u52a8\u6001\u8bb0\u5fc6\u6a21\u5757\u548c\u4e13\u95e8\u5316\u8f7b\u91cf\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ece\u6587\u672c\u4e2d\u68c0\u6d4b\u6027\u683c\u7279\u5f81\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u8de8\u591a\u4e2a\u5e16\u5b50\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5728\u8bed\u4e49\u7a00\u758f\u73af\u5883\u4e2d\u63d0\u53d6\u4ee3\u8868\u6027\u7279\u5f81\u7684\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6846\u67b6\u6765\u63d0\u5347\u6027\u683c\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u5927\u8111\u76ae\u5c42\u8fdb\u884c\u5168\u5c40\u8bed\u4e49\u63a8\u7406\uff0c\u52a8\u6001\u8bb0\u5fc6\u6a21\u5757\u6a21\u62df\u524d\u989d\u53f6\u76ae\u5c42\u8fdb\u884c\u81ea\u9002\u5e94\u95e8\u63a7\u548c\u9009\u62e9\u6027\u4fdd\u7559\uff0c\u4e13\u95e8\u5316\u8f7b\u91cf\u6a21\u578b\u6a21\u62df\u57fa\u5e95\u795e\u7ecf\u8282\u901a\u8fc7\u8d62\u5bb6\u901a\u5403\u673a\u5236\u52a8\u6001\u8def\u7531\u3002", "result": "\u5728Kaggle\u548cPandora\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHIPPD\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u53d7\u5927\u8111\u542f\u53d1\u7684\u5206\u5c42\u4fe1\u606f\u5904\u7406\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u6027\u683c\u68c0\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u795e\u7ecf\u79d1\u5b66\u539f\u7406\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.09704", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09704", "abs": "https://arxiv.org/abs/2510.09704", "authors": ["Matthew Schlegel", "Matthew E. Taylor", "Mostafa Farrokhabadi"], "title": "Operator Learning for Power Systems Simulation", "comment": null, "summary": "Time domain simulation, i.e., modeling the system's evolution over time, is a\ncrucial tool for studying and enhancing power system stability and dynamic\nperformance. However, these simulations become computationally intractable for\nrenewable-penetrated grids, due to the small simulation time step required to\ncapture renewable energy resources' ultra-fast dynamic phenomena in the range\nof 1-50 microseconds. This creates a critical need for solutions that are both\nfast and scalable, posing a major barrier for the stable integration of\nrenewable energy resources and thus climate change mitigation. This paper\nexplores operator learning, a family of machine learning methods that learn\nmappings between functions, as a surrogate model for these costly simulations.\nThe paper investigates, for the first time, the fundamental concept of\nsimulation time step-invariance, which enables models trained on coarse time\nsteps to generalize to fine-resolution dynamics. Three operator learning\nmethods are benchmarked on a simple test system that, while not incorporating\npractical complexities of renewable-penetrated grids, serves as a first\nproof-of-concept to demonstrate the viability of time step-invariance. Models\nare evaluated on (i) zero-shot super-resolution, where training is performed on\na coarse simulation time step and inference is performed at super-resolution,\nand (ii) generalization between stable and unstable dynamic regimes. This work\naddresses a key challenge in the integration of renewable energy for the\nmitigation of climate change by benchmarking operator learning methods to model\nphysical systems.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4f7f\u7528\u7b97\u5b50\u5b66\u4e60\u4f5c\u4e3a\u7535\u529b\u7cfb\u7edf\u65f6\u57df\u4eff\u771f\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u9996\u6b21\u7814\u7a76\u4e86\u4eff\u771f\u65f6\u95f4\u6b65\u957f\u4e0d\u53d8\u6027\u6982\u5ff5\uff0c\u4f7f\u5728\u7c97\u65f6\u95f4\u6b65\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u591f\u6cdb\u5316\u5230\u7cbe\u7ec6\u5206\u8fa8\u7387\u52a8\u6001\u3002", "motivation": "\u53ef\u518d\u751f\u80fd\u6e90\u6e17\u900f\u7535\u7f51\u7684\u65f6\u57df\u4eff\u771f\u56e0\u9700\u8981\u5fae\u79d2\u7ea7\u65f6\u95f4\u6b65\u957f\u800c\u8ba1\u7b97\u56f0\u96be\uff0c\u8fd9\u963b\u788d\u4e86\u53ef\u518d\u751f\u80fd\u6e90\u7684\u7a33\u5b9a\u96c6\u6210\u548c\u6c14\u5019\u53d8\u5316\u7f13\u89e3\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\u4f5c\u4e3a\u4eff\u771f\u66ff\u4ee3\u6a21\u578b\uff0c\u5728\u7b80\u5355\u6d4b\u8bd5\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u65f6\u95f4\u6b65\u957f\u4e0d\u53d8\u6027\u6982\u5ff5\uff0c\u8bc4\u4f30\u96f6\u6837\u672c\u8d85\u5206\u8fa8\u7387\u548c\u7a33\u5b9a/\u4e0d\u7a33\u5b9a\u52a8\u6001\u673a\u5236\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u7814\u7a76\u8bc1\u660e\u4e86\u65f6\u95f4\u6b65\u957f\u4e0d\u53d8\u6027\u7684\u53ef\u884c\u6027\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u7c97\u65f6\u95f4\u6b65\u4e0a\u8bad\u7ec3\u540e\u6cdb\u5316\u5230\u7cbe\u7ec6\u5206\u8fa8\u7387\u52a8\u6001\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\u6765\u5efa\u6a21\u7269\u7406\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u53ef\u518d\u751f\u80fd\u6e90\u96c6\u6210\u548c\u6c14\u5019\u53d8\u5316\u7f13\u89e3\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2510.10238", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10238", "abs": "https://arxiv.org/abs/2510.10238", "authors": ["Zixuan Qin", "Kunlin Lyu", "Qingchen Yu", "Yifan Sun", "Zhaoxin Fan"], "title": "The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities", "comment": null, "summary": "Large Language Models (LLMs) have become foundational tools in natural\nlanguage processing, powering a wide range of applications and research. Many\nstudies have shown that LLMs share significant similarities with the human\nbrain. Recent neuroscience research has found that a small subset of biological\nneurons in the human brain are crucial for core cognitive functions, which\nraises a fundamental question: do LLMs also contain a small subset of critical\nneurons? In this paper, we investigate this question by proposing a\nPerturbation-based Causal Identification of Critical Neurons method to\nsystematically locate such critical neurons in LLMs. Our findings reveal three\nkey insights: (1) LLMs contain ultra-sparse critical neuron sets. Disrupting\nthese critical neurons can cause a 72B-parameter model with over 1.1 billion\nneurons to completely collapse, with perplexity increasing by up to 20 orders\nof magnitude; (2) These critical neurons are not uniformly distributed, but\ntend to concentrate in the outer layers, particularly within the MLP down\\_proj\ncomponents; (3) Performance degradation exhibits sharp phase transitions,\nrather than a gradual decline, when these critical neurons are disrupted.\nThrough comprehensive experiments across diverse model architectures and\nscales, we provide deeper analysis of these phenomena and their implications\nfor LLM robustness and interpretability. These findings can offer guidance for\ndeveloping more robust model architectures and improving deployment security in\nsafety-critical applications.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u4e2d\u5b58\u5728\u8d85\u7a00\u758f\u7684\u5173\u952e\u795e\u7ecf\u5143\u96c6\u5408\uff0c\u7834\u574f\u8fd9\u4e9b\u795e\u7ecf\u5143\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u8fd9\u4e9b\u795e\u7ecf\u5143\u4e3b\u8981\u5206\u5e03\u5728\u6a21\u578b\u5916\u5c42MLP\u7ec4\u4ef6\u4e2d\uff0c\u4e14\u6027\u80fd\u9000\u5316\u5448\u73b0\u6025\u5267\u7684\u76f8\u53d8\u7279\u5f81\u3002", "motivation": "\u53d7\u4eba\u7c7b\u5927\u8111\u4e2d\u5c11\u6570\u5173\u952e\u795e\u7ecf\u5143\u5bf9\u8ba4\u77e5\u529f\u80fd\u81f3\u5173\u91cd\u8981\u7684\u542f\u53d1\uff0c\u7814\u7a76\u63a2\u7d22LLMs\u662f\u5426\u4e5f\u5b58\u5728\u7c7b\u4f3c\u7684\u5173\u952e\u795e\u7ecf\u5143\u5b50\u96c6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6270\u52a8\u7684\u56e0\u679c\u5173\u952e\u795e\u7ecf\u5143\u8bc6\u522b\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5b9a\u4f4dLLMs\u4e2d\u7684\u5173\u952e\u795e\u7ecf\u5143\u3002", "result": "\u53d1\u73b0\uff1a(1) LLMs\u5305\u542b\u8d85\u7a00\u758f\u5173\u952e\u795e\u7ecf\u5143\uff0c\u7834\u574f\u5b83\u4eec\u53ef\u4f7f72B\u53c2\u6570\u6a21\u578b\u5b8c\u5168\u5d29\u6e83\uff1b(2) \u5173\u952e\u795e\u7ecf\u5143\u96c6\u4e2d\u5206\u5e03\u5728\u5916\u5c42MLP\u7ec4\u4ef6\uff1b(3) \u6027\u80fd\u9000\u5316\u5448\u73b0\u6025\u5267\u76f8\u53d8\u800c\u975e\u6e10\u8fdb\u4e0b\u964d\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u6a21\u578b\u67b6\u6784\u548c\u63d0\u9ad8\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2510.09891", "categories": ["cs.LG", "cs.AI", "physics.ao-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09891", "abs": "https://arxiv.org/abs/2510.09891", "authors": ["Parsa Gooya", "Reinel Sospedra-Alfonso"], "title": "Probabilistic bias adjustment of seasonal predictions of Arctic Sea Ice Concentration", "comment": null, "summary": "Seasonal forecast of Arctic sea ice concentration is key to mitigate the\nnegative impact and assess potential opportunities posed by the rapid decline\nof sea ice coverage. Seasonal prediction systems based on climate models often\nshow systematic biases and complex spatio-temporal errors that grow with the\nforecasts. Consequently, operational predictions are routinely bias corrected\nand calibrated using retrospective forecasts. For predictions of Arctic sea ice\nconcentration, error corrections are mainly based on one-to-one post-processing\nmethods including climatological mean or linear regression correction and, more\nrecently, machine learning. Such deterministic adjustments are confined at best\nto the limited number of costly-to-run ensemble members of the raw forecast.\nHowever, decision-making requires proper quantification of uncertainty and\nlikelihood of events, particularly of extremes. We introduce a probabilistic\nerror correction framework based on a conditional Variational Autoencoder model\nto map the conditional distribution of observations given the biased model\nprediction. This method naturally allows for generating large ensembles of\nadjusted forecasts. We evaluate our model using deterministic and probabilistic\nmetrics and show that the adjusted forecasts are better calibrated, closer to\nthe observational distribution, and have smaller errors than climatological\nmean adjusted forecasts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u6982\u7387\u8bef\u5dee\u6821\u6b63\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u5317\u6781\u6d77\u51b0\u6d53\u5ea6\u7684\u5b63\u8282\u6027\u9884\u6d4b\uff0c\u80fd\u591f\u751f\u6210\u5927\u91cf\u8c03\u6574\u540e\u7684\u9884\u6d4b\u96c6\u5408\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u5317\u6781\u6d77\u51b0\u6d53\u5ea6\u5b63\u8282\u6027\u9884\u6d4b\u7cfb\u7edf\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\u548c\u590d\u6742\u7684\u65f6\u7a7a\u8bef\u5dee\uff0c\u4f20\u7edf\u7684\u4e00\u5bf9\u4e00\u540e\u5904\u7406\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u51b3\u7b56\u5236\u5b9a\u9700\u8981\u9002\u5f53\u7684\u6982\u7387\u4fe1\u606f\u7279\u522b\u662f\u6781\u7aef\u4e8b\u4ef6\u7684\u6982\u7387\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u6a21\u578b\u6765\u6620\u5c04\u7ed9\u5b9a\u6709\u504f\u5dee\u6a21\u578b\u9884\u6d4b\u6761\u4ef6\u4e0b\u89c2\u6d4b\u503c\u7684\u6761\u4ef6\u5206\u5e03\uff0c\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u81ea\u7136\u5730\u751f\u6210\u5927\u91cf\u8c03\u6574\u540e\u7684\u9884\u6d4b\u96c6\u5408\u3002", "result": "\u8c03\u6574\u540e\u7684\u9884\u6d4b\u5728\u6821\u51c6\u6027\u3001\u4e0e\u89c2\u6d4b\u5206\u5e03\u7684\u63a5\u8fd1\u7a0b\u5ea6\u4ee5\u53ca\u8bef\u5dee\u65b9\u9762\u90fd\u4f18\u4e8e\u57fa\u4e8e\u6c14\u5019\u5b66\u5e73\u5747\u503c\u8c03\u6574\u7684\u9884\u6d4b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6982\u7387\u8bef\u5dee\u6821\u6b63\u6846\u67b6\u80fd\u591f\u6709\u6548\u6539\u8fdb\u5317\u6781\u6d77\u51b0\u6d53\u5ea6\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u4e3a\u51b3\u7b56\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u6982\u7387\u4fe1\u606f\u3002"}}
{"id": "2510.11316", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11316", "abs": "https://arxiv.org/abs/2510.11316", "authors": ["Kaj Munhoz Arfvidsson", "Loizos Hadjiloizou", "Frank J. Jiang", "Karl H. Johansson", "Jonas M\u00e5rtensson"], "title": "pyspect: An Extensible Toolbox for Automatic Construction of Temporal Logic Trees via Reachability Analysis", "comment": "To be published in the 64th IEEE Conference on Decision and Control", "summary": "In this paper, we present pyspect, a Python toolbox that simplifies the use\nof reachability analysis for temporal logic problems. Currently, satisfying\ncomplex requirements in cyber-physical systems requires significant manual\neffort and domain expertise to develop the underlying reachability programs.\nThis high development effort limits the broader adoption of reachability\nanalysis for complex verification problems. To address this, pyspect provides a\nmethod-agnostic approach to performing reachability analysis for verifying a\ntemporal logic specification via temporal logic trees (TLTs). It enables the\nspecification of complex safety and liveness requirements using high-level\nlogic formulations that are independent of any particular reachability\ntechnique or set representation. As a result, pyspect allows for the comparison\nof different reachability implementations, such as Hamilton-Jacobi and Hybrid\nZonotope-based reachability analysis, for the same temporal logic\nspecification. This design separates the concerns of implementation developers\n(who develop numerical procedures for reachability) and end-users (who write\nspecifications). Through a simple vehicle example, we demonstrate how pyspect\nsimplifies the synthesis of reachability programs, promotes specification\nreusability, and facilitates side-by-side comparisons of reachability\ntechniques for complex tasks.", "AI": {"tldr": "pyspect\u662f\u4e00\u4e2aPython\u5de5\u5177\u7bb1\uff0c\u901a\u8fc7\u65f6\u6001\u903b\u8f91\u6811\u7b80\u5316\u4e86\u65f6\u6001\u903b\u8f91\u95ee\u9898\u7684\u53ef\u8fbe\u6027\u5206\u6790\uff0c\u4f7f\u590d\u6742\u7684\u5b89\u5168\u6027\u548c\u6d3b\u6027\u9700\u6c42\u89c4\u8303\u72ec\u7acb\u4e8e\u5177\u4f53\u7684\u53ef\u8fbe\u6027\u6280\u672f\u3002", "motivation": "\u5f53\u524d\u5728\u4fe1\u606f\u7269\u7406\u7cfb\u7edf\u4e2d\u6ee1\u8db3\u590d\u6742\u9700\u6c42\u9700\u8981\u5927\u91cf\u624b\u52a8\u5de5\u4f5c\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u6765\u5f00\u53d1\u53ef\u8fbe\u6027\u7a0b\u5e8f\uff0c\u8fd9\u79cd\u9ad8\u5f00\u53d1\u6210\u672c\u9650\u5236\u4e86\u53ef\u8fbe\u6027\u5206\u6790\u5728\u590d\u6742\u9a8c\u8bc1\u95ee\u9898\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "pyspect\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b9\u6cd5\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u6001\u903b\u8f91\u6811\u6267\u884c\u53ef\u8fbe\u6027\u5206\u6790\u6765\u9a8c\u8bc1\u65f6\u6001\u903b\u8f91\u89c4\u8303\uff0c\u652f\u6301\u4f7f\u7528\u9ad8\u7ea7\u903b\u8f91\u516c\u5f0f\u6765\u6307\u5b9a\u590d\u6742\u9700\u6c42\uff0c\u72ec\u7acb\u4e8e\u4efb\u4f55\u7279\u5b9a\u7684\u53ef\u8fbe\u6027\u6280\u672f\u6216\u96c6\u5408\u8868\u793a\u3002", "result": "pyspect\u5141\u8bb8\u6bd4\u8f83\u4e0d\u540c\u53ef\u8fbe\u6027\u5b9e\u73b0\uff08\u5982Hamilton-Jacobi\u548c\u6df7\u5408Zonotope\u57fa\u53ef\u8fbe\u6027\u5206\u6790\uff09\u5bf9\u76f8\u540c\u65f6\u6001\u903b\u8f91\u89c4\u8303\u7684\u6548\u679c\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u8f66\u8f86\u793a\u4f8b\u5c55\u793a\u4e86\u5176\u7b80\u5316\u53ef\u8fbe\u6027\u7a0b\u5e8f\u5408\u6210\u3001\u4fc3\u8fdb\u89c4\u8303\u53ef\u91cd\u7528\u6027\u548c\u4fc3\u8fdb\u53ef\u8fbe\u6027\u6280\u672f\u6bd4\u8f83\u7684\u80fd\u529b\u3002", "conclusion": "pyspect\u7684\u8bbe\u8ba1\u5206\u79bb\u4e86\u5b9e\u73b0\u5f00\u53d1\u8005\uff08\u5f00\u53d1\u53ef\u8fbe\u6027\u6570\u503c\u7a0b\u5e8f\uff09\u548c\u6700\u7ec8\u7528\u6237\uff08\u7f16\u5199\u89c4\u8303\uff09\u7684\u5173\u6ce8\u70b9\uff0c\u7b80\u5316\u4e86\u590d\u6742\u4efb\u52a1\u7684\u89c4\u8303\u7f16\u5199\u548c\u53ef\u8fbe\u6027\u5206\u6790\u8fc7\u7a0b\u3002"}}
{"id": "2510.10474", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.10474", "abs": "https://arxiv.org/abs/2510.10474", "authors": ["Jingyi Wu", "Junying Liang"], "title": "When or What? Understanding Consumer Engagement on Digital Platforms", "comment": "21 pages, 6 figures, 3 tables", "summary": "Understanding what drives popularity is critical in today's digital service\neconomy, where content creators compete for consumer attention. Prior studies\nhave primarily emphasized the role of content features, yet creators often\nmisjudge what audiences actually value. This study applies Latent Dirichlet\nAllocation (LDA) modeling to a large corpus of TED Talks, treating the platform\nas a case of digital service provision in which creators (speakers) and\nconsumers (audiences) interact. By comparing the thematic supply of creators\nwith the demand expressed in audience engagement, we identify persistent\nmismatches between producer offerings and consumer preferences. Our\nlongitudinal analysis further reveals that temporal dynamics exert a stronger\ninfluence on consumer engagement than thematic content, suggesting that when\ncontent is delivered may matter more than what is delivered. These findings\nchallenge the dominant assumption that content features are the primary drivers\nof popularity and highlight the importance of timing and contextual factors in\nshaping consumer responses. The results provide new insights into consumer\nattention dynamics on digital platforms and carry practical implications for\nmarketers, platform managers, and content creators seeking to optimize audience\nengagement strategies.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790TED\u6f14\u8bb2\u6570\u636e\u53d1\u73b0\uff0c\u5185\u5bb9\u521b\u4f5c\u8005\u4e0e\u89c2\u4f17\u504f\u597d\u4e4b\u95f4\u5b58\u5728\u6301\u7eed\u9519\u914d\uff0c\u4e14\u65f6\u95f4\u52a8\u6001\u5bf9\u89c2\u4f17\u53c2\u4e0e\u5ea6\u7684\u5f71\u54cd\u6bd4\u4e3b\u9898\u5185\u5bb9\u66f4\u5f3a\uff0c\u6311\u6218\u4e86\u5185\u5bb9\u7279\u5f81\u662f\u53d7\u6b22\u8fce\u5ea6\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\u7684\u5047\u8bbe\u3002", "motivation": "\u5728\u6570\u5b57\u670d\u52a1\u7ecf\u6d4e\u4e2d\uff0c\u7406\u89e3\u4ec0\u4e48\u9a71\u52a8\u53d7\u6b22\u8fce\u5ea6\u81f3\u5173\u91cd\u8981\u3002\u4ee5\u5f80\u7814\u7a76\u4e3b\u8981\u5f3a\u8c03\u5185\u5bb9\u7279\u5f81\u7684\u4f5c\u7528\uff0c\u4f46\u521b\u4f5c\u8005\u7ecf\u5e38\u8bef\u5224\u89c2\u4f17\u771f\u6b63\u91cd\u89c6\u7684\u5185\u5bb9\u3002", "method": "\u5e94\u7528\u6f5c\u5728\u72c4\u5229\u514b\u96f7\u5206\u914d(LDA)\u6a21\u578b\u5206\u6790\u5927\u91cfTED\u6f14\u8bb2\u6570\u636e\uff0c\u5c06\u5e73\u53f0\u89c6\u4e3a\u521b\u4f5c\u8005(\u6f14\u8bb2\u8005)\u4e0e\u6d88\u8d39\u8005(\u89c2\u4f17)\u4e92\u52a8\u7684\u6570\u5b57\u670d\u52a1\u63d0\u4f9b\u6848\u4f8b\uff0c\u6bd4\u8f83\u521b\u4f5c\u8005\u7684\u4e3b\u9898\u4f9b\u7ed9\u4e0e\u89c2\u4f17\u53c2\u4e0e\u5ea6\u8868\u8fbe\u7684\u9700\u6c42\u3002", "result": "\u8bc6\u522b\u51fa\u521b\u4f5c\u8005\u4f9b\u7ed9\u4e0e\u6d88\u8d39\u8005\u504f\u597d\u4e4b\u95f4\u7684\u6301\u7eed\u9519\u914d\uff0c\u7eb5\u5411\u5206\u6790\u663e\u793a\u65f6\u95f4\u52a8\u6001\u5bf9\u6d88\u8d39\u8005\u53c2\u4e0e\u5ea6\u7684\u5f71\u54cd\u6bd4\u4e3b\u9898\u5185\u5bb9\u66f4\u5f3a\uff0c\u8868\u660e\u5185\u5bb9\u4ea4\u4ed8\u65f6\u673a\u53ef\u80fd\u6bd4\u5185\u5bb9\u672c\u8eab\u66f4\u91cd\u8981\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86\u5185\u5bb9\u7279\u5f81\u662f\u53d7\u6b22\u8fce\u5ea6\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\u7684\u5047\u8bbe\uff0c\u5f3a\u8c03\u4e86\u65f6\u673a\u548c\u60c5\u5883\u56e0\u7d20\u5728\u5851\u9020\u6d88\u8d39\u8005\u53cd\u5e94\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u6570\u5b57\u5e73\u53f0\u4e0a\u7684\u6d88\u8d39\u8005\u6ce8\u610f\u529b\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2510.11102", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.11102", "abs": "https://arxiv.org/abs/2510.11102", "authors": ["Michel de Lara"], "title": "Increasing Value of Information Implies Separable Utility", "comment": null, "summary": "We consider decision-making under incomplete information about an unknown\nstate of nature. Utility acts (that is, utility vectors indexed by states of\nnature) and beliefs (probability distributions over the states of nature) are\nnaturally paired by bilinear duality, giving the expected utility. With this\npairing, an expected utility maximizer (DM) is characterized by a continuous\nclosed convex comprehensive set of utility acts (c-utility act set). We show\nthat DM M values information more than DM L if and only if the c-utility act\nset of DM M is obtained by Minkowski addition from the cutility act set of DM\nL. In the classic setting of decision theory, this is interpreted as the\nequivalence between more valuable information, on the one hand, and multiplying\ndecisions and adding utility, on the other hand (additively separable utility).\nWe also introduce the algebraic structure of dioid to describe two operations\nbetween DMs: union (adding options) and fusion (multiplying options and adding\nutilities). We say that DM M is more exible by union (resp. by fusion) than DM\nL if DM M is obtained by union (resp. by fusion) from DM L. Our main result is\nthat DM M values information more than DM L if and only if DM M is more exible\nby fusion than DM L. We also study when exibility by union can lead to more\nvaluable information.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4fe1\u606f\u4e0d\u5b8c\u5168\u60c5\u51b5\u4e0b\u7684\u51b3\u7b56\u7406\u8bba\uff0c\u5efa\u7acb\u4e86\u4fe1\u606f\u4ef7\u503c\u4e0e\u51b3\u7b56\u8005\u7075\u6d3b\u6027\u4e4b\u95f4\u7684\u7b49\u4ef7\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u5728\u72b6\u6001\u4fe1\u606f\u4e0d\u5b8c\u5168\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u8861\u91cf\u4fe1\u606f\u7684\u4ef7\u503c\u4ee5\u53ca\u51b3\u7b56\u8005\u7075\u6d3b\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u53cc\u7ebf\u6027\u5bf9\u5076\u5c06\u6548\u7528\u884c\u4e3a\u4e0e\u4fe1\u5ff5\u914d\u5bf9\uff0c\u5f15\u5165\u95ed\u51f8\u7efc\u5408\u6548\u7528\u884c\u4e3a\u96c6\u6765\u8868\u5f81\u51b3\u7b56\u8005\uff0c\u5e76\u5e94\u7528Minkowski\u52a0\u6cd5\u548cdioid\u4ee3\u6570\u7ed3\u6784\u5206\u6790\u51b3\u7b56\u8005\u7684\u7075\u6d3b\u6027\u3002", "result": "\u8bc1\u660e\u51b3\u7b56\u8005M\u6bd4\u51b3\u7b56\u8005L\u66f4\u91cd\u89c6\u4fe1\u606f\u5f53\u4e14\u4ec5\u5f53M\u7684\u6548\u7528\u884c\u4e3a\u96c6\u53ef\u901a\u8fc7Minkowski\u52a0\u6cd5\u4eceL\u7684\u96c6\u5408\u5f97\u5230\uff0c\u8fd9\u7b49\u4ef7\u4e8eM\u5728\u878d\u5408\u64cd\u4f5c\u4e0a\u6bd4L\u66f4\u7075\u6d3b\u3002", "conclusion": "\u4fe1\u606f\u4ef7\u503c\u4e0e\u51b3\u7b56\u8005\u7684\u878d\u5408\u7075\u6d3b\u6027\u4e4b\u95f4\u5b58\u5728\u7b49\u4ef7\u5173\u7cfb\uff0c\u4e3a\u7406\u89e3\u4e0d\u5b8c\u5168\u4fe1\u606f\u4e0b\u7684\u51b3\u7b56\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2510.09913", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09913", "abs": "https://arxiv.org/abs/2510.09913", "authors": ["Shangbin Feng", "Wenhao Yu", "Yike Wang", "Hongming Zhang", "Yulia Tsvetkov", "Dong Yu"], "title": "Don't Throw Away Your Pretrained Model", "comment": null, "summary": "Alignment training has tradeoffs: it helps language models (LMs) gain in\nreasoning and instruction following but might lose out on skills such as\ncreativity and calibration, where unaligned base models are better at. We aim\nto make the best of both worlds through model collaboration, where different\nmodels in the training pipeline collaborate and complement each other. Since LM\nresponses feature interleaving skills that favor different models, we propose\nSwitch Generation, where pretrained and aligned model versions take turns to\n``speak'' in a response sequence. Specifically, we train a switcher LM by\nlearning from outcomes of choosing different models to generate the next\nsegment across diverse queries and contexts. At inference time, the switcher LM\nguides different model checkpoints to dynamically generate the next segment\nwhere their strengths are most needed. Extensive experiments with 8 model\ncollaboration baselines and 18 datasets show that 1) model collaboration\nconsistently outperforms individual models on 16 out of 18 tasks, and 2) Switch\nGeneration further outperforms baselines by 12.9% on average. Further analysis\nreveals that Switch Generation discovers compositional skills to solve problems\nwhere individual models struggle and generalizes to unseen models and tasks,\nreusing and repurposing by-products in expensive model training pipelines that\nare otherwise discarded.", "AI": {"tldr": "\u63d0\u51faSwitch Generation\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba9\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5bf9\u9f50\u6a21\u578b\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u4ea4\u66ff\"\u53d1\u8a00\"\uff0c\u5b9e\u73b0\u6a21\u578b\u534f\u4f5c\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u548c\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u7684\u540c\u65f6\uff0c\u4fdd\u7559\u521b\u9020\u6027\u548c\u6821\u51c6\u7b49\u6280\u80fd\u3002", "motivation": "\u5bf9\u9f50\u8bad\u7ec3\u5b58\u5728\u6743\u8861\uff1a\u867d\u7136\u80fd\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u548c\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u4f46\u53ef\u80fd\u635f\u5931\u521b\u9020\u6027\u548c\u6821\u51c6\u7b49\u6280\u80fd\u3002\u76ee\u6807\u662f\u8ba9\u8bad\u7ec3\u6d41\u6c34\u7ebf\u4e2d\u7684\u4e0d\u540c\u6a21\u578b\u534f\u4f5c\u4e92\u8865\uff0c\u5b9e\u73b0\u4e24\u5168\u5176\u7f8e\u3002", "method": "\u63d0\u51faSwitch Generation\u65b9\u6cd5\uff0c\u8bad\u7ec3\u4e00\u4e2a\u5207\u6362\u5668\u8bed\u8a00\u6a21\u578b\uff0c\u5b66\u4e60\u5728\u4e0d\u540c\u67e5\u8be2\u548c\u4e0a\u4e0b\u6587\u4e2d\u9009\u62e9\u4e0d\u540c\u6a21\u578b\u751f\u6210\u4e0b\u4e00\u4e2a\u7247\u6bb5\u3002\u63a8\u7406\u65f6\uff0c\u5207\u6362\u5668\u6307\u5bfc\u4e0d\u540c\u6a21\u578b\u68c0\u67e5\u70b9\u52a8\u6001\u751f\u6210\u6700\u9700\u8981\u7684\u7247\u6bb5\u3002", "result": "\u57288\u4e2a\u6a21\u578b\u534f\u4f5c\u57fa\u7ebf\u548c18\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1a1\uff09\u6a21\u578b\u534f\u4f5c\u572816/18\u4efb\u52a1\u4e0a\u6301\u7eed\u4f18\u4e8e\u5355\u4e2a\u6a21\u578b\uff1b2\uff09Switch Generation\u6bd4\u57fa\u7ebf\u5e73\u5747\u63d0\u534712.9%\u3002", "conclusion": "Switch Generation\u80fd\u591f\u53d1\u73b0\u7ec4\u5408\u6280\u80fd\u6765\u89e3\u51b3\u5355\u4e2a\u6a21\u578b\u96be\u4ee5\u5904\u7406\u7684\u95ee\u9898\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u6a21\u578b\u548c\u4efb\u52a1\uff0c\u6709\u6548\u91cd\u7528\u548c\u91cd\u65b0\u5229\u7528\u6602\u8d35\u6a21\u578b\u8bad\u7ec3\u6d41\u6c34\u7ebf\u4e2d\u7684\u526f\u4ea7\u54c1\u3002"}}
{"id": "2510.10285", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10285", "abs": "https://arxiv.org/abs/2510.10285", "authors": ["Haolang Lu", "Bolun Chu", "WeiYe Fu", "Guoshun Nan", "Junning Liu", "Minghui Pan", "Qiankun Li", "Yi Yu", "Hua Wang", "Kun Wang"], "title": "Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control", "comment": "preprint", "summary": "Multimodal large reasoning models (MLRMs) are rapidly advancing\nvision-language reasoning and are emerging as a foundation for cross-modal\nintelligence. Hallucination remains a persistent failure mode, manifesting\nitself as erroneous reasoning chains and misinterpretation of visual content.\nIn this study, we observe that attention heads exhibit a staged division:\nshallow heads predominantly serve perception, while deeper heads shift toward\nsymbolic reasoning, revealing two major causes of hallucination, namely\nperceptual bias and reasoning drift. To address these issues, we propose a\nlightweight and interpretable two-step plugin, Functional Head Identification\nand Class-conditioned Rescaling, which locates perception- and\nreasoning-oriented heads and regulates their contributions without retraining.\nEvaluations on three real-world MLRMs (Kimi-VL, Ocean-R1, R1-Onevision), six\nbenchmarks across three domains, and four baselines show that our plugin\nachieves an average improvement of 5% and up to 15%, with only <1% additional\ncomputation and 9% of baseline latency. Our approach is completely\nmodel-agnostic and significantly enhances both the reliability and\ninterpretability of the off-the-shelf MLRMs, thereby enabling their safe\ndeployment in high-stakes applications. Our code is available at\nhttps://anonymous.4open.science/r/Functional-Attention-Control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u63d2\u4ef6\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u8c03\u8282\u611f\u77e5\u4e0e\u63a8\u7406\u5bfc\u5411\u7684\u6ce8\u610f\u529b\u5934\u6765\u51cf\u5c11\u591a\u6a21\u6001\u5927\u63a8\u7406\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5e73\u5747\u63d0\u53475%\uff0c\u6700\u9ad8\u63d0\u534715%\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u63a8\u7406\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u8868\u73b0\u4e3a\u9519\u8bef\u7684\u63a8\u7406\u94fe\u548c\u89c6\u89c9\u5185\u5bb9\u8bef\u89e3\u3002\u7814\u7a76\u53d1\u73b0\u6ce8\u610f\u529b\u5934\u5b58\u5728\u9636\u6bb5\u6027\u5206\u5de5\uff1a\u6d45\u5c42\u8d1f\u8d23\u611f\u77e5\uff0c\u6df1\u5c42\u8f6c\u5411\u7b26\u53f7\u63a8\u7406\uff0c\u63ed\u793a\u4e86\u611f\u77e5\u504f\u5dee\u548c\u63a8\u7406\u6f02\u79fb\u662f\u5e7b\u89c9\u7684\u4e24\u5927\u539f\u56e0\u3002", "method": "\u63d0\u51fa\u4e24\u6b65\u63d2\u4ef6\u65b9\u6cd5\uff1a\u529f\u80fd\u5934\u8bc6\u522b\u548c\u7c7b\u6761\u4ef6\u91cd\u7f29\u653e\u3002\u9996\u5148\u5b9a\u4f4d\u611f\u77e5\u548c\u63a8\u7406\u5bfc\u5411\u7684\u6ce8\u610f\u529b\u5934\uff0c\u7136\u540e\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8c03\u8282\u5b83\u4eec\u7684\u8d21\u732e\u5ea6\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754cMLRM\u6a21\u578b\u3001\u516d\u4e2a\u8de8\u4e09\u4e2a\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u56db\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u4e0a\uff0c\u8be5\u63d2\u4ef6\u5e73\u5747\u63d0\u53475%\uff0c\u6700\u9ad8\u63d0\u534715%\uff0c\u4ec5\u589e\u52a0<1%\u7684\u8ba1\u7b97\u5f00\u9500\u548c9%\u7684\u57fa\u7ebf\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b8c\u5168\u6a21\u578b\u65e0\u5173\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6210MLRM\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u5b89\u5168\u90e8\u7f72\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u3002"}}
{"id": "2510.09895", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09895", "abs": "https://arxiv.org/abs/2510.09895", "authors": ["Yubo Li", "Rema Padman"], "title": "Chain-of-Influence: Tracing Interdependencies Across Time and Features in Clinical Predictive Modelings", "comment": null, "summary": "Modeling clinical time-series data is hampered by the challenge of capturing\nlatent, time-varying dependencies among features. State-of-the-art approaches\noften rely on black-box mechanisms or simple aggregation, failing to explicitly\nmodel how the influence of one clinical variable propagates through others over\ntime. We propose $\\textbf{Chain-of-Influence (CoI)}$, an interpretable deep\nlearning framework that constructs an explicit, time-unfolded graph of feature\ninteractions. CoI leverages a multi-level attention architecture: first, a\ntemporal attention layer identifies critical time points in a patient's record;\nsecond, a cross-feature attention layer models the directed influence from\nfeatures at these time points to subsequent features. This design enables the\ntracing of influence pathways, providing a granular audit trail that shows how\nany feature at any time contributes to the final prediction, both directly and\nthrough its influence on other variables. We evaluate CoI on mortality and\ndisease progression tasks using the MIMIC-IV dataset and a private chronic\nkidney disease cohort. Our framework significantly outperforms existing methods\nin predictive accuracy. More importantly, through case studies, we show that\nCoI can uncover clinically meaningful, patient-specific patterns of disease\nprogression that are opaque to other models, offering unprecedented\ntransparency into the temporal and cross-feature dependencies that inform\nclinical decision-making.", "AI": {"tldr": "\u63d0\u51fa\u4e86Chain-of-Influence (CoI)\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u663e\u5f0f\u7684\u65f6\u95f4\u5c55\u5f00\u7279\u5f81\u4ea4\u4e92\u56fe\u6765\u5efa\u6a21\u4e34\u5e8a\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u4f9d\u8d56\u5173\u7cfb\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u663e\u5f0f\u5efa\u6a21\u4e34\u5e8a\u53d8\u91cf\u4e4b\u95f4\u968f\u65f6\u95f4\u53d8\u5316\u7684\u4f9d\u8d56\u5173\u7cfb\u4f20\u64ad\uff0c\u9ed1\u76d2\u673a\u5236\u6216\u7b80\u5355\u805a\u5408\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528\u591a\u5c42\u6ce8\u610f\u529b\u67b6\u6784\uff1a\u65f6\u95f4\u6ce8\u610f\u529b\u5c42\u8bc6\u522b\u5173\u952e\u65f6\u95f4\u70b9\uff0c\u8de8\u7279\u5f81\u6ce8\u610f\u529b\u5c42\u5efa\u6a21\u8fd9\u4e9b\u65f6\u95f4\u70b9\u7279\u5f81\u5bf9\u540e\u7eed\u7279\u5f81\u7684\u5b9a\u5411\u5f71\u54cd\uff0c\u6784\u5efa\u5f71\u54cd\u8def\u5f84\u56fe\u3002", "result": "\u5728MIMIC-IV\u6570\u636e\u96c6\u548c\u6162\u6027\u80be\u75c5\u961f\u5217\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cCoI\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u63ed\u793a\u4e34\u5e8a\u4e0a\u91cd\u8981\u7684\u60a3\u8005\u7279\u5f02\u6027\u75be\u75c5\u8fdb\u5c55\u6a21\u5f0f\u3002", "conclusion": "CoI\u6846\u67b6\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u900f\u660e\u5ea6\uff0c\u80fd\u591f\u8ffd\u8e2a\u7279\u5f81\u5f71\u54cd\u8def\u5f84\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u65f6\u5e8f\u548c\u8de8\u7279\u5f81\u4f9d\u8d56\u5173\u7cfb\u6d1e\u5bdf\u3002"}}
{"id": "2510.11331", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11331", "abs": "https://arxiv.org/abs/2510.11331", "authors": ["Bingjie Zhu", "Zhixiong Chen", "Liqiang Zhao", "Hyundong Shin", "Arumugam Nallanathan"], "title": "Efficient LLM Inference over Heterogeneous Edge Networks with Speculative Decoding", "comment": null, "summary": "Large language model (LLM) inference at the network edge is a promising\nserving paradigm that leverages distributed edge resources to run inference\nnear users and enhance privacy. Existing edge-based LLM inference systems\ntypically adopt autoregressive decoding (AD), which only generates one token\nper forward pass. This iterative process, compounded by the limited\ncomputational resources of edge nodes, results in high serving latency and\nconstrains the system's ability to support multiple users under growing\ndemands.To address these challenges, we propose a speculative decoding\n(SD)-based LLM serving framework that deploys small and large models across\nheterogeneous edge nodes to collaboratively deliver inference services.\nSpecifically, the small model rapidly generates draft tokens that the large\nmodel verifies in parallel, enabling multi-token generation per forward pass\nand thus reducing serving latency. To improve resource utilization of edge\nnodes, we incorporate pipeline parallelism to overlap drafting and verification\nacross multiple inference tasks. Based on this framework, we analyze and derive\na comprehensive latency model incorporating both communication and inference\nlatency. Then, we formulate a joint optimization problem for speculation\nlength, task batching, and wireless communication resource allocation to\nminimize total serving latency. To address this problem, we derive the\nclosed-form solutions for wireless communication resource allocation, and\ndevelop a dynamic programming algorithm for joint batching and speculation\ncontrol strategies. Experimental results demonstrate that the proposed\nframework achieves lower serving latency compared to AD-based serving systems.\nIn addition,the proposed joint optimization method delivers up to 44.9% latency\nreduction compared to benchmark schemes.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u63a8\u6d4b\u89e3\u7801\u7684\u8fb9\u7f18LLM\u670d\u52a1\u6846\u67b6\uff0c\u901a\u8fc7\u5c0f\u6a21\u578b\u751f\u6210\u8349\u7a3f\u4ee4\u724c\u3001\u5927\u6a21\u578b\u5e76\u884c\u9a8c\u8bc1\uff0c\u7ed3\u5408\u6d41\u6c34\u7ebf\u5e76\u884c\u548c\u8054\u5408\u4f18\u5316\uff0c\u663e\u8457\u964d\u4f4e\u670d\u52a1\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8fb9\u7f18\u7684LLM\u63a8\u7406\u7cfb\u7edf\u91c7\u7528\u81ea\u56de\u5f52\u89e3\u7801\uff0c\u6bcf\u6b21\u524d\u5411\u4f20\u9012\u53ea\u751f\u6210\u4e00\u4e2a\u4ee4\u724c\uff0c\u52a0\u4e0a\u8fb9\u7f18\u8282\u70b9\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\uff0c\u5bfc\u81f4\u9ad8\u670d\u52a1\u5ef6\u8fdf\uff0c\u96be\u4ee5\u652f\u6301\u591a\u7528\u6237\u9700\u6c42\u3002", "method": "\u91c7\u7528\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u5728\u5f02\u6784\u8fb9\u7f18\u8282\u70b9\u90e8\u7f72\u5927\u5c0f\u6a21\u578b\u534f\u4f5c\u63a8\u7406\uff1b\u7ed3\u5408\u6d41\u6c34\u7ebf\u5e76\u884c\u91cd\u53e0\u8349\u7a3f\u548c\u9a8c\u8bc1\u8fc7\u7a0b\uff1b\u5efa\u7acb\u5305\u542b\u901a\u4fe1\u548c\u63a8\u7406\u5ef6\u8fdf\u7684\u7efc\u5408\u5ef6\u8fdf\u6a21\u578b\uff1b\u8054\u5408\u4f18\u5316\u63a8\u6d4b\u957f\u5ea6\u3001\u4efb\u52a1\u6279\u5904\u7406\u548c\u65e0\u7ebf\u8d44\u6e90\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u57fa\u4e8e\u81ea\u56de\u5f52\u89e3\u7801\u7684\u670d\u52a1\u7cfb\u7edf\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u670d\u52a1\u5ef6\u8fdf\uff1b\u63d0\u51fa\u7684\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u76f8\u6bd4\u57fa\u51c6\u65b9\u6848\u53ef\u964d\u4f4e44.9%\u7684\u5ef6\u8fdf\u3002", "conclusion": "\u57fa\u4e8e\u63a8\u6d4b\u89e3\u7801\u7684\u8fb9\u7f18LLM\u670d\u52a1\u6846\u67b6\u80fd\u6709\u6548\u964d\u4f4e\u670d\u52a1\u5ef6\u8fdf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7b56\u7565\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3a\u8fb9\u7f18LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10998", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10998", "abs": "https://arxiv.org/abs/2510.10998", "authors": ["Mahika Phutane", "Hayoung Jung", "Matthew Kim", "Tanushree Mitra", "Aditya Vashistha"], "title": "ABLEIST: Intersectional Disability Bias in LLM-Generated Hiring Scenarios", "comment": "28 pages, 11 figures, 16 tables. In submission", "summary": "Large language models (LLMs) are increasingly under scrutiny for perpetuating\nidentity-based discrimination in high-stakes domains such as hiring,\nparticularly against people with disabilities (PwD). However, existing research\nremains largely Western-centric, overlooking how intersecting forms of\nmarginalization--such as gender and caste--shape experiences of PwD in the\nGlobal South. We conduct a comprehensive audit of six LLMs across 2,820 hiring\nscenarios spanning diverse disability, gender, nationality, and caste profiles.\nTo capture subtle intersectional harms and biases, we introduce ABLEIST\n(Ableism, Inspiration, Superhumanization, and Tokenism), a set of five\nableism-specific and three intersectional harm metrics grounded in disability\nstudies literature. Our results reveal significant increases in ABLEIST harms\ntowards disabled candidates--harms that many state-of-the-art models failed to\ndetect. These harms were further amplified by sharp increases in intersectional\nharms (e.g., Tokenism) for gender and caste-marginalized disabled candidates,\nhighlighting critical blind spots in current safety tools and the need for\nintersectional safety evaluations of frontier models in high-stakes domains\nlike hiring.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5ba1\u8ba1\u4e866\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u57282820\u4e2a\u62db\u8058\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u5bf9\u6b8b\u969c\u4eba\u58eb\u5b58\u5728\u663e\u8457\u7684\u6b67\u89c6\u6027\u504f\u89c1\uff0c\u7279\u522b\u662f\u5f53\u6b8b\u969c\u4e0e\u6027\u522b\u3001\u79cd\u59d3\u7b49\u8eab\u4efd\u4ea4\u53c9\u65f6\uff0c\u504f\u89c1\u66f4\u52a0\u4e25\u91cd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u897f\u65b9\u80cc\u666f\uff0c\u5ffd\u89c6\u4e86\u5168\u7403\u5357\u65b9\u5730\u533a\u6b8b\u969c\u4eba\u58eb\u9762\u4e34\u7684\u4ea4\u53c9\u6027\u8fb9\u7f18\u5316\u95ee\u9898\uff08\u5982\u6027\u522b\u548c\u79cd\u59d3\uff09\uff0c\u9700\u8981\u8bc4\u4f30LLMs\u5728\u62db\u8058\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u5bf9\u6b8b\u969c\u4eba\u58eb\u7684\u6b67\u89c6\u95ee\u9898\u3002", "method": "\u5f15\u5165ABLEIST\u6846\u67b6\uff08\u5305\u542b5\u4e2a\u6b8b\u969c\u6b67\u89c6\u7279\u5b9a\u6307\u6807\u548c3\u4e2a\u4ea4\u53c9\u6027\u4f24\u5bb3\u6307\u6807\uff09\uff0c\u5bf96\u4e2aLLMs\u57282820\u4e2a\u62db\u8058\u573a\u666f\u4e2d\u8fdb\u884c\u5168\u9762\u5ba1\u8ba1\uff0c\u6db5\u76d6\u4e0d\u540c\u6b8b\u969c\u3001\u6027\u522b\u3001\u56fd\u7c4d\u548c\u79cd\u59d3\u80cc\u666f\u3002", "result": "\u53d1\u73b0\u5bf9\u6b8b\u969c\u5019\u9009\u4eba\u7684ABLEIST\u4f24\u5bb3\u663e\u8457\u589e\u52a0\uff0c\u8bb8\u591a\u6700\u5148\u8fdb\u6a21\u578b\u672a\u80fd\u68c0\u6d4b\u5230\u8fd9\u4e9b\u4f24\u5bb3\u3002\u6027\u522b\u548c\u79cd\u59d3\u8fb9\u7f18\u5316\u7684\u6b8b\u969c\u5019\u9009\u4eba\u7684\u4ea4\u53c9\u6027\u4f24\u5bb3\uff08\u5982\u8c61\u5f81\u4e3b\u4e49\uff09\u6025\u5267\u589e\u52a0\u3002", "conclusion": "\u5f53\u524d\u5b89\u5168\u5de5\u5177\u5b58\u5728\u5173\u952e\u76f2\u70b9\uff0c\u9700\u8981\u5728\u62db\u8058\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u5bf9\u524d\u6cbf\u6a21\u578b\u8fdb\u884c\u4ea4\u53c9\u6027\u5b89\u5168\u8bc4\u4f30\u3002"}}
{"id": "2510.11158", "categories": ["math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2510.11158", "abs": "https://arxiv.org/abs/2510.11158", "authors": ["Alessandro Calvia", "Federico Cannerozzi", "Giorgio Ferrari"], "title": "Optimal Policy Characterization for a Class of Multi-Dimensional Ergodic Singular Stochastic Control Problems", "comment": null, "summary": "In ergodic singular stochastic control problems, a decision-maker can\ninstantaneously adjust the evolution of a state variable using a control of\nbounded variation, with the goal of minimizing a long-term average cost\nfunctional. The cost of control is proportional to the magnitude of\nadjustments. This paper characterizes the optimal policy and the value in a\nclass of multi-dimensional ergodic singular stochastic control problems. These\nproblems involve a linearly controlled one-dimensional stochastic differential\nequation, whose coefficients, along with the cost functional to be optimized,\ndepend on a multi-dimensional uncontrolled process Y. We first provide general\nverification theorems providing an optimal control in terms of a Skorokhod\nreflection at Y-dependent free boundaries, which emerge from the analysis of an\nauxiliary Dynkin game. We then fully solve two two-dimensional optimal\ninventory management problems. To the best of our knowledge, this is the first\npaper to establish a connection between multi-dimensional ergodic singular\nstochastic control and optimal stopping, and to exploit this connection to\nachieve a complete solution in a genuinely two-dimensional setting.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u7ef4\u904d\u5386\u5947\u5f02\u968f\u673a\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u6790\u8f85\u52a9Dynkin\u535a\u5f08\uff0c\u5efa\u7acb\u4e86\u6700\u4f18\u63a7\u5236\u4e0eSkorokhod\u53cd\u5c04\u5728Y\u76f8\u5173\u81ea\u7531\u8fb9\u754c\u4e0a\u7684\u8054\u7cfb\uff0c\u5e76\u5b8c\u5168\u89e3\u51b3\u4e86\u4e24\u4e2a\u4e8c\u7ef4\u6700\u4f18\u5e93\u5b58\u7ba1\u7406\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u591a\u7ef4\u904d\u5386\u5947\u5f02\u968f\u673a\u63a7\u5236\u95ee\u9898\uff0c\u5176\u4e2d\u51b3\u7b56\u8005\u53ef\u4ee5\u901a\u8fc7\u6709\u754c\u53d8\u5dee\u63a7\u5236\u77ac\u65f6\u8c03\u6574\u72b6\u6001\u53d8\u91cf\uff0c\u76ee\u6807\u662f\u6700\u5c0f\u5316\u957f\u671f\u5e73\u5747\u6210\u672c\u51fd\u6570\u3002\u63a7\u5236\u6210\u672c\u4e0e\u8c03\u6574\u5e45\u5ea6\u6210\u6b63\u6bd4\u3002", "method": "\u9996\u5148\u63d0\u4f9b\u4e00\u822c\u9a8c\u8bc1\u5b9a\u7406\uff0c\u901a\u8fc7\u5206\u6790\u8f85\u52a9Dynkin\u535a\u5f08\uff0c\u5f97\u5230\u57fa\u4e8eY\u76f8\u5173\u81ea\u7531\u8fb9\u754c\u7684Skorokhod\u53cd\u5c04\u7684\u6700\u4f18\u63a7\u5236\u3002\u7136\u540e\u5b8c\u5168\u89e3\u51b3\u4e24\u4e2a\u4e8c\u7ef4\u6700\u4f18\u5e93\u5b58\u7ba1\u7406\u95ee\u9898\u3002", "result": "\u5efa\u7acb\u4e86\u591a\u7ef4\u904d\u5386\u5947\u5f02\u968f\u673a\u63a7\u5236\u4e0e\u6700\u4f18\u505c\u6b62\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5e76\u5728\u771f\u6b63\u7684\u4e8c\u7ef4\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u5b8c\u5168\u89e3\u3002\u8fd9\u662f\u9996\u6b21\u5b9e\u73b0\u8fd9\u79cd\u8054\u7cfb\u5e76\u8fbe\u6210\u5b8c\u6574\u89e3\u7684\u8bba\u6587\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5c06\u591a\u7ef4\u904d\u5386\u5947\u5f02\u968f\u673a\u63a7\u5236\u4e0e\u6700\u4f18\u505c\u6b62\u7406\u8bba\u8054\u7cfb\u8d77\u6765\uff0c\u901a\u8fc7\u8f85\u52a9Dynkin\u535a\u5f08\u5206\u6790\uff0c\u63d0\u4f9b\u4e86\u57fa\u4e8e\u81ea\u7531\u8fb9\u754c\u7684\u6700\u4f18\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u5728\u4e8c\u7ef4\u5e93\u5b58\u7ba1\u7406\u95ee\u9898\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.09915", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09915", "abs": "https://arxiv.org/abs/2510.09915", "authors": ["Sicong Huang", "Qianqi Yan", "Shengze Wang", "Ian Lane"], "title": "Enhancing Faithfulness in Abstractive Summarization via Span-Level Fine-Tuning", "comment": null, "summary": "Abstractive summarization using large language models (LLMs) has become an\nessential tool for condensing information. However, despite their ability to\ngenerate fluent summaries, these models sometimes produce unfaithful summaries,\nintroducing hallucinations at the word, phrase, or concept level. Existing\nmitigation strategies, such as post-processing corrections or contrastive\nlearning with synthetically generated negative samples, fail to fully address\nthe diverse errors that can occur in LLM-generated summaries. In this paper, we\ninvestigate fine-tuning strategies to reduce the occurrence of unfaithful spans\nin generated summaries. First, we automatically generate summaries for the set\nof source documents in the training set with a variety of LLMs and then use\nGPT-4o to annotate any hallucinations it detects at the span-level. Leveraging\nthese annotations, we fine-tune LLMs with both hallucination-free summaries and\nannotated unfaithful spans to enhance model faithfulness. In this paper, we\nintroduce a new dataset that contains both faithful and unfaithful summaries\nwith span-level labels and we evaluate three techniques to fine-tuning a LLM to\nimprove the faithfulness of the resulting summarization: gradient ascent,\nunlikelihood training, and task vector negation. Experimental results show that\nall three approaches successfully leverage span-level annotations to improve\nfaithfulness, with unlikelihood training being the most effective.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u901a\u8fc7\u5fae\u8c03\u7b56\u7565\u51cf\u5c11LLM\u751f\u6210\u6458\u8981\u4e2d\u7684\u4e0d\u5fe0\u5b9e\u5185\u5bb9\uff0c\u63d0\u51fa\u4e86\u5305\u542b\u5fe0\u5b9e\u548c\u4e0d\u5fe0\u5b9e\u6458\u8981\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u5fae\u8c03\u65b9\u6cd5\uff0c\u5176\u4e2dunlikelihood\u8bad\u7ec3\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u751f\u6210\u6d41\u7545\u6458\u8981\uff0c\u4f46\u7ecf\u5e38\u4ea7\u751f\u5305\u542b\u5e7b\u89c9\u7684\u4e0d\u5fe0\u5b9e\u5185\u5bb9\uff0c\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u65e0\u6cd5\u5b8c\u5168\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u591a\u79cdLLM\u81ea\u52a8\u751f\u6210\u8bad\u7ec3\u96c6\u6458\u8981\uff0c\u7528GPT-4o\u8fdb\u884cspan\u7ea7\u5e7b\u89c9\u6807\u6ce8\uff0c\u7136\u540e\u901a\u8fc7\u68af\u5ea6\u4e0a\u5347\u3001unlikelihood\u8bad\u7ec3\u548c\u4efb\u52a1\u5411\u91cf\u5426\u5b9a\u4e09\u79cd\u65b9\u6cd5\u5fae\u8c03LLM\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6240\u6709\u4e09\u79cd\u65b9\u6cd5\u90fd\u80fd\u6709\u6548\u5229\u7528span\u7ea7\u6807\u6ce8\u63d0\u9ad8\u5fe0\u5b9e\u6027\uff0c\u5176\u4e2dunlikelihood\u8bad\u7ec3\u6548\u679c\u6700\u597d\u3002", "conclusion": "span\u7ea7\u6807\u6ce8\u80fd\u6709\u6548\u63d0\u5347LLM\u6458\u8981\u751f\u6210\u7684\u5fe0\u5b9e\u6027\uff0cunlikelihood\u8bad\u7ec3\u662f\u6700\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.09712", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09712", "abs": "https://arxiv.org/abs/2510.09712", "authors": ["Zhao Tong", "Chunlin Gong", "Yimeng Gu", "Haichao Shi", "Qiang Liu", "Shu Wu", "Xiao-Yu Zhang"], "title": "Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments", "comment": "10 pages, 12 figures", "summary": "The spread of fake news online distorts public judgment and erodes trust in\nsocial media platforms. Although recent fake news detection (FND) models\nperform well in standard settings, they remain vulnerable to adversarial\ncomments-authored by real users or by large language models (LLMs)-that subtly\nshift model decisions. In view of this, we first present a comprehensive\nevaluation of comment attacks to existing fake news detectors and then\nintroduce a group-adaptive adversarial training strategy to improve the\nrobustness of FND models. To be specific, our approach comprises three steps:\n(1) dividing adversarial comments into three psychologically grounded\ncategories: perceptual, cognitive, and societal; (2) generating diverse,\ncategory-specific attacks via LLMs to enhance adversarial training; and (3)\napplying a Dirichlet-based adaptive sampling mechanism (InfoDirichlet Adjusting\nMechanism) that dynamically adjusts the learning focus across different comment\ncategories during training. Experiments on benchmark datasets show that our\nmethod maintains strong detection accuracy while substantially increasing\nrobustness to a wide range of adversarial comment perturbations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7fa4\u4f53\u81ea\u9002\u5e94\u5bf9\u6297\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u5bf9\u6297\u6027\u8bc4\u8bba\u5206\u4e3a\u611f\u77e5\u3001\u8ba4\u77e5\u548c\u793e\u4f1a\u4e09\u4e2a\u5fc3\u7406\u5b66\u7c7b\u522b\uff0c\u5229\u7528LLM\u751f\u6210\u591a\u6837\u5316\u7684\u7c7b\u522b\u7279\u5b9a\u653b\u51fb\uff0c\u5e76\u5e94\u7528\u57fa\u4e8eDirichlet\u7684\u81ea\u9002\u5e94\u91c7\u6837\u673a\u5236\u6765\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u91cd\u70b9\uff0c\u4ece\u800c\u63d0\u5347\u5047\u65b0\u95fb\u68c0\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5047\u65b0\u95fb\u68c0\u6d4b\u6a21\u578b\u5728\u6807\u51c6\u8bbe\u7f6e\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u7531\u771f\u5b9e\u7528\u6237\u6216\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5bf9\u6297\u6027\u8bc4\u8bba\u653b\u51fb\u4ecd\u7136\u8106\u5f31\uff0c\u8fd9\u4e9b\u653b\u51fb\u4f1a\u5fae\u5999\u5730\u6539\u53d8\u6a21\u578b\u51b3\u7b56\uff0c\u5f71\u54cd\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u7684\u4fe1\u4efb\u5ea6\u3002", "method": "1) \u5c06\u5bf9\u6297\u6027\u8bc4\u8bba\u5206\u4e3a\u4e09\u4e2a\u5fc3\u7406\u5b66\u57fa\u7840\u7c7b\u522b\uff1a\u611f\u77e5\u3001\u8ba4\u77e5\u548c\u793e\u4f1a\uff1b2) \u901a\u8fc7LLM\u751f\u6210\u591a\u6837\u5316\u7684\u7c7b\u522b\u7279\u5b9a\u653b\u51fb\u4ee5\u589e\u5f3a\u5bf9\u6297\u8bad\u7ec3\uff1b3) \u5e94\u7528\u57fa\u4e8eDirichlet\u7684\u81ea\u9002\u5e94\u91c7\u6837\u673a\u5236(InfoDirichlet Adjusting Mechanism)\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u8c03\u6574\u4e0d\u540c\u8bc4\u8bba\u7c7b\u522b\u7684\u5b66\u4e60\u91cd\u70b9\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5f3a\u68c0\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u5404\u79cd\u5bf9\u6297\u6027\u8bc4\u8bba\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7fa4\u4f53\u81ea\u9002\u5e94\u5bf9\u6297\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u5047\u65b0\u95fb\u68c0\u6d4b\u6a21\u578b\u5bf9\u5bf9\u6297\u6027\u8bc4\u8bba\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2510.10331", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10331", "abs": "https://arxiv.org/abs/2510.10331", "authors": ["Hanchen Su", "Wei Luo", "Wei Han", "Yu Elaine Liu", "Yufeng Wayne Zhang", "Cen Mia Zhao", "Ying Joy Zhang", "Yashar Mehdad"], "title": "LLM-Friendly Knowledge Representation for Customer Support", "comment": null, "summary": "We propose a practical approach by integrating Large Language Models (LLMs)\nwith a framework designed to navigate the complexities of Airbnb customer\nsupport operations. In this paper, our methodology employs a novel reformatting\ntechnique, the Intent, Context, and Action (ICA) format, which transforms\npolicies and workflows into a structure more comprehensible to LLMs.\nAdditionally, we develop a synthetic data generation strategy to create\ntraining data with minimal human intervention, enabling cost-effective\nfine-tuning of our model. Our internal experiments (not applied to Airbnb\nproducts) demonstrate that our approach of restructuring workflows and\nfine-tuning LLMs with synthetic data significantly enhances their performance,\nsetting a new benchmark for their application in customer support. Our solution\nis not only cost-effective but also improves customer support, as evidenced by\nboth accuracy and manual processing time evaluation metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0eAirbnb\u5ba2\u670d\u8fd0\u8425\u6846\u67b6\u7ed3\u5408\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7ICA\u683c\u5f0f\u91cd\u6784\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u4f7f\u7528\u5408\u6210\u6570\u636e\u5fae\u8c03\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5ba2\u670d\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3Airbnb\u5ba2\u670d\u8fd0\u8425\u7684\u590d\u6742\u6027\uff0c\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5ba2\u670d\u652f\u6301\u4e2d\u7684\u7406\u89e3\u548c\u5e94\u7528\u80fd\u529b\u3002", "method": "\u91c7\u7528\u610f\u56fe\u3001\u4e0a\u4e0b\u6587\u548c\u884c\u52a8(ICA)\u683c\u5f0f\u91cd\u6784\u653f\u7b56\u548c\u6d41\u7a0b\uff0c\u5f00\u53d1\u5408\u6210\u6570\u636e\u751f\u6210\u7b56\u7565\u8fdb\u884c\u4f4e\u6210\u672c\u6a21\u578b\u5fae\u8c03\u3002", "result": "\u5185\u90e8\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5728\u51c6\u786e\u6027\u548c\u5904\u7406\u65f6\u95f4\u8bc4\u4f30\u6307\u6807\u4e0a\u90fd\u6709\u6539\u5584\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6210\u672c\u6548\u76ca\u9ad8\uff0c\u8fd8\u4e3a\u5ba2\u670d\u652f\u6301\u4e2d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2510.09923", "categories": ["cs.LG", "math.OC", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09923", "abs": "https://arxiv.org/abs/2510.09923", "authors": ["Nikola Surjanovic", "Alexandre Bouchard-C\u00f4t\u00e9", "Trevor Campbell"], "title": "AutoGD: Automatic Learning Rate Selection for Gradient Descent", "comment": null, "summary": "The performance of gradient-based optimization methods, such as standard\ngradient descent (GD), greatly depends on the choice of learning rate. However,\nit can require a non-trivial amount of user tuning effort to select an\nappropriate learning rate schedule. When such methods appear as inner loops of\nother algorithms, expecting the user to tune the learning rates may be\nimpractical. To address this, we introduce AutoGD: a gradient descent method\nthat automatically determines whether to increase or decrease the learning rate\nat a given iteration. We establish the convergence of AutoGD, and show that we\ncan recover the optimal rate of GD (up to a constant) for a broad class of\nfunctions without knowledge of smoothness constants. Experiments on a variety\nof traditional problems and variational inference optimization tasks\ndemonstrate strong performance of the method, along with its extensions to\nAutoBFGS and AutoLBFGS.", "AI": {"tldr": "AutoGD\u662f\u4e00\u79cd\u81ea\u52a8\u8c03\u6574\u5b66\u4e60\u7387\u7684\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\uff0c\u65e0\u9700\u7528\u6237\u624b\u52a8\u8c03\u53c2\uff0c\u80fd\u591f\u81ea\u52a8\u51b3\u5b9a\u4f55\u65f6\u589e\u52a0\u6216\u51cf\u5c11\u5b66\u4e60\u7387\uff0c\u5e76\u5728\u591a\u79cd\u4f18\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u68af\u5ea6\u4e0b\u964d\u7b49\u4f18\u5316\u65b9\u6cd5\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u5b66\u4e60\u7387\u7684\u9009\u62e9\uff0c\u4f46\u624b\u52a8\u8c03\u6574\u5b66\u4e60\u7387\u9700\u8981\u5927\u91cf\u7528\u6237\u8c03\u53c2\u5de5\u4f5c\uff0c\u7279\u522b\u662f\u5728\u4f5c\u4e3a\u5176\u4ed6\u7b97\u6cd5\u5185\u5faa\u73af\u65f6\uff0c\u671f\u671b\u7528\u6237\u8c03\u53c2\u662f\u4e0d\u73b0\u5b9e\u7684\u3002", "method": "\u63d0\u51faAutoGD\u65b9\u6cd5\uff0c\u81ea\u52a8\u5224\u65ad\u5728\u7ed9\u5b9a\u8fed\u4ee3\u4e2d\u5e94\u8be5\u589e\u52a0\u8fd8\u662f\u51cf\u5c11\u5b66\u4e60\u7387\uff0c\u65e0\u9700\u77e5\u9053\u5e73\u6ed1\u5ea6\u5e38\u6570\uff0c\u5e76\u6269\u5c55\u5230AutoBFGS\u548cAutoLBFGS\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eAutoGD\u5728\u4f20\u7edf\u95ee\u9898\u548c\u53d8\u5206\u63a8\u65ad\u4f18\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u6062\u590dGD\u7684\u6700\u4f18\u6536\u655b\u901f\u7387\uff08\u76f8\u5dee\u4e00\u4e2a\u5e38\u6570\uff09\u3002", "conclusion": "AutoGD\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u81ea\u52a8\u5b66\u4e60\u7387\u8c03\u6574\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u7528\u6237\u8c03\u53c2\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6536\u655b\u6027\u80fd\u3002"}}
{"id": "2510.11386", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11386", "abs": "https://arxiv.org/abs/2510.11386", "authors": ["Yuechen Liu", "Boqi Meng"], "title": "High-Order Quarter-Wave Plate Optimization for Linear Birefringence Suppression in Reflective FOCS", "comment": null, "summary": "Fiber optic current sensors (FOCS) are widely adopted in modern power grids\ndue to high sensitivity, excellent insulation, and strong immunity to\nelectromagnetic interference. This prominence necessitates precise\ninvestigation into their error sources and corresponding optimization. This\nstudy examines reflective FOCS based on the Faraday effect. A theoretical model\nis established to simulate phase error caused by linear birefringence from the\nquarter-wave plate. Conventional methods using circular birefringence are\nanalyzed, revealing inherent limitations. Innovatively, a compensation strategy\nemploying high-order quarter-wave plates is proposed to effectively eliminate\nlinear birefringence effects. This approach significantly enhances the accuracy\nand practicality of FOCS in precision metrology.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u6cd5\u62c9\u7b2c\u6548\u5e94\u7684\u53cd\u5c04\u5f0f\u5149\u7ea4\u7535\u6d41\u4f20\u611f\u5668\uff0c\u5efa\u7acb\u4e86\u7406\u8bba\u6a21\u578b\u5206\u6790\u56db\u5206\u4e4b\u4e00\u6ce2\u7247\u5f15\u8d77\u7684\u7ebf\u6027\u53cc\u6298\u5c04\u76f8\u4f4d\u8bef\u5dee\uff0c\u63d0\u51fa\u4e86\u4f7f\u7528\u9ad8\u9636\u56db\u5206\u4e4b\u4e00\u6ce2\u7247\u7684\u8865\u507f\u7b56\u7565\u6765\u6d88\u9664\u7ebf\u6027\u53cc\u6298\u5c04\u6548\u5e94\u3002", "motivation": "\u5149\u7ea4\u7535\u6d41\u4f20\u611f\u5668\u5728\u73b0\u4ee3\u7535\u7f51\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u8bef\u5dee\u6e90\u9700\u8981\u7cbe\u786e\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u53cd\u5c04\u5f0fFOCS\u4e2d\u7531\u56db\u5206\u4e4b\u4e00\u6ce2\u7247\u5f15\u8d77\u7684\u7ebf\u6027\u53cc\u6298\u5c04\u76f8\u4f4d\u8bef\u5dee\u95ee\u9898\u3002", "method": "\u5efa\u7acb\u4e86\u7406\u8bba\u6a21\u578b\u6a21\u62df\u56db\u5206\u4e4b\u4e00\u6ce2\u7247\u5f15\u8d77\u7684\u7ebf\u6027\u53cc\u6298\u5c04\u76f8\u4f4d\u8bef\u5dee\uff0c\u5206\u6790\u4e86\u4f20\u7edf\u4f7f\u7528\u5706\u5f62\u53cc\u6298\u5c04\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u521b\u65b0\u6027\u5730\u63d0\u51fa\u4e86\u4f7f\u7528\u9ad8\u9636\u56db\u5206\u4e4b\u4e00\u6ce2\u7247\u7684\u8865\u507f\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u7684\u9ad8\u9636\u56db\u5206\u4e4b\u4e00\u6ce2\u7247\u8865\u507f\u7b56\u7565\u80fd\u6709\u6548\u6d88\u9664\u7ebf\u6027\u53cc\u6298\u5c04\u6548\u5e94\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5149\u7ea4\u7535\u6d41\u4f20\u611f\u5668\u5728\u7cbe\u5bc6\u8ba1\u91cf\u4e2d\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u9ad8\u9636\u56db\u5206\u4e4b\u4e00\u6ce2\u7247\u8865\u507f\u7ebf\u6027\u53cc\u6298\u5c04\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u53cd\u5c04\u5f0f\u5149\u7ea4\u7535\u6d41\u4f20\u611f\u5668\u7684\u7cbe\u5ea6\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.11586", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.11586", "abs": "https://arxiv.org/abs/2510.11586", "authors": ["Georg Ahnert", "Anna-Carolina Haensch", "Barbara Plank", "Markus Strohmaier"], "title": "Survey Response Generation: Generating Closed-Ended Survey Responses In-Silico with Large Language Models", "comment": null, "summary": "Many in-silico simulations of human survey responses with large language\nmodels (LLMs) focus on generating closed-ended survey responses, whereas LLMs\nare typically trained to generate open-ended text instead. Previous research\nhas used a diverse range of methods for generating closed-ended survey\nresponses with LLMs, and a standard practice remains to be identified. In this\npaper, we systematically investigate the impact that various Survey Response\nGeneration Methods have on predicted survey responses. We present the results\nof 32 mio. simulated survey responses across 8 Survey Response Generation\nMethods, 4 political attitude surveys, and 10 open-weight language models. We\nfind significant differences between the Survey Response Generation Methods in\nboth individual-level and subpopulation-level alignment. Our results show that\nRestricted Generation Methods perform best overall, and that reasoning output\ndoes not consistently improve alignment. Our work underlines the significant\nimpact that Survey Response Generation Methods have on simulated survey\nresponses, and we develop practical recommendations on the application of\nSurvey Response Generation Methods.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e868\u79cd\u8c03\u67e5\u54cd\u5e94\u751f\u6210\u65b9\u6cd5\u5bf9LLM\u6a21\u62df\u8c03\u67e5\u54cd\u5e94\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9650\u5236\u6027\u751f\u6210\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u63a8\u7406\u8f93\u51fa\u4e0d\u80fd\u6301\u7eed\u6539\u5584\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4f7f\u7528\u591a\u79cd\u65b9\u6cd5\u751f\u6210\u5c01\u95ed\u5f0f\u8c03\u67e5\u54cd\u5e94\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5b9e\u8df5\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u751f\u6210\u65b9\u6cd5\u5bf9\u9884\u6d4b\u8c03\u67e5\u54cd\u5e94\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u75288\u79cd\u8c03\u67e5\u54cd\u5e94\u751f\u6210\u65b9\u6cd5\u30014\u4e2a\u653f\u6cbb\u6001\u5ea6\u8c03\u67e5\u548c10\u4e2a\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\uff0c\u751f\u6210\u4e863200\u4e07\u4e2a\u6a21\u62df\u8c03\u67e5\u54cd\u5e94\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u4e0d\u540c\u751f\u6210\u65b9\u6cd5\u5728\u4e2a\u4f53\u5c42\u9762\u548c\u5b50\u7fa4\u4f53\u5c42\u9762\u7684\u5bf9\u9f50\u6548\u679c\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u9650\u5236\u6027\u751f\u6210\u65b9\u6cd5\u6574\u4f53\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u8c03\u67e5\u54cd\u5e94\u751f\u6210\u65b9\u6cd5\u5bf9\u6a21\u62df\u8c03\u67e5\u54cd\u5e94\u6709\u663e\u8457\u5f71\u54cd\uff0c\u7814\u7a76\u4e3a\u5e94\u7528\u8fd9\u4e9b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2510.11312", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.11312", "abs": "https://arxiv.org/abs/2510.11312", "authors": ["Konstantinos Oikonomidis", "Jan Quan", "Panagiotis Patrinos"], "title": "Nonlinearly Preconditioned Gradient Methods: Momentum and Stochastic Analysis", "comment": "NeurIPS 2025 poster", "summary": "We study nonlinearly preconditioned gradient methods for smooth nonconvex\noptimization problems, focusing on sigmoid preconditioners that inherently\nperform a form of gradient clipping akin to the widely used gradient clipping\ntechnique. Building upon this idea, we introduce a novel heavy ball-type\nalgorithm and provide convergence guarantees under a generalized smoothness\ncondition that is less restrictive than traditional Lipschitz smoothness, thus\ncovering a broader class of functions. Additionally, we develop a stochastic\nvariant of the base method and study its convergence properties under different\nnoise assumptions. We compare the proposed algorithms with baseline methods on\ndiverse tasks from machine learning including neural network training.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528sigmoid\u9884\u6761\u4ef6\u5668\u7684\u975e\u7ebf\u6027\u9884\u6761\u4ef6\u68af\u5ea6\u65b9\u6cd5\uff0c\u7528\u4e8e\u5149\u6ed1\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5e26\u6709\u52a8\u91cf\u9879\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u5e7f\u4e49\u5149\u6ed1\u6027\u6761\u4ef6\u4e0b\u63d0\u4f9b\u6536\u655b\u4fdd\u8bc1\u3002", "motivation": "\u7814\u7a76\u975e\u7ebf\u6027\u9884\u6761\u4ef6\u68af\u5ea6\u65b9\u6cd5\uff0c\u7279\u522b\u662fsigmoid\u9884\u6761\u4ef6\u5668\uff0c\u5b83\u5929\u7136\u5b9e\u73b0\u4e86\u68af\u5ea6\u88c1\u526a\u529f\u80fd\uff0c\u65e8\u5728\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u7b97\u6cd5\u6765\u5904\u7406\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8esigmoid\u9884\u6761\u4ef6\u5668\u7684\u91cd\u7403\u578b\u7b97\u6cd5\u53ca\u5176\u968f\u673a\u53d8\u4f53\uff0c\u5728\u5e7f\u4e49\u5149\u6ed1\u6027\u6761\u4ef6\u4e0b\u5206\u6790\u6536\u655b\u6027\uff0c\u5e76\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u5728\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e0a\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u65b0\u7b97\u6cd5\u5728\u5e7f\u4e49\u5149\u6ed1\u6027\u6761\u4ef6\u4e0b\u5177\u6709\u6536\u655b\u4fdd\u8bc1\uff0c\u80fd\u591f\u5904\u7406\u6bd4\u4f20\u7edfLipschitz\u5149\u6ed1\u6027\u66f4\u5e7f\u6cdb\u7684\u51fd\u6570\u7c7b\uff0c\u5e76\u5728\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "sigmoid\u9884\u6761\u4ef6\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u7136\u7684\u68af\u5ea6\u88c1\u526a\u673a\u5236\uff0c\u6240\u63d0\u51fa\u7684\u91cd\u7403\u578b\u7b97\u6cd5\u5728\u5e7f\u4e49\u5149\u6ed1\u6027\u6761\u4ef6\u4e0b\u6709\u6548\uff0c\u4e3a\u5904\u7406\u975e\u51f8\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002"}}
{"id": "2510.09935", "categories": ["cs.CL", "cs.AI", "68T50, 68T45, 68T07", "I.2.7; I.2.10; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.09935", "abs": "https://arxiv.org/abs/2510.09935", "authors": ["Weibin Cai", "Jiayu Li", "Reza Zafarani"], "title": "Unpacking Hateful Memes: Presupposed Context and False Claims", "comment": null, "summary": "While memes are often humorous, they are frequently used to disseminate hate,\ncausing serious harm to individuals and society. Current approaches to hateful\nmeme detection mainly rely on pre-trained language models. However, less focus\nhas been dedicated to \\textit{what make a meme hateful}. Drawing on insights\nfrom philosophy and psychology, we argue that hateful memes are characterized\nby two essential features: a \\textbf{presupposed context} and the expression of\n\\textbf{false claims}. To capture presupposed context, we develop \\textbf{PCM}\nfor modeling contextual information across modalities. To detect false claims,\nwe introduce the \\textbf{FACT} module, which integrates external knowledge and\nharnesses cross-modal reference graphs. By combining PCM and FACT, we introduce\n\\textbf{\\textsf{SHIELD}}, a hateful meme detection framework designed to\ncapture the fundamental nature of hate. Extensive experiments show that SHIELD\noutperforms state-of-the-art methods across datasets and metrics, while\ndemonstrating versatility on other tasks, such as fake news detection.", "AI": {"tldr": "SHIELD\u6846\u67b6\u901a\u8fc7\u5efa\u6a21\u9884\u8bbe\u4e0a\u4e0b\u6587\u548c\u68c0\u6d4b\u865a\u5047\u58f0\u660e\u6765\u68c0\u6d4b\u4ec7\u6068\u8868\u60c5\u5305\uff0c\u5728\u591a\u6570\u636e\u96c6\u548c\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u4ec7\u6068\u8868\u60c5\u5305\u68c0\u6d4b\u4e3b\u8981\u4f9d\u8d56\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u8f83\u5c11\u5173\u6ce8\u4ec7\u6068\u8868\u60c5\u5305\u7684\u672c\u8d28\u7279\u5f81\u3002\u57fa\u4e8e\u54f2\u5b66\u548c\u5fc3\u7406\u5b66\u6d1e\u5bdf\uff0c\u4ec7\u6068\u8868\u60c5\u5305\u5177\u6709\u9884\u8bbe\u4e0a\u4e0b\u6587\u548c\u8868\u8fbe\u865a\u5047\u58f0\u660e\u4e24\u4e2a\u5173\u952e\u7279\u5f81\u3002", "method": "\u5f00\u53d1PCM\u6a21\u5757\u5efa\u6a21\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5f15\u5165FACT\u6a21\u5757\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u5e76\u5229\u7528\u8de8\u6a21\u6001\u53c2\u8003\u56fe\u68c0\u6d4b\u865a\u5047\u58f0\u660e\uff0c\u7ed3\u5408\u4e24\u8005\u6784\u5efaSHIELD\u6846\u67b6\u3002", "result": "SHIELD\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5728\u5047\u65b0\u95fb\u68c0\u6d4b\u7b49\u5176\u4ed6\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5efa\u6a21\u9884\u8bbe\u4e0a\u4e0b\u6587\u548c\u68c0\u6d4b\u865a\u5047\u58f0\u660e\uff0cSHIELD\u80fd\u591f\u6709\u6548\u6355\u6349\u4ec7\u6068\u8868\u60c5\u5305\u7684\u672c\u8d28\u7279\u5f81\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u68c0\u6d4b\u6548\u679c\u3002"}}
{"id": "2510.09717", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09717", "abs": "https://arxiv.org/abs/2510.09717", "authors": ["Zhenlong Liu", "Hao Zeng", "Weiran Huang", "Hongxin Wei"], "title": "High-Power Training Data Identification with Provable Statistical Guarantees", "comment": null, "summary": "Identifying training data within large-scale models is critical for copyright\nlitigation, privacy auditing, and ensuring fair evaluation. The conventional\napproaches treat it as a simple binary classification task without statistical\nguarantees. A recent approach is designed to control the false discovery rate\n(FDR), but its guarantees rely on strong, easily violated assumptions. In this\npaper, we introduce Provable Training Data Identification (PTDI), a rigorous\nmethod that identifies a set of training data with strict false discovery rate\n(FDR) control. Specifically, our method computes p-values for each data point\nusing a set of known unseen data, and then constructs a conservative estimator\nfor the data usage proportion of the test set, which allows us to scale these\np-values. Our approach then selects the final set of training data by\nidentifying all points whose scaled p-values fall below a data-dependent\nthreshold. This entire procedure enables the discovery of training data with\nprovable, strict FDR control and significantly boosted power. Extensive\nexperiments across a wide range of models (LLMs and VLMs), and datasets\ndemonstrate that PTDI strictly controls the FDR and achieves higher power.", "AI": {"tldr": "\u63d0\u51faPTDI\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97p\u503c\u548c\u6784\u5efa\u4fdd\u5b88\u4f30\u8ba1\u5668\uff0c\u5728\u4e25\u683c\u63a7\u5236\u9519\u8bef\u53d1\u73b0\u7387\u7684\u524d\u63d0\u4e0b\u8bc6\u522b\u8bad\u7ec3\u6570\u636e", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u6216\u4f9d\u8d56\u4e8e\u6613\u8fdd\u53cd\u7684\u5f3a\u5047\u8bbe\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u4e25\u683c\u63a7\u5236FDR\u7684\u8bad\u7ec3\u6570\u636e\u8bc6\u522b\u65b9\u6cd5", "method": "\u4f7f\u7528\u5df2\u77e5\u672a\u89c1\u6570\u636e\u8ba1\u7b97\u6bcf\u4e2a\u6570\u636e\u70b9\u7684p\u503c\uff0c\u6784\u5efa\u6d4b\u8bd5\u96c6\u6570\u636e\u4f7f\u7528\u6bd4\u4f8b\u7684\u4fdd\u5b88\u4f30\u8ba1\u5668\u6765\u7f29\u653ep\u503c\uff0c\u7136\u540e\u57fa\u4e8e\u6570\u636e\u4f9d\u8d56\u9608\u503c\u9009\u62e9\u8bad\u7ec3\u6570\u636e\u96c6", "result": "\u5728\u591a\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPTDI\u80fd\u4e25\u683c\u63a7\u5236FDR\u5e76\u5b9e\u73b0\u66f4\u9ad8\u7684\u68c0\u6d4b\u80fd\u529b", "conclusion": "PTDI\u63d0\u4f9b\u4e86\u4e00\u79cd\u5177\u6709\u4e25\u683cFDR\u63a7\u5236\u4fdd\u8bc1\u7684\u8bad\u7ec3\u6570\u636e\u8bc6\u522b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u80fd\u529b"}}
{"id": "2510.09959", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09959", "abs": "https://arxiv.org/abs/2510.09959", "authors": ["Jun Yin", "Runcheng Cai", "Shiliang Sun"], "title": "Clustering Result Re-guided Incomplete Multi-view Spectral Clustering", "comment": null, "summary": "Incomplete multi-view spectral clustering generalizes spectral clustering to\nmulti-view data and simultaneously realizes the partition of multi-view data\nwith missing views. For this category of method, K-means algorithm needs to be\nperformed to generate the clustering result after the procedure of feature\nextraction. More importantly, the connectivity of samples reflected by the\nclustering result is not utilized effectively. To overcome these defects, we\npropose Clustering Result re-Guided Incomplete Multi-view Spectral Clustering\n(CRG_IMSC). CRG_IMSC obtains the clustering result directly by imposing\nnonnegative constraint to the extracted feature. Furthermore, it constructs the\nconnectivity matrix according to the result of spectral clustering, and\nminimizes the residual of self-representation based on the connectivity matrix.\nA novel iterative algorithm using multiplicative update is developed to solve\nthe optimization problem of CRG_IMSC, and its convergence is proved rigorously.\nOn benchmark datasets, for multi-view data, CRG_IMSC performs better than\nstate-of-the-art clustering methods, and the experimental results also\ndemonstrate the convergence of CRG_IMSC algorithm.", "AI": {"tldr": "\u63d0\u51faCRG_IMSC\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u8d1f\u7ea6\u675f\u76f4\u63a5\u83b7\u5f97\u805a\u7c7b\u7ed3\u679c\uff0c\u5e76\u5229\u7528\u8c31\u805a\u7c7b\u7ed3\u679c\u6784\u5efa\u8fde\u901a\u6027\u77e9\u9635\uff0c\u6539\u8fdb\u4e0d\u5b8c\u5168\u591a\u89c6\u56fe\u8c31\u805a\u7c7b\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4e0d\u5b8c\u5168\u591a\u89c6\u56fe\u8c31\u805a\u7c7b\u9700\u8981\u5728\u7279\u5f81\u63d0\u53d6\u540e\u4f7f\u7528K-means\u7b97\u6cd5\u751f\u6210\u805a\u7c7b\u7ed3\u679c\uff0c\u4e14\u672a\u80fd\u6709\u6548\u5229\u7528\u6837\u672c\u8fde\u901a\u6027\u4fe1\u606f\u3002", "method": "\u65bd\u52a0\u975e\u8d1f\u7ea6\u675f\u76f4\u63a5\u83b7\u5f97\u805a\u7c7b\u7ed3\u679c\uff0c\u57fa\u4e8e\u8c31\u805a\u7c7b\u7ed3\u679c\u6784\u5efa\u8fde\u901a\u6027\u77e9\u9635\uff0c\u6700\u5c0f\u5316\u57fa\u4e8e\u8fde\u901a\u6027\u77e9\u9635\u7684\u81ea\u8868\u793a\u6b8b\u5dee\uff0c\u4f7f\u7528\u4e58\u6cd5\u66f4\u65b0\u8fed\u4ee3\u7b97\u6cd5\u6c42\u89e3\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cCRG_IMSC\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u805a\u7c7b\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6027\u3002", "conclusion": "CRG_IMSC\u901a\u8fc7\u76f4\u63a5\u83b7\u5f97\u805a\u7c7b\u7ed3\u679c\u548c\u6709\u6548\u5229\u7528\u6837\u672c\u8fde\u901a\u6027\uff0c\u63d0\u5347\u4e86\u4e0d\u5b8c\u5168\u591a\u89c6\u56fe\u8c31\u805a\u7c7b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.11388", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11388", "abs": "https://arxiv.org/abs/2510.11388", "authors": ["Sheng-Wen Cheng", "Teng-Hu Cheng"], "title": "Data-Driven Estimation of Quadrotor Motor Efficiency via Residual Minimization", "comment": null, "summary": "A data-driven framework is proposed for online estimation of quadrotor motor\nefficiency via residual minimization. The problem is formulated as a\nconstrained nonlinear optimization that minimizes trajectory residuals between\nmeasured flight data and predictions generated by a quadrotor dynamics model. A\nsliding-window strategy enables online estimation, and the optimization is\nefficiently solved using an iteratively reweighted least squares (IRLS) scheme\ncombined with a primal-dual interior-point method, with inequality constraints\nenforced through a logarithmic barrier function. Robust z-score weighting is\nemployed to reject outliers, which is particularly effective in motor clipping\nscenarios where the proposed estimator exhibits smaller spikes than an EKF\nbaseline. Compared to traditional filter-based approaches, the batch-mode\nformulation offers greater flexibility by selectively incorporating informative\ndata segments. This structure is well-suited for onboard implementation,\nparticularly for applications such as fault detection and isolation (FDI),\nhealth monitoring, and predictive maintenance in aerial robotic systems.\nSimulation results under various degradation scenarios demonstrate the accuracy\nand robustness of the proposed estimator.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6b8b\u5dee\u6700\u5c0f\u5316\u7684\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7ebf\u4f30\u8ba1\u56db\u65cb\u7ffc\u7535\u673a\u6548\u7387\uff0c\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u7b56\u7565\u548cIRLS\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u7535\u673a\u6545\u969c\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8eEKF\u57fa\u7ebf\u3002", "motivation": "\u4f20\u7edf\u6ee4\u6ce2\u5668\u65b9\u6cd5\u5728\u7535\u673a\u6548\u7387\u4f30\u8ba1\u65b9\u9762\u7075\u6d3b\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9009\u62e9\u6027\u5229\u7528\u4fe1\u606f\u6570\u636e\u6bb5\u3001\u9002\u5408\u673a\u8f7d\u5b9e\u73b0\u7684\u5728\u7ebf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u6545\u969c\u68c0\u6d4b\u548c\u5065\u5eb7\u76d1\u6d4b\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u7ea6\u675f\u975e\u7ebf\u6027\u4f18\u5316\uff0c\u6700\u5c0f\u5316\u5b9e\u6d4b\u98de\u884c\u6570\u636e\u4e0e\u56db\u65cb\u7ffc\u52a8\u529b\u5b66\u6a21\u578b\u9884\u6d4b\u4e4b\u95f4\u7684\u8f68\u8ff9\u6b8b\u5dee\u3002\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u7b56\u7565\u5b9e\u73b0\u5728\u7ebf\u4f30\u8ba1\uff0c\u4f7f\u7528IRLS\u7ed3\u5408\u539f\u59cb-\u5bf9\u5076\u5185\u70b9\u6cd5\u9ad8\u6548\u6c42\u89e3\uff0c\u901a\u8fc7\u5bf9\u6570\u969c\u788d\u51fd\u6570\u5904\u7406\u4e0d\u7b49\u5f0f\u7ea6\u675f\uff0c\u5e76\u91c7\u7528\u7a33\u5065z-score\u52a0\u6743\u6291\u5236\u5f02\u5e38\u503c\u3002", "result": "\u5728\u591a\u79cd\u9000\u5316\u573a\u666f\u4e0b\u7684\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u4f30\u8ba1\u5668\u5728\u7535\u673a\u524a\u6ce2\u60c5\u51b5\u4e0b\u6bd4EKF\u57fa\u7ebf\u5177\u6709\u66f4\u5c0f\u7684\u5c16\u5cf0\uff0c\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6279\u5904\u7406\u6a21\u5f0f\u6846\u67b6\u4e3a\u673a\u8f7d\u5b9e\u73b0\u63d0\u4f9b\u4e86\u66f4\u5927\u7075\u6d3b\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7a7a\u4e2d\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6545\u969c\u68c0\u6d4b\u4e0e\u9694\u79bb\u3001\u5065\u5eb7\u76d1\u6d4b\u548c\u9884\u6d4b\u6027\u7ef4\u62a4\u5e94\u7528\u3002"}}
{"id": "2510.11325", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.11325", "abs": "https://arxiv.org/abs/2510.11325", "authors": ["Jingyi Zhang"], "title": "A model reduction method based on nonlinear optimization for multiscale stochastic optimal control problems", "comment": null, "summary": "This paper presents a nonlinear optimization-based model reduction method for\nmultiscale stochastic optimal control problems governed by stochastic partial\ndifferential equations. The proposed approach constructs a non-intrusive,\ndata-driven reduced-order model by employing a parameter-separable structure to\nhandle stochastic dependencies and directly minimizing the L2 norm of the\noutput error via gradient-based optimization. Compared to existing methods,\nthis framework offers three significant advantages: it is entirely data-driven,\nrelying solely on output measurements without requiring access to internal\nsystem matrices; it guarantees approximation accuracy for control outputs,\naligning directly with the optimization objective; and its computational\ncomplexity is independent of the original PDE dimension, ensuring feasibility\nfor real-time control applications. Numerical experiments on stochastic\ndiffusion and advection-diffusion equations demonstrate the method's\neffectiveness and efficiency, providing a systematic solution for the real-time\ncontrol of complex uncertain systems and bridging the gap between model\nreduction theory and practical engineering.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u7ebf\u6027\u4f18\u5316\u7684\u6a21\u578b\u964d\u9636\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u5c3a\u5ea6\u968f\u673a\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u964d\u9636\u6a21\u578b\u76f4\u63a5\u6700\u5c0f\u5316\u8f93\u51fa\u8bef\u5dee\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u63a7\u5236\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3\u591a\u5c3a\u5ea6\u968f\u673a\u6700\u4f18\u63a7\u5236\u95ee\u9898\u4e2d\u6a21\u578b\u590d\u6742\u5ea6\u8fc7\u9ad8\u3001\u96be\u4ee5\u5b9e\u65f6\u5e94\u7528\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u968f\u673a\u504f\u5fae\u5206\u65b9\u7a0b\u63a7\u5236\u7684\u7cfb\u7edf\u4e2d\u3002", "method": "\u91c7\u7528\u53c2\u6570\u53ef\u5206\u79bb\u7ed3\u6784\u5904\u7406\u968f\u673a\u4f9d\u8d56\u6027\uff0c\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u76f4\u63a5\u6700\u5c0f\u5316\u8f93\u51fa\u8bef\u5dee\u7684L2\u8303\u6570\uff0c\u6784\u5efa\u975e\u4fb5\u5165\u5f0f\u6570\u636e\u9a71\u52a8\u964d\u9636\u6a21\u578b\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u968f\u673a\u6269\u6563\u548c\u5bf9\u6d41\u6269\u6563\u65b9\u7a0b\u4e2d\u6709\u6548\u4e14\u9ad8\u6548\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u539f\u59cbPDE\u7ef4\u5ea6\u65e0\u5173\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u4e0d\u786e\u5b9a\u7cfb\u7edf\u7684\u5b9e\u65f6\u63a7\u5236\u63d0\u4f9b\u4e86\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\uff0c\u5f25\u5408\u4e86\u6a21\u578b\u964d\u9636\u7406\u8bba\u4e0e\u5b9e\u9645\u5de5\u7a0b\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2510.09947", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09947", "abs": "https://arxiv.org/abs/2510.09947", "authors": ["Mir Tafseer Nayeem", "Sawsan Alqahtani", "Md Tahmid Rahman Laskar", "Tasnim Mohiuddin", "M Saiful Bari"], "title": "Beyond Fertility: Analyzing STRR as a Metric for Multilingual Tokenization Evaluation", "comment": "NeurIPS 2025 Workshop", "summary": "Tokenization is a crucial but under-evaluated step in large language models\n(LLMs). The standard metric, fertility (the average number of tokens per word),\ncaptures compression efficiency but obscures how vocabularies are allocated\nacross languages and domains. We analyze six widely used tokenizers across\nseven languages and two domains, finding stable fertility for English, high\nfertility for Chinese, and little domain sensitivity. To address fertility's\nblind spots, we propose the Single Token Retention Rate (STRR), which measures\nthe proportion of words preserved as single tokens. STRR reveals systematic\nprioritization of English, strong support for Chinese, and fragmentation in\nHindi, offering an interpretable view of cross-lingual fairness. Our results\nshow that STRR complements fertility and provides practical guidance for\ndesigning more equitable multilingual tokenizers.", "AI": {"tldr": "\u63d0\u51fa\u5355\u4ee4\u724c\u4fdd\u7559\u7387\uff08STRR\uff09\u4f5c\u4e3a\u8bc4\u4f30\u5206\u8bcd\u5668\u8de8\u8bed\u8a00\u516c\u5e73\u6027\u7684\u65b0\u6307\u6807\uff0c\u63ed\u793a\u73b0\u6709\u5206\u8bcd\u5668\u5bf9\u82f1\u8bed\u7684\u7cfb\u7edf\u6027\u4f18\u5148\u548c\u5bf9\u5370\u5730\u8bed\u7684\u5206\u5272\u95ee\u9898\u3002", "motivation": "\u6807\u51c6\u7684\u5206\u8bcd\u8bc4\u4f30\u6307\u6807fertility\uff08\u751f\u80b2\u7387\uff09\u53ea\u5173\u6ce8\u538b\u7f29\u6548\u7387\uff0c\u4f46\u63a9\u76d6\u4e86\u8bcd\u6c47\u8868\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u9886\u57df\u95f4\u7684\u5206\u914d\u4e0d\u516c\u5e73\u95ee\u9898\u3002", "method": "\u5206\u67906\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u5206\u8bcd\u5668\u57287\u79cd\u8bed\u8a00\u548c2\u4e2a\u9886\u57df\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51faSTRR\u6307\u6807\u6765\u8861\u91cf\u5355\u8bcd\u88ab\u4fdd\u7559\u4e3a\u5355\u4e2a\u4ee4\u724c\u7684\u6bd4\u4f8b\u3002", "result": "\u53d1\u73b0\u82f1\u8bed\u5206\u8bcd\u7a33\u5b9a\u6027\u9ad8\uff0c\u4e2d\u6587\u5206\u8bcd\u7387\u8f83\u9ad8\uff0c\u5370\u5730\u8bed\u5b58\u5728\u4e25\u91cd\u5206\u5272\u95ee\u9898\uff1bSTRR\u6307\u6807\u6709\u6548\u63ed\u793a\u4e86\u8de8\u8bed\u8a00\u516c\u5e73\u6027\u95ee\u9898\u3002", "conclusion": "STRR\u8865\u5145\u4e86\u4f20\u7edffertility\u6307\u6807\u7684\u4e0d\u8db3\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u516c\u5e73\u7684\u591a\u8bed\u8a00\u5206\u8bcd\u5668\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2510.09718", "categories": ["cs.LG", "68T05, 68T10", "I.5.3; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.09718", "abs": "https://arxiv.org/abs/2510.09718", "authors": ["A. Jung"], "title": "Federated k-Means via Generalized Total Variation Minimization", "comment": null, "summary": "We consider the problem of federated clustering, where interconnected devices\nhave access to private local datasets and need to jointly cluster the overall\ndataset without sharing their local dataset. Our focus is on hard clustering\nbased on the k-means principle. We formulate federated k-means clustering as an\ninstance of GTVMin. This formulation naturally lends to a federated k-means\nalgorithm where each device updates local cluster centroids by solving a\nmodified local k-means problem. The modification involves adding a penalty term\nto measure the discrepancy between the cluster centroid of neighbouring\ndevices. Our federated k-means algorithm is privacy-friendly as it only\nrequires sharing aggregated information among interconnected devices.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8ek-means\u7684\u8054\u90a6\u805a\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7GTVMin\u516c\u5f0f\u5316\u5b9e\u73b0\u8bbe\u5907\u95f4\u8054\u5408\u805a\u7c7b\u800c\u65e0\u9700\u5171\u4eab\u672c\u5730\u6570\u636e", "motivation": "\u89e3\u51b3\u8054\u90a6\u805a\u7c7b\u95ee\u9898\uff0c\u4f7f\u4e92\u8054\u8bbe\u5907\u80fd\u591f\u5728\u4e0d\u5171\u4eab\u672c\u5730\u6570\u636e\u96c6\u7684\u60c5\u51b5\u4e0b\u8054\u5408\u805a\u7c7b\u6574\u4f53\u6570\u636e\u96c6", "method": "\u5c06\u8054\u90a6k-means\u805a\u7c7b\u516c\u5f0f\u5316\u4e3aGTVMin\u5b9e\u4f8b\uff0c\u6bcf\u4e2a\u8bbe\u5907\u901a\u8fc7\u6c42\u89e3\u4fee\u6539\u540e\u7684\u672c\u5730k-means\u95ee\u9898\u6765\u66f4\u65b0\u672c\u5730\u805a\u7c7b\u4e2d\u5fc3\uff0c\u4fee\u6539\u5305\u62ec\u6dfb\u52a0\u60e9\u7f5a\u9879\u6765\u8861\u91cf\u76f8\u90bb\u8bbe\u5907\u805a\u7c7b\u4e2d\u5fc3\u7684\u5dee\u5f02", "result": "\u5f00\u53d1\u51fa\u9690\u79c1\u53cb\u597d\u7684\u8054\u90a6k-means\u7b97\u6cd5\uff0c\u4ec5\u9700\u8981\u5728\u4e92\u8054\u8bbe\u5907\u95f4\u5171\u4eab\u805a\u5408\u4fe1\u606f", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u8054\u90a6\u73af\u5883\u4e0b\u7684k-means\u805a\u7c7b\uff0c\u4fdd\u62a4\u4e86\u6570\u636e\u9690\u79c1"}}
{"id": "2510.10409", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10409", "abs": "https://arxiv.org/abs/2510.10409", "authors": ["Siddartha Devic", "Charlotte Peale", "Arwen Bradley", "Sinead Williamson", "Preetum Nakkiran", "Aravind Gollakota"], "title": "Trace Length is a Simple Uncertainty Signal in Reasoning Models", "comment": null, "summary": "Uncertainty quantification for LLMs is a key research direction towards\naddressing hallucination and other issues that limit their reliable deployment.\nIn this work, we show that reasoning trace length is a simple and useful\nconfidence estimator in large reasoning models. Through comprehensive\nexperiments across multiple models, datasets, and prompts, we show that trace\nlength performs in comparable but complementary ways to other zero-shot\nconfidence estimators such as verbalized confidence. Our work reveals that\nreasoning post-training fundamentally alters the relationship between trace\nlength and accuracy, going beyond prior work that had shown that post-training\ncauses traces to grow longer in general (e.g., \"overthinking\"). We investigate\nthe mechanisms behind trace length's performance as a confidence signal,\nobserving that the effect remains even after adjusting for confounders such as\nproblem difficulty and GRPO-induced length bias. We identify high-entropy or\n\"forking\" tokens as playing a key role in the mechanism. Our findings\ndemonstrate that reasoning post-training enhances uncertainty quantification\nbeyond verbal expressions, and establish trace length as a practical confidence\nmeasure for large reasoning models.", "AI": {"tldr": "\u63a8\u7406\u8f68\u8ff9\u957f\u5ea6\u662f\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u7b80\u5355\u6709\u6548\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u5668\uff0c\u4e0e\u96f6\u6837\u672c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u4f46\u4e92\u8865\u3002", "motivation": "\u89e3\u51b3LLM\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u90e8\u7f72\u53ef\u9760\u6027\uff0c\u9700\u8981\u6709\u6548\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u591a\u6a21\u578b\u3001\u591a\u6570\u636e\u96c6\u3001\u591a\u63d0\u793a\u7684\u7efc\u5408\u5b9e\u9a8c\uff0c\u5206\u6790\u63a8\u7406\u8f68\u8ff9\u957f\u5ea6\u4e0e\u51c6\u786e\u7387\u7684\u5173\u7cfb\uff0c\u5e76\u7814\u7a76\u5176\u673a\u5236\u3002", "result": "\u63a8\u7406\u8f68\u8ff9\u957f\u5ea6\u4f5c\u4e3a\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u5668\u8868\u73b0\u826f\u597d\uff0c\u5373\u4f7f\u8c03\u6574\u4e86\u95ee\u9898\u96be\u5ea6\u548cGRPO\u8bf1\u5bfc\u7684\u957f\u5ea6\u504f\u5dee\u540e\u4ecd\u7136\u6709\u6548\u3002\u9ad8\u71b5\u6216\"\u5206\u53c9\"token\u5728\u673a\u5236\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u63a8\u7406\u540e\u8bad\u7ec3\u589e\u5f3a\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\uff0c\u8f68\u8ff9\u957f\u5ea6\u53ef\u4f5c\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b9e\u7528\u7f6e\u4fe1\u5ea6\u5ea6\u91cf\u3002"}}
{"id": "2510.09965", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09965", "abs": "https://arxiv.org/abs/2510.09965", "authors": ["Shuo Zhao", "Yongqiang Li", "Yu Feng", "Zhongsheng Hou", "Yuanjing Feng"], "title": "Homomorphic Mappings for Value-Preserving State Aggregation in Markov Decision Processes", "comment": null, "summary": "State aggregation aims to reduce the computational complexity of solving\nMarkov Decision Processes (MDPs) while preserving the performance of the\noriginal system. A fundamental challenge lies in optimizing policies within the\naggregated, or abstract, space such that the performance remains optimal in the\nground MDP-a property referred to as {\"}optimal policy equivalence {\"}.\n  This paper presents an abstraction framework based on the notion of\nhomomorphism, in which two Markov chains are deemed homomorphic if their value\nfunctions exhibit a linear relationship. Within this theoretical framework, we\nestablish a sufficient condition for the equivalence of optimal policy.\n  We further examine scenarios where the sufficient condition is not met and\nderive an upper bound on the approximation error and a performance lower bound\nfor the objective function under the ground MDP. We propose Homomorphic Policy\nGradient (HPG), which guarantees optimal policy equivalence under sufficient\nconditions, and its extension, Error-Bounded HPG (EBHPG), which balances\ncomputational efficiency and the performance loss induced by aggregation. In\nthe experiments, we validated the theoretical results and conducted comparative\nevaluations against seven algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u540c\u6001\u6982\u5ff5\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u72b6\u6001\u805a\u5408\u6846\u67b6\uff0c\u5efa\u7acb\u4e86\u6700\u4f18\u7b56\u7565\u7b49\u4ef7\u6027\u7684\u5145\u5206\u6761\u4ef6\uff0c\u5e76\u5f00\u53d1\u4e86\u4fdd\u8bc1\u6700\u4f18\u7b56\u7565\u7b49\u4ef7\u7684\u540c\u6001\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u53ca\u5176\u8bef\u5dee\u6709\u754c\u6269\u5c55\u7248\u672c\u3002", "motivation": "\u72b6\u6001\u805a\u5408\u65e8\u5728\u964d\u4f4e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u7cfb\u7edf\u7684\u6027\u80fd\u3002\u6838\u5fc3\u6311\u6218\u662f\u5728\u805a\u5408\u7a7a\u95f4\u4e2d\u4f18\u5316\u7b56\u7565\uff0c\u4f7f\u5176\u5728\u539f\u59cbMDP\u4e2d\u4ecd\u4fdd\u6301\u6700\u4f18\u6027\u80fd\uff0c\u5373\u6700\u4f18\u7b56\u7565\u7b49\u4ef7\u6027\u3002", "method": "\u57fa\u4e8e\u540c\u6001\u6982\u5ff5\u6784\u5efa\u62bd\u8c61\u6846\u67b6\uff0c\u5176\u4e2d\u4e24\u4e2a\u9a6c\u5c14\u53ef\u592b\u94fe\u5982\u679c\u5176\u4ef7\u503c\u51fd\u6570\u5b58\u5728\u7ebf\u6027\u5173\u7cfb\u5219\u88ab\u89c6\u4e3a\u540c\u6001\u3002\u63d0\u51fa\u4e86\u540c\u6001\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u53ca\u5176\u8bef\u5dee\u6709\u754c\u6269\u5c55\u7248\u672c\u3002", "result": "\u5efa\u7acb\u4e86\u6700\u4f18\u7b56\u7565\u7b49\u4ef7\u7684\u5145\u5206\u6761\u4ef6\uff0c\u63a8\u5bfc\u4e86\u8fd1\u4f3c\u8bef\u5dee\u4e0a\u754c\u548c\u6027\u80fd\u4e0b\u754c\u3002\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\uff0c\u5e76\u4e0e\u4e03\u79cd\u7b97\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u8bc4\u4f30\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u548c\u7b97\u6cd5\u5728\u4fdd\u8bc1\u6700\u4f18\u7b56\u7565\u7b49\u4ef7\u6027\u7684\u540c\u65f6\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u805a\u5408\u5f15\u8d77\u7684\u6027\u80fd\u635f\u5931\uff0c\u4e3a\u72b6\u6001\u805a\u5408\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2510.11393", "categories": ["eess.SY", "cs.SY", "math.DS"], "pdf": "https://arxiv.org/pdf/2510.11393", "abs": "https://arxiv.org/abs/2510.11393", "authors": ["Farhad Mehdifar", "Charalampos P. Bechlioulis", "Dimos V. Dimarogonas"], "title": "Robust Closed-Form Control for MIMO Nonlinear Systems under Conflicting Time-Varying Hard and Soft Constraints", "comment": "18 pages, 6 figures", "summary": "This paper introduces a novel robust closed-form control law to handle\ntime-varying hard and soft constraints in uncertain high-relative-degree\nnonlinear MIMO systems. These constraints represent spatiotemporal\nspecifications in mechanical systems' operational space, with hard constraints\nensuring safety-critical requirements and soft constraints encoding performance\nor task objectives. Initially, all constraints are consolidated into two\nseparate scalar time-varying hard and soft constraint functions, whose positive\nlevel sets define feasible regions. A closed-form control law is developed to\nenforce these constraints using appropriately designed reciprocal barriers and\nnonlinear transformation functions. When conflicts between hard and soft\nconstraints arise, the control law prioritizes hard constraints by virtually\nrelaxing soft constraints via a dynamic relaxation law. Notably, the proposed\ncontrol law maintains low complexity by avoiding approximation schemes for\ncoping with system uncertainties. Simulation results confirm the effectiveness\nof the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u9c81\u68d2\u95ed\u5f0f\u63a7\u5236\u5f8b\uff0c\u7528\u4e8e\u5904\u7406\u4e0d\u786e\u5b9a\u9ad8\u9636\u975e\u7ebf\u6027MIMO\u7cfb\u7edf\u4e2d\u7684\u65f6\u53d8\u786c\u7ea6\u675f\u548c\u8f6f\u7ea6\u675f\uff0c\u901a\u8fc7\u52a8\u6001\u677e\u5f1b\u673a\u5236\u5728\u7ea6\u675f\u51b2\u7a81\u65f6\u4f18\u5148\u4fdd\u8bc1\u786c\u7ea6\u675f\u5b89\u5168\u3002", "motivation": "\u673a\u68b0\u7cfb\u7edf\u64cd\u4f5c\u7a7a\u95f4\u4e2d\u7684\u65f6\u7a7a\u89c4\u8303\u9700\u8981\u540c\u65f6\u5904\u7406\u5b89\u5168\u5173\u952e\u7684\u786c\u7ea6\u675f\u548c\u6027\u80fd\u4efb\u52a1\u7684\u8f6f\u7ea6\u675f\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u7ea6\u675f\u51b2\u7a81\u65f6\u6709\u6548\u5e73\u8861\u4f18\u5148\u7ea7\u5e76\u5e94\u5bf9\u7cfb\u7edf\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5c06\u6240\u6709\u7ea6\u675f\u6574\u5408\u4e3a\u65f6\u53d8\u786c\u7ea6\u675f\u548c\u8f6f\u7ea6\u675f\u51fd\u6570\uff0c\u8bbe\u8ba1\u4e92\u6613\u969c\u788d\u51fd\u6570\u548c\u975e\u7ebf\u6027\u53d8\u6362\u51fd\u6570\u6784\u5efa\u95ed\u5f0f\u63a7\u5236\u5f8b\uff0c\u901a\u8fc7\u52a8\u6001\u677e\u5f1b\u5f8b\u5728\u7ea6\u675f\u51b2\u7a81\u65f6\u865a\u62df\u677e\u5f1b\u8f6f\u7ea6\u675f\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63a7\u5236\u5f8b\u5728\u4fdd\u6301\u4f4e\u590d\u6742\u5ea6\u7684\u540c\u65f6\u6210\u529f\u5904\u7406\u4e86\u7ea6\u675f\u51b2\u7a81\u548c\u7cfb\u7edf\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u7ea6\u675f\u5904\u7406\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e0d\u786e\u5b9a\u9ad8\u9636\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u4f18\u5148\u4fdd\u8bc1\u5b89\u5168\u7ea6\u675f\uff0c\u540c\u65f6\u517c\u987e\u6027\u80fd\u76ee\u6807\u3002"}}
{"id": "2510.11334", "categories": ["math.OC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.11334", "abs": "https://arxiv.org/abs/2510.11334", "authors": ["Fabio Ancona", "Mohamed Bentaibi", "Francesco Rossi"], "title": "Exponential convergence of multiagent systems with lack of connection", "comment": null, "summary": "Finding conditions ensuring consensus, i.e. convergence to a common value,\nfor a networked system is of crucial interest, both for theoretical reasons and\napplications. This goal is harder to achieve when connections between agents\nare temporarily lost.\n  Here, we prove that known conditions (introduced by Moreau) ensure an\nexponential convergence to consensus, with explicit rate of convergence. The\nkey result is related to the length of the graph (i.e. the number of\nconnections to reach a common agent): if this is large, then convergence is\nslow.\n  This general result also provides conditions for convergence of second-order\ncooperative systems with lack of connections.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86Moreau\u6761\u4ef6\u80fd\u786e\u4fdd\u7f51\u7edc\u7cfb\u7edf\u4ee5\u6307\u6570\u901f\u5ea6\u8fbe\u6210\u5171\u8bc6\uff0c\u5e76\u7ed9\u51fa\u4e86\u660e\u786e\u7684\u6536\u655b\u901f\u7387\u3002\u5173\u952e\u53d1\u73b0\u662f\u56fe\u957f\u5ea6\uff08\u5230\u8fbe\u5171\u540c\u4ee3\u7406\u6240\u9700\u7684\u8fde\u63a5\u6570\uff09\u8d8a\u5927\uff0c\u6536\u655b\u8d8a\u6162\u3002", "motivation": "\u7814\u7a76\u7f51\u7edc\u7cfb\u7edf\u8fbe\u6210\u5171\u8bc6\u7684\u6761\u4ef6\u5bf9\u7406\u8bba\u548c\u5e94\u7528\u90fd\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u8fde\u63a5\u6682\u65f6\u4e22\u5931\u7684\u60c5\u51b5\u4e0b\u66f4\u5177\u6311\u6218\u6027\u3002", "method": "\u57fa\u4e8eMoreau\u6761\u4ef6\uff0c\u5206\u6790\u56fe\u957f\u5ea6\u5bf9\u6536\u655b\u901f\u5ea6\u7684\u5f71\u54cd\uff0c\u8bc1\u660e\u6307\u6570\u6536\u655b\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u5df2\u77e5\u6761\u4ef6\u80fd\u786e\u4fdd\u6307\u6570\u6536\u655b\u5230\u5171\u8bc6\uff0c\u5e76\u7ed9\u51fa\u4e86\u660e\u786e\u7684\u6536\u655b\u901f\u7387\u3002\u53d1\u73b0\u56fe\u957f\u5ea6\u8d8a\u5927\u6536\u655b\u8d8a\u6162\u3002", "conclusion": "\u8be5\u901a\u7528\u7ed3\u679c\u4e5f\u4e3a\u8fde\u63a5\u7f3a\u5931\u7684\u4e8c\u9636\u534f\u4f5c\u7cfb\u7edf\u7684\u6536\u655b\u63d0\u4f9b\u4e86\u6761\u4ef6\u3002"}}
{"id": "2510.09988", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09988", "abs": "https://arxiv.org/abs/2510.09988", "authors": ["Jiaqi Wei", "Xiang Zhang", "Yuejin Yang", "Wenxuan Huang", "Juntai Cao", "Sheng Xu", "Xiang Zhuang", "Zhangyang Gao", "Muhammad Abdul-Mageed", "Laks V. S. Lakshmanan", "Chenyu You", "Wanli Ouyang", "Siqi Sun"], "title": "Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A Survey", "comment": null, "summary": "Deliberative tree search is a cornerstone of modern Large Language Model\n(LLM) research, driving the pivot from brute-force scaling toward algorithmic\nefficiency. This single paradigm unifies two critical frontiers:\n\\textbf{Test-Time Scaling (TTS)}, which deploys on-demand computation to solve\nhard problems, and \\textbf{Self-Improvement}, which uses search-generated data\nto durably enhance model parameters. However, this burgeoning field is\nfragmented and lacks a common formalism, particularly concerning the ambiguous\nrole of the reward signal -- is it a transient heuristic or a durable learning\ntarget? This paper resolves this ambiguity by introducing a unified framework\nthat deconstructs search algorithms into three core components: the\n\\emph{Search Mechanism}, \\emph{Reward Formulation}, and \\emph{Transition\nFunction}. We establish a formal distinction between transient \\textbf{Search\nGuidance} for TTS and durable \\textbf{Parametric Reward Modeling} for\nSelf-Improvement. Building on this formalism, we introduce a component-centric\ntaxonomy, synthesize the state-of-the-art, and chart a research roadmap toward\nmore systematic progress in creating autonomous, self-improving agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06LLM\u7684\u5ba1\u8bae\u6811\u641c\u7d22\u5206\u89e3\u4e3a\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u641c\u7d22\u673a\u5236\u3001\u5956\u52b1\u516c\u5f0f\u548c\u8f6c\u79fb\u51fd\u6570\uff0c\u660e\u786e\u4e86\u641c\u7d22\u6307\u5bfc\u4e0e\u53c2\u6570\u5316\u5956\u52b1\u5efa\u6a21\u7684\u533a\u522b\uff0c\u4e3a\u81ea\u4e3b\u81ea\u6539\u8fdb\u4ee3\u7406\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u8def\u5f84\u3002", "motivation": "\u5f53\u524d\u5ba1\u8bae\u6811\u641c\u7d22\u9886\u57df\u7f3a\u4e4f\u7edf\u4e00\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u5956\u52b1\u4fe1\u53f7\u7684\u89d2\u8272\u4e0a\u5b58\u5728\u6a21\u7cca\u6027\u2014\u2014\u5b83\u7a76\u7adf\u662f\u4e34\u65f6\u542f\u53d1\u5f0f\u8fd8\u662f\u6301\u4e45\u5b66\u4e60\u76ee\u6807\uff1f\u8fd9\u79cd\u6a21\u7cca\u6027\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u7cfb\u7edf\u6027\u53d1\u5c55\u3002", "method": "\u5f15\u5165\u7edf\u4e00\u6846\u67b6\u5c06\u641c\u7d22\u7b97\u6cd5\u5206\u89e3\u4e3a\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u641c\u7d22\u673a\u5236\u3001\u5956\u52b1\u516c\u5f0f\u548c\u8f6c\u79fb\u51fd\u6570\uff0c\u5e76\u6b63\u5f0f\u533a\u5206\u4e86\u7528\u4e8e\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u77ac\u65f6\u641c\u7d22\u6307\u5bfc\u548c\u7528\u4e8e\u81ea\u6539\u8fdb\u7684\u6301\u4e45\u53c2\u6570\u5316\u5956\u52b1\u5efa\u6a21\u3002", "result": "\u5efa\u7acb\u4e86\u7ec4\u4ef6\u4e2d\u5fc3\u5206\u7c7b\u6cd5\uff0c\u7efc\u5408\u4e86\u6700\u5148\u8fdb\u6280\u672f\uff0c\u5e76\u4e3a\u521b\u5efa\u81ea\u4e3b\u81ea\u6539\u8fdb\u4ee3\u7406\u7684\u7cfb\u7edf\u6027\u8fdb\u5c55\u7ed8\u5236\u4e86\u7814\u7a76\u8def\u7ebf\u56fe\u3002", "conclusion": "\u8be5\u6846\u67b6\u89e3\u51b3\u4e86\u5956\u52b1\u4fe1\u53f7\u89d2\u8272\u7684\u6a21\u7cca\u6027\uff0c\u4e3a\u5ba1\u8bae\u6811\u641c\u7d22\u9886\u57df\u7684\u7edf\u4e00\u5f62\u5f0f\u5316\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u4ece\u66b4\u529b\u6269\u5c55\u5411\u7b97\u6cd5\u6548\u7387\u7684\u8f6c\u53d8\uff0c\u5e76\u4e3a\u81ea\u4e3b\u81ea\u6539\u8fdb\u4ee3\u7406\u7684\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.09719", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09719", "abs": "https://arxiv.org/abs/2510.09719", "authors": ["Chenxu Wang", "Hao Li", "Yiqun Zhang", "Linyao Chen", "Jianhao Chen", "Ping Jian", "Peng Ye", "Qiaosheng Zhang", "Shuyue Hu"], "title": "ICL-Router: In-Context Learned Model Representations for LLM Routing", "comment": null, "summary": "Large language models (LLMs) often exhibit complementary strengths. Model\nrouting harnesses these strengths by dynamically directing each query to the\nmost suitable model, given a candidate model pool. However, routing performance\nrelies on accurate model representations, and adding new models typically\nrequires retraining, limiting scalability. To address these challenges, we\npropose a novel routing method using in-context vectors to represent model\ncapabilities. The method proceeds in two stages. First, queries are embedded\nand projected into vectors, with a projector and LLM-based router trained to\nreconstruct the original queries, aligning vector representations with the\nrouter's semantic space. Second, each candidate model is profiled on a query\nset, and the router learns -- based on in-context vectors of query and model\nperformance -- to predict whether each model can correctly answer new queries.\nExtensive experiments demonstrate that our method achieves state-of-the-art\nrouting performance in both in-distribution and out-of-distribution tasks.\nMoreover, our method allows for seamless integration of new models without\nretraining the router. The code is available at\nhttps://github.com/lalalamdbf/ICL-Router.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u4e0a\u4e0b\u6587\u5411\u91cf\u8868\u793a\u6a21\u578b\u80fd\u529b\u7684\u65b0\u578b\u8def\u7531\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u52a8\u6001\u67e5\u8be2\u8def\u7531\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u96c6\u6210\u65b0\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u8def\u7531\u65b9\u6cd5\u4f9d\u8d56\u51c6\u786e\u7684\u6a21\u578b\u8868\u793a\uff0c\u6dfb\u52a0\u65b0\u6a21\u578b\u901a\u5e38\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u67e5\u8be2\u5d4c\u5165\u548c\u6295\u5f71\u5230\u5411\u91cf\u7a7a\u95f4\uff0c\u8bad\u7ec3\u6295\u5f71\u5668\u548c\u57fa\u4e8eLLM\u7684\u8def\u7531\u5668\u91cd\u5efa\u67e5\u8be2\uff1b2\uff09\u57fa\u4e8e\u67e5\u8be2\u548c\u6a21\u578b\u6027\u80fd\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\uff0c\u8def\u7531\u5668\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u662f\u5426\u80fd\u6b63\u786e\u56de\u7b54\u65b0\u67e5\u8be2\u3002", "result": "\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u4efb\u52a1\u4e2d\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8def\u7531\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u65e0\u7f1d\u96c6\u6210\u65b0\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e0a\u4e0b\u6587\u5411\u91cf\u6709\u6548\u8868\u793a\u6a21\u578b\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u8def\u7531\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2510.10454", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10454", "abs": "https://arxiv.org/abs/2510.10454", "authors": ["Sihang Zeng", "Yujuan Fu", "Sitong Zhou", "Zixuan Yu", "Lucas Jing Liu", "Jun Wen", "Matthew Thompson", "Ruth Etzioni", "Meliha Yetisgen"], "title": "Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction", "comment": "Accepted by NeurIPS 2025 GenAI4Health Workshop", "summary": "Large language models (LLMs) offer a generalizable approach for modeling\npatient trajectories, but suffer from the long and noisy nature of electronic\nhealth records (EHR) data in temporal reasoning. To address these challenges,\nwe introduce Traj-CoA, a multi-agent system involving chain-of-agents for\npatient trajectory modeling. Traj-CoA employs a chain of worker agents to\nprocess EHR data in manageable chunks sequentially, distilling critical events\ninto a shared long-term memory module, EHRMem, to reduce noise and preserve a\ncomprehensive timeline. A final manager agent synthesizes the worker agents'\nsummary and the extracted timeline in EHRMem to make predictions. In a\nzero-shot one-year lung cancer risk prediction task based on five-year EHR\ndata, Traj-CoA outperforms baselines of four categories. Analysis reveals that\nTraj-CoA exhibits clinically aligned temporal reasoning, establishing it as a\npromisingly robust and generalizable approach for modeling complex patient\ntrajectories.", "AI": {"tldr": "Traj-CoA\u662f\u4e00\u4e2a\u7528\u4e8e\u60a3\u8005\u8f68\u8ff9\u5efa\u6a21\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u94fe\u5f0f\u667a\u80fd\u4f53\u5904\u7406\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\uff0c\u4f7f\u7528\u5171\u4eab\u8bb0\u5fc6\u6a21\u5757\u51cf\u5c11\u566a\u58f0\u5e76\u4fdd\u7559\u65f6\u95f4\u7ebf\uff0c\u5728\u80ba\u764c\u98ce\u9669\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5efa\u6a21\u60a3\u8005\u8f68\u8ff9\u65b9\u9762\u5177\u6709\u901a\u7528\u6027\uff0c\u4f46\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u7684\u957f\u5e8f\u5217\u548c\u566a\u58f0\u7279\u6027\u7ed9\u65f6\u5e8f\u63a8\u7406\u5e26\u6765\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5305\u62ec\u5de5\u4f5c\u667a\u80fd\u4f53\u94fe\u987a\u5e8f\u5904\u7406EHR\u6570\u636e\u5757\uff0c\u5c06\u5173\u952e\u4e8b\u4ef6\u63d0\u53d6\u5230\u5171\u4eab\u957f\u671f\u8bb0\u5fc6\u6a21\u5757EHRMem\u4e2d\uff0c\u6700\u540e\u7531\u7ba1\u7406\u667a\u80fd\u4f53\u7efc\u5408\u603b\u7ed3\u548c\u65f6\u95f4\u7ebf\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u57fa\u4e8e\u4e94\u5e74EHR\u6570\u636e\u7684\u96f6\u6837\u672c\u4e00\u5e74\u80ba\u764c\u98ce\u9669\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cTraj-CoA\u5728\u56db\u7c7b\u57fa\u7ebf\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5206\u6790\u663e\u793a\u5176\u5177\u6709\u4e34\u5e8a\u5bf9\u9f50\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "Traj-CoA\u4e3a\u5efa\u6a21\u590d\u6742\u60a3\u8005\u8f68\u8ff9\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u4e14\u53ef\u63a8\u5e7f\u7684\u65b9\u6cd5\uff0c\u5728\u4e34\u5e8a\u65f6\u5e8f\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.10000", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10000", "abs": "https://arxiv.org/abs/2510.10000", "authors": ["Bach C. Le", "Tung V. Dao", "Binh T. Nguyen", "Hong T. M. Chu"], "title": "Tight Robustness Certificates and Wasserstein Distributional Attacks for Deep Neural Networks", "comment": null, "summary": "Wasserstein distributionally robust optimization (WDRO) provides a framework\nfor adversarial robustness, yet existing methods based on global Lipschitz\ncontinuity or strong duality often yield loose upper bounds or require\nprohibitive computation. In this work, we address these limitations by\nintroducing a primal approach and adopting a notion of exact Lipschitz\ncertificate to tighten this upper bound of WDRO. In addition, we propose a\nnovel Wasserstein distributional attack (WDA) that directly constructs a\ncandidate for the worst-case distribution. Compared to existing point-wise\nattack and its variants, our WDA offers greater flexibility in the number and\nlocation of attack points. In particular, by leveraging the piecewise-affine\nstructure of ReLU networks on their activation cells, our approach results in\nan exact tractable characterization of the corresponding WDRO problem.\nExtensive evaluations demonstrate that our method achieves competitive robust\naccuracy against state-of-the-art baselines while offering tighter certificates\nthan existing methods. Our code is available at\nhttps://github.com/OLab-Repo/WDA", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7cbe\u786eLipschitz\u8bc1\u4e66\u7684Wasserstein\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u539f\u59cb\u65b9\u6cd5\uff0c\u4ee5\u53caWasserstein\u5206\u5e03\u653b\u51fb(WDA)\uff0c\u5728ReLU\u7f51\u7edc\u4e0a\u5b9e\u73b0\u66f4\u7d27\u7684\u9c81\u68d2\u6027\u8bc1\u4e66\u548c\u7ade\u4e89\u6027\u7684\u9c81\u68d2\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5168\u5c40Lipschitz\u8fde\u7eed\u6027\u6216\u5f3a\u5bf9\u5076\u6027\u7684WDRO\u65b9\u6cd5\u5b58\u5728\u4e0a\u754c\u8fc7\u677e\u6216\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7d27\u7684\u9c81\u68d2\u6027\u8bc1\u4e66\u548c\u66f4\u7075\u6d3b\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u539f\u59cb\u65b9\u6cd5\u7ed3\u5408\u7cbe\u786eLipschitz\u8bc1\u4e66\u6765\u6536\u7d27WDRO\u4e0a\u754c\uff0c\u63d0\u51faWasserstein\u5206\u5e03\u653b\u51fb\u76f4\u63a5\u6784\u9020\u6700\u574f\u60c5\u51b5\u5206\u5e03\uff0c\u5229\u7528ReLU\u7f51\u7edc\u5728\u6fc0\u6d3b\u5355\u5143\u4e0a\u7684\u5206\u6bb5\u4eff\u5c04\u7ed3\u6784\u5b9e\u73b0\u7cbe\u786e\u53ef\u5904\u7406\u8868\u5f81\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u9c81\u68d2\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u4f9b\u66f4\u7d27\u7684\u8bc1\u4e66\uff0c\u4e14WDA\u5728\u653b\u51fb\u70b9\u6570\u91cf\u548c\u4f4d\u7f6e\u65b9\u9762\u66f4\u7075\u6d3b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u539f\u59cb\u65b9\u6cd5\u548cWDA\u5728Wasserstein\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u4e2d\u5b9e\u73b0\u4e86\u66f4\u7d27\u7684\u9c81\u68d2\u6027\u8bc1\u4e66\u548c\u66f4\u597d\u7684\u7075\u6d3b\u6027\uff0c\u4e3a\u5bf9\u6297\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.11405", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11405", "abs": "https://arxiv.org/abs/2510.11405", "authors": ["Samuel Oliveira", "Mostafa Tavakkoli Anbarani", "Gregory Beal", "Ilya Kovalenko", "Marcelo Teixeira", "Andr\u00e9 B. Leal", "R\u00f4mulo Meira-G\u00f3es"], "title": "Robust Recovery and Control of Cyber-physical Discrete Event Systems under Actuator Attacks", "comment": "This work has been accepted for publication in the 64th IEEE\n  Conference on Decision and Control (CDC). The final published version will be\n  available on IEEE Xplore", "summary": "Critical real-world applications strongly rely on Cyber-physical systems\n(CPS), but their dependence on communication networks introduces significant\nsecurity risks, as attackers can exploit vulnerabilities to compromise their\nintegrity and availability. This work explores the topic of cybersecurity in\nthe context of CPS modeled as discrete event systems (DES), focusing on\nrecovery strategies following the detection of cyberattacks. Specifically, we\naddress actuator enablement attacks and propose a method that preserves the\nsystem's full valid behavior under normal conditions. Upon detecting an attack,\nour proposed solution aims to guide the system toward a restricted yet robust\nbehavior, ensuring operational continuity and resilience. Additionally, we\nintroduce a property termed AE-robust recoverability, which characterizes the\nnecessary and sufficient conditions for recovering a system from attacks while\npreventing further vulnerabilities. Finally, we showcase the proposed solution\nthrough a case study based on a manufacturing system.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4fe1\u606f\u7269\u7406\u7cfb\u7edf\u4e2d\u6267\u884c\u5668\u4f7f\u80fd\u653b\u51fb\u7684\u6062\u590d\u7b56\u7565\uff0c\u901a\u8fc7\u9650\u5236\u7cfb\u7edf\u884c\u4e3a\u786e\u4fdd\u64cd\u4f5c\u8fde\u7eed\u6027\uff0c\u5e76\u5f15\u5165\u4e86AE-\u9c81\u68d2\u53ef\u6062\u590d\u6027\u6982\u5ff5\u3002", "motivation": "\u4fe1\u606f\u7269\u7406\u7cfb\u7edf\u4f9d\u8d56\u901a\u4fe1\u7f51\u7edc\uff0c\u5b58\u5728\u88ab\u653b\u51fb\u8005\u5229\u7528\u6f0f\u6d1e\u7834\u574f\u7cfb\u7edf\u5b8c\u6574\u6027\u548c\u53ef\u7528\u6027\u7684\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u6709\u6548\u7684\u653b\u51fb\u540e\u6062\u590d\u7b56\u7565\u3002", "method": "\u5c06CPS\u5efa\u6a21\u4e3a\u79bb\u6563\u4e8b\u4ef6\u7cfb\u7edf\uff0c\u9488\u5bf9\u6267\u884c\u5668\u4f7f\u80fd\u653b\u51fb\u63d0\u51fa\u6062\u590d\u65b9\u6cd5\uff0c\u5728\u68c0\u6d4b\u5230\u653b\u51fb\u540e\u5f15\u5bfc\u7cfb\u7edf\u8fdb\u5165\u53d7\u9650\u4f46\u9c81\u68d2\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u63d0\u51fa\u4e86AE-\u9c81\u68d2\u53ef\u6062\u590d\u6027\u6982\u5ff5\uff0c\u7ed9\u51fa\u4e86\u7cfb\u7edf\u4ece\u653b\u51fb\u4e2d\u6062\u590d\u7684\u5fc5\u8981\u548c\u5145\u5206\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u5236\u9020\u7cfb\u7edf\u6848\u4f8b\u9a8c\u8bc1\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u6b63\u5e38\u6761\u4ef6\u4e0b\u4fdd\u6301\u7cfb\u7edf\u5b8c\u6574\u884c\u4e3a\uff0c\u5728\u653b\u51fb\u68c0\u6d4b\u540e\u786e\u4fdd\u64cd\u4f5c\u8fde\u7eed\u6027\uff0c\u4e3aCPS\u7f51\u7edc\u5b89\u5168\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6062\u590d\u673a\u5236\u3002"}}
{"id": "2510.11381", "categories": ["math.OC", "math.DS"], "pdf": "https://arxiv.org/pdf/2510.11381", "abs": "https://arxiv.org/abs/2510.11381", "authors": ["Othman Cherkaoui Dekkaki"], "title": "Optimal Control of a Bioeconomic Crop-Energy System with Energy Reinvestment", "comment": "Submitted for Journal of Mathematical Modeling", "summary": "We develop an optimal control model for allocating agricultural crop residues\nbetween bioenergy production and soil fertility restoration. The system\ncaptures a novel circular feedback: a fraction of cumulative energy output is\nreinvested into soil productivity, linking energy use with ecological\nregeneration. The dynamics are governed by a nonlinear three-state system\ndescribing soil fertility, residue biomass, and accumulated energy, with a\nsingle control representing the proportion of biomass diverted to energy. The\nobjective is to maximize a discounted net benefit that accounts for energy\nrevenue, soil value, and operational costs. We apply the Pontryagin Maximum\nPrinciple in current-value form to derive necessary optimality conditions and\ncharacterize the structure of optimal controls. Numerical simulations based on\ndirect optimization reveal interior and switching regimes, and show how\nplanning horizon and reinvestment efficiency influence optimal strategies. The\nresults highlight the strategic role of energy reinvestment in achieving\nsustainable residue management.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u519c\u4e1a\u4f5c\u7269\u6b8b\u832c\u5728\u751f\u7269\u80fd\u6e90\u751f\u4ea7\u548c\u571f\u58e4\u80a5\u529b\u6062\u590d\u4e4b\u95f4\u5206\u914d\u7684\u6700\u4f18\u63a7\u5236\u6a21\u578b\uff0c\u5305\u542b\u80fd\u6e90\u4ea7\u51fa\u518d\u6295\u8d44\u4e8e\u571f\u58e4\u751f\u4ea7\u529b\u7684\u5faa\u73af\u53cd\u9988\u673a\u5236\u3002", "motivation": "\u89e3\u51b3\u519c\u4e1a\u6b8b\u832c\u7ba1\u7406\u4e2d\u7684\u53ef\u6301\u7eed\u6027\u95ee\u9898\uff0c\u5e73\u8861\u751f\u7269\u80fd\u6e90\u751f\u4ea7\u548c\u571f\u58e4\u751f\u6001\u6062\u590d\u7684\u53cc\u91cd\u76ee\u6807\uff0c\u901a\u8fc7\u80fd\u6e90\u518d\u6295\u8d44\u673a\u5236\u8fde\u63a5\u80fd\u6e90\u4f7f\u7528\u4e0e\u751f\u6001\u518d\u751f\u3002", "method": "\u91c7\u7528\u975e\u7ebf\u6027\u4e09\u72b6\u6001\u7cfb\u7edf\u63cf\u8ff0\u571f\u58e4\u80a5\u529b\u3001\u6b8b\u832c\u751f\u7269\u91cf\u548c\u7d2f\u79ef\u80fd\u91cf\uff0c\u4f7f\u7528\u5e9e\u7279\u91cc\u4e9a\u91d1\u6700\u5927\u503c\u539f\u7406\u63a8\u5bfc\u6700\u4f18\u6027\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u8fdb\u884c\u6570\u503c\u6a21\u62df\u3002", "result": "\u6570\u503c\u6a21\u62df\u63ed\u793a\u4e86\u5185\u90e8\u548c\u5207\u6362\u63a7\u5236\u673a\u5236\uff0c\u663e\u793a\u89c4\u5212\u671f\u9650\u548c\u518d\u6295\u8d44\u6548\u7387\u5982\u4f55\u5f71\u54cd\u6700\u4f18\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u80fd\u6e90\u518d\u6295\u8d44\u5728\u53ef\u6301\u7eed\u6b8b\u832c\u7ba1\u7406\u4e2d\u7684\u6218\u7565\u4f5c\u7528\u3002", "conclusion": "\u80fd\u6e90\u518d\u6295\u8d44\u662f\u5b9e\u73b0\u53ef\u6301\u7eed\u519c\u4e1a\u6b8b\u832c\u7ba1\u7406\u7684\u5173\u952e\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u5e73\u8861\u80fd\u6e90\u751f\u4ea7\u548c\u571f\u58e4\u751f\u6001\u6062\u590d\u7684\u53cc\u91cd\u76ee\u6807\u3002"}}
{"id": "2510.09994", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09994", "abs": "https://arxiv.org/abs/2510.09994", "authors": ["Yimin Xiao", "Yongle Zhang", "Dayeon Ki", "Calvin Bao", "Marianna J. Martindale", "Charlotte Vaughn", "Ge Gao", "Marine Carpuat"], "title": "Toward Machine Translation Literacy: How Lay Users Perceive and Rely on Imperfect Translations", "comment": "EMNLP 2025", "summary": "As Machine Translation (MT) becomes increasingly commonplace, understanding\nhow the general public perceives and relies on imperfect MT is crucial for\ncontextualizing MT research in real-world applications. We present a human\nstudy conducted in a public museum (n=452), investigating how fluency and\nadequacy errors impact bilingual and non-bilingual users' reliance on MT during\ncasual use. Our findings reveal that non-bilingual users often over-rely on MT\ndue to a lack of evaluation strategies and alternatives, while experiencing the\nimpact of errors can prompt users to reassess future reliance. This highlights\nthe need for MT evaluation and NLP explanation techniques to promote not only\nMT quality, but also MT literacy among its users.", "AI": {"tldr": "\u7814\u7a76\u8c03\u67e5\u4e86\u5728\u535a\u7269\u9986\u73af\u5883\u4e2d\uff0c\u673a\u5668\u7ffb\u8bd1\u7684\u6d41\u7545\u6027\u548c\u5145\u5206\u6027\u9519\u8bef\u5982\u4f55\u5f71\u54cd\u53cc\u8bed\u548c\u975e\u53cc\u8bed\u7528\u6237\u5bf9MT\u7684\u4f9d\u8d56\u7a0b\u5ea6\u3002\u53d1\u73b0\u975e\u53cc\u8bed\u7528\u6237\u7531\u4e8e\u7f3a\u4e4f\u8bc4\u4f30\u7b56\u7565\u800c\u8fc7\u5ea6\u4f9d\u8d56MT\uff0c\u4f46\u9519\u8bef\u4f53\u9a8c\u4f1a\u4fc3\u4f7f\u7528\u6237\u91cd\u65b0\u8bc4\u4f30\u4f9d\u8d56\u3002", "motivation": "\u968f\u7740\u673a\u5668\u7ffb\u8bd1\u65e5\u76ca\u666e\u53ca\uff0c\u4e86\u89e3\u516c\u4f17\u5982\u4f55\u611f\u77e5\u548c\u4f9d\u8d56\u4e0d\u5b8c\u7f8e\u7684MT\u5bf9\u4e8e\u5c06MT\u7814\u7a76\u7f6e\u4e8e\u5b9e\u9645\u5e94\u7528\u80cc\u666f\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5728\u516c\u5171\u535a\u7269\u9986\u8fdb\u884c\u4eba\u7c7b\u7814\u7a76\uff08n=452\uff09\uff0c\u8c03\u67e5\u6d41\u7545\u6027\u548c\u5145\u5206\u6027\u9519\u8bef\u5982\u4f55\u5f71\u54cd\u53cc\u8bed\u548c\u975e\u53cc\u8bed\u7528\u6237\u5728\u4f11\u95f2\u4f7f\u7528\u4e2d\u5bf9MT\u7684\u4f9d\u8d56\u3002", "result": "\u975e\u53cc\u8bed\u7528\u6237\u7531\u4e8e\u7f3a\u4e4f\u8bc4\u4f30\u7b56\u7565\u548c\u66ff\u4ee3\u65b9\u6848\u800c\u7ecf\u5e38\u8fc7\u5ea6\u4f9d\u8d56MT\uff0c\u4f46\u7ecf\u5386\u9519\u8bef\u5f71\u54cd\u4f1a\u4fc3\u4f7f\u7528\u6237\u91cd\u65b0\u8bc4\u4f30\u672a\u6765\u7684\u4f9d\u8d56\u7a0b\u5ea6\u3002", "conclusion": "MT\u8bc4\u4f30\u548cNLP\u89e3\u91ca\u6280\u672f\u4e0d\u4ec5\u9700\u8981\u63d0\u5347MT\u8d28\u91cf\uff0c\u8fd8\u9700\u8981\u4fc3\u8fdb\u7528\u6237\u5bf9MT\u7684\u8ba4\u77e5\u7d20\u517b\u3002"}}
{"id": "2510.09723", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05 (Primary), 68T50", "I.2.6; I.2.7; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.09723", "abs": "https://arxiv.org/abs/2510.09723", "authors": ["Gregory D. Baker"], "title": "It's 2025 -- Narrative Learning is the new baseline to beat for explainable machine learning", "comment": "18 pages, 5 figures", "summary": "In this paper, we introduce Narrative Learning, a methodology where models\nare defined entirely in natural language and iteratively refine their\nclassification criteria using explanatory prompts rather than traditional\nnumerical optimisation. We report on experiments to evaluate the accuracy and\npotential of this approach using 3 synthetic and 3 natural datasets and compare\nthem against 7 baseline explainable machine learning models. We demonstrate\nthat on 5 out of 6 of these datasets, Narrative Learning became more accurate\nthan the baseline explainable models in 2025 or earlier because of improvements\nin language models. We also report on trends in the lexicostatistics of these\nmodels' outputs as a proxy for the comprehensibility of the explanations.", "AI": {"tldr": "Narrative Learning\u662f\u4e00\u79cd\u5b8c\u5168\u7528\u81ea\u7136\u8bed\u8a00\u5b9a\u4e49\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u91ca\u6027\u63d0\u793a\u800c\u975e\u4f20\u7edf\u6570\u503c\u4f18\u5316\u6765\u8fed\u4ee3\u6539\u8fdb\u5206\u7c7b\u6807\u51c6\u3002", "motivation": "\u63a2\u7d22\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5b9a\u4e49\u6a21\u578b\u5e76\u901a\u8fc7\u89e3\u91ca\u6027\u63d0\u793a\u8fed\u4ee3\u6539\u8fdb\u5206\u7c7b\u6807\u51c6\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u66ff\u4ee3\u4f20\u7edf\u7684\u6570\u503c\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u4f7f\u75283\u4e2a\u5408\u6210\u6570\u636e\u96c6\u548c3\u4e2a\u81ea\u7136\u6570\u636e\u96c6\uff0c\u4e0e7\u4e2a\u57fa\u7ebf\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u901a\u8fc7\u89e3\u91ca\u6027\u63d0\u793a\u8fed\u4ee3\u6539\u8fdb\u5206\u7c7b\u6807\u51c6\u3002", "result": "\u57286\u4e2a\u6570\u636e\u96c6\u4e2d\u76845\u4e2a\u4e0a\uff0cNarrative Learning\u7531\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u6539\u8fdb\uff0c\u57282025\u5e74\u6216\u66f4\u65e9\u65f6\u53d8\u5f97\u6bd4\u57fa\u7ebf\u53ef\u89e3\u91ca\u6a21\u578b\u66f4\u51c6\u786e\u3002", "conclusion": "Narrative Learning\u5c55\u793a\u4e86\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5b9a\u4e49\u6a21\u578b\u5e76\u901a\u8fc7\u89e3\u91ca\u6027\u63d0\u793a\u8fed\u4ee3\u6539\u8fdb\u7684\u6f5c\u529b\uff0c\u968f\u7740\u8bed\u8a00\u6a21\u578b\u7684\u6539\u8fdb\uff0c\u5176\u51c6\u786e\u6027\u6709\u671b\u8d85\u8d8a\u4f20\u7edf\u53ef\u89e3\u91ca\u6a21\u578b\u3002"}}
{"id": "2510.10461", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10461", "abs": "https://arxiv.org/abs/2510.10461", "authors": ["Hongjie Zheng", "Zesheng Shi", "Ping Yi"], "title": "MedCoAct: Confidence-Aware Multi-Agent Collaboration for Complete Clinical Decision", "comment": null, "summary": "Autonomous agents utilizing Large Language Models (LLMs) have demonstrated\nremarkable capabilities in isolated medical tasks like diagnosis and image\nanalysis, but struggle with integrated clinical workflows that connect\ndiagnostic reasoning and medication decisions. We identify a core limitation:\nexisting medical AI systems process tasks in isolation without the\ncross-validation and knowledge integration found in clinical teams, reducing\ntheir effectiveness in real-world healthcare scenarios. To transform the\nisolation paradigm into a collaborative approach, we propose MedCoAct, a\nconfidence-aware multi-agent framework that simulates clinical collaboration by\nintegrating specialized doctor and pharmacist agents, and present a benchmark,\nDrugCareQA, to evaluate medical AI capabilities in integrated diagnosis and\ntreatment workflows. Our results demonstrate that MedCoAct achieves 67.58\\%\ndiagnostic accuracy and 67.58\\% medication recommendation accuracy,\noutperforming single agent framework by 7.04\\% and 7.08\\% respectively. This\ncollaborative approach generalizes well across diverse medical domains, proving\nespecially effective for telemedicine consultations and routine clinical\nscenarios, while providing interpretable decision-making pathways.", "AI": {"tldr": "\u63d0\u51fa\u4e86MedCoAct\u6846\u67b6\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6a21\u62df\u4e34\u5e8a\u56e2\u961f\u5408\u4f5c\uff0c\u5728\u96c6\u6210\u8bca\u65ad\u548c\u6cbb\u7597\u5de5\u4f5c\u6d41\u4e2d\u663e\u8457\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u7cfb\u7edf", "motivation": "\u73b0\u6709\u533b\u7597AI\u7cfb\u7edf\u5728\u5904\u7406\u5b64\u7acb\u4efb\u52a1\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u8bca\u65ad\u63a8\u7406\u548c\u7528\u836f\u51b3\u7b56\u6574\u5408\u7684\u4e34\u5e8a\u5de5\u4f5c\u6d41\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u4e34\u5e8a\u56e2\u961f\u4e2d\u7684\u4ea4\u53c9\u9a8c\u8bc1\u548c\u77e5\u8bc6\u6574\u5408", "method": "MedCoAct\u6846\u67b6\u6574\u5408\u4e13\u4e1a\u533b\u751f\u548c\u836f\u5242\u5e08\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u534f\u4f5c\u673a\u5236\uff0c\u5e76\u521b\u5efaDrugCareQA\u57fa\u51c6\u6765\u8bc4\u4f30\u96c6\u6210\u8bca\u65ad\u548c\u6cbb\u7597\u80fd\u529b", "result": "MedCoAct\u8fbe\u523067.58%\u7684\u8bca\u65ad\u51c6\u786e\u7387\u548c67.58%\u7684\u7528\u836f\u63a8\u8350\u51c6\u786e\u7387\uff0c\u5206\u522b\u6bd4\u5355\u667a\u80fd\u4f53\u6846\u67b6\u63d0\u9ad87.04%\u548c7.08%\uff0c\u5728\u8fdc\u7a0b\u533b\u7597\u548c\u5e38\u89c4\u4e34\u5e8a\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02", "conclusion": "\u534f\u4f5c\u65b9\u6cd5\u5728\u4e0d\u540c\u533b\u7597\u9886\u57df\u6cdb\u5316\u826f\u597d\uff0c\u4e3a\u73b0\u5b9e\u533b\u7597\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u8def\u5f84\uff0c\u5c06\u5b64\u7acb\u8303\u5f0f\u8f6c\u53d8\u4e3a\u534f\u4f5c\u8303\u5f0f"}}
{"id": "2510.10029", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10029", "abs": "https://arxiv.org/abs/2510.10029", "authors": ["Ruoxing Yang"], "title": "Experience-Efficient Model-Free Deep Reinforcement Learning Using Pre-Training", "comment": null, "summary": "We introduce PPOPT - Proximal Policy Optimization using Pretraining, a novel,\nmodel-free deep-reinforcement-learning algorithm that leverages pretraining to\nachieve high training efficiency and stability on very small training samples\nin physics-based environments. Reinforcement learning agents typically rely on\nlarge samples of environment interactions to learn a policy. However, frequent\ninteractions with a (computer-simulated) environment may incur high\ncomputational costs, especially when the environment is complex. Our main\ninnovation is a new policy neural network architecture that consists of a\npretrained neural network middle section sandwiched between two fully-connected\nnetworks. Pretraining part of the network on a different environment with\nsimilar physics will help the agent learn the target environment with high\nefficiency because it will leverage a general understanding of the\ntransferrable physics characteristics from the pretraining environment. We\ndemonstrate that PPOPT outperforms baseline classic PPO on small training\nsamples both in terms of rewards gained and general training stability. While\nPPOPT underperforms against classic model-based methods such as DYNA DDPG, the\nmodel-free nature of PPOPT allows it to train in significantly less time than\nits model-based counterparts. Finally, we present our implementation of PPOPT\nas open-source software, available at github.com/Davidrxyang/PPOPT.", "AI": {"tldr": "PPOPT\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u6a21\u578b\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u5728\u7269\u7406\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5c0f\u6837\u672c\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u5927\u91cf\u73af\u5883\u4ea4\u4e92\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u73af\u5883\u4e2d\u3002PPOPT\u65e8\u5728\u901a\u8fc7\u9884\u8bad\u7ec3\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u51cf\u5c11\u73af\u5883\u4ea4\u4e92\u9700\u6c42\u3002", "method": "\u91c7\u7528\u65b0\u578b\u7b56\u7565\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u4e2d\u95f4\u90e8\u5206\u4e3a\u9884\u8bad\u7ec3\u7f51\u7edc\uff0c\u4e24\u7aef\u4e3a\u5168\u8fde\u63a5\u7f51\u7edc\u3002\u5728\u5177\u6709\u76f8\u4f3c\u7269\u7406\u7279\u6027\u7684\u4e0d\u540c\u73af\u5883\u4e2d\u9884\u8bad\u7ec3\u90e8\u5206\u7f51\u7edc\uff0c\u4ee5\u5229\u7528\u53ef\u8f6c\u79fb\u7684\u7269\u7406\u7279\u6027\u7406\u89e3\u3002", "result": "PPOPT\u5728\u5c0f\u6837\u672c\u8bad\u7ec3\u4e2d\u4f18\u4e8e\u7ecf\u5178PPO\uff0c\u5728\u5956\u52b1\u83b7\u53d6\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002\u867d\u7136\u4e0d\u5982DYNA DDPG\u7b49\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4f46\u65e0\u6a21\u578b\u7279\u6027\u4f7f\u5176\u8bad\u7ec3\u65f6\u95f4\u663e\u8457\u66f4\u77ed\u3002", "conclusion": "PPOPT\u901a\u8fc7\u9884\u8bad\u7ec3\u6709\u6548\u63d0\u9ad8\u4e86\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u5728\u5c0f\u6837\u672c\u73af\u5883\u4e2d\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u4e3a\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.11413", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11413", "abs": "https://arxiv.org/abs/2510.11413", "authors": ["Sofia Girardello", "Giulia Michieletto", "Angelo Cenedese", "Antonio Franchi", "Chiara Gabellieri"], "title": "Trajectory control of a suspended load with non-stopping flying carriers", "comment": null, "summary": "This paper presents the first closed-loop control framework for cooperative\npayload transportation with non-stopping flying carriers. Building upon\ngrasp-matrix formulations and internal force redundancy, we propose a feedback\nwrench controller that actively regulates the payload's pose while an\noptimization layer dynamically shapes internal-force oscillations to guarantee\npersistent carrier motion. Preliminary experimental results on multirotor UAVs\nvalidate the model assumptions, and numerical simulations demonstrate that the\nmethod successfully prevents carrier stagnation, achieves accurate load\ntracking, and generates physically feasible trajectories with smooth velocity\nprofiles. The proposed framework not only advances the state of the art but\nalso offers a reliable, versatile solution for future real-world applications\nrequiring load transportation by coordinated non-stopping flying carriers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u975e\u505c\u98de\u8f7d\u5177\u534f\u540c\u8f7d\u8377\u8fd0\u8f93\u7684\u95ed\u73af\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u9988\u529b\u77e9\u63a7\u5236\u5668\u8c03\u8282\u8f7d\u8377\u59ff\u6001\uff0c\u4f18\u5316\u5c42\u52a8\u6001\u8c03\u6574\u5185\u529b\u632f\u8361\u4fdd\u8bc1\u8f7d\u5177\u6301\u7eed\u8fd0\u52a8\u3002", "motivation": "\u89e3\u51b3\u591a\u98de\u884c\u5668\u534f\u540c\u8fd0\u8f93\u4e2d\u8f7d\u5177\u505c\u6ede\u95ee\u9898\uff0c\u5b9e\u73b0\u975e\u505c\u98de\u8f7d\u5177\u7684\u53ef\u9760\u8f7d\u8377\u8fd0\u8f93\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u6293\u53d6\u77e9\u9635\u516c\u5f0f\u548c\u5185\u529b\u5197\u4f59\uff0c\u8bbe\u8ba1\u53cd\u9988\u529b\u77e9\u63a7\u5236\u5668\u8c03\u8282\u8f7d\u8377\u4f4d\u59ff\uff0c\u901a\u8fc7\u4f18\u5316\u5c42\u52a8\u6001\u5851\u9020\u5185\u529b\u632f\u8361\u6765\u4fdd\u8bc1\u8f7d\u5177\u6301\u7eed\u8fd0\u52a8\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u5047\u8bbe\uff0c\u6570\u503c\u6a21\u62df\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u9632\u6b62\u8f7d\u5177\u505c\u6ede\u3001\u5b9e\u73b0\u7cbe\u786e\u8f7d\u8377\u8ddf\u8e2a\uff0c\u5e76\u751f\u6210\u7269\u7406\u53ef\u884c\u4e14\u901f\u5ea6\u5e73\u6ed1\u7684\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63a8\u8fdb\u4e86\u6280\u672f\u524d\u6cbf\uff0c\u8fd8\u4e3a\u9700\u8981\u534f\u8c03\u975e\u505c\u98de\u8f7d\u5177\u8fdb\u884c\u8f7d\u8377\u8fd0\u8f93\u7684\u672a\u6765\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.11396", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.11396", "abs": "https://arxiv.org/abs/2510.11396", "authors": ["Achraf Bouhmady", "Othman Cherkaoui Dekkaki"], "title": "Hamilton-Jacobi Reachability for Viability Analysis of Constrained Waste-to-Energy Systems under Adversarial Uncertainty", "comment": "Submitted", "summary": "This paper investigates the problem of maintaining the safe operation of\nWaste-to-Energy (WtE) systems under operational constraints and uncertain waste\ninflows. We model this as a robust viability problem, formulated as a zero-sum\ndifferential game between a control policy and an adversarial disturbance.\nWithin a Hamilton-Jacobi framework, the viability kernel is characterized as\nthe zero sublevel set of a value function satisfying a constrained\nHamilton-Jacobi-Bellman (HJB) equation in the viscosity sense. This formulation\nprovides formal guarantees for ensuring that system trajectories remain within\nprescribed operational limits under worst-case scenarios. Compared to existing\nviability studies, this work introduces a rigorous HJB-based characterization\nexplicitly incorporating uncertainty, tailored to nonlinear WtE dynamics. A\nnumerical scheme based on the Local Lax-Friedrichs method is employed to\napproximate the viability kernel. Numerical experiments illustrate how\nincreasing inflow uncertainty significantly reduces the viability domain,\nshrinking the safe operating envelope. The proposed method is computationally\ntractable for systems of moderate dimension and offers a basis for synthesizing\nrobust control policies, contributing to the design of resilient and\nsustainable WtE infrastructures.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5728\u8fd0\u884c\u7ea6\u675f\u548c\u4e0d\u786e\u5b9a\u5e9f\u7269\u8f93\u5165\u4e0b\u7ef4\u6301\u5e9f\u7269\u8f6c\u80fd\u6e90\u7cfb\u7edf\u5b89\u5168\u8fd0\u884c\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u9c81\u68d2\u751f\u5b58\u6027\u5206\u6790\u548c\u54c8\u5bc6\u987f-\u96c5\u53ef\u6bd4\u6846\u67b6\u63d0\u4f9b\u5f62\u5f0f\u5316\u4fdd\u8bc1\u3002", "motivation": "\u5e9f\u7269\u8f6c\u80fd\u6e90\u7cfb\u7edf\u9762\u4e34\u8fd0\u884c\u7ea6\u675f\u548c\u4e0d\u786e\u5b9a\u5e9f\u7269\u8f93\u5165\u7684\u6311\u6218\uff0c\u9700\u8981\u786e\u4fdd\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7cfb\u7edf\u8f68\u8ff9\u4ecd\u80fd\u4fdd\u6301\u5728\u89c4\u5b9a\u7684\u8fd0\u884c\u9650\u5236\u5185\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u96f6\u548c\u5fae\u5206\u535a\u5f08\uff0c\u4f7f\u7528\u54c8\u5bc6\u987f-\u96c5\u53ef\u6bd4-\u8d1d\u5c14\u66fc\u65b9\u7a0b\u5728\u7c98\u5ea6\u610f\u4e49\u4e0b\u63cf\u8ff0\u751f\u5b58\u6838\uff0c\u5e76\u91c7\u7528\u5c40\u90e8Lax-Friedrichs\u65b9\u6cd5\u8fdb\u884c\u6570\u503c\u8fd1\u4f3c\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u589e\u52a0\u8f93\u5165\u4e0d\u786e\u5b9a\u6027\u4f1a\u663e\u8457\u51cf\u5c0f\u751f\u5b58\u57df\uff0c\u7f29\u5c0f\u5b89\u5168\u8fd0\u884c\u8303\u56f4\u3002\u8be5\u65b9\u6cd5\u5728\u4e2d\u7b49\u7ef4\u5ea6\u7cfb\u7edf\u4e2d\u8ba1\u7b97\u53ef\u884c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u5408\u6210\u9c81\u68d2\u63a7\u5236\u7b56\u7565\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u8bbe\u8ba1\u5177\u6709\u5f39\u6027\u548c\u53ef\u6301\u7eed\u6027\u7684\u5e9f\u7269\u8f6c\u80fd\u6e90\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2510.10003", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.10003", "abs": "https://arxiv.org/abs/2510.10003", "authors": ["Jianjin Wang", "Runsong Zhao", "Xiaoqian Liu", "Yuan Ge", "Ziqiang Xu", "Tong Xiao", "Shengxiang Gao", "Zhengtao Yu", "Jingbo Zhu"], "title": "MTP-S2UT: Enhancing Speech-to-Speech Translation Quality with Multi-token Prediction", "comment": "Copyright 2026 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Current direct speech-to-speech translation methods predominantly employ\nspeech tokens as intermediate representations. However, a single speech token\nis not dense in semantics, so we generally need multiple tokens to express a\ncomplete semantic unit. To address this limitation, we introduce multi-token\nprediction (MTP) loss into speech-to-unit translation (S2UT) models, enabling\nmodels to predict multiple subsequent tokens at each position, thereby\ncapturing more complete semantics and enhancing information density per\nposition. Initial MTP implementations apply the loss at the final layer, which\nimproves output representation but initiates information enrichment too late.\nWe hypothesize that advancing the information enrichment process to\nintermediate layers can achieve earlier and more effective enhancement of\nhidden representation. Consequently, we propose MTP-S2UT loss, applying MTP\nloss to hidden representation where CTC loss is computed. Experiments\ndemonstrate that all MTP loss variants consistently improve the quality of S2UT\ntranslation, with MTP-S2UT achieving the best performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u8bed\u97f3\u5230\u5355\u5143\u7ffb\u8bd1(S2UT)\u6a21\u578b\u4e2d\u5f15\u5165\u591a\u4ee4\u724c\u9884\u6d4b(MTP)\u635f\u5931\uff0c\u901a\u8fc7\u5728\u4e2d\u95f4\u5c42\u5e94\u7528MTP\u635f\u5931\u6765\u589e\u5f3a\u9690\u85cf\u8868\u793a\u7684\u4fe1\u606f\u5bc6\u5ea6\uff0c\u4ece\u800c\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\u65b9\u6cd5\u4f7f\u7528\u8bed\u97f3\u4ee4\u724c\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u4f46\u5355\u4e2a\u4ee4\u724c\u8bed\u4e49\u5bc6\u5ea6\u4e0d\u8db3\uff0c\u9700\u8981\u591a\u4e2a\u4ee4\u724c\u624d\u80fd\u8868\u8fbe\u5b8c\u6574\u8bed\u4e49\u5355\u5143\u3002", "method": "\u63d0\u51faMTP-S2UT\u635f\u5931\uff0c\u5728\u8ba1\u7b97CTC\u635f\u5931\u7684\u9690\u85cf\u8868\u793a\u5c42\u5e94\u7528\u591a\u4ee4\u724c\u9884\u6d4b\u635f\u5931\uff0c\u4f7f\u6a21\u578b\u80fd\u5728\u6bcf\u4e2a\u4f4d\u7f6e\u9884\u6d4b\u591a\u4e2a\u540e\u7eed\u4ee4\u724c\uff0c\u63d0\u524d\u589e\u5f3a\u4fe1\u606f\u5bc6\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6240\u6709MTP\u635f\u5931\u53d8\u4f53\u90fd\u80fd\u63d0\u5347S2UT\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5176\u4e2dMTP-S2UT\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u5728\u4e2d\u95f4\u5c42\u5e94\u7528MTP\u635f\u5931\u80fd\u66f4\u65e9\u3001\u66f4\u6709\u6548\u5730\u589e\u5f3a\u9690\u85cf\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\u6027\u80fd\u3002"}}
{"id": "2510.09732", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09732", "abs": "https://arxiv.org/abs/2510.09732", "authors": ["P. van Oerle", "R. H. Bemthuis", "F. A. Bukhsh"], "title": "Evaluating LLM-Based Process Explanations under Progressive Behavioral-Input Reduction", "comment": "12 pages, 2 figures, 3 tables; to appear in Enterprise Design,\n  Operations, and Computing. EDOC 2025 Workshops, Lecture Notes in Business\n  Information Processing (LNBIP), Springer, 2025. Part of 29th International\n  Conference on Enterprise Design, Operations, and Computing (EDOC)", "summary": "Large Language Models (LLMs) are increasingly used to generate textual\nexplanations of process models discovered from event logs. Producing\nexplanations from large behavioral abstractions (e.g., directly-follows graphs\nor Petri nets) can be computationally expensive. This paper reports an\nexploratory evaluation of explanation quality under progressive\nbehavioral-input reduction, where models are discovered from progressively\nsmaller prefixes of a fixed log. Our pipeline (i) discovers models at multiple\ninput sizes, (ii) prompts an LLM to generate explanations, and (iii) uses a\nsecond LLM to assess completeness, bottleneck identification, and suggested\nimprovements. On synthetic logs, explanation quality is largely preserved under\nmoderate reduction, indicating a practical cost-quality trade-off. The study is\nexploratory, as the scores are LLM-based (comparative signals rather than\nground truth) and the data are synthetic. The results suggest a path toward\nmore computationally efficient, LLM-assisted process analysis in\nresource-constrained settings.", "AI": {"tldr": "\u8bc4\u4f30\u5728\u9010\u6b65\u51cf\u5c11\u884c\u4e3a\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\uff0cLLM\u751f\u6210\u8fc7\u7a0b\u6a21\u578b\u89e3\u91ca\u7684\u8d28\u91cf\uff0c\u53d1\u73b0\u9002\u5ea6\u51cf\u5c11\u8f93\u5165\u65f6\u89e3\u91ca\u8d28\u91cf\u57fa\u672c\u4fdd\u6301\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u8fc7\u7a0b\u5206\u6790\u65b9\u6cd5\u3002", "motivation": "\u4f7f\u7528LLM\u751f\u6210\u8fc7\u7a0b\u6a21\u578b\u89e3\u91ca\u65f6\uff0c\u76f4\u63a5\u4ece\u5927\u578b\u884c\u4e3a\u62bd\u8c61\uff08\u5982\u76f4\u63a5\u8ddf\u968f\u56fe\u6216Petri\u7f51\uff09\u751f\u6210\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u63a2\u7d22\u5728\u51cf\u5c11\u8f93\u5165\u60c5\u51b5\u4e0b\u7684\u89e3\u91ca\u8d28\u91cf\u3002", "method": "\u5efa\u7acb\u6d41\u6c34\u7ebf\uff1a(i)\u4ece\u56fa\u5b9a\u65e5\u5fd7\u7684\u9010\u6b65\u7f29\u5c0f\u524d\u7f00\u4e2d\u53d1\u73b0\u6a21\u578b\uff0c(ii)\u63d0\u793aLLM\u751f\u6210\u89e3\u91ca\uff0c(iii)\u4f7f\u7528\u7b2c\u4e8c\u4e2aLLM\u8bc4\u4f30\u5b8c\u6574\u6027\u3001\u74f6\u9888\u8bc6\u522b\u548c\u5efa\u8bae\u6539\u8fdb\u3002", "result": "\u5728\u5408\u6210\u65e5\u5fd7\u4e0a\uff0c\u9002\u5ea6\u51cf\u5c11\u8f93\u5165\u65f6\u89e3\u91ca\u8d28\u91cf\u57fa\u672c\u4fdd\u6301\uff0c\u8868\u660e\u5b58\u5728\u5b9e\u7528\u7684\u6210\u672c-\u8d28\u91cf\u6743\u8861\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u3001LLM\u8f85\u52a9\u8fc7\u7a0b\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u8def\u5f84\uff0c\u4f46\u9700\u6ce8\u610f\u8bc4\u4f30\u57fa\u4e8eLLM\u8bc4\u5206\u4e14\u6570\u636e\u4e3a\u5408\u6210\u6570\u636e\u3002"}}
{"id": "2510.10494", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10494", "abs": "https://arxiv.org/abs/2510.10494", "authors": ["Martina G. Vilas", "Safoora Yousefi", "Besmira Nushi", "Eric Horvitz", "Vidhisha Balachandran"], "title": "Tracing the Traces: Latent Temporal Signals for Efficient and Accurate Reasoning", "comment": null, "summary": "Reasoning models improve their problem-solving ability through inference-time\nscaling, allocating more compute via longer token budgets. Identifying which\nreasoning traces are likely to succeed remains a key opportunity: reliably\npredicting productive paths can substantially reduce wasted computation and\nimprove overall efficiency. We introduce Latent-Trajectory signals that\ncharacterize the temporal evolution of a model's internal representations\nduring the generation of intermediate reasoning tokens. By measuring the\noverall change in latent representations between the start and end of\nreasoning, the change accumulated across intermediate steps, and the extent to\nwhich these changes advance toward the final state, we show that these signals\npredict solution accuracy more reliably than both cross-layer metrics and\noutput-based confidence measures. When used to guide answer selection across\nmultiple sampled generations, Latent-Trajectory signals make test-time scaling\nmore effective and efficient than majority voting, reducing token usage by up\nto 70% while preserving and even improving accuracy by 2.6% on average.\nMoreover, these predictive signals often emerge early in the reasoning trace,\nenabling early selection and allocation of compute to the most promising\ncandidates. Our findings contribute not only practical strategies for\ninference-time efficiency, but also a deeper interpretability perspective on\nhow reasoning processes are represented and differentiated in latent space.", "AI": {"tldr": "\u63d0\u51faLatent-Trajectory\u4fe1\u53f7\u6765\u9884\u6d4b\u63a8\u7406\u8fc7\u7a0b\u6210\u529f\u7387\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5185\u90e8\u8868\u5f81\u7684\u65f6\u95f4\u6f14\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u5e76\u51cf\u5c11\u8ba1\u7b97\u6d6a\u8d39\u3002", "motivation": "\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u589e\u52a0\u8ba1\u7b97\u8d44\u6e90\u6765\u63d0\u5347\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u4f46\u8bc6\u522b\u54ea\u4e9b\u63a8\u7406\u8def\u5f84\u53ef\u80fd\u6210\u529f\u662f\u5173\u952e\u673a\u4f1a\uff0c\u53ef\u9760\u9884\u6d4b\u6709\u6548\u8def\u5f84\u53ef\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u6d6a\u8d39\u5e76\u63d0\u9ad8\u6574\u4f53\u6548\u7387\u3002", "method": "\u5f15\u5165Latent-Trajectory\u4fe1\u53f7\uff0c\u901a\u8fc7\u6d4b\u91cf\u63a8\u7406\u5f00\u59cb\u548c\u7ed3\u675f\u65f6\u6f5c\u5728\u8868\u5f81\u7684\u603b\u4f53\u53d8\u5316\u3001\u4e2d\u95f4\u6b65\u9aa4\u7d2f\u79ef\u7684\u53d8\u5316\u4ee5\u53ca\u8fd9\u4e9b\u53d8\u5316\u5411\u6700\u7ec8\u72b6\u6001\u63a8\u8fdb\u7684\u7a0b\u5ea6\u6765\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u51c6\u786e\u6027\u3002", "result": "Latent-Trajectory\u4fe1\u53f7\u6bd4\u8de8\u5c42\u6307\u6807\u548c\u57fa\u4e8e\u8f93\u51fa\u7684\u7f6e\u4fe1\u5ea6\u6d4b\u91cf\u66f4\u53ef\u9760\u5730\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u51c6\u786e\u6027\uff0c\u5728\u591a\u4e2a\u91c7\u6837\u751f\u6210\u4e2d\u6307\u5bfc\u7b54\u6848\u9009\u62e9\u65f6\uff0c\u53ef\u4f7f\u6d4b\u8bd5\u65f6\u6269\u5c55\u6bd4\u591a\u6570\u6295\u7968\u66f4\u6709\u6548\u548c\u9ad8\u6548\uff0c\u51cf\u5c11token\u4f7f\u7528\u91cf\u8fbe70%\uff0c\u540c\u65f6\u5e73\u5747\u63d0\u9ad8\u51c6\u786e\u60272.6%\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e0d\u4ec5\u4e3a\u63a8\u7406\u65f6\u6548\u7387\u63d0\u4f9b\u4e86\u5b9e\u7528\u7b56\u7565\uff0c\u8fd8\u4ece\u53ef\u89e3\u91ca\u6027\u89d2\u5ea6\u6df1\u5165\u63ed\u793a\u4e86\u63a8\u7406\u8fc7\u7a0b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u8868\u793a\u548c\u533a\u5206\u65b9\u5f0f\u3002"}}
{"id": "2510.10089", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10089", "abs": "https://arxiv.org/abs/2510.10089", "authors": ["Zixuan Gong", "Jiaye Teng", "Yong Liu"], "title": "What Makes Looped Transformers Perform Better Than Non-Recursive Ones (Provably)", "comment": null, "summary": "While looped transformers (termed as Looped-Attn) often outperform standard\ntransformers (termed as Single-Attn) on complex reasoning tasks, the\ntheoretical basis for this advantage remains underexplored. In this paper, we\nexplain this phenomenon through the lens of loss landscape geometry, inspired\nby empirical observations of their distinct dynamics at both sample and Hessian\nlevels. To formalize this, we extend the River-Valley landscape model by\ndistinguishing between U-shaped valleys (flat) and V-shaped valleys (steep).\nBased on empirical observations, we conjecture that the recursive architecture\nof Looped-Attn induces a landscape-level inductive bias towards River-V-Valley.\nTheoretical derivations based on this inductive bias guarantee a better loss\nconvergence along the river due to valley hopping, and further encourage\nlearning about complex patterns compared to the River-U-Valley induced by\nSingle-Attn. Building on this insight, we propose SHIFT (Staged HIerarchical\nFramework for Progressive Training), a staged training framework that\naccelerates the training process of Looped-Attn while achieving comparable\nperformances.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u635f\u5931\u666f\u89c2\u51e0\u4f55\u89c6\u89d2\u89e3\u91ca\u4e86\u5faa\u73afTransformer\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6807\u51c6Transformer\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63d0\u51fa\u4e86River-Valley\u666f\u89c2\u6a21\u578b\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86SHIFT\u5206\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u3002", "motivation": "\u5faa\u73afTransformer\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6Transformer\uff0c\u4f46\u5176\u7406\u8bba\u4f18\u52bf\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u4ece\u635f\u5931\u666f\u89c2\u51e0\u4f55\u7684\u89d2\u5ea6\u89e3\u91ca\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u6269\u5c55River-Valley\u666f\u89c2\u6a21\u578b\uff0c\u533a\u5206U\u5f62\u8c37\uff08\u5e73\u5766\uff09\u548cV\u5f62\u8c37\uff08\u9661\u5ced\uff09\uff0c\u63d0\u51fa\u5faa\u73afTransformer\u67b6\u6784\u8bf1\u5bfc\u4e86River-V-Valley\u666f\u89c2\u504f\u7f6e\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86SHIFT\u5206\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u3002", "result": "\u7406\u8bba\u63a8\u5bfc\u8868\u660e\uff0c\u57fa\u4e8eV\u5f62\u8c37\u504f\u7f6e\u7684\u5faa\u73afTransformer\u5728\u635f\u5931\u6536\u655b\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u80fd\u591f\u5b66\u4e60\u66f4\u590d\u6742\u7684\u6a21\u5f0f\u3002SHIFT\u6846\u67b6\u52a0\u901f\u4e86\u5faa\u73afTransformer\u7684\u8bad\u7ec3\u8fc7\u7a0b\u5e76\u8fbe\u5230\u53ef\u6bd4\u8f83\u7684\u6027\u80fd\u3002", "conclusion": "\u5faa\u73afTransformer\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u6e90\u4e8e\u5176\u8bf1\u5bfc\u7684River-V-Valley\u666f\u89c2\u504f\u7f6e\uff0cSHIFT\u6846\u67b6\u6709\u6548\u5229\u7528\u4e86\u8fd9\u79cd\u504f\u7f6e\u6765\u52a0\u901f\u8bad\u7ec3\u3002"}}
{"id": "2510.11476", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11476", "abs": "https://arxiv.org/abs/2510.11476", "authors": ["Nan Gu", "Ge Chen", "Junjie Qin"], "title": "The Role of Flexible Connection in Accelerating Load Interconnection in Distribution Networks", "comment": null, "summary": "This paper investigates the role of flexible connection in accelerating the\ninterconnection of large loads amid rising electricity demand from data centers\nand electrification. Flexible connection allows new loads to defer or curtail\nconsumption during rare, grid-constrained periods, enabling faster access\nwithout major infrastructure upgrades. To quantify how flexible connection\nunlocks load hosting capacity, we formulate a flexibility-aware hosting\ncapacity analysis problem that explicitly limits the number of\nutility-controlled interventions per year, ensuring infrequent disruption.\nEfficient solution methods are developed for this nonconvex problem and applied\nto real load data and test feeders. Empirical results reveal that modest\nflexibility, i.e., few interventions with small curtailments or delays, can\nunlock substantial hosting capacity. Theoretical analysis further explains and\ngeneralizes these findings, highlighting the broad potential of flexible\nconnection.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u67d4\u6027\u8fde\u63a5\u5728\u52a0\u901f\u5927\u578b\u8d1f\u8377\u63a5\u5165\u7535\u7f51\u4e2d\u7684\u4f5c\u7528\uff0c\u901a\u8fc7\u5141\u8bb8\u8d1f\u8377\u5728\u7535\u7f51\u53d7\u9650\u65f6\u6bb5\u63a8\u8fdf\u6216\u524a\u51cf\u7528\u7535\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u57fa\u7840\u8bbe\u65bd\u5347\u7ea7\u5373\u53ef\u5feb\u901f\u63a5\u5165\u65b0\u8d1f\u8377\u3002", "motivation": "\u968f\u7740\u6570\u636e\u4e2d\u5fc3\u548c\u7535\u6c14\u5316\u5e26\u6765\u7684\u7535\u529b\u9700\u6c42\u589e\u957f\uff0c\u9700\u8981\u627e\u5230\u52a0\u901f\u5927\u578b\u8d1f\u8377\u63a5\u5165\u7535\u7f51\u7684\u65b9\u6cd5\uff0c\u907f\u514d\u6602\u8d35\u7684\u57fa\u7840\u8bbe\u65bd\u5347\u7ea7\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8003\u8651\u7075\u6d3b\u6027\u7684\u627f\u8f7d\u80fd\u529b\u5206\u6790\u95ee\u9898\uff0c\u660e\u786e\u9650\u5236\u6bcf\u5e74\u516c\u7528\u4e8b\u4e1a\u63a7\u5236\u5e72\u9884\u6b21\u6570\uff0c\u786e\u4fdd\u5e72\u6270\u9891\u7387\u4f4e\u3002\u5f00\u53d1\u4e86\u9ad8\u6548\u6c42\u89e3\u65b9\u6cd5\uff0c\u5e76\u5e94\u7528\u4e8e\u5b9e\u9645\u8d1f\u8377\u6570\u636e\u548c\u6d4b\u8bd5\u9988\u7ebf\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u9002\u5ea6\u7684\u7075\u6d3b\u6027\uff08\u5373\u5c11\u91cf\u5e72\u9884\u3001\u5c0f\u5e45\u5ea6\u524a\u51cf\u6216\u5ef6\u8fdf\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8d1f\u8377\u627f\u8f7d\u80fd\u529b\u3002\u7406\u8bba\u5206\u6790\u8fdb\u4e00\u6b65\u89e3\u91ca\u5e76\u63a8\u5e7f\u4e86\u8fd9\u4e9b\u53d1\u73b0\u3002", "conclusion": "\u67d4\u6027\u8fde\u63a5\u5177\u6709\u5e7f\u6cdb\u6f5c\u529b\uff0c\u80fd\u591f\u5728\u4e0d\u9891\u7e41\u5e72\u6270\u8d1f\u8377\u7684\u60c5\u51b5\u4e0b\uff0c\u5927\u5e45\u63d0\u5347\u7535\u7f51\u5bf9\u5927\u578b\u8d1f\u8377\u7684\u627f\u8f7d\u80fd\u529b\u3002"}}
{"id": "2510.11433", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.11433", "abs": "https://arxiv.org/abs/2510.11433", "authors": ["H\u00f2a T. B\u00f9i", "Minh N. B\u00f9i", "Christian Clason"], "title": "Variational Analysis in Spectral Decomposition Systems", "comment": null, "summary": "This work is concerned with variational analysis of so-called spectral\nfunctions and spectral sets of matrices that only depend on eigenvalues of the\nmatrix. Based on our previous work [H. T. B\\`ui, M. N. B\\`ui, and C. Clason,\nConvex analysis in spectral decomposition systems, arXiv 2503.14981] on convex\nanalysis of such functions, we consider the question in the abstract framework\nof spectral decomposition systems, which covers a wide range of previously\nstudied settings, including eigenvalue decomposition of Hermitian matrices and\nsingular value decomposition of rectangular matrices, and allows deriving new\nresults in more general settings such as normal decomposition systems and\nsigned singular value decompositions. The main results characterize Fr\\'echet\nand limiting normal cones to spectral sets as well as Fr\\'echet, limiting, and\nClarke subdifferentials of spectral functions in terms of the reduced\nfunctions. For the latter, we also characterize Fr\\'echet differentiability.\nFinally, we obtain a generalization of Lidski\\u{\\i}'s theorem on the spectrum\nof additive perturbations of Hermitian matrices to arbitrary spectral\ndecomposition systems.", "AI": {"tldr": "\u672c\u6587\u5728\u8c31\u5206\u89e3\u7cfb\u7edf\u7684\u62bd\u8c61\u6846\u67b6\u4e0b\uff0c\u7814\u7a76\u4e86\u77e9\u9635\u8c31\u51fd\u6570\u548c\u8c31\u96c6\u7684\u53d8\u5206\u5206\u6790\uff0c\u5305\u62ec\u6cd5\u9525\u3001\u6b21\u5fae\u5206\u7684\u523b\u753b\uff0c\u4ee5\u53caLidskii\u5b9a\u7406\u7684\u63a8\u5e7f\u3002", "motivation": "\u57fa\u4e8e\u5148\u524d\u5173\u4e8e\u8c31\u51fd\u6570\u51f8\u5206\u6790\u7684\u5de5\u4f5c\uff0c\u65e8\u5728\u5728\u8c31\u5206\u89e3\u7cfb\u7edf\u7684\u62bd\u8c61\u6846\u67b6\u4e0b\u7edf\u4e00\u7814\u7a76\u77e9\u9635\u8c31\u51fd\u6570\u548c\u8c31\u96c6\u7684\u53d8\u5206\u6027\u8d28\uff0c\u6db5\u76d6\u66f4\u4e00\u822c\u7684\u8bbe\u7f6e\u3002", "method": "\u91c7\u7528\u8c31\u5206\u89e3\u7cfb\u7edf\u7684\u62bd\u8c61\u6846\u67b6\uff0c\u901a\u8fc7\u7ea6\u5316\u51fd\u6570\u6765\u523b\u753b\u8c31\u96c6\u7684\u6cd5\u9525\u548c\u8c31\u51fd\u6570\u7684\u6b21\u5fae\u5206\uff0c\u5e76\u63a8\u5e7fLidskii\u5b9a\u7406\u3002", "result": "\u5efa\u7acb\u4e86\u8c31\u96c6Fr\\'echet\u548c\u6781\u9650\u6cd5\u9525\u7684\u523b\u753b\uff0c\u8c31\u51fd\u6570Fr\\'echet\u3001\u6781\u9650\u548cClarke\u6b21\u5fae\u5206\u7684\u523b\u753b\uff0c\u4ee5\u53caFr\\'echet\u53ef\u5fae\u6027\u7684\u7279\u5f81\uff0c\u5e76\u5c06Lidskii\u5b9a\u7406\u63a8\u5e7f\u5230\u4efb\u610f\u8c31\u5206\u89e3\u7cfb\u7edf\u3002", "conclusion": "\u8c31\u5206\u89e3\u7cfb\u7edf\u7684\u62bd\u8c61\u6846\u67b6\u4e3a\u7edf\u4e00\u7814\u7a76\u77e9\u9635\u8c31\u51fd\u6570\u548c\u8c31\u96c6\u7684\u53d8\u5206\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\uff0c\u5e76\u5f97\u5230\u4e86Lidskii\u5b9a\u7406\u7684\u63a8\u5e7f\u3002"}}
{"id": "2510.10009", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10009", "abs": "https://arxiv.org/abs/2510.10009", "authors": ["Shu Zhao", "Tan Yu", "Anbang Xu"], "title": "Beyond the limitation of a single query: Train your LLM for query expansion with Reinforcement Learning", "comment": null, "summary": "Reasoning-augmented search agents, such as Search-R1, are trained to reason,\nsearch, and generate the final answer iteratively. Nevertheless, due to their\nlimited capabilities in reasoning and search, their performance on multi-hop QA\nbenchmarks remains far from satisfactory. To handle complex or compound\nqueries, we train an LLM-based search agent with the native capability of query\nexpansion through reinforcement learning. In each turn, our search agent\nproposes several query variants, which are searched simultaneously to cover\nmore relevant information. Meanwhile, given limited post-training data and\ncomputing resources, it is very challenging for a search agent to master\nmultiple tasks, including query generation, retrieved information\nunderstanding, and answer generation. Therefore, we propose incorporating a\npre-trained squeezer model that helps the search agent understand the retrieved\ndocuments, allowing the search agent to focus on query generation for high\nretrieval recall. With the assistance of the squeezer model, we discover that\neven a small-scale 3B LLM can demonstrate a strong capability of query\nexpansion and achieve state-of-the-art accuracy on the multi-hop QA benchmarks.\nTo be specific, our experiments across seven question-answering benchmarks\ndemonstrate that our method, named ExpandSearch, achieves an average\nimprovement of 4.4% compared to state-of-the-art baselines, with strong gains\non multi-hop reasoning tasks requiring diverse evidence aggregation.", "AI": {"tldr": "\u63d0\u51fa\u4e86ExpandSearch\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u641c\u7d22\u4ee3\u7406\uff0c\u5177\u5907\u67e5\u8be2\u6269\u5c55\u80fd\u529b\uff0c\u4f7f\u7528\u6324\u538b\u5668\u6a21\u578b\u8f85\u52a9\u7406\u89e3\u68c0\u7d22\u6587\u6863\uff0c\u57287\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53474.4%\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u589e\u5f3a\u641c\u7d22\u4ee3\u7406\u5728\u590d\u6742\u591a\u8df3QA\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u63a8\u7406\u548c\u641c\u7d22\u80fd\u529b\u4e0d\u8db3\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u590d\u5408\u67e5\u8be2\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u641c\u7d22\u4ee3\u7406\uff0c\u6bcf\u8f6e\u751f\u6210\u591a\u4e2a\u67e5\u8be2\u53d8\u4f53\u5e76\u884c\u641c\u7d22\uff1b\u5f15\u5165\u9884\u8bad\u7ec3\u6324\u538b\u5668\u6a21\u578b\u5e2e\u52a9\u7406\u89e3\u68c0\u7d22\u6587\u6863\uff0c\u8ba9\u641c\u7d22\u4ee3\u7406\u4e13\u6ce8\u4e8e\u67e5\u8be2\u751f\u6210\u4ee5\u63d0\u9ad8\u68c0\u7d22\u53ec\u56de\u7387\u3002", "result": "\u57287\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53474.4%\u51c6\u786e\u7387\uff0c\u5728\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5373\u4f7f3B\u5c0f\u6a21\u578b\u4e5f\u80fd\u5c55\u73b0\u5f3a\u5927\u7684\u67e5\u8be2\u6269\u5c55\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u67e5\u8be2\u6269\u5c55\u548c\u6324\u538b\u5668\u6a21\u578b\u7684\u7ed3\u5408\uff0c\u5c0f\u89c4\u6a21LLM\u4e5f\u80fd\u5728\u591a\u8df3QA\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.09734", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09734", "abs": "https://arxiv.org/abs/2510.09734", "authors": ["Jindong Tian", "Yifei Ding", "Ronghui Xu", "Hao Miao", "Chenjuan Guo", "Bin Yang"], "title": "ARROW: An Adaptive Rollout and Routing Method for Global Weather Forecasting", "comment": "16 pages, 6 figures, conference", "summary": "Weather forecasting is a fundamental task in spatiotemporal data analysis,\nwith broad applications across a wide range of domains. Existing data-driven\nforecasting methods typically model atmospheric dynamics over a fixed short\ntime interval (e.g., 6 hours) and rely on naive autoregression-based rollout\nfor long-term forecasting (e.g., 138 hours). However, this paradigm suffers\nfrom two key limitations: (1) it often inadequately models the spatial and\nmulti-scale temporal dependencies inherent in global weather systems, and (2)\nthe rollout strategy struggles to balance error accumulation with the capture\nof fine-grained atmospheric variations. In this study, we propose ARROW, an\nAdaptive-Rollout Multi-scale temporal Routing method for Global Weather\nForecasting. To contend with the first limitation, we construct a\nmulti-interval forecasting model that forecasts weather across different time\nintervals. Within the model, the Shared-Private Mixture-of-Experts captures\nboth shared patterns and specific characteristics of atmospheric dynamics\nacross different time scales, while Ring Positional Encoding accurately encodes\nthe circular latitude structure of the Earth when representing spatial\ninformation. For the second limitation, we develop an adaptive rollout\nscheduler based on reinforcement learning, which selects the most suitable time\ninterval to forecast according to the current weather state. Experimental\nresults demonstrate that ARROW achieves state-of-the-art performance in global\nweather forecasting, establishing a promising paradigm in this field.", "AI": {"tldr": "\u63d0\u51faARROW\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u65f6\u95f4\u8def\u7531\u548c\u81ea\u9002\u5e94\u6eda\u52a8\u7b56\u7565\u6539\u8fdb\u5168\u7403\u5929\u6c14\u9884\u62a5\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u7a7a\u4f9d\u8d56\u5efa\u6a21\u548c\u8bef\u5dee\u7d2f\u79ef\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9a71\u52a8\u7684\u5929\u6c14\u9884\u62a5\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u56fa\u5b9a\u77ed\u65f6\u95f4\u95f4\u9694\u5efa\u6a21\uff0c\u4f9d\u8d56\u6734\u7d20\u81ea\u56de\u5f52\u8fdb\u884c\u957f\u671f\u9884\u6d4b\uff0c\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a(1) \u65e0\u6cd5\u5145\u5206\u5efa\u6a21\u5168\u7403\u5929\u6c14\u7cfb\u7edf\u7684\u7a7a\u95f4\u548c\u591a\u5c3a\u5ea6\u65f6\u95f4\u4f9d\u8d56\u6027\uff1b(2) \u6eda\u52a8\u7b56\u7565\u96be\u4ee5\u5e73\u8861\u8bef\u5dee\u7d2f\u79ef\u4e0e\u7cbe\u7ec6\u5927\u6c14\u53d8\u5316\u6355\u6349\u3002", "method": "\u6784\u5efa\u591a\u95f4\u9694\u9884\u6d4b\u6a21\u578b\uff0c\u4f7f\u7528\u5171\u4eab-\u79c1\u6709\u4e13\u5bb6\u6df7\u5408\u673a\u5236\u6355\u83b7\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684\u5171\u4eab\u6a21\u5f0f\u548c\u7279\u5b9a\u7279\u5f81\uff0c\u91c7\u7528\u73af\u5f62\u4f4d\u7f6e\u7f16\u7801\u51c6\u786e\u8868\u793a\u5730\u7403\u7eac\u5ea6\u7ed3\u6784\uff0c\u5e76\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u5f00\u53d1\u81ea\u9002\u5e94\u6eda\u52a8\u8c03\u5ea6\u5668\u6765\u9009\u62e9\u6700\u5408\u9002\u7684\u65f6\u95f4\u95f4\u9694\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eARROW\u5728\u5168\u7403\u5929\u6c14\u9884\u62a5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "ARROW\u4e3a\u5168\u7403\u5929\u6c14\u9884\u62a5\u9886\u57df\u5efa\u7acb\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.10549", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10549", "abs": "https://arxiv.org/abs/2510.10549", "authors": ["Xinbang Dai", "Huikang Hu", "Yongrui Chen", "Jiaqi Li", "Rihui Jin", "Yuyang Zhang", "Xiaoguang Li", "Lifeng Shang", "Guilin Qi"], "title": "ELAIPBench: A Benchmark for Expert-Level Artificial Intelligence Paper Understanding", "comment": "25 pages, 20 figures", "summary": "While large language models (LLMs) excel at many domain-specific tasks, their\nability to deeply comprehend and reason about full-length academic papers\nremains underexplored. Existing benchmarks often fall short of capturing such\ndepth, either due to surface-level question design or unreliable evaluation\nmetrics. To address this gap, we introduce ELAIPBench, a benchmark curated by\ndomain experts to evaluate LLMs' comprehension of artificial intelligence (AI)\nresearch papers. Developed through an incentive-driven, adversarial annotation\nprocess, ELAIPBench features 403 multiple-choice questions from 137 papers. It\nspans three difficulty levels and emphasizes non-trivial reasoning rather than\nshallow retrieval. Our experiments show that the best-performing LLM achieves\nan accuracy of only 39.95%, far below human performance. Moreover, we observe\nthat frontier LLMs equipped with a thinking mode or a retrieval-augmented\ngeneration (RAG) system fail to improve final results-even harming accuracy due\nto overthinking or noisy retrieval. These findings underscore the significant\ngap between current LLM capabilities and genuine comprehension of academic\npapers.", "AI": {"tldr": "ELAIPBench\u662f\u4e00\u4e2a\u7531\u9886\u57df\u4e13\u5bb6\u7b56\u5212\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5bf9AI\u7814\u7a76\u8bba\u6587\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5305\u542b403\u4e2a\u591a\u9009\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u6700\u4f73LLM\u51c6\u786e\u7387\u4ec539.95%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30LLMs\u5bf9\u5b66\u672f\u8bba\u6587\u7684\u6df1\u5ea6\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8981\u4e48\u95ee\u9898\u8bbe\u8ba1\u80a4\u6d45\uff0c\u8981\u4e48\u8bc4\u4f30\u6307\u6807\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u66f4\u4e13\u4e1a\u7684\u57fa\u51c6\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6fc0\u52b1\u9a71\u52a8\u7684\u5bf9\u6297\u6027\u6807\u6ce8\u8fc7\u7a0b\u5f00\u53d1ELAIPBench\u57fa\u51c6\uff0c\u5305\u542b137\u7bc7\u8bba\u6587\u7684403\u4e2a\u591a\u9009\u95ee\u9898\uff0c\u6db5\u76d6\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\uff0c\u5f3a\u8c03\u975e\u5e73\u51e1\u63a8\u7406\u800c\u975e\u6d45\u5c42\u68c0\u7d22\u3002", "result": "\u6700\u4f73\u6027\u80fd\u7684LLM\u51c6\u786e\u7387\u4ec5\u4e3a39.95%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u8868\u73b0\u3002\u914d\u5907\u601d\u7ef4\u6a21\u5f0f\u6216RAG\u7cfb\u7edf\u7684\u524d\u6cbfLLM\u672a\u80fd\u6539\u5584\u7ed3\u679c\uff0c\u751a\u81f3\u56e0\u8fc7\u5ea6\u601d\u8003\u6216\u566a\u58f0\u68c0\u7d22\u800c\u964d\u4f4e\u51c6\u786e\u6027\u3002", "conclusion": "\u5f53\u524dLLM\u80fd\u529b\u4e0e\u771f\u6b63\u7406\u89e3\u5b66\u672f\u8bba\u6587\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7684\u7814\u7a76\u6765\u63d0\u5347\u6a21\u578b\u5bf9\u590d\u6742\u5b66\u672f\u5185\u5bb9\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.10140", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10140", "abs": "https://arxiv.org/abs/2510.10140", "authors": ["Yue Deng", "Francisco Santos", "Pang-Ning Tan", "Lifeng Luo"], "title": "Adversarial Attacks on Downstream Weather Forecasting Models: Application to Tropical Cyclone Trajectory Prediction", "comment": null, "summary": "Deep learning based weather forecasting (DLWF) models leverage past weather\nobservations to generate future forecasts, supporting a wide range of\ndownstream tasks, including tropical cyclone (TC) trajectory prediction. In\nthis paper, we investigate their vulnerability to adversarial attacks, where\nsubtle perturbations to the upstream weather forecasts can alter the downstream\nTC trajectory predictions. Although research on adversarial attacks in DLWF\nmodels has grown recently, generating perturbed upstream forecasts that\nreliably steer downstream output toward attacker-specified trajectories remains\na challenge. First, conventional TC detection systems are opaque,\nnon-differentiable black boxes, making standard gradient-based attacks\ninfeasible. Second, the extreme rarity of TC events leads to severe class\nimbalance problem, making it difficult to develop efficient attack methods that\nwill produce the attacker's target trajectories. Furthermore, maintaining\nphysical consistency in adversarially generated forecasts presents another\nsignificant challenge. To overcome these limitations, we propose Cyc-Attack, a\nnovel method that perturbs the upstream forecasts of DLWF models to generate\nadversarial trajectories. First, we pre-train a differentiable surrogate model\nto approximate the TC detector's output, enabling the construction of\ngradient-based attacks. Cyc-Attack also employs skewness-aware loss function\nwith kernel dilation strategy to address the imbalance problem. Finally, a\ndistance-based gradient weighting scheme and regularization are used to\nconstrain the perturbations and eliminate spurious trajectories to ensure the\nadversarial forecasts are realistic and not easily detectable.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faCyc-Attack\u65b9\u6cd5\uff0c\u901a\u8fc7\u6270\u52a8\u6df1\u5ea6\u5b66\u4e60\u5929\u6c14\u9884\u62a5\u6a21\u578b\u7684\u4e0a\u6e38\u9884\u62a5\u6765\u751f\u6210\u5bf9\u6297\u6027\u70ed\u5e26\u6c14\u65cb\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u653b\u51fb\u65b9\u6cd5\u5728\u975e\u53ef\u5fae\u5206\u68c0\u6d4b\u7cfb\u7edf\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u4e0a\u7684\u6311\u6218\u3002", "motivation": "\u7814\u7a76\u6df1\u5ea6\u5b66\u4e60\u5929\u6c14\u9884\u62a5\u6a21\u578b\u5bf9\u5bf9\u6297\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u7279\u522b\u662f\u5982\u4f55\u901a\u8fc7\u5fae\u5c0f\u7684\u4e0a\u6e38\u9884\u62a5\u6270\u52a8\u6765\u6539\u53d8\u4e0b\u6e38\u70ed\u5e26\u6c14\u65cb\u8f68\u8ff9\u9884\u6d4b\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u53ef\u5fae\u5206\u68c0\u6d4b\u7cfb\u7edf\u548c\u6781\u7aef\u7a00\u6709\u4e8b\u4ef6\u5bfc\u81f4\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u4e0a\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51faCyc-Attack\u65b9\u6cd5\uff1a\u9884\u8bad\u7ec3\u53ef\u5fae\u5206\u66ff\u4ee3\u6a21\u578b\u8fd1\u4f3c\u70ed\u5e26\u6c14\u65cb\u68c0\u6d4b\u5668\u8f93\u51fa\uff1b\u4f7f\u7528\u504f\u5ea6\u611f\u77e5\u635f\u5931\u51fd\u6570\u548c\u6838\u81a8\u80c0\u7b56\u7565\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff1b\u91c7\u7528\u57fa\u4e8e\u8ddd\u79bb\u7684\u68af\u5ea6\u52a0\u6743\u65b9\u6848\u548c\u6b63\u5219\u5316\u786e\u4fdd\u6270\u52a8\u7269\u7406\u4e00\u81f4\u6027\u3002", "result": "Cyc-Attack\u80fd\u591f\u6709\u6548\u751f\u6210\u5bf9\u6297\u6027\u70ed\u5e26\u6c14\u65cb\u8f68\u8ff9\uff0c\u6210\u529f\u64cd\u7eb5\u4e0b\u6e38\u9884\u6d4b\u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u7684\u5bf9\u6297\u6027\u9884\u62a5\u7684\u7269\u7406\u5408\u7406\u6027\u548c\u9690\u853d\u6027\u3002", "conclusion": "Cyc-Attack\u65b9\u6cd5\u514b\u670d\u4e86\u4f20\u7edf\u5bf9\u6297\u653b\u51fb\u5728\u5929\u6c14\u9884\u62a5\u6a21\u578b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u5929\u6c14\u9884\u62a5\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.11515", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11515", "abs": "https://arxiv.org/abs/2510.11515", "authors": ["Shanthan Kumar Padisala", "Bharatkumar Hegde", "Ibrahim Haskara", "Satadru Dey"], "title": "A Physics-Informed Reinforcement Learning Approach for Degradation-Aware Long-Term Charging Optimization in Batteries", "comment": null, "summary": "Batteries degrade with usage and continuous cycling. This aging is typically\nreflected through the resistance growth and the capacity fade of battery cells.\nOver the years, various charging methods have been presented in the literature\nthat proposed current profiles in order to enable optimal, fast, and/or\nhealth-conscious charging. However, very few works have attempted to make the\nubiquitous Constant Current Constant Voltage (CCCV) charging protocol adaptive\nto the changing battery health as it cycles. This work aims to address this gap\nand proposes a framework that optimizes the constant current part of the CCCV\nprotocol adapting to long-term battery degradation. Specifically, a\nphysics-informed Reinforcement Learning (RL) approach has been used that not\nonly estimates a key battery degradation mechanism, namely, Loss of Active\nMaterial (LAM), but also adjusts the current magnitude of CCCV as a result of\nthis particular degradation. The proposed framework has been implemented by\ncombining PyBamm, an open-source battery modeling tool, and Stable-baselines\nwhere the RL agent was trained using a Proximal Policy Optimization (PPO)\nnetwork. Simulation results show the potential of the proposed framework for\nenhancing the widely used CCCV protocol by embedding physics information in RL\nalgorithm. A comparative study of this proposed agent has also been discussed\nwith 2 other charging protocols generated by a non-physics-based RL agent and a\nconstant CCCV for all the cycles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94CCCV\u5145\u7535\u534f\u8bae\u4f18\u5316\u6846\u67b6\uff0c\u80fd\u591f\u6839\u636e\u7535\u6c60\u957f\u671f\u9000\u5316\u8c03\u6574\u5145\u7535\u7535\u6d41", "motivation": "\u89e3\u51b3\u4f20\u7edfCCCV\u5145\u7535\u534f\u8bae\u65e0\u6cd5\u9002\u5e94\u7535\u6c60\u5065\u5eb7\u72b6\u6001\u53d8\u5316\u7684\u95ee\u9898\uff0c\u5f88\u5c11\u6709\u5de5\u4f5c\u5c1d\u8bd5\u4f7fCCCV\u534f\u8bae\u968f\u7535\u6c60\u5faa\u73af\u800c\u81ea\u9002\u5e94\u8c03\u6574", "method": "\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408PyBamm\u7535\u6c60\u5efa\u6a21\u5de5\u5177\u548cStable-baselines\uff0c\u91c7\u7528PPO\u7f51\u7edc\u8bad\u7ec3RL\u667a\u80fd\u4f53\uff0c\u4f30\u8ba1\u7535\u6c60\u6d3b\u6027\u6750\u6599\u635f\u5931\u5e76\u76f8\u5e94\u8c03\u6574CCCV\u7535\u6d41", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u589e\u5f3a\u5e7f\u6cdb\u4f7f\u7528\u7684CCCV\u534f\u8bae\uff0c\u901a\u8fc7\u5c06\u7269\u7406\u4fe1\u606f\u5d4c\u5165RL\u7b97\u6cd5\u5b9e\u73b0\u4f18\u5316", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5c55\u793a\u4e86\u5c06\u7269\u7406\u4fe1\u606f\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u4e2d\u4f18\u5316\u7535\u6c60\u5145\u7535\u534f\u8bae\u7684\u6f5c\u529b\uff0c\u5e76\u4e0e\u975e\u7269\u7406RL\u667a\u80fd\u4f53\u548c\u6052\u5b9aCCCV\u534f\u8bae\u8fdb\u884c\u4e86\u6bd4\u8f83\u7814\u7a76"}}
{"id": "2510.11440", "categories": ["math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11440", "abs": "https://arxiv.org/abs/2510.11440", "authors": ["Abbas Khademi", "Antonio Silveti-Falls"], "title": "Adaptive Conditional Gradient Descent", "comment": null, "summary": "Selecting an effective step-size is a fundamental challenge in first-order\noptimization, especially for problems with non-Euclidean geometries. This paper\npresents a novel adaptive step-size strategy for optimization algorithms that\nrely on linear minimization oracles, as used in the Conditional Gradient or\nnon-Euclidean Normalized Steepest Descent algorithms. Using a simple heuristic\nto estimate a local Lipschitz constant for the gradient, we can determine\nstep-sizes that guarantee sufficient decrease at each iteration. More\nprecisely, we establish convergence guarantees for our proposed Adaptive\nConditional Gradient Descent algorithm, which covers as special cases both the\nclassical Conditional Gradient algorithm and non-Euclidean Normalized Steepest\nDescent algorithms with adaptive step-sizes. Our analysis covers optimization\nof continuously differentiable functions in non-convex, quasar-convex, and\nstrongly convex settings, achieving convergence rates that match\nstate-of-the-art theoretical bounds. Comprehensive numerical experiments\nvalidate our theoretical findings and illustrate the practical effectiveness of\nAdaptive Conditional Gradient Descent. The results exhibit competitive\nperformance, underscoring the potential of the adaptive step-size for\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6761\u4ef6\u68af\u5ea6\u6cd5\u548c\u975e\u6b27\u51e0\u91cc\u5f97\u5f52\u4e00\u5316\u6700\u901f\u4e0b\u964d\u6cd5\u7684\u81ea\u9002\u5e94\u6b65\u957f\u7b56\u7565\uff0c\u901a\u8fc7\u4f30\u8ba1\u5c40\u90e8Lipschitz\u5e38\u6570\u6765\u4fdd\u8bc1\u6bcf\u6b21\u8fed\u4ee3\u7684\u5145\u5206\u4e0b\u964d\uff0c\u5728\u975e\u51f8\u3001\u62df\u51f8\u548c\u5f3a\u51f8\u8bbe\u7f6e\u4e0b\u90fd\u83b7\u5f97\u4e86\u6536\u655b\u4fdd\u8bc1\u3002", "motivation": "\u89e3\u51b3\u4e00\u9636\u4f18\u5316\u4e2d\u6b65\u957f\u9009\u62e9\u7684\u57fa\u672c\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u975e\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\u95ee\u9898\u4e2d\uff0c\u4f20\u7edf\u56fa\u5b9a\u6b65\u957f\u7b56\u7565\u5f80\u5f80\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u4f7f\u7528\u7b80\u5355\u542f\u53d1\u5f0f\u65b9\u6cd5\u4f30\u8ba1\u68af\u5ea6\u7684\u5c40\u90e8Lipschitz\u5e38\u6570\uff0c\u57fa\u4e8e\u6b64\u786e\u5b9a\u80fd\u591f\u4fdd\u8bc1\u5145\u5206\u4e0b\u964d\u7684\u81ea\u9002\u5e94\u6b65\u957f\uff0c\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u6761\u4ef6\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8be5\u7b97\u6cd5\u5728\u975e\u51f8\u3001\u62df\u51f8\u548c\u5f3a\u51f8\u8bbe\u7f6e\u4e0b\u90fd\u80fd\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u7406\u8bba\u6536\u655b\u754c\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u5e76\u5c55\u793a\u4e86\u5b9e\u9645\u6709\u6548\u6027\u3002", "conclusion": "\u81ea\u9002\u5e94\u6b65\u957f\u7b56\u7565\u5728\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u81ea\u9002\u5e94\u6761\u4ef6\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u4e3a\u4f9d\u8d56\u7ebf\u6027\u6700\u5c0f\u5316oracle\u7684\u4f18\u5316\u7b97\u6cd5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6b65\u957f\u9009\u62e9\u65b9\u6848\u3002"}}
{"id": "2510.10013", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10013", "abs": "https://arxiv.org/abs/2510.10013", "authors": ["Yuyi Huang", "Runzhe Zhan", "Lidia S. Chao", "Ailin Tao", "Derek F. Wong"], "title": "Path Drift in Large Reasoning Models:How First-Person Commitments Override Safety", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed for complex\nreasoning tasks, Long Chain-of-Thought (Long-CoT) prompting has emerged as a\nkey paradigm for structured inference. Despite early-stage safeguards enabled\nby alignment techniques such as RLHF, we identify a previously underexplored\nvulnerability: reasoning trajectories in Long-CoT models can drift from aligned\npaths, resulting in content that violates safety constraints. We term this\nphenomenon Path Drift. Through empirical analysis, we uncover three behavioral\ntriggers of Path Drift: (1) first-person commitments that induce goal-driven\nreasoning that delays refusal signals; (2) ethical evaporation, where\nsurface-level disclaimers bypass alignment checkpoints; (3) condition chain\nescalation, where layered cues progressively steer models toward unsafe\ncompletions. Building on these insights, we introduce a three-stage Path Drift\nInduction Framework comprising cognitive load amplification, self-role priming,\nand condition chain hijacking. Each stage independently reduces refusal rates,\nwhile their combination further compounds the effect. To mitigate these risks,\nwe propose a path-level defense strategy incorporating role attribution\ncorrection and metacognitive reflection (reflective safety cues). Our findings\nhighlight the need for trajectory-level alignment oversight in long-form\nreasoning beyond token-level alignment.", "AI": {"tldr": "\u8bba\u6587\u8bc6\u522b\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u7684\u8def\u5f84\u6f02\u79fb\u6f0f\u6d1e\uff0c\u5373\u63a8\u7406\u8f68\u8ff9\u4f1a\u504f\u79bb\u5b89\u5168\u5bf9\u9f50\u8def\u5f84\uff0c\u5bfc\u81f4\u8fdd\u53cd\u5b89\u5168\u7ea6\u675f\u7684\u5185\u5bb9\u3002\u63d0\u51fa\u4e86\u8bf1\u5bfc\u6846\u67b6\u548c\u9632\u5fa1\u7b56\u7565\u3002", "motivation": "\u5c3d\u7ba1LLM\u901a\u8fc7RLHF\u7b49\u6280\u672f\u8fdb\u884c\u4e86\u5b89\u5168\u5bf9\u9f50\uff0c\u4f46\u5728\u590d\u6742\u7684\u957f\u94fe\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u63a8\u7406\u8f68\u8ff9\u4ecd\u53ef\u80fd\u504f\u79bb\u5b89\u5168\u8def\u5f84\uff0c\u4ea7\u751f\u8fdd\u53cd\u5b89\u5168\u7ea6\u675f\u7684\u5185\u5bb9\uff0c\u8fd9\u662f\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u6f0f\u6d1e\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u8bc6\u522b\u4e86\u4e09\u79cd\u8def\u5f84\u6f02\u79fb\u884c\u4e3a\u89e6\u53d1\u56e0\u7d20\uff0c\u63d0\u51fa\u4e86\u5305\u542b\u8ba4\u77e5\u8d1f\u8377\u653e\u5927\u3001\u81ea\u6211\u89d2\u8272\u542f\u52a8\u548c\u6761\u4ef6\u94fe\u52ab\u6301\u7684\u4e09\u9636\u6bb5\u8def\u5f84\u6f02\u79fb\u8bf1\u5bfc\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5305\u542b\u89d2\u8272\u5f52\u56e0\u4fee\u6b63\u548c\u5143\u8ba4\u77e5\u53cd\u601d\u7684\u8def\u5f84\u7ea7\u9632\u5fa1\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6bcf\u4e2a\u8bf1\u5bfc\u9636\u6bb5\u90fd\u80fd\u72ec\u7acb\u964d\u4f4e\u62d2\u7edd\u7387\uff0c\u7ec4\u5408\u4f7f\u7528\u65f6\u6548\u679c\u8fdb\u4e00\u6b65\u589e\u5f3a\u3002\u8def\u5f84\u6f02\u79fb\u73b0\u8c61\u786e\u5b9e\u5b58\u5728\u4e14\u53ef\u901a\u8fc7\u7cfb\u7edf\u65b9\u6cd5\u8bf1\u5bfc\u3002", "conclusion": "\u957f\u94fe\u63a8\u7406\u9700\u8981\u8d85\u8d8a\u8bcd\u5143\u7ea7\u5bf9\u9f50\u7684\u8f68\u8ff9\u7ea7\u5bf9\u9f50\u76d1\u7763\uff0c\u8def\u5f84\u6f02\u79fb\u63ed\u793a\u4e86\u5f53\u524d\u5b89\u5168\u5bf9\u9f50\u5728\u590d\u6742\u63a8\u7406\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.09735", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09735", "abs": "https://arxiv.org/abs/2510.09735", "authors": ["Qianyou Sun", "Jiexin Zheng", "Bohan Jin", "Lihua Chen", "Yijie Peng"], "title": "InterCorpRel-LLM: Enhancing Financial Relational Understanding with Graph-Language Models", "comment": null, "summary": "Identifying inter-firm relationships such as supply and competitive ties is\ncritical for financial analysis and corporate governance, yet remains\nchallenging due to the scale, sparsity, and contextual dependence of corporate\ndata. Graph-based methods capture structure but miss semantic depth, while\nlarge language models (LLMs) excel at text but remain limited in their ability\nto represent relational dependencies. To address this, we propose\nInterCorpRel-LLM, a cross-modal framework that integrates GNNs with LLMs,\nsupported by a proprietary dataset derived from FactSet supply chain records\nand three tailored training tasks: company graph matching, industry\nclassification, and supply relation prediction. This design enables effective\njoint modeling of structure and semantics. Experiments show that\nInterCorpRel-LLM substantially outperforms strong baselines, including GPT-5,\non a supply relation identification task, achieving an F-score of 0.8543 vs.\n0.2287 with only a 7B-parameter backbone and lightweight training. The model\nalso generalizes to zero-shot competitor identification, underscoring its\nability to capture nuanced inter-firm dynamics. Our framework thus provides\nanalysts and strategists with a robust tool for mapping and reasoning about\ncomplex corporate networks, enhancing decision-making and risk management in\ndynamic markets.", "AI": {"tldr": "\u63d0\u51fa\u4e86InterCorpRel-LLM\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u8bc6\u522b\u4f01\u4e1a\u95f4\u7684\u4f9b\u5e94\u548c\u7ade\u4e89\u5173\u7cfb\uff0c\u5728\u4f9b\u5e94\u94fe\u5173\u7cfb\u8bc6\u522b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8eGPT-5\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u8bc6\u522b\u4f01\u4e1a\u95f4\u5173\u7cfb\u5bf9\u91d1\u878d\u5206\u6790\u548c\u516c\u53f8\u6cbb\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u6570\u636e\u89c4\u6a21\u5927\u3001\u7a00\u758f\u6027\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7b49\u6311\u6218\u3002\u56fe\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u6df1\u5ea6\uff0c\u800cLLMs\u5728\u5173\u7cfb\u4f9d\u8d56\u8868\u793a\u65b9\u9762\u6709\u9650\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001\u6846\u67b6InterCorpRel-LLM\uff0c\u6574\u5408GNNs\u548cLLMs\uff0c\u4f7f\u7528FactSet\u4f9b\u5e94\u94fe\u8bb0\u5f55\u6784\u5efa\u4e13\u6709\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e09\u4e2a\u5b9a\u5236\u8bad\u7ec3\u4efb\u52a1\uff1a\u516c\u53f8\u56fe\u5339\u914d\u3001\u884c\u4e1a\u5206\u7c7b\u548c\u4f9b\u5e94\u5173\u7cfb\u9884\u6d4b\u3002", "result": "\u5728\u4f9b\u5e94\u94fe\u5173\u7cfb\u8bc6\u522b\u4efb\u52a1\u4e0a\uff0cInterCorpRel-LLM\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\uff08\u5305\u62ecGPT-5\uff09\uff0cF-score\u8fbe\u52300.8543 vs 0.2287\uff0c\u4ec5\u4f7f\u75287B\u53c2\u6570\u9aa8\u5e72\u7f51\u7edc\u548c\u8f7b\u91cf\u8bad\u7ec3\u3002\u6a21\u578b\u8fd8\u80fd\u96f6\u6837\u672c\u6cdb\u5316\u5230\u7ade\u4e89\u8005\u8bc6\u522b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5206\u6790\u5e08\u548c\u7b56\u7565\u5e08\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u6620\u5c04\u548c\u63a8\u7406\u590d\u6742\u7684\u4f01\u4e1a\u7f51\u7edc\uff0c\u589e\u5f3a\u52a8\u6001\u5e02\u573a\u4e2d\u7684\u51b3\u7b56\u5236\u5b9a\u548c\u98ce\u9669\u7ba1\u7406\u80fd\u529b\u3002"}}
{"id": "2510.10592", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10592", "abs": "https://arxiv.org/abs/2510.10592", "authors": ["Hong Su"], "title": "A Layered Intuition -- Method Model with Scope Extension for LLM Reasoning", "comment": null, "summary": "Existing studies have introduced method-based reasoning and scope extension\nas approaches to enhance Large Language Model (LLM) performance beyond direct\nmatrix mappings. Building on these foundations, this paper summarizes and\nintegrates these ideas into a unified Intuition-Method Layered Model with Scope\nExtension, designed to address indirected (unseen) issues more systematically.\nIn this framework, intuition-based thinking provides rapid first-reaction\nanswers, while method-based thinking decouples questions and solutions into\ntransferable reasoning units. Scope extension is then applied to broaden\napplicability, including vertical (cause analysis), horizontal (parallel and\ngeneralized issues), and for the first time, temporal and spatial extensions,\nwhich expand reasoning across time and contextual dimensions. These extensions\nare organized into systematic knowledge trees that interconnect into a\nknowledge network, thereby increasing adaptability. To quantitatively evaluate\nthis process, we propose the entropy of method extension, which measures the\nindependence and diversity of extensions as an indicator of the system's\ncapacity to solve unseen questions. By logically connecting existing approaches\nwith new extensions and introducing an entropy-based evaluation framework, this\nwork advances toward a more robust and extensible reasoning paradigm for LLMs\nin real-world problem-solving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u76f4\u89c9-\u65b9\u6cd5\u5206\u5c42\u6a21\u578b\uff0c\u901a\u8fc7\u8303\u56f4\u6269\u5c55\u6765\u7cfb\u7edf\u6027\u5730\u89e3\u51b3LLM\u4e2d\u7684\u95f4\u63a5\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8e\u71b5\u7684\u65b9\u6cd5\u6269\u5c55\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5df2\u5f15\u5165\u57fa\u4e8e\u65b9\u6cd5\u7684\u63a8\u7406\u548c\u8303\u56f4\u6269\u5c55\u6765\u63d0\u5347LLM\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6574\u5408\u3002\u672c\u6587\u65e8\u5728\u5c06\u8fd9\u4e9b\u601d\u60f3\u7edf\u4e00\u5230\u4e00\u4e2a\u6846\u67b6\u4e2d\uff0c\u66f4\u7cfb\u7edf\u5730\u5904\u7406\u672a\u89c1\u8fc7\u7684\u95f4\u63a5\u95ee\u9898\u3002", "method": "\u6784\u5efa\u76f4\u89c9-\u65b9\u6cd5\u5206\u5c42\u6a21\u578b\uff1a\u76f4\u89c9\u601d\u7ef4\u63d0\u4f9b\u5feb\u901f\u521d\u6b65\u7b54\u6848\uff0c\u65b9\u6cd5\u601d\u7ef4\u5c06\u95ee\u9898\u89e3\u8026\u4e3a\u53ef\u8f6c\u79fb\u7684\u63a8\u7406\u5355\u5143\u3002\u5f15\u5165\u5782\u76f4\u3001\u6c34\u5e73\u3001\u65f6\u95f4\u548c\u7a7a\u95f4\u56db\u4e2a\u7ef4\u5ea6\u7684\u8303\u56f4\u6269\u5c55\uff0c\u5e76\u7ec4\u7ec7\u6210\u7cfb\u7edf\u5316\u7684\u77e5\u8bc6\u6811\u7f51\u7edc\u3002", "result": "\u63d0\u51fa\u4e86\u65b9\u6cd5\u6269\u5c55\u71b5\u4f5c\u4e3a\u5b9a\u91cf\u8bc4\u4f30\u6307\u6807\uff0c\u8861\u91cf\u6269\u5c55\u7684\u72ec\u7acb\u6027\u548c\u591a\u6837\u6027\uff0c\u53cd\u6620\u7cfb\u7edf\u89e3\u51b3\u672a\u89c1\u95ee\u9898\u7684\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u903b\u8f91\u8fde\u63a5\u73b0\u6709\u65b9\u6cd5\u4e0e\u65b0\u6269\u5c55\uff0c\u5e76\u5f15\u5165\u71b5\u8bc4\u4f30\u6846\u67b6\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3aLLM\u5728\u73b0\u5b9e\u95ee\u9898\u89e3\u51b3\u4e2d\u63a8\u8fdb\u4e86\u66f4\u9c81\u68d2\u548c\u53ef\u6269\u5c55\u7684\u63a8\u7406\u8303\u5f0f\u3002"}}
{"id": "2510.11583", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11583", "abs": "https://arxiv.org/abs/2510.11583", "authors": ["Siddhartha Upadhyay", "Ratnangshu Das", "Pushpak Jagtap"], "title": "Smooth Spatiotemporal Tube Synthesis for Prescribed-Time Reach-Avoid-Stay Control", "comment": null, "summary": "In this work, we address the issue of controller synthesis for a\ncontrol-affine nonlinear system to meet prescribed time reach-avoid-stay\nspecifications. Our goal is to improve upon previous methods based on\nspatiotemporal tubes (STTs) by eliminating the need for circumvent functions,\nwhich often lead to abrupt tube modifications and high control effort. We\npropose an adaptive framework that constructs smooth STTs around static unsafe\nsets, enabling continuous avoidance while guiding the system toward the target\nwithin the prescribed time. A closed-form, approximation-free control law is\nderived to ensure the system trajectory remains within the tube and satisfies\nthe RAS task. The effectiveness of the proposed approach is demonstrated\nthrough a case study, showing a significant reduction in control effort\ncompared to prior methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u6846\u67b6\u6765\u6784\u5efa\u5e73\u6ed1\u7684\u65f6\u7a7a\u7ba1\uff0c\u7528\u4e8e\u63a7\u5236\u4eff\u5c04\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u5230\u8fbe-\u907f\u969c-\u505c\u7559\u4efb\u52a1\uff0c\u6d88\u9664\u4e86\u5bf9\u89c4\u907f\u51fd\u6570\u7684\u9700\u6c42\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a7\u5236\u52aa\u529b\u3002", "motivation": "\u6539\u8fdb\u57fa\u4e8e\u65f6\u7a7a\u7ba1\u7684\u63a7\u5236\u5668\u7efc\u5408\u65b9\u6cd5\uff0c\u6d88\u9664\u89c4\u907f\u51fd\u6570\u5bfc\u81f4\u7684\u7a81\u53d8\u538b\u7ba1\u4fee\u6539\u548c\u9ad8\u63a7\u5236\u52aa\u529b\u95ee\u9898\u3002", "method": "\u6784\u5efa\u56f4\u7ed5\u9759\u6001\u4e0d\u5b89\u5168\u96c6\u7684\u5e73\u6ed1\u65f6\u7a7a\u7ba1\uff0c\u63a8\u5bfc\u51fa\u65e0\u8fd1\u4f3c\u7684\u95ed\u5f0f\u63a7\u5236\u5f8b\uff0c\u786e\u4fdd\u7cfb\u7edf\u8f68\u8ff9\u4fdd\u6301\u5728\u7ba1\u5185\u5e76\u6ee1\u8db3RAS\u4efb\u52a1\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u63a7\u5236\u52aa\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u6846\u67b6\u80fd\u591f\u8fde\u7eed\u907f\u969c\u5e76\u5f15\u5bfc\u7cfb\u7edf\u5728\u89c4\u5b9a\u65f6\u95f4\u5185\u5230\u8fbe\u76ee\u6807\uff0c\u540c\u65f6\u4fdd\u6301\u5e73\u6ed1\u7684\u63a7\u5236\u884c\u4e3a\u3002"}}
{"id": "2510.11497", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.11497", "abs": "https://arxiv.org/abs/2510.11497", "authors": ["Marius Roland", "Nagisa Sugishita", "Alexandre Forel", "Youssouf Emine", "Ricardo Fukasawa", "Thibaut Vidal"], "title": "The Branch-and-Bound Tree Closure", "comment": null, "summary": "This paper investigates the a-posteriori analysis of Branch-and-Bound~(BB)\ntrees to extract structural information about the feasible region of\nmixed-binary linear programs. We introduce three novel outer approximations of\nthe feasible region, systematically constructed from a BB tree. These are: a\ntight formulation based on disjunctive programming, a branching-based\nformulation derived from the tree's branching logic, and a mixing-set\nformulation derived from the on-off properties inside the tree. We establish an\ninclusion hierarchy, which ranks the approximations by their theoretical\nstrength \\wrt to the original feasible region. The analysis is extended to the\ngeneration of valid inequalities, revealing a separation-time hierarchy that\nmirrors the inclusion hierarchy in reverse. This highlights a trade-off between\nthe tightness of an approximation and the computational cost of generating cuts\nfrom it. Motivated by the computational expense of the stronger approximations,\nwe introduce a new family of valid inequalities called star tree inequalities.\nAlthough their closure forms the weakest of the proposed approximations, their\npractical appeal lies in an efficient, polynomial-time combinatorial separation\nalgorithm. A computational study on multi-dimensional knapsack and set-covering\nproblems empirically validates the theoretical findings. Moreover, these\nexperiments confirm that computationally useful valid inequalities can be\ngenerated from BB trees obtained by solving optimization problems considered in\npractice.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e09\u79cd\u57fa\u4e8e\u5206\u652f\u5b9a\u754c\u6811\u7684\u5916\u8fd1\u4f3c\u65b9\u6cd5\u6765\u5206\u6790\u6df7\u5408\u4e8c\u8fdb\u5236\u7ebf\u6027\u89c4\u5212\u7684\u53ef\u884c\u57df\uff0c\u5efa\u7acb\u4e86\u5305\u542b\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u661f\u6811\u4e0d\u7b49\u5f0f\u65cf\uff0c\u901a\u8fc7\u8ba1\u7b97\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\u3002", "motivation": "\u7814\u7a76\u5206\u652f\u5b9a\u754c\u6811\u7684\u540e\u9a8c\u5206\u6790\uff0c\u4ee5\u63d0\u53d6\u6df7\u5408\u4e8c\u8fdb\u5236\u7ebf\u6027\u89c4\u5212\u53ef\u884c\u57df\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u65e8\u5728\u5f00\u53d1\u6709\u6548\u7684\u8fd1\u4f3c\u65b9\u6cd5\u548c\u6709\u6548\u4e0d\u7b49\u5f0f\u751f\u6210\u6280\u672f\u3002", "method": "\u5f15\u5165\u4e09\u79cd\u65b0\u9896\u7684\u5916\u8fd1\u4f3c\uff1a\u57fa\u4e8e\u6790\u53d6\u89c4\u5212\u7684\u7d27\u81f4\u516c\u5f0f\u3001\u57fa\u4e8e\u5206\u652f\u903b\u8f91\u7684\u5206\u652f\u516c\u5f0f\u3001\u4ee5\u53ca\u57fa\u4e8e\u5f00\u5173\u7279\u6027\u7684\u6df7\u5408\u96c6\u516c\u5f0f\uff1b\u63d0\u51fa\u661f\u6811\u4e0d\u7b49\u5f0f\u65cf\u53ca\u5176\u591a\u9879\u5f0f\u65f6\u95f4\u5206\u79bb\u7b97\u6cd5\u3002", "result": "\u5efa\u7acb\u4e86\u8fd1\u4f3c\u65b9\u6cd5\u7684\u5305\u542b\u5c42\u6b21\u7ed3\u6784\uff0c\u63ed\u793a\u4e86\u8fd1\u4f3c\u7d27\u81f4\u5ea6\u4e0e\u5272\u5e73\u9762\u751f\u6210\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\uff1b\u8ba1\u7b97\u5b9e\u9a8c\u5728\u591a\u7ef4\u80cc\u5305\u548c\u96c6\u5408\u8986\u76d6\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "\u4ece\u5b9e\u9645\u4f18\u5316\u95ee\u9898\u4e2d\u83b7\u5f97\u7684\u5206\u652f\u5b9a\u754c\u6811\u53ef\u4ee5\u751f\u6210\u8ba1\u7b97\u4e0a\u6709\u7528\u7684\u6709\u6548\u4e0d\u7b49\u5f0f\uff0c\u661f\u6811\u4e0d\u7b49\u5f0f\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8fd1\u4f3c\u5f3a\u5ea6\u4e4b\u95f4\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2510.10025", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10025", "abs": "https://arxiv.org/abs/2510.10025", "authors": ["Jiaqi Liu", "Lanruo Wang", "Su Liu", "Xin Hu"], "title": "Lightweight Baselines for Medical Abstract Classification: DistilBERT with Cross-Entropy as a Strong Default", "comment": "Healthcare AI, Medical Text Classification, Lightweight LLMs,\n  DistilBERT, Reproducibility", "summary": "Large language models work well for many NLP tasks, but they are hard to\ndeploy in health settings with strict cost, latency, and privacy limits. We\nrevisit a lightweight recipe for medical abstract classification and ask how\nfar compact encoders can go under a controlled budget. Using the public medical\nabstracts corpus, we finetune BERT base and DistilBERT with three objectives\nstandard cross-entropy, class weighted cross entropy, and focal loss keeping\ntokenizer, sequence length, optimizer, and schedule fixed. DistilBERT with\nplain cross-entropy gives the best balance on the test set while using far\nfewer parameters than BERT base. We report accuracy, Macro F1, and Weighted F1,\nrelease the evaluation code, and include confusion analyses to make error\npatterns clear. Our results suggest a practical default: start with a compact\nencoder and cross-entropy, then add calibration and task-specific checks before\nmoving to heavier models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u533b\u7597\u573a\u666f\u4e0b\u4f7f\u7528\u8f7b\u91cf\u7ea7BERT\u6a21\u578b\u8fdb\u884c\u533b\u5b66\u6458\u8981\u5206\u7c7b\uff0c\u53d1\u73b0DistilBERT\u5728\u4ea4\u53c9\u71b5\u635f\u5931\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u533b\u7597NLP\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u73af\u5883\u4e2d\u90e8\u7f72\u9762\u4e34\u6210\u672c\u3001\u5ef6\u8fdf\u548c\u9690\u79c1\u9650\u5236\uff0c\u9700\u8981\u63a2\u7d22\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u533b\u5b66\u6458\u8981\u8bed\u6599\u5e93\uff0c\u5728\u56fa\u5b9a\u5206\u8bcd\u5668\u3001\u5e8f\u5217\u957f\u5ea6\u3001\u4f18\u5316\u5668\u548c\u8c03\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9BERT base\u548cDistilBERT\u8fdb\u884c\u5fae\u8c03\uff0c\u6bd4\u8f83\u4e09\u79cd\u635f\u5931\u51fd\u6570\uff1a\u6807\u51c6\u4ea4\u53c9\u71b5\u3001\u7c7b\u522b\u52a0\u6743\u4ea4\u53c9\u71b5\u548c\u7126\u70b9\u635f\u5931\u3002", "result": "DistilBERT\u5728\u6807\u51c6\u4ea4\u53c9\u71b5\u635f\u5931\u4e0b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u540c\u65f6\u4f7f\u7528\u7684\u53c2\u6570\u8fdc\u5c11\u4e8eBERT base\u3002\u62a5\u544a\u4e86\u51c6\u786e\u7387\u3001\u5b8fF1\u548c\u52a0\u6743F1\u6307\u6807\u3002", "conclusion": "\u5efa\u8bae\u4ece\u7d27\u51d1\u7f16\u7801\u5668\u548c\u4ea4\u53c9\u71b5\u5f00\u59cb\uff0c\u7136\u540e\u6dfb\u52a0\u6821\u51c6\u548c\u4efb\u52a1\u7279\u5b9a\u68c0\u67e5\uff0c\u6700\u540e\u518d\u8003\u8651\u66f4\u91cd\u7684\u6a21\u578b\uff0c\u4e3a\u533b\u7597NLP\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2510.09739", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.09739", "abs": "https://arxiv.org/abs/2510.09739", "authors": ["Ayoub Bouguettaya", "Elizabeth M. Stuart"], "title": "Machine learning methods fail to provide cohesive atheoretical construction of personality traits from semantic embeddings", "comment": "1 figure, 12 pages", "summary": "The lexical hypothesis posits that personality traits are encoded in language\nand is foundational to models like the Big Five. We created a bottom-up\npersonality model from a classic adjective list using machine learning and\ncompared its descriptive utility against the Big Five by analyzing one million\nReddit comments. The Big Five, particularly Agreeableness, Conscientiousness,\nand Neuroticism, provided a far more powerful and interpretable description of\nthese online communities. In contrast, our machine-learning clusters provided\nno meaningful distinctions, failed to recover the Extraversion trait, and\nlacked the psychometric coherence of the Big Five. These results affirm the\nrobustness of the Big Five and suggest personality's semantic structure is\ncontext-dependent. Our findings show that while machine learning can help check\nthe ecological validity of established psychological theories, it may not be\nable to replace them.", "AI": {"tldr": "\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u4ece\u5f62\u5bb9\u8bcd\u5217\u8868\u6784\u5efa\u81ea\u4e0b\u800c\u4e0a\u7684\u4eba\u683c\u6a21\u578b\uff0c\u4e0eBig Five\u6a21\u578b\u5728Reddit\u8bc4\u8bba\u6570\u636e\u4e0a\u6bd4\u8f83\uff0c\u53d1\u73b0Big Five\u6a21\u578b\u5728\u63cf\u8ff0\u5728\u7ebf\u793e\u533a\u65b9\u9762\u66f4\u5f3a\u5927\u548c\u53ef\u89e3\u91ca\u3002", "motivation": "\u9a8c\u8bc1\u8bcd\u6c47\u5047\u8bf4\uff0c\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u662f\u5426\u80fd\u66ff\u4ee3\u6216\u6539\u8fdb\u4f20\u7edf\u4eba\u683c\u6a21\u578b\u5982Big Five\u3002", "method": "\u4f7f\u7528\u7ecf\u5178\u5f62\u5bb9\u8bcd\u5217\u8868\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6784\u5efa\u4eba\u683c\u6a21\u578b\uff0c\u5e76\u5728100\u4e07\u6761Reddit\u8bc4\u8bba\u6570\u636e\u4e0a\u4e0eBig Five\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002", "result": "Big Five\u6a21\u578b\uff08\u7279\u522b\u662f\u5b9c\u4eba\u6027\u3001\u5c3d\u8d23\u6027\u548c\u795e\u7ecf\u8d28\uff09\u5728\u63cf\u8ff0\u5728\u7ebf\u793e\u533a\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u673a\u5668\u5b66\u4e60\u805a\u7c7b\u672a\u80fd\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u533a\u5206\uff0c\u672a\u80fd\u6062\u590d\u5916\u5411\u6027\u7279\u8d28\uff0c\u4e14\u7f3a\u4e4fBig Five\u7684\u5fc3\u7406\u6d4b\u91cf\u4e00\u81f4\u6027\u3002", "conclusion": "Big Five\u6a21\u578b\u5177\u6709\u7a33\u5065\u6027\uff0c\u4eba\u683c\u7684\u8bed\u4e49\u7ed3\u6784\u662f\u60c5\u5883\u4f9d\u8d56\u7684\uff0c\u673a\u5668\u5b66\u4e60\u53ef\u4ee5\u68c0\u9a8c\u73b0\u6709\u5fc3\u7406\u5b66\u7406\u8bba\u7684\u751f\u6001\u6548\u5ea6\uff0c\u4f46\u53ef\u80fd\u65e0\u6cd5\u66ff\u4ee3\u5b83\u4eec\u3002"}}
{"id": "2510.10596", "categories": ["cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.10596", "abs": "https://arxiv.org/abs/2510.10596", "authors": ["Ruolan Cheng", "Yong Deng", "Seraf\u00edn Moral", "Jos\u00e9 Ram\u00f3n Trillo"], "title": "A Distance Measure for Random Permutation Set: From the Layer-2 Belief Structure Perspective", "comment": null, "summary": "Random permutation set (RPS) is a recently proposed framework designed to\nrepresent order-structured uncertain information. Measuring the distance\nbetween permutation mass functions is a key research topic in RPS theory\n(RPST). This paper conducts an in-depth analysis of distances between RPSs from\ntwo different perspectives: random finite set (RFS) and transferable belief\nmodel (TBM). Adopting the layer-2 belief structure interpretation of RPS, we\nregard RPST as a refinement of TBM, where the order in the ordered focus set\nrepresents qualitative propensity. Starting from the permutation, we introduce\na new definition of the cumulative Jaccard index to quantify the similarity\nbetween two permutations and further propose a distance measure method for RPSs\nbased on the cumulative Jaccard index matrix. The metric and structural\nproperties of the proposed distance measure are investigated, including the\npositive definiteness analysis of the cumulative Jaccard index matrix, and a\ncorrection scheme is provided. The proposed method has a natural\ntop-weightiness property: inconsistencies between higher-ranked elements tend\nto result in greater distance values. Two parameters are provided to the\ndecision-maker to adjust the weight and truncation depth. Several numerical\nexamples are used to compare the proposed method with the existing method. The\nexperimental results show that the proposed method not only overcomes the\nshortcomings of the existing method and is compatible with the Jousselme\ndistance, but also has higher sensitivity and flexibility.", "AI": {"tldr": "\u672c\u6587\u4ece\u968f\u673a\u6709\u9650\u96c6\u548c\u53ef\u8f6c\u79fb\u4fe1\u5ff5\u6a21\u578b\u4e24\u4e2a\u89d2\u5ea6\u6df1\u5165\u5206\u6790\u4e86\u968f\u673a\u7f6e\u6362\u96c6\u4e4b\u95f4\u7684\u8ddd\u79bb\u5ea6\u91cf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7d2f\u79efJaccard\u6307\u6570\u7684\u8ddd\u79bb\u6d4b\u91cf\u65b9\u6cd5\uff0c\u5177\u6709\u81ea\u7136\u7684\u4e0a\u4f4d\u6743\u91cd\u7279\u6027\u548c\u66f4\u9ad8\u7684\u7075\u654f\u5ea6\u4e0e\u7075\u6d3b\u6027\u3002", "motivation": "\u968f\u673a\u7f6e\u6362\u96c6\u662f\u8868\u793a\u6709\u5e8f\u7ed3\u6784\u4e0d\u786e\u5b9a\u4fe1\u606f\u7684\u65b0\u6846\u67b6\uff0c\u6d4b\u91cf\u7f6e\u6362\u8d28\u91cf\u51fd\u6570\u4e4b\u95f4\u7684\u8ddd\u79bb\u662fRPS\u7406\u8bba\u7684\u5173\u952e\u7814\u7a76\u8bfe\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u8ddd\u79bb\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "\u91c7\u7528RPS\u7684\u5c42-2\u4fe1\u5ff5\u7ed3\u6784\u89e3\u91ca\uff0c\u4ece\u7f6e\u6362\u51fa\u53d1\u5f15\u5165\u7d2f\u79efJaccard\u6307\u6570\u6765\u91cf\u5316\u4e24\u4e2a\u7f6e\u6362\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u5e76\u57fa\u4e8e\u7d2f\u79efJaccard\u6307\u6570\u77e9\u9635\u63d0\u51faRPS\u7684\u8ddd\u79bb\u6d4b\u91cf\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5177\u6709\u81ea\u7136\u7684\u4e0a\u4f4d\u6743\u91cd\u7279\u6027\uff1a\u8f83\u9ad8\u6392\u540d\u5143\u7d20\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\u4f1a\u5bfc\u81f4\u66f4\u5927\u7684\u8ddd\u79bb\u503c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7f3a\u70b9\uff0c\u4e0eJousselme\u8ddd\u79bb\u517c\u5bb9\uff0c\u4e14\u5177\u6709\u66f4\u9ad8\u7684\u7075\u654f\u5ea6\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u57fa\u4e8e\u7d2f\u79efJaccard\u6307\u6570\u7684\u8ddd\u79bb\u6d4b\u91cf\u65b9\u6cd5\u4e3a\u968f\u673a\u7f6e\u6362\u96c6\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8ddd\u79bb\u5ea6\u91cf\u5de5\u5177\uff0c\u5177\u6709\u7406\u8bba\u5408\u7406\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.10374", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10374", "abs": "https://arxiv.org/abs/2510.10374", "authors": ["Ziyi Wei", "Huaiyang Zhong", "Xiaocheng Li"], "title": "Exploration-free Algorithms for Multi-group Mean Estimation", "comment": null, "summary": "We address the problem of multi-group mean estimation, which seeks to\nallocate a finite sampling budget across multiple groups to obtain uniformly\naccurate estimates of their means. Unlike classical multi-armed bandits, whose\nobjective is to minimize regret by identifying and exploiting the best arm, the\noptimal allocation in this setting requires sampling every group on the order\nof $\\Theta(T)$ times. This fundamental distinction makes exploration-free\nalgorithms both natural and effective. Our work makes three contributions.\nFirst, we strengthen the existing results on subgaussian variance concentration\nusing the Hanson-Wright inequality and identify a class of strictly subgaussian\ndistributions that yield sharper guarantees. Second, we design exploration-free\nnon-adaptive and adaptive algorithms, and we establish tighter regret bounds\nthan the existing results. Third, we extend the framework to contextual bandit\nsettings, an underexplored direction, and propose algorithms that leverage side\ninformation with provable guarantees. Overall, these results position\nexploration-free allocation as a principled and efficient approach to\nmulti-group mean estimation, with potential applications in experimental\ndesign, personalization, and other domains requiring accurate multi-group\ninference.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u7ec4\u5747\u503c\u4f30\u8ba1\u95ee\u9898\uff0c\u63d0\u51fa\u63a2\u7d22\u65e0\u5173\u7684\u5206\u914d\u7b97\u6cd5\uff0c\u5728\u6709\u9650\u91c7\u6837\u9884\u7b97\u4e0b\u83b7\u5f97\u5404\u7ec4\u5747\u503c\u7684\u7edf\u4e00\u51c6\u786e\u4f30\u8ba1\u3002\u4e0e\u7ecf\u5178\u591a\u81c2\u8001\u864e\u673a\u4e0d\u540c\uff0c\u8be5\u95ee\u9898\u9700\u8981\u91c7\u6837\u6bcf\u4e2a\u7ec4\u0398(T)\u6b21\uff0c\u56e0\u6b64\u63a2\u7d22\u65e0\u5173\u7b97\u6cd5\u65e2\u81ea\u7136\u53c8\u6709\u6548\u3002", "motivation": "\u89e3\u51b3\u591a\u7ec4\u5747\u503c\u4f30\u8ba1\u95ee\u9898\uff0c\u5728\u6709\u9650\u91c7\u6837\u9884\u7b97\u4e0b\u83b7\u5f97\u6240\u6709\u7ec4\u5747\u503c\u7684\u7edf\u4e00\u51c6\u786e\u4f30\u8ba1\u3002\u4e0e\u7ecf\u5178\u591a\u81c2\u8001\u864e\u673a\u4e13\u6ce8\u4e8e\u8bc6\u522b\u6700\u4f73\u81c2\u4e0d\u540c\uff0c\u8be5\u95ee\u9898\u9700\u8981\u5e73\u8861\u91c7\u6837\u6240\u6709\u7ec4\uff0c\u56e0\u6b64\u63a2\u7d22\u65e0\u5173\u65b9\u6cd5\u66f4\u4e3a\u5408\u9002\u3002", "method": "\u4f7f\u7528Hanson-Wright\u4e0d\u7b49\u5f0f\u52a0\u5f3a\u4e9a\u9ad8\u65af\u65b9\u5dee\u96c6\u4e2d\u6027\u7ed3\u679c\uff0c\u8bc6\u522b\u4e25\u683c\u4e9a\u9ad8\u65af\u5206\u5e03\u7c7b\u522b\u4ee5\u83b7\u5f97\u66f4\u5c16\u9510\u4fdd\u8bc1\u3002\u8bbe\u8ba1\u63a2\u7d22\u65e0\u5173\u7684\u975e\u81ea\u9002\u5e94\u548c\u81ea\u9002\u5e94\u7b97\u6cd5\uff0c\u5e76\u5c06\u6846\u67b6\u6269\u5c55\u5230\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u8bbe\u7f6e\u3002", "result": "\u5efa\u7acb\u4e86\u6bd4\u73b0\u6709\u7ed3\u679c\u66f4\u7d27\u5bc6\u7684\u9057\u61be\u754c\uff0c\u63d0\u51fa\u4e86\u5728\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u8bbe\u7f6e\u4e2d\u5229\u7528\u4fa7\u4fe1\u606f\u7684\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u8bc1\u660e\u7684\u4fdd\u8bc1\u3002", "conclusion": "\u63a2\u7d22\u65e0\u5173\u5206\u914d\u662f\u591a\u7ec4\u5747\u503c\u4f30\u8ba1\u7684\u539f\u5219\u6027\u548c\u9ad8\u6548\u65b9\u6cd5\uff0c\u5728\u5b9e\u9a8c\u8bbe\u8ba1\u3001\u4e2a\u6027\u5316\u7b49\u9700\u8981\u51c6\u786e\u591a\u7ec4\u63a8\u65ad\u7684\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.11692", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.11692", "abs": "https://arxiv.org/abs/2510.11692", "authors": ["Samuel G. Gessow", "Brett T. Lopez"], "title": "Analysis of the Geometric Heat Flow Equation: Computing Geodesics in Real-Time with Convergence Guarantees", "comment": null, "summary": "We present an analysis on the convergence properties of the so-called\ngeometric heat flow equation for computing geodesics (shortest-path~curves) on\nRiemannian manifolds. Computing geodesics numerically in real-time has become\nan important capability in several fields, including control and motion\nplanning. The geometric heat flow equation involves solving a parabolic partial\ndifferential equation whose solution is a geodesic. In practice, solving this\nPDE numerically can be done efficiently, and tends to be more numerically\nstable and exhibit a better rate of convergence compared to numerical\noptimization. We prove that the geometric heat flow equation is globally\nexponentially stable in $L_2$ if the curvature of the Riemannian manifold is\nnot too positive, and that asymptotic convergence in $L_2$ is always\nguaranteed. We also present a pseudospectral method that leverages Chebyshev\npolynomials to accurately compute geodesics in only a few milliseconds for\nnon-contrived manifolds. Our analysis was verified with our custom\npseudospectral method by computing geodesics on common non-Euclidean surfaces,\nand in feedback for a contraction-based controller with a non-flat metric for a\nnonlinear system.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u9ece\u66fc\u6d41\u5f62\u4e0a\u8ba1\u7b97\u6d4b\u5730\u7ebf\u7684\u51e0\u4f55\u70ed\u6d41\u65b9\u7a0b\u7684\u6536\u655b\u6027\uff0c\u8bc1\u660e\u4e86\u5728\u66f2\u7387\u4e0d\u8fc7\u4e8e\u6b63\u7684\u60c5\u51b5\u4e0b\u5177\u6709\u5168\u5c40\u6307\u6570\u7a33\u5b9a\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eChebyshev\u591a\u9879\u5f0f\u7684\u4f2a\u8c31\u65b9\u6cd5\u5b9e\u73b0\u6beb\u79d2\u7ea7\u6d4b\u5730\u7ebf\u8ba1\u7b97\u3002", "motivation": "\u5b9e\u65f6\u8ba1\u7b97\u6d4b\u5730\u7ebf\u5728\u63a7\u5236\u548c\u8fd0\u52a8\u89c4\u5212\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u51e0\u4f55\u70ed\u6d41\u65b9\u7a0b\u76f8\u6bd4\u6570\u503c\u4f18\u5316\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6570\u503c\u7a33\u5b9a\u6027\u548c\u6536\u655b\u901f\u5ea6\u3002", "method": "\u4f7f\u7528\u51e0\u4f55\u70ed\u6d41\u65b9\u7a0b\u6c42\u89e3\u629b\u7269\u578b\u504f\u5fae\u5206\u65b9\u7a0b\u6765\u8ba1\u7b97\u6d4b\u5730\u7ebf\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8eChebyshev\u591a\u9879\u5f0f\u7684\u4f2a\u8c31\u65b9\u6cd5\u8fdb\u884c\u9ad8\u6548\u6570\u503c\u8ba1\u7b97\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u66f2\u7387\u4e0d\u8fc7\u4e8e\u6b63\u7684\u9ece\u66fc\u6d41\u5f62\u4e0a\uff0c\u51e0\u4f55\u70ed\u6d41\u65b9\u7a0b\u5177\u6709\u5168\u5c40\u6307\u6570\u7a33\u5b9a\u6027\uff0c\u4e14\u603b\u80fd\u4fdd\u8bc1L2\u8303\u6570\u4e0b\u7684\u6e10\u8fd1\u6536\u655b\u3002\u4f2a\u8c31\u65b9\u6cd5\u80fd\u5728\u51e0\u6beb\u79d2\u5185\u51c6\u786e\u8ba1\u7b97\u975e\u5e73\u51e1\u6d41\u5f62\u4e0a\u7684\u6d4b\u5730\u7ebf\u3002", "conclusion": "\u51e0\u4f55\u70ed\u6d41\u65b9\u7a0b\u4e3a\u5b9e\u65f6\u8ba1\u7b97\u6d4b\u5730\u7ebf\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u9ad8\u6548\u6570\u503c\u65b9\u6cd5\uff0c\u5728\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u63a7\u5236\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u6027\u80fd\u3002"}}
{"id": "2510.11519", "categories": ["math.OC", "49K35, 90C30, 90C31"], "pdf": "https://arxiv.org/pdf/2510.11519", "abs": "https://arxiv.org/abs/2510.11519", "authors": ["Yang Zhou", "Xiaojun Chen"], "title": "Robust Least Squares Problems with Binary Uncertain Data", "comment": null, "summary": "We propose a Binary Robust Least Squares (BRLS) model that encompasses key\nrobust least squares formulations, such as those involving uncertain binary\nlabels and adversarial noise constrained within a hypercube. We show that the\ngeometric structure of the noise propagation matrix, particularly whether its\ncolumns form acute or obtuse angles, implies the supermodularity or\nsubmodularity of the inner maximization problem. This structural property\nenables us to integrate powerful combinatorial optimization tools into a\ngradient-based minimax algorithmic framework. For the robust linear least\nsquares problem with the supermodularity, we establish the relationship between\nthe minimax points of BRLS and saddle points of its continuous relaxation, and\npropose a projected gradient algorithm computing $\\epsilon$-global minimax\npoints in $O(\\epsilon^{-2})$ iterations. For the robust nonlinear least squares\nproblem with supermodularity, we develop a revised framework that finds\n$\\epsilon$-stationary points in the sense of expectation within\n$O(\\epsilon^{-4})$ iterations. For the robust linear least squares problem with\nthe submodularity, we employ a double greedy algorithm as a subsolver,\nguaranteeing a $(\\frac{1}{3}, \\epsilon)$-approximate minimax point in\n$O(\\epsilon^{-2})$ iterations. Numerical experiments on health status\nprediction and phase retrieval demonstrate that BRLS achieves superior\nrobustness against structured noise compared to classical least squares\nproblems and LASSO.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e8c\u8fdb\u5236\u9c81\u68d2\u6700\u5c0f\u4e8c\u4e58\u6a21\u578b\uff0c\u9488\u5bf9\u542b\u4e0d\u786e\u5b9a\u4e8c\u8fdb\u5236\u6807\u7b7e\u548c\u8d85\u7acb\u65b9\u4f53\u7ea6\u675f\u5bf9\u6297\u566a\u58f0\u7684\u95ee\u9898\u3002\u5229\u7528\u566a\u58f0\u4f20\u64ad\u77e9\u9635\u7684\u51e0\u4f55\u7ed3\u6784\u6027\u8d28\uff0c\u5c06\u7ec4\u5408\u4f18\u5316\u5de5\u5177\u96c6\u6210\u5230\u68af\u5ea6\u6781\u5c0f\u6781\u5927\u7b97\u6cd5\u6846\u67b6\u4e2d\uff0c\u4e3a\u4e0d\u540c\u60c5\u51b5\u8bbe\u8ba1\u4e86\u9ad8\u6548\u6c42\u89e3\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6700\u5c0f\u4e8c\u4e58\u65b9\u6cd5\u5bf9\u7ed3\u6784\u5316\u566a\u58f0\uff08\u5982\u4e0d\u786e\u5b9a\u4e8c\u8fdb\u5236\u6807\u7b7e\u548c\u5bf9\u6297\u566a\u58f0\uff09\u4e0d\u591f\u9c81\u68d2\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u8fd9\u7c7b\u566a\u58f0\u7684\u9c81\u68d2\u4f18\u5316\u6a21\u578b\u3002", "method": "\u57fa\u4e8e\u566a\u58f0\u4f20\u64ad\u77e9\u9635\u7684\u51e0\u4f55\u7ed3\u6784\uff08\u5217\u5411\u91cf\u5f62\u6210\u7684\u89d2\u5ea6\uff09\uff0c\u5c06\u5185\u5c42\u6700\u5927\u5316\u95ee\u9898\u5206\u7c7b\u4e3a\u8d85\u6a21\u6216\u5b50\u6a21\u95ee\u9898\uff0c\u5206\u522b\u8bbe\u8ba1\u6295\u5f71\u68af\u5ea6\u7b97\u6cd5\u3001\u4fee\u6b63\u6846\u67b6\u548c\u53cc\u8d2a\u5a6a\u7b97\u6cd5\u6765\u6c42\u89e3\u3002", "result": "\u5bf9\u4e8e\u8d85\u6a21\u60c5\u51b5\uff0c\u7ebf\u6027\u95ee\u9898\u53ef\u5728O(\u03b5\u207b\u00b2)\u8fed\u4ee3\u5185\u627e\u5230\u03b5-\u5168\u5c40\u6781\u5c0f\u6781\u5927\u70b9\uff0c\u975e\u7ebf\u6027\u95ee\u9898\u5728O(\u03b5\u207b\u2074)\u8fed\u4ee3\u5185\u627e\u5230\u671f\u671b\u610f\u4e49\u4e0b\u7684\u03b5-\u7a33\u5b9a\u70b9\uff1b\u5bf9\u4e8e\u5b50\u6a21\u60c5\u51b5\uff0c\u53ef\u5728O(\u03b5\u207b\u00b2)\u8fed\u4ee3\u5185\u627e\u5230(1/3, \u03b5)-\u8fd1\u4f3c\u6781\u5c0f\u6781\u5927\u70b9\u3002", "conclusion": "BRLS\u6a21\u578b\u5728\u5065\u5eb7\u72b6\u6001\u9884\u6d4b\u548c\u76f8\u4f4d\u6062\u590d\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u6bd4\u4f20\u7edf\u6700\u5c0f\u4e8c\u4e58\u548cLASSO\u66f4\u597d\u7684\u6297\u7ed3\u6784\u5316\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.10062", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10062", "abs": "https://arxiv.org/abs/2510.10062", "authors": ["Adnan El Assadi", "Isaac Chung", "Roman Solomatin", "Niklas Muennighoff", "Kenneth Enevoldsen"], "title": "HUME: Measuring the Human-Model Performance Gap in Text Embedding Task", "comment": "Submitted to ICLR 2026", "summary": "Comparing human and model performance offers a valuable perspective for\nunderstanding the strengths and limitations of embedding models, highlighting\nwhere they succeed and where they fail to capture meaning and nuance. However,\nsuch comparisons are rarely made, as human performance on embedding tasks is\ndifficult to measure. To fill this gap, we introduce HUME: Human Evaluation\nFramework for Text Embeddings. While frameworks like MTEB provide broad model\nevaluation, they lack reliable estimates of human performance, limiting the\ninterpretability of model scores. We measure human performance across 16 MTEB\ndatasets spanning reranking, classification, clustering, and semantic textual\nsimilarity across linguistically diverse high- and low-resource languages.\nHumans achieve an average performance of 77.6% compared to 80.1% for the best\nembedding model, although variation is substantial: models reach near-ceiling\nperformance on some datasets while struggling on others, suggesting dataset\nissues and revealing shortcomings in low-resource languages. We provide human\nperformance baselines, insight into task difficulty patterns, and an extensible\nevaluation framework that enables a more meaningful interpretation of the model\nand informs the development of both models and benchmarks. Our code, dataset,\nand leaderboard are publicly available at\nhttps://github.com/embeddings-benchmark/mteb.", "AI": {"tldr": "\u63d0\u51fa\u4e86HUME\u6846\u67b6\u6765\u6d4b\u91cf\u6587\u672c\u5d4c\u5165\u4efb\u52a1\u4e2d\u7684\u4eba\u7c7b\u8868\u73b0\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u7f3a\u4e4f\u4eba\u7c7b\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u901a\u8fc716\u4e2aMTEB\u6570\u636e\u96c6\u6bd4\u8f83\u4eba\u7c7b\u4e0e\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u5d4c\u5165\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\u5982MTEB\u7f3a\u4e4f\u53ef\u9760\u7684\u4eba\u7c7b\u8868\u73b0\u4f30\u8ba1\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5206\u6570\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u96be\u4ee5\u7406\u89e3\u6a21\u578b\u5728\u6355\u6349\u8bed\u4e49\u548c\u7ec6\u5fae\u5dee\u522b\u65b9\u9762\u7684\u771f\u5b9e\u80fd\u529b\u3002", "method": "\u5f00\u53d1HUME\u4eba\u7c7b\u8bc4\u4f30\u6846\u67b6\uff0c\u572816\u4e2aMTEB\u6570\u636e\u96c6\u4e0a\u6d4b\u91cf\u4eba\u7c7b\u8868\u73b0\uff0c\u6db5\u76d6\u91cd\u6392\u5e8f\u3001\u5206\u7c7b\u3001\u805a\u7c7b\u548c\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u4efb\u52a1\uff0c\u8986\u76d6\u9ad8\u8d44\u6e90\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002", "result": "\u4eba\u7c7b\u5e73\u5747\u8868\u73b0\u8fbe\u523077.6%\uff0c\u6700\u4f73\u5d4c\u5165\u6a21\u578b\u4e3a80.1%\uff1b\u6a21\u578b\u5728\u67d0\u4e9b\u6570\u636e\u96c6\u4e0a\u63a5\u8fd1\u5929\u82b1\u677f\u6027\u80fd\uff0c\u4f46\u5728\u5176\u4ed6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4eba\u7c7b\u6027\u80fd\u57fa\u51c6\u3001\u4efb\u52a1\u96be\u5ea6\u6a21\u5f0f\u6d1e\u5bdf\u548c\u53ef\u6269\u5c55\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u6a21\u578b\u8bc4\u4f30\u66f4\u6709\u610f\u4e49\uff0c\u5e76\u4e3a\u6a21\u578b\u548c\u57fa\u51c6\u5f00\u53d1\u63d0\u4f9b\u4fe1\u606f\u3002"}}
{"id": "2510.09740", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09740", "abs": "https://arxiv.org/abs/2510.09740", "authors": ["Atharv Goel", "Sharat Agarwal", "Saket Anand", "Chetan Arora"], "title": "Reliable Active Learning from Unreliable Labels via Neural Collapse Geometry", "comment": "Accepted to NeurIPS 2025 Workshop on Reliable ML from Unreliable Data", "summary": "Active Learning (AL) promises to reduce annotation cost by prioritizing\ninformative samples, yet its reliability is undermined when labels are noisy or\nwhen the data distribution shifts. In practice, annotators make mistakes, rare\ncategories are ambiguous, and conventional AL heuristics (uncertainty,\ndiversity) often amplify such errors by repeatedly selecting mislabeled or\nredundant samples. We propose Reliable Active Learning via Neural Collapse\nGeometry (NCAL-R), a framework that leverages the emergent geometric\nregularities of deep networks to counteract unreliable supervision. Our method\nintroduces two complementary signals: (i) a Class-Mean Alignment Perturbation\nscore, which quantifies how candidate samples structurally stabilize or distort\ninter-class geometry, and (ii) a Feature Fluctuation score, which captures\ntemporal instability of representations across training checkpoints. By\ncombining these signals, NCAL-R prioritizes samples that both preserve class\nseparation and highlight ambiguous regions, mitigating the effect of noisy or\nredundant labels. Experiments on ImageNet-100 and CIFAR100 show that NCAL-R\nconsistently outperforms standard AL baselines, achieving higher accuracy with\nfewer labels, improved robustness under synthetic label noise, and stronger\ngeneralization to out-of-distribution data. These results suggest that\nincorporating geometric reliability criteria into acquisition decisions can\nmake Active Learning less brittle to annotation errors and distribution shifts,\na key step toward trustworthy deployment in real-world labeling pipelines. Our\ncode is available at https://github.com/Vision-IIITD/NCAL.", "AI": {"tldr": "\u63d0\u51fa\u4e86NCAL-R\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u5d29\u6e83\u51e0\u4f55\u5b66\u89e3\u51b3\u4e3b\u52a8\u5b66\u4e60\u4e2d\u6807\u7b7e\u566a\u58f0\u548c\u6570\u636e\u5206\u5e03\u504f\u79fb\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u7ed3\u5408\u7c7b\u5747\u503c\u5bf9\u9f50\u6270\u52a8\u548c\u7279\u5f81\u6ce2\u52a8\u8bc4\u5206\u6765\u9009\u62e9\u6837\u672c\u3002", "motivation": "\u4f20\u7edf\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u5728\u6807\u7b7e\u566a\u58f0\u548c\u6570\u636e\u5206\u5e03\u504f\u79fb\u4e0b\u4e0d\u53ef\u9760\uff0c\u56e0\u4e3a\u4e0d\u786e\u5b9a\u6027\u3001\u591a\u6837\u6027\u7b49\u542f\u53d1\u5f0f\u65b9\u6cd5\u4f1a\u653e\u5927\u9519\u8bef\u6807\u7b7e\u6216\u5197\u4f59\u6837\u672c\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u4e92\u8865\u4fe1\u53f7\uff1a\u7c7b\u5747\u503c\u5bf9\u9f50\u6270\u52a8\u8bc4\u5206\uff08\u91cf\u5316\u6837\u672c\u5bf9\u7c7b\u95f4\u51e0\u4f55\u7ed3\u6784\u7684\u5f71\u54cd\uff09\u548c\u7279\u5f81\u6ce2\u52a8\u8bc4\u5206\uff08\u6355\u6349\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8868\u793a\u7684\u65f6\u95f4\u4e0d\u7a33\u5b9a\u6027\uff09\uff0c\u7ed3\u5408\u8fd9\u4e9b\u4fe1\u53f7\u9009\u62e9\u80fd\u4fdd\u6301\u7c7b\u522b\u5206\u79bb\u5e76\u7a81\u51fa\u6a21\u7cca\u533a\u57df\u7684\u6837\u672c\u3002", "result": "\u5728ImageNet-100\u548cCIFAR100\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNCAL-R\u5728\u5408\u6210\u6807\u7b7e\u566a\u58f0\u4e0b\u5177\u6709\u66f4\u9ad8\u51c6\u786e\u7387\u3001\u66f4\u5f3a\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u6cdb\u5316\u5230\u5206\u5e03\u5916\u6570\u636e\u3002", "conclusion": "\u5c06\u51e0\u4f55\u53ef\u9760\u6027\u6807\u51c6\u7eb3\u5165\u4e3b\u52a8\u5b66\u4e60\u91c7\u96c6\u51b3\u7b56\u53ef\u4ee5\u51cf\u5c11\u5bf9\u6807\u6ce8\u9519\u8bef\u548c\u5206\u5e03\u504f\u79fb\u7684\u8106\u5f31\u6027\uff0c\u662f\u5b9e\u73b0\u771f\u5b9e\u4e16\u754c\u6807\u6ce8\u7ba1\u9053\u4e2d\u53ef\u4fe1\u90e8\u7f72\u7684\u5173\u952e\u6b65\u9aa4\u3002"}}
{"id": "2510.10603", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10603", "abs": "https://arxiv.org/abs/2510.10603", "authors": ["WenTao Liu", "Siyu Song", "Hao Hao", "Aimin Zhou"], "title": "EA4LLM: A Gradient-Free Approach to Large Language Model Optimization via Evolutionary Algorithms", "comment": null, "summary": "In recent years, large language models (LLMs) have made remarkable progress,\nwith model optimization primarily relying on gradient-based optimizers such as\nAdam. However, these gradient-based methods impose stringent hardware\nrequirements, demanding high-concurrency, high-memory GPUs. Moreover, they\nrequire all neural network operations to be differentiable, thereby excluding\nmany promising non-differentiable architectures from practical use. To address\nthese limitations, we propose a method for optimizing LLMs using evolutionary\nalgorithms (EA4LLM) and, for the first time, successfully demonstrate its\ncapability to train a 1-billion-parameter LLM from the pre-trained stage. We\nconduct extensive experiments and provide key insights into how evolutionary\nalgorithms can effectively optimize neural networks. Our work challenges the\nprevailing assumption that gradient-based optimization is the only viable\napproach for training neural networks. It also holds significant potential to\nreduce the computational cost of training large language models, thereby\nenabling groups with limited computational resources to participate in deep\nlearning research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u8fdb\u5316\u7b97\u6cd5\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5(EA4LLM)\uff0c\u9996\u6b21\u6210\u529f\u8bad\u7ec3\u4e8610\u4ebf\u53c2\u6570\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6311\u6218\u4e86\u57fa\u4e8e\u68af\u5ea6\u4f18\u5316\u662f\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u552f\u4e00\u53ef\u884c\u65b9\u6cd5\u7684\u666e\u904d\u5047\u8bbe\u3002", "motivation": "\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u5668\u5982Adam\u5bf9\u786c\u4ef6\u8981\u6c42\u4e25\u683c\uff0c\u9700\u8981\u9ad8\u5e76\u53d1\u3001\u9ad8\u5185\u5b58\u7684GPU\uff0c\u4e14\u8981\u6c42\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u64cd\u4f5c\u53ef\u5fae\u5206\uff0c\u6392\u9664\u4e86\u8bb8\u591a\u6709\u524d\u666f\u7684\u4e0d\u53ef\u5fae\u5206\u67b6\u6784\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u8fdb\u5316\u7b97\u6cd5\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6210\u529f\u4ece\u9884\u8bad\u7ec3\u9636\u6bb5\u5f00\u59cb\u8bad\u7ec310\u4ebf\u53c2\u6570\u7684LLM\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u5173\u4e8e\u8fdb\u5316\u7b97\u6cd5\u5982\u4f55\u6709\u6548\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u663e\u8457\u6f5c\u529b\u964d\u4f4e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\uff0c\u4f7f\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u7fa4\u4f53\u80fd\u591f\u53c2\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u3002"}}
{"id": "2510.10544", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10544", "abs": "https://arxiv.org/abs/2510.10544", "authors": ["Abdelkrim Zitouni", "Mehdi Hennequin", "Juba Agoun", "Ryan Horache", "Nadia Kabachi", "Omar Rivasplata"], "title": "PAC-Bayesian Reinforcement Learning Trains Generalizable Policies", "comment": null, "summary": "We derive a novel PAC-Bayesian generalization bound for reinforcement\nlearning that explicitly accounts for Markov dependencies in the data, through\nthe chain's mixing time. This contributes to overcoming challenges in obtaining\ngeneralization guarantees for reinforcement learning, where the sequential\nnature of data breaks the independence assumptions underlying classical bounds.\nOur bound provides non-vacuous certificates for modern off-policy algorithms\nlike Soft Actor-Critic. We demonstrate the bound's practical utility through\nPB-SAC, a novel algorithm that optimizes the bound during training to guide\nexploration. Experiments across continuous control tasks show that our approach\nprovides meaningful confidence certificates while maintaining competitive\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8003\u8651\u9a6c\u5c14\u53ef\u592b\u4f9d\u8d56\u6027\u7684PAC-Bayesian\u5f3a\u5316\u5b66\u4e60\u6cdb\u5316\u754c\uff0c\u901a\u8fc7\u94fe\u7684\u6df7\u5408\u65f6\u95f4\u663e\u5f0f\u5904\u7406\u6570\u636e\u4f9d\u8d56\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u4f18\u5316\u8be5\u754c\u7684PB-SAC\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7531\u4e8e\u6570\u636e\u5e8f\u5217\u6027\u7834\u574f\u72ec\u7acb\u6027\u5047\u8bbe\u800c\u96be\u4ee5\u83b7\u5f97\u6cdb\u5316\u4fdd\u8bc1\u7684\u6311\u6218\u3002", "method": "\u63a8\u5bfc\u4e86\u8003\u8651\u9a6c\u5c14\u53ef\u592b\u4f9d\u8d56\u6027\u7684PAC-Bayesian\u6cdb\u5316\u754c\uff0c\u5e76\u8bbe\u8ba1\u4e86PB-SAC\u7b97\u6cd5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f18\u5316\u8be5\u754c\u6765\u6307\u5bfc\u63a2\u7d22\u3002", "result": "\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u610f\u4e49\u7684\u7f6e\u4fe1\u8bc1\u4e66\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6cdb\u5316\u754c\u4e3a\u975e\u7b56\u7565\u7b97\u6cd5\u63d0\u4f9b\u4e86\u975e\u5e73\u51e1\u7684\u6cdb\u5316\u4fdd\u8bc1\uff0cPB-SAC\u7b97\u6cd5\u5728\u5b9e\u8df5\u4e2d\u6709\u6548\u5229\u7528\u4e86\u8fd9\u4e9b\u4fdd\u8bc1\u6765\u6307\u5bfc\u63a2\u7d22\u3002"}}
{"id": "2510.11554", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2510.11554", "abs": "https://arxiv.org/abs/2510.11554", "authors": ["Shaoze Li", "Junhao Wu", "Cheng Lu", "Zhibin Deng", "Shu-Cherng Fang"], "title": "An Efficient Solution Method for Solving Convex Separable Quadratic Optimization Problems", "comment": null, "summary": "Convex separable quadratic optimization problems occur in many practical\napplications. In this paper, based on an iterative resolution scheme of the KKT\nsystem, we develop an efficient method for solving a quadratic programming\nproblem with a convex separable objective function subject to multiple convex\nseparable constraints. We show that the proposed approach leads to a dual\ncoordinate ascent algorithm and provide a convergence proof. Numerical\nexperiments support the superior performance of the proposed method to that of\nthe Gurobi solver, especially for solving large-scale convex separate quadratic\nprogramming problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKKT\u7cfb\u7edf\u8fed\u4ee3\u6c42\u89e3\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5177\u6709\u51f8\u53ef\u5206\u76ee\u6807\u51fd\u6570\u548c\u591a\u4e2a\u51f8\u53ef\u5206\u7ea6\u675f\u7684\u4e8c\u6b21\u89c4\u5212\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u8f6c\u5316\u4e3a\u5bf9\u5076\u5750\u6807\u4e0a\u5347\u7b97\u6cd5\uff0c\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u4e8eGurobi\u6c42\u89e3\u5668\u3002", "motivation": "\u51f8\u53ef\u5206\u4e8c\u6b21\u4f18\u5316\u95ee\u9898\u5728\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u4e2d\u9891\u7e41\u51fa\u73b0\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u6c42\u89e3\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5927\u89c4\u6a21\u95ee\u9898\u3002", "method": "\u57fa\u4e8eKKT\u7cfb\u7edf\u7684\u8fed\u4ee3\u6c42\u89e3\u65b9\u6848\uff0c\u5f00\u53d1\u4e86\u5bf9\u5076\u5750\u6807\u4e0a\u5347\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u6536\u655b\u6027\u8bc1\u660e\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6c42\u89e3\u5927\u89c4\u6a21\u51f8\u53ef\u5206\u4e8c\u6b21\u89c4\u5212\u95ee\u9898\u65f6\uff0c\u6027\u80fd\u4f18\u4e8eGurobi\u6c42\u89e3\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6c42\u89e3\u51f8\u53ef\u5206\u4e8c\u6b21\u89c4\u5212\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u95ee\u9898\u3002"}}
{"id": "2510.10063", "categories": ["cs.CL", "cs.AI", "68T50, 68T07, 68T27", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.10063", "abs": "https://arxiv.org/abs/2510.10063", "authors": ["Yibo Yang"], "title": "CLMN: Concept based Language Models via Neural Symbolic Reasoning", "comment": "7 pages, 2 figures", "summary": "Deep learning has advanced NLP, but interpretability remains limited,\nespecially in healthcare and finance. Concept bottleneck models tie predictions\nto human concepts in vision, but NLP versions either use binary activations\nthat harm text representations or latent concepts that weaken semantics, and\nthey rarely model dynamic concept interactions such as negation and context. We\nintroduce the Concept Language Model Network (CLMN), a neural-symbolic\nframework that keeps both performance and interpretability. CLMN represents\nconcepts as continuous, human-readable embeddings and applies fuzzy-logic\nreasoning to learn adaptive interaction rules that state how concepts affect\neach other and the final decision. The model augments original text features\nwith concept-aware representations and automatically induces interpretable\nlogic rules. Across multiple datasets and pre-trained language models, CLMN\nachieves higher accuracy than existing concept-based methods while improving\nexplanation quality. These results show that integrating neural representations\nwith symbolic reasoning in a unified concept space can yield practical,\ntransparent NLP systems.", "AI": {"tldr": "CLMN\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u8fde\u7eed\u53ef\u8bfb\u7684\u6982\u5ff5\u5d4c\u5165\u548c\u6a21\u7cca\u903b\u8f91\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347NLP\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u52a8\u6001\u6982\u5ff5\u4ea4\u4e92\u65b9\u9762\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728NLP\u4e2d\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u53ef\u89e3\u91ca\u6027\u4ecd\u7136\u6709\u9650\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u548c\u91d1\u878d\u9886\u57df\u3002\u73b0\u6709\u7684\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u8981\u4e48\u4f7f\u7528\u635f\u5bb3\u6587\u672c\u8868\u793a\u7684\u4e8c\u5143\u6fc0\u6d3b\uff0c\u8981\u4e48\u4f7f\u7528\u524a\u5f31\u8bed\u4e49\u7684\u6f5c\u5728\u6982\u5ff5\uff0c\u4e14\u5f88\u5c11\u5efa\u6a21\u52a8\u6001\u6982\u5ff5\u4ea4\u4e92\u3002", "method": "CLMN\u5c06\u6982\u5ff5\u8868\u793a\u4e3a\u8fde\u7eed\u3001\u4eba\u7c7b\u53ef\u8bfb\u7684\u5d4c\u5165\uff0c\u5e94\u7528\u6a21\u7cca\u903b\u8f91\u63a8\u7406\u5b66\u4e60\u81ea\u9002\u5e94\u4ea4\u4e92\u89c4\u5219\uff0c\u8bf4\u660e\u6982\u5ff5\u5982\u4f55\u76f8\u4e92\u5f71\u54cd\u53ca\u5f71\u54cd\u6700\u7ec8\u51b3\u7b56\u3002\u6a21\u578b\u901a\u8fc7\u6982\u5ff5\u611f\u77e5\u8868\u793a\u589e\u5f3a\u539f\u59cb\u6587\u672c\u7279\u5f81\uff0c\u5e76\u81ea\u52a8\u63a8\u5bfc\u53ef\u89e3\u91ca\u7684\u903b\u8f91\u89c4\u5219\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e0a\uff0cCLMN\u6bd4\u73b0\u6709\u57fa\u4e8e\u6982\u5ff5\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u89e3\u91ca\u8d28\u91cf\u3002", "conclusion": "\u5728\u7edf\u4e00\u6982\u5ff5\u7a7a\u95f4\u4e2d\u6574\u5408\u795e\u7ecf\u8868\u793a\u4e0e\u7b26\u53f7\u63a8\u7406\u53ef\u4ee5\u4ea7\u751f\u5b9e\u7528\u3001\u900f\u660e\u7684NLP\u7cfb\u7edf\u3002"}}
{"id": "2510.10633", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10633", "abs": "https://arxiv.org/abs/2510.10633", "authors": ["Jiabao Shi", "Minfeng Qi", "Lefeng Zhang", "Di Wang", "Yingjie Zhao", "Ziying Li", "Yalong Xing", "Ningran Li"], "title": "Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion", "comment": "16 pages, 13 figures", "summary": "Multimodal text-to-image generation remains constrained by the difficulty of\nmaintaining semantic alignment and professional-level detail across diverse\nvisual domains. We propose a multi-agent reinforcement learning framework that\ncoordinates domain-specialized agents (e.g., focused on architecture,\nportraiture, and landscape imagery) within two coupled subsystems: a text\nenhancement module and an image generation module, each augmented with\nmultimodal integration components. Agents are trained using Proximal Policy\nOptimization (PPO) under a composite reward function that balances semantic\nsimilarity, linguistic visual quality, and content diversity. Cross-modal\nalignment is enforced through contrastive learning, bidirectional attention,\nand iterative feedback between text and image. Across six experimental\nsettings, our system significantly enriches generated content (word count\nincreased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusion\nmethods, Transformer-based strategies achieve the highest composite score\n(0.521), despite occasional stability issues. Multimodal ensembles yield\nmoderate consistency (ranging from 0.444 to 0.481), reflecting the persistent\nchallenges of cross-modal semantic grounding. These findings underscore the\npromise of collaborative, specialization-driven architectures for advancing\nreliable multimodal generative systems.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u534f\u8c03\u9886\u57df\u4e13\u5bb6\u667a\u80fd\u4f53\u8fdb\u884c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u901a\u8fc7PPO\u8bad\u7ec3\u548c\u590d\u5408\u5956\u52b1\u51fd\u6570\u63d0\u5347\u8bed\u4e49\u5bf9\u9f50\u548c\u7ec6\u8282\u8d28\u91cf", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8bed\u4e49\u5bf9\u9f50\u548c\u4e13\u4e1a\u7ec6\u8282\u4fdd\u6301\u7684\u6311\u6218", "method": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u6587\u672c\u589e\u5f3a\u548c\u56fe\u50cf\u751f\u6210\u4e24\u4e2a\u8026\u5408\u5b50\u7cfb\u7edf\uff0c\u4f7f\u7528PPO\u8bad\u7ec3\u548c\u590d\u5408\u5956\u52b1\u51fd\u6570\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u3001\u53cc\u5411\u6ce8\u610f\u529b\u548c\u8fed\u4ee3\u53cd\u9988", "result": "\u5728\u516d\u4e2a\u5b9e\u9a8c\u573a\u666f\u4e2d\u663e\u8457\u4e30\u5bcc\u751f\u6210\u5185\u5bb9\uff08\u8bcd\u6570\u589e\u52a01614%\uff09\uff0cROUGE-1\u5206\u6570\u964d\u4f4e69.7%\uff0cTransformer\u878d\u5408\u65b9\u6cd5\u83b7\u5f97\u6700\u9ad8\u7efc\u5408\u5f97\u52060.521", "conclusion": "\u534f\u4f5c\u5f0f\u3001\u4e13\u4e1a\u5316\u9a71\u52a8\u7684\u67b6\u6784\u5728\u63a8\u8fdb\u53ef\u9760\u591a\u6a21\u6001\u751f\u6210\u7cfb\u7edf\u65b9\u9762\u5177\u6709\u524d\u666f"}}
{"id": "2510.10730", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10730", "abs": "https://arxiv.org/abs/2510.10730", "authors": ["Jiazheng Sun", "Weixin Wang", "Pan Xu"], "title": "Provable Anytime Ensemble Sampling Algorithms in Nonlinear Contextual Bandits", "comment": "40 pages, 1 figure", "summary": "We provide a unified algorithmic framework for ensemble sampling in nonlinear\ncontextual bandits and develop corresponding regret bounds for two most common\nnonlinear contextual bandit settings: Generalized Linear Ensemble Sampling\n(\\texttt{GLM-ES}) for generalized linear bandits and Neural Ensemble Sampling\n(\\texttt{Neural-ES}) for neural contextual bandits. Both methods maintain\nmultiple estimators for the reward model parameters via maximum likelihood\nestimation on randomly perturbed data. We prove high-probability frequentist\nregret bounds of $\\mathcal{O}(d^{3/2} \\sqrt{T} + d^{9/2})$ for \\texttt{GLM-ES}\nand $\\mathcal{O}(\\widetilde{d} \\sqrt{T})$ for \\texttt{Neural-ES}, where $d$ is\nthe dimension of feature vectors, $\\widetilde{d}$ is the effective dimension of\na neural tangent kernel matrix, and $T$ is the number of rounds. These regret\nbounds match the state-of-the-art results of randomized exploration algorithms\nin nonlinear contextual bandit settings. In the theoretical analysis, we\nintroduce techniques that address challenges specific to nonlinear models.\nPractically, we remove fixed-time horizon assumptions by developing anytime\nversions of our algorithms, suitable when $T$ is unknown. Finally, we\nempirically evaluate \\texttt{GLM-ES}, \\texttt{Neural-ES}, and their anytime\nvariants, demonstrating strong performance. Overall, our results establish\nensemble sampling as a provable and practical randomized exploration approach\nfor nonlinear contextual bandits.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u96c6\u6210\u91c7\u6837\u7b97\u6cd5\u6846\u67b6\uff0c\u7528\u4e8e\u975e\u7ebf\u6027\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u5305\u62ecGLM-ES\uff08\u5e7f\u4e49\u7ebf\u6027\u96c6\u6210\u91c7\u6837\uff09\u548cNeural-ES\uff08\u795e\u7ecf\u7f51\u7edc\u96c6\u6210\u91c7\u6837\uff09\u4e24\u79cd\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5e94\u7684\u9057\u61be\u754c\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u7684\u975e\u7ebf\u6027\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u7b97\u6cd5\u5728\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9645\u5e94\u7528\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u53c8\u5b9e\u7528\u7684\u968f\u673a\u63a2\u7d22\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5728\u968f\u673a\u6270\u52a8\u6570\u636e\u4e0a\u7ef4\u62a4\u591a\u4e2a\u5956\u52b1\u6a21\u578b\u53c2\u6570\u4f30\u8ba1\u5668\uff0c\u5f00\u53d1\u4e86GLM-ES\u548cNeural-ES\u4e24\u79cd\u96c6\u6210\u91c7\u6837\u7b97\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u65e0\u9700\u56fa\u5b9a\u65f6\u95f4\u8303\u56f4\u7684\u968f\u65f6\u7248\u672c\u3002", "result": "\u8bc1\u660e\u4e86GLM-ES\u7684\u9057\u61be\u754c\u4e3aO(d\u00b3/\u00b2\u221aT + d\u2079/\u00b2)\uff0cNeural-ES\u7684\u9057\u61be\u754c\u4e3aO(\u02dcd\u221aT)\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u968f\u673a\u63a2\u7d22\u7b97\u6cd5\u7ed3\u679c\u76f8\u5339\u914d\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u5f3a\u5927\u6027\u80fd\u3002", "conclusion": "\u96c6\u6210\u91c7\u6837\u4e3a\u975e\u7ebf\u6027\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u8bc1\u660e\u4e14\u5b9e\u7528\u7684\u968f\u673a\u63a2\u7d22\u65b9\u6cd5\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.11571", "categories": ["math.OC", "cs.DS", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.11571", "abs": "https://arxiv.org/abs/2510.11571", "authors": ["Fran\u00e7ois Cl\u00e9ment", "Stefan Steinerberger"], "title": "Robust Online Sampling from Possibly Moving Target Distributions", "comment": null, "summary": "We suppose we are given a list of points $x_1, \\dots, x_n \\in \\mathbb{R}$, a\ntarget probability measure $\\mu$ and are asked to add additional points\n$x_{n+1}, \\dots, x_{n+m}$ so that $x_1, \\dots, x_{n+m}$ is as close as possible\nto the distribution of $\\mu$; additionally, we want this to be true uniformly\nfor all $m$. We propose a simple method that achieves this goal. It selects new\npoints in regions where the existing set is lacking points and avoids regions\nthat are already overly crowded. If we replace $\\mu$ by another measure $\\mu_2$\nin the middle of the computation, the method dynamically adjusts and allows us\nto keep the original sampling points. $x_{n+1}$ can be computed in\n$\\mathcal{O}(n)$ steps and we obtain state-of-the-art results. It appears to be\nan interesting dynamical system in its own right; we analyze a continuous\nmean-field version that reflects much of the same behavior.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6709\u7f3a\u9677\u7684\u533a\u57df\u6dfb\u52a0\u65b0\u70b9\uff0c\u907f\u514d\u8fc7\u5ea6\u62e5\u6324\u533a\u57df\uff0c\u4f7f\u70b9\u96c6\u5206\u5e03\u5747\u5300\u63a5\u8fd1\u76ee\u6807\u6982\u7387\u6d4b\u5ea6\u03bc\u3002", "motivation": "\u89e3\u51b3\u5982\u4f55\u5728\u73b0\u6709\u70b9\u96c6\u57fa\u7840\u4e0a\u6dfb\u52a0\u65b0\u70b9\uff0c\u4f7f\u6574\u4f53\u5206\u5e03\u5747\u5300\u63a5\u8fd1\u76ee\u6807\u6982\u7387\u5206\u5e03\uff0c\u5e76\u4fdd\u6301\u5bf9\u6240\u6709m\u7684\u4e00\u81f4\u6027\u3002", "method": "\u5728\u73b0\u6709\u70b9\u96c6\u7f3a\u4e4f\u70b9\u7684\u533a\u57df\u9009\u62e9\u65b0\u70b9\uff0c\u907f\u514d\u8fc7\u5ea6\u62e5\u6324\u533a\u57df\uff0c\u652f\u6301\u52a8\u6001\u8c03\u6574\u76ee\u6807\u6d4b\u5ea6\u3002", "result": "\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u9ad8\uff08x_{n+1}\u53ef\u5728O(n)\u6b65\u5185\u8ba1\u7b97\uff09\uff0c\u83b7\u5f97\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u80fd\u52a8\u6001\u9002\u5e94\u6d4b\u5ea6\u53d8\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5b9e\u7528\u9ad8\u6548\uff0c\u8fd8\u5f62\u6210\u4e86\u4e00\u4e2a\u6709\u8da3\u7684\u52a8\u529b\u7cfb\u7edf\uff0c\u8fde\u7eed\u5e73\u5747\u573a\u7248\u672c\u5206\u6790\u663e\u793a\u7c7b\u4f3c\u884c\u4e3a\u7279\u5f81\u3002"}}
{"id": "2510.10072", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10072", "abs": "https://arxiv.org/abs/2510.10072", "authors": ["Hua Cai", "Shuang Zhao", "Liang Zhang", "Xuli Shen", "Qing Xu", "Weilin Shen", "Zihao Wen", "Tianke Ban"], "title": "Unilaw-R1: A Large Language Model for Legal Reasoning with Reinforcement Learning and Iterative Inference", "comment": null, "summary": "Reasoning-focused large language models (LLMs) are rapidly evolving across\nvarious domains, yet their capabilities in handling complex legal problems\nremains underexplored. In this paper, we introduce Unilaw-R1, a large language\nmodel tailored for legal reasoning. With a lightweight 7-billion parameter\nscale, Unilaw-R1 significantly reduces deployment cost while effectively\ntackling three core challenges in the legal domain: insufficient legal\nknowledge, unreliable reasoning logic, and weak business generalization. To\naddress these issues, we first construct Unilaw-R1-Data, a high-quality dataset\ncontaining 17K distilled and screened chain-of-thought (CoT) samples. Based on\nthis, we adopt a two-stage training strategy combining Supervised Fine-Tuning\n(SFT) and Reinforcement Learning (RL), which significantly boosts the\nperformance on complex legal reasoning tasks and supports interpretable\ndecision-making in legal AI applications. To assess legal reasoning ability, we\nalso introduce Unilaw-R1-Eval, a dedicated benchmark designed to evaluate\nmodels across single- and multi-choice legal tasks. Unilaw-R1 demonstrates\nstrong results on authoritative benchmarks, outperforming all models of similar\nscale and achieving performance on par with the much larger\nDeepSeek-R1-Distill-Qwen-32B (54.9%). Following domain-specific training, it\nalso showed significant gains on LawBench and LexEval, exceeding\nQwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%.", "AI": {"tldr": "Unilaw-R1\u662f\u4e00\u4e2a\u4e13\u4e3a\u6cd5\u5f8b\u63a8\u7406\u8bbe\u8ba1\u768470\u4ebf\u53c2\u6570\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u5728\u6743\u5a01\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u540c\u89c4\u6a21\u6a21\u578b\u5e76\u4e0e\u66f4\u5927\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "motivation": "\u5f53\u524d\u4e13\u6ce8\u4e8e\u63a8\u7406\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u9886\u57df\u5904\u7406\u590d\u6742\u95ee\u9898\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u89e3\u51b3\u6cd5\u5f8b\u77e5\u8bc6\u4e0d\u8db3\u3001\u63a8\u7406\u903b\u8f91\u4e0d\u53ef\u9760\u548c\u4e1a\u52a1\u6cdb\u5316\u80fd\u529b\u5f31\u4e09\u5927\u6311\u6218\u3002", "method": "\u6784\u5efa\u5305\u542b17K\u9ad8\u8d28\u91cf\u601d\u7ef4\u94fe\u6837\u672c\u7684\u6570\u636e\u96c6\uff0c\u91c7\u7528\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5f00\u53d1\u4e13\u95e8\u7684\u6cd5\u5f8b\u63a8\u7406\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u5728\u6743\u5a01\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u5f3a\u52b2\uff0c\u8d85\u8d8a\u4e86\u6240\u6709\u540c\u89c4\u6a21\u6a21\u578b\uff0c\u4e0e\u66f4\u5927\u7684DeepSeek-R1-Distill-Qwen-32B\u6027\u80fd\u76f8\u5f53(54.9%)\uff0c\u5728LawBench\u548cLexEval\u4e0a\u5e73\u5747\u8d85\u8fc7Qwen-2.5-7B-Instruct 6.6%\u3002", "conclusion": "Unilaw-R1\u8bc1\u660e\u4e86\u8f7b\u91cf\u7ea7\u6a21\u578b\u901a\u8fc7\u4e13\u95e8\u7684\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u7b56\u7565\u53ef\u4ee5\u5728\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\uff0c\u4e3a\u6cd5\u5f8bAI\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2510.09762", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09762", "abs": "https://arxiv.org/abs/2510.09762", "authors": ["Ruo Yang", "Sai Krishna Reddy Mudhiganti", "Manali Sharma"], "title": "PatentVision: A multimodal method for drafting patent applications", "comment": null, "summary": "Patent drafting is complex due to its need for detailed technical\ndescriptions, legal compliance, and visual elements. Although Large Vision\nLanguage Models (LVLMs) show promise across various tasks, their application in\nautomating patent writing remains underexplored. In this paper, we present\nPatentVision, a multimodal framework that integrates textual and visual inputs\nsuch as patent claims and drawings to generate complete patent specifications.\nBuilt on advanced LVLMs, PatentVision enhances accuracy by combining fine tuned\nvision language models with domain specific training tailored to patents.\nExperiments reveal it surpasses text only methods, producing outputs with\ngreater fidelity and alignment with human written standards. Its incorporation\nof visual data allows it to better represent intricate design features and\nfunctional connections, leading to richer and more precise results. This study\nunderscores the value of multimodal techniques in patent automation, providing\na scalable tool to reduce manual workloads and improve consistency.\nPatentVision not only advances patent drafting but also lays the groundwork for\nbroader use of LVLMs in specialized areas, potentially transforming\nintellectual property management and innovation processes.", "AI": {"tldr": "PatentVision\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6574\u5408\u4e13\u5229\u6743\u5229\u8981\u6c42\u548c\u56fe\u7eb8\u7b49\u6587\u672c\u548c\u89c6\u89c9\u8f93\u5165\uff0c\u81ea\u52a8\u751f\u6210\u5b8c\u6574\u7684\u4e13\u5229\u8bf4\u660e\u4e66\uff0c\u8d85\u8d8a\u4e86\u7eaf\u6587\u672c\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "motivation": "\u4e13\u5229\u64b0\u5199\u590d\u6742\u4e14\u9700\u8981\u8be6\u7ec6\u6280\u672f\u63cf\u8ff0\u3001\u6cd5\u5f8b\u5408\u89c4\u6027\u548c\u89c6\u89c9\u5143\u7d20\uff0c\u800c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u4e13\u5229\u5199\u4f5c\u65b9\u9762\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u57fa\u4e8e\u5148\u8fdb\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6784\u5efa\uff0c\u7ed3\u5408\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u9488\u5bf9\u4e13\u5229\u7684\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\uff0c\u6574\u5408\u6587\u672c\u548c\u89c6\u89c9\u8f93\u5165\u6765\u751f\u6210\u4e13\u5229\u8bf4\u660e\u4e66\u3002", "result": "\u5b9e\u9a8c\u663e\u793aPatentVision\u8d85\u8d8a\u4e86\u7eaf\u6587\u672c\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u8f93\u51fa\u5177\u6709\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\u548c\u4e0e\u4eba\u5de5\u64b0\u5199\u6807\u51c6\u7684\u4e00\u81f4\u6027\uff0c\u80fd\u591f\u66f4\u597d\u5730\u8868\u793a\u590d\u6742\u7684\u8bbe\u8ba1\u7279\u5f81\u548c\u529f\u80fd\u8fde\u63a5\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u6280\u672f\u5728\u4e13\u5229\u81ea\u52a8\u5316\u4e2d\u7684\u4ef7\u503c\uff0c\u4e3a\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\u3001\u63d0\u9ad8\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u5de5\u5177\uff0c\u5e76\u4e3aLVLMs\u5728\u4e13\u4e1a\u9886\u57df\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.10639", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10639", "abs": "https://arxiv.org/abs/2510.10639", "authors": ["Haemin Choi", "Gayathri Nadarajan"], "title": "Automatic Piecewise Linear Regression for Predicting Student Learning Satisfaction", "comment": null, "summary": "Although student learning satisfaction has been widely studied, modern\ntechniques such as interpretable machine learning and neural networks have not\nbeen sufficiently explored. This study demonstrates that a recent model that\ncombines boosting with interpretability, automatic piecewise linear\nregression(APLR), offers the best fit for predicting learning satisfaction\namong several state-of-the-art approaches. Through the analysis of APLR's\nnumerical and visual interpretations, students' time management and\nconcentration abilities, perceived helpfulness to classmates, and participation\nin offline courses have the most significant positive impact on learning\nsatisfaction. Surprisingly, involvement in creative activities did not\npositively affect learning satisfaction. Moreover, the contributing factors can\nbe interpreted on an individual level, allowing educators to customize\ninstructions according to student profiles.", "AI": {"tldr": "APLR\u6a21\u578b\u5728\u9884\u6d4b\u5b66\u4e60\u6ee1\u610f\u5ea6\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u53d1\u73b0\u65f6\u95f4\u7ba1\u7406\u3001\u4e13\u6ce8\u529b\u3001\u5e2e\u52a9\u540c\u5b66\u548c\u7ebf\u4e0b\u8bfe\u7a0b\u53c2\u4e0e\u5bf9\u5b66\u4e60\u6ee1\u610f\u5ea6\u6709\u663e\u8457\u6b63\u5411\u5f71\u54cd\uff0c\u800c\u521b\u610f\u6d3b\u52a8\u53c2\u4e0e\u65e0\u79ef\u6781\u5f71\u54cd\u3002", "motivation": "\u867d\u7136\u5b66\u751f\u5b66\u4e60\u6ee1\u610f\u5ea6\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u548c\u795e\u7ecf\u7f51\u7edc\u7b49\u73b0\u4ee3\u6280\u672f\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u7ed3\u5408\u63d0\u5347\u548c\u53ef\u89e3\u91ca\u6027\u7684\u81ea\u52a8\u5206\u6bb5\u7ebf\u6027\u56de\u5f52(APLR)\u6a21\u578b\uff0c\u5e76\u4e0e\u591a\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "APLR\u6a21\u578b\u63d0\u4f9b\u4e86\u6700\u4f73\u62df\u5408\uff0c\u901a\u8fc7\u6570\u503c\u548c\u53ef\u89c6\u5316\u89e3\u91ca\u53d1\u73b0\u65f6\u95f4\u7ba1\u7406\u3001\u4e13\u6ce8\u529b\u3001\u5e2e\u52a9\u540c\u5b66\u548c\u7ebf\u4e0b\u8bfe\u7a0b\u53c2\u4e0e\u5bf9\u5b66\u4e60\u6ee1\u610f\u5ea6\u6709\u6700\u663e\u8457\u6b63\u5411\u5f71\u54cd\uff0c\u521b\u610f\u6d3b\u52a8\u53c2\u4e0e\u65e0\u79ef\u6781\u5f71\u54cd\u3002", "conclusion": "\u8d21\u732e\u56e0\u7d20\u53ef\u5728\u4e2a\u4f53\u5c42\u9762\u89e3\u91ca\uff0c\u4f7f\u6559\u80b2\u8005\u80fd\u591f\u6839\u636e\u5b66\u751f\u6863\u6848\u5b9a\u5236\u6559\u5b66\u6307\u5bfc\u3002"}}
{"id": "2510.10854", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10854", "abs": "https://arxiv.org/abs/2510.10854", "authors": ["Aadithya Srikanth", "Mudit Gaur", "Vaneet Aggarwal"], "title": "Discrete State Diffusion Models: A Sample Complexity Perspective", "comment": null, "summary": "Diffusion models have demonstrated remarkable performance in generating\nhigh-dimensional samples across domains such as vision, language, and the\nsciences. Although continuous-state diffusion models have been extensively\nstudied both empirically and theoretically, discrete-state diffusion models,\nessential for applications involving text, sequences, and combinatorial\nstructures, remain significantly less understood from a theoretical standpoint.\nIn particular, all existing analyses of discrete-state models assume score\nestimation error bounds without studying sample complexity results. In this\nwork, we present a principled theoretical framework for discrete-state\ndiffusion, providing the first sample complexity bound of\n$\\widetilde{\\mathcal{O}}(\\epsilon^{-2})$. Our structured decomposition of the\nscore estimation error into statistical, approximation, optimization, and\nclipping components offers critical insights into how discrete-state models can\nbe trained efficiently. This analysis addresses a fundamental gap in the\nliterature and establishes the theoretical tractability and practical relevance\nof discrete-state diffusion models.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u4e3a\u79bb\u6563\u72b6\u6001\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u7ed9\u51fa\u4e86\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7406\u8bba\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u867d\u7136\u8fde\u7eed\u72b6\u6001\u6269\u6563\u6a21\u578b\u5df2\u6709\u5145\u5206\u7814\u7a76\uff0c\u4f46\u79bb\u6563\u72b6\u6001\u6269\u6563\u6a21\u578b\uff08\u7528\u4e8e\u6587\u672c\u3001\u5e8f\u5217\u548c\u7ec4\u5408\u7ed3\u6784\uff09\u7684\u7406\u8bba\u7406\u89e3\u4ecd\u7136\u4e0d\u8db3\uff0c\u73b0\u6709\u5206\u6790\u90fd\u5047\u8bbe\u5206\u6570\u4f30\u8ba1\u8bef\u5dee\u754c\u9650\u800c\u672a\u7814\u7a76\u6837\u672c\u590d\u6742\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u5206\u6570\u4f30\u8ba1\u8bef\u5dee\u5206\u89e3\u4e3a\u7edf\u8ba1\u3001\u8fd1\u4f3c\u3001\u4f18\u5316\u548c\u88c1\u526a\u56db\u4e2a\u7ec4\u6210\u90e8\u5206\u3002", "result": "\u83b7\u5f97\u4e86\u79bb\u6563\u72b6\u6001\u6269\u6563\u6a21\u578b\u7684\u7b2c\u4e00\u4e2a\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650 $\\widetilde{\\mathcal{O}}(\\epsilon^{-2})$\u3002", "conclusion": "\u8be5\u5206\u6790\u586b\u8865\u4e86\u6587\u732e\u4e2d\u7684\u57fa\u672c\u7a7a\u767d\uff0c\u786e\u7acb\u4e86\u79bb\u6563\u72b6\u6001\u6269\u6563\u6a21\u578b\u7684\u7406\u8bba\u53ef\u5904\u7406\u6027\u548c\u5b9e\u9645\u76f8\u5173\u6027\u3002"}}
{"id": "2510.11676", "categories": ["math.OC", "cs.AI", "cs.LG", "stat.ML", "49M05, 49M37, 90C25, 90C30"], "pdf": "https://arxiv.org/pdf/2510.11676", "abs": "https://arxiv.org/abs/2510.11676", "authors": ["Chuan He", "Zhaosong Lu"], "title": "Accelerated stochastic first-order method for convex optimization under heavy-tailed noise", "comment": null, "summary": "We study convex composite optimization problems, where the objective function\nis given by the sum of a prox-friendly function and a convex function whose\nsubgradients are estimated under heavy-tailed noise. Existing work often\nemploys gradient clipping or normalization techniques in stochastic first-order\nmethods to address heavy-tailed noise. In this paper, we demonstrate that a\nvanilla stochastic algorithm -- without additional modifications such as\nclipping or normalization -- can achieve optimal complexity for these problems.\nIn particular, we establish that an accelerated stochastic proximal subgradient\nmethod achieves a first-order oracle complexity that is universally optimal for\nsmooth, weakly smooth, and nonsmooth convex optimization, as well as for\nstochastic convex optimization under heavy-tailed noise. Numerical experiments\nare further provided to validate our theoretical results.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u4e00\u4e2a\u666e\u901a\u7684\u968f\u673a\u8fd1\u7aef\u6b21\u68af\u5ea6\u65b9\u6cd5\u65e0\u9700\u68af\u5ea6\u88c1\u526a\u6216\u5f52\u4e00\u5316\u7b49\u989d\u5916\u4fee\u6539\uff0c\u5c31\u80fd\u5728\u91cd\u5c3e\u566a\u58f0\u4e0b\u5b9e\u73b0\u51f8\u590d\u5408\u4f18\u5316\u95ee\u9898\u7684\u6700\u4f18\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u968f\u673a\u4e00\u9636\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u68af\u5ea6\u88c1\u526a\u6216\u5f52\u4e00\u5316\u6280\u672f\u6765\u5904\u7406\u91cd\u5c3e\u566a\u58f0\uff0c\u672c\u6587\u65e8\u5728\u8bc1\u660e\u65e0\u9700\u8fd9\u4e9b\u989d\u5916\u4fee\u6539\u7684\u666e\u901a\u968f\u673a\u7b97\u6cd5\u4e5f\u80fd\u8fbe\u5230\u6700\u4f18\u590d\u6742\u5ea6\u3002", "method": "\u4f7f\u7528\u52a0\u901f\u968f\u673a\u8fd1\u7aef\u6b21\u68af\u5ea6\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e0d\u9700\u8981\u68af\u5ea6\u88c1\u526a\u6216\u5f52\u4e00\u5316\u7b49\u989d\u5916\u4fee\u6539\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5149\u6ed1\u3001\u5f31\u5149\u6ed1\u548c\u975e\u5149\u6ed1\u51f8\u4f18\u5316\u4ee5\u53ca\u91cd\u5c3e\u566a\u58f0\u4e0b\u7684\u968f\u673a\u51f8\u4f18\u5316\u4e2d\u5747\u5b9e\u73b0\u4e86\u666e\u904d\u6700\u4f18\u7684\u4e00\u9636oracle\u590d\u6742\u5ea6\u3002", "conclusion": "\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\uff0c\u8868\u660e\u666e\u901a\u968f\u673a\u7b97\u6cd5\u5728\u91cd\u5c3e\u566a\u58f0\u4e0b\u4e5f\u80fd\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002"}}
{"id": "2510.10077", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10077", "abs": "https://arxiv.org/abs/2510.10077", "authors": ["Wenqing Wang", "Muhammad Asif Ali", "Ali Shoker", "Ruohan Yang", "Junyang Chen", "Ying Sha", "Huan Wang"], "title": "A-IPO: Adaptive Intent-driven Preference Optimization", "comment": null, "summary": "Human preferences are diverse and dynamic, shaped by regional, cultural, and\nsocial factors. Existing alignment methods like Direct Preference Optimization\n(DPO) and its variants often default to majority views, overlooking minority\nopinions and failing to capture latent user intentions in prompts.\n  To address these limitations, we introduce \\underline{\\textbf{A}}daptive\n\\textbf{\\underline{I}}ntent-driven \\textbf{\\underline{P}}reference\n\\textbf{\\underline{O}}ptimization (\\textbf{A-IPO}). Specifically,A-IPO\nintroduces an intention module that infers the latent intent behind each user\nprompt and explicitly incorporates this inferred intent into the reward\nfunction, encouraging stronger alignment between the preferred model's\nresponses and the user's underlying intentions. We demonstrate, both\ntheoretically and empirically, that incorporating an intention--response\nsimilarity term increases the preference margin (by a positive shift of\n$\\lambda\\,\\Delta\\mathrm{sim}$ in the log-odds), resulting in clearer separation\nbetween preferred and dispreferred responses compared to DPO.\n  For evaluation, we introduce two new benchmarks, Real-pref, Attack-pref along\nwith an extended version of an existing dataset, GlobalOpinionQA-Ext, to assess\nreal-world and adversarial preference alignment.\n  Through explicit modeling of diverse user intents,A-IPO facilitates\npluralistic preference optimization while simultaneously enhancing adversarial\nrobustness in preference alignment. Comprehensive empirical evaluation\ndemonstrates that A-IPO consistently surpasses existing baselines, yielding\nsubstantial improvements across key metrics: up to +24.8 win-rate and +45.6\nResponse-Intention Consistency on Real-pref; up to +38.6 Response Similarity\nand +52.2 Defense Success Rate on Attack-pref; and up to +54.6 Intention\nConsistency Score on GlobalOpinionQA-Ext.", "AI": {"tldr": "A-IPO\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u610f\u56fe\u9a71\u52a8\u7684\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a8\u65ad\u7528\u6237\u63d0\u793a\u80cc\u540e\u7684\u6f5c\u5728\u610f\u56fe\u5e76\u878d\u5165\u5956\u52b1\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u5ffd\u89c6\u5c11\u6570\u610f\u89c1\u548c\u7528\u6237\u6f5c\u5728\u610f\u56fe\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u5982DPO\u5f80\u5f80\u9ed8\u8ba4\u591a\u6570\u89c2\u70b9\uff0c\u5ffd\u89c6\u4e86\u5c11\u6570\u610f\u89c1\uff0c\u4e14\u65e0\u6cd5\u6355\u6349\u7528\u6237\u63d0\u793a\u4e2d\u7684\u6f5c\u5728\u610f\u56fe\u3002\u4eba\u7c7b\u504f\u597d\u5177\u6709\u591a\u6837\u6027\u548c\u52a8\u6001\u6027\uff0c\u53d7\u5730\u533a\u3001\u6587\u5316\u548c\u793e\u4f1a\u56e0\u7d20\u5f71\u54cd\u3002", "method": "A-IPO\u5f15\u5165\u610f\u56fe\u6a21\u5757\u63a8\u65ad\u7528\u6237\u63d0\u793a\u7684\u6f5c\u5728\u610f\u56fe\uff0c\u5e76\u5c06\u63a8\u65ad\u7684\u610f\u56fe\u660e\u786e\u7eb3\u5165\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7\u610f\u56fe-\u54cd\u5e94\u76f8\u4f3c\u6027\u9879\u589e\u52a0\u504f\u597d\u8fb9\u9645\uff0c\u5b9e\u73b0\u66f4\u6e05\u6670\u7684\u9996\u9009\u548c\u975e\u9996\u9009\u54cd\u5e94\u5206\u79bb\u3002", "result": "A-IPO\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff1aReal-pref\u4e0a\u80dc\u7387\u63d0\u534724.8%\uff0c\u54cd\u5e94\u610f\u56fe\u4e00\u81f4\u6027\u63d0\u534745.6%\uff1bAttack-pref\u4e0a\u54cd\u5e94\u76f8\u4f3c\u6027\u63d0\u534738.6%\uff0c\u9632\u5fa1\u6210\u529f\u7387\u63d0\u534752.2%\uff1bGlobalOpinionQA-Ext\u4e0a\u610f\u56fe\u4e00\u81f4\u6027\u5f97\u5206\u63d0\u534754.6%\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u591a\u6837\u7528\u6237\u610f\u56fe\uff0cA-IPO\u4fc3\u8fdb\u4e86\u591a\u5143\u504f\u597d\u4f18\u5316\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u504f\u597d\u5bf9\u9f50\u7684\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5bf9\u6297\u6027\u504f\u597d\u5bf9\u9f50\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.09764", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09764", "abs": "https://arxiv.org/abs/2510.09764", "authors": ["Wanting Mao", "Maxwell A Xu", "Harish Haresamudram", "Mithun Saha", "Santosh Kumar", "James Matthew Rehg"], "title": "Leveraging Shared Prototypes for a Multimodal Pulse Motion Foundation Model", "comment": null, "summary": "Modeling multi-modal time-series data is critical for capturing system-level\ndynamics, particularly in biosignals where modalities such as ECG, PPG, EDA,\nand accelerometry provide complementary perspectives on interconnected\nphysiological processes. While recent self-supervised learning (SSL) advances\nhave improved unimodal representation learning, existing multi-modal approaches\noften rely on CLIP-style contrastive objectives that overfit to easily aligned\nfeatures and misclassify valid cross-modal relationships as negatives,\nresulting in fragmented and non-generalizable embeddings. To overcome these\nlimitations, we propose ProtoMM, a novel SSL framework that introduces a shared\nprototype dictionary to anchor heterogeneous modalities in a common embedding\nspace. By clustering representations around shared prototypes rather than\nexplicit negative sampling, our method captures complementary information\nacross modalities and provides a coherent \"common language\" for physiological\nsignals. In this work, we focus on developing a Pulse Motion foundation model\nwith ProtoMM and demonstrate that our approach outperforms contrastive-only and\nprior multimodal SSL methods, achieving state-of-the-art performance while\noffering improved interpretability of learned features.", "AI": {"tldr": "ProtoMM\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5171\u4eab\u539f\u578b\u5b57\u5178\u5728\u5171\u540c\u5d4c\u5165\u7a7a\u95f4\u4e2d\u951a\u5b9a\u5f02\u6784\u6a21\u6001\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5efa\u6a21\u4e2d\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCLIP\u98ce\u683c\u5bf9\u6bd4\u76ee\u6807\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u5bb9\u6613\u8fc7\u62df\u5408\u5230\u6613\u4e8e\u5bf9\u9f50\u7684\u7279\u5f81\uff0c\u5e76\u5c06\u6709\u6548\u7684\u8de8\u6a21\u6001\u5173\u7cfb\u8bef\u5206\u7c7b\u4e3a\u8d1f\u6837\u672c\uff0c\u5bfc\u81f4\u788e\u7247\u5316\u548c\u4e0d\u53ef\u6cdb\u5316\u7684\u5d4c\u5165\u8868\u793a\u3002", "method": "\u63d0\u51faProtoMM\u6846\u67b6\uff0c\u4f7f\u7528\u5171\u4eab\u539f\u578b\u5b57\u5178\u805a\u7c7b\u8868\u793a\uff0c\u800c\u4e0d\u662f\u663e\u5f0f\u8d1f\u91c7\u6837\uff0c\u4ece\u800c\u5728\u5171\u540c\u5d4c\u5165\u7a7a\u95f4\u4e2d\u6355\u83b7\u8de8\u6a21\u6001\u7684\u4e92\u8865\u4fe1\u606f\u3002", "result": "ProtoMM\u5728Pulse Motion\u57fa\u7840\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8e\u4ec5\u5bf9\u6bd4\u5b66\u4e60\u548c\u5148\u524d\u7684\u591a\u6a21\u6001SSL\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "ProtoMM\u901a\u8fc7\u5171\u4eab\u539f\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u751f\u7406\u4fe1\u53f7\u7684\"\u5171\u540c\u8bed\u8a00\"\uff0c\u6539\u8fdb\u4e86\u5b66\u4e60\u7279\u5f81\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u3002"}}
{"id": "2510.10640", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10640", "abs": "https://arxiv.org/abs/2510.10640", "authors": ["Piyush Pant", "Marcellius William Suntoro", "Ayesha Siddiqua", "Muhammad Shehryaar Sharif", "Daniyal Ahmed"], "title": "Equity-Aware Geospatial AI for Forecasting Demand-Driven Hospital Locations in Germany", "comment": "7 pages. Application:\n  https://equity-aware-geospatial-ai-project.streamlit.app/ Codebase:\n  https://github.com/mwsyow/equity-aware-geospatial-ai-project/", "summary": "This paper presents EA-GeoAI, an integrated framework for demand forecasting\nand equitable hospital planning in Germany through 2030. We combine\ndistrict-level demographic shifts, aging population density, and infrastructure\nbalances into a unified Equity Index. An interpretable Agentic AI optimizer\nthen allocates beds and identifies new facility sites to minimize unmet need\nunder budget and travel-time constraints. This approach bridges GeoAI,\nlong-term forecasting, and equity measurement to deliver actionable\nrecommendations for policymakers.", "AI": {"tldr": "EA-GeoAI\u6846\u67b6\u7ed3\u5408\u5730\u7406AI\u3001\u957f\u671f\u9884\u6d4b\u548c\u516c\u5e73\u6027\u5ea6\u91cf\uff0c\u4e3a\u5fb7\u56fd2030\u5e74\u533b\u9662\u89c4\u5212\u63d0\u4f9b\u9700\u6c42\u9884\u6d4b\u548c\u516c\u5e73\u8d44\u6e90\u5206\u914d\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u5fb7\u56fd\u533b\u9662\u8d44\u6e90\u5206\u914d\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u8003\u8651\u4eba\u53e3\u8001\u9f84\u5316\u3001\u5730\u533a\u5dee\u5f02\u548c\u57fa\u7840\u8bbe\u65bd\u5e73\u8861\uff0c\u786e\u4fdd\u533b\u7597\u8d44\u6e90\u6309\u9700\u5206\u914d\u3002", "method": "\u6574\u5408\u5730\u533a\u4eba\u53e3\u53d8\u5316\u3001\u8001\u9f84\u5316\u5bc6\u5ea6\u548c\u57fa\u7840\u8bbe\u65bd\u5e73\u8861\u6784\u5efa\u516c\u5e73\u6307\u6570\uff0c\u4f7f\u7528\u53ef\u89e3\u91ca\u7684\u667a\u80fdAI\u4f18\u5316\u5668\u5728\u9884\u7b97\u548c\u901a\u52e4\u65f6\u95f4\u7ea6\u675f\u4e0b\u5206\u914d\u5e8a\u4f4d\u548c\u786e\u5b9a\u65b0\u8bbe\u65bd\u4f4d\u7f6e\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u6700\u5c0f\u5316\u672a\u6ee1\u8db3\u9700\u6c42\u3001\u8003\u8651\u516c\u5e73\u6027\u7684\u533b\u9662\u89c4\u5212\u6846\u67b6\uff0c\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u53ef\u64cd\u4f5c\u5efa\u8bae\u3002", "conclusion": "EA-GeoAI\u6210\u529f\u8fde\u63a5\u4e86\u5730\u7406AI\u3001\u957f\u671f\u9884\u6d4b\u548c\u516c\u5e73\u6027\u6d4b\u91cf\uff0c\u4e3a\u533b\u7597\u8d44\u6e90\u516c\u5e73\u5206\u914d\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.10902", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10902", "abs": "https://arxiv.org/abs/2510.10902", "authors": ["Mahmoud Abdelghafar", "Maryam Aliakbarpour", "Chris Jermaine"], "title": "Quantifying Information Disclosure During Gradient Descent Using Gradient Uniqueness", "comment": null, "summary": "Disclosing private information via publication of a machine learning model is\noften a concern. Intuitively, publishing a learned model should be less risky\nthan publishing a dataset. But how much risk is there? In this paper, we\npresent a principled disclosure metric called \\emph{gradient uniqueness} that\nis derived from an upper bound on the amount of information disclosure from\npublishing a learned model. Gradient uniqueness provides an intuitive way to\nperform privacy auditing. The mathematical derivation of gradient uniqueness is\ngeneral, and does not make any assumption on the model architecture, dataset\ntype, or the strategy of an attacker. We examine a simple defense based on\nmonitoring gradient uniqueness, and find that it achieves privacy comparable to\nclassical methods such as DP-SGD, while being substantially better in terms of\n(utility) testing accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u68af\u5ea6\u552f\u4e00\u6027\u7684\u9690\u79c1\u62ab\u9732\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u53d1\u5e03\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65f6\u7684\u9690\u79c1\u98ce\u9669\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u5bf9\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u96c6\u7c7b\u578b\u6216\u653b\u51fb\u7b56\u7565\u505a\u4efb\u4f55\u5047\u8bbe\u3002", "motivation": "\u53d1\u5e03\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65f6\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u9700\u8981\u4e00\u79cd\u539f\u5219\u6027\u7684\u65b9\u6cd5\u6765\u91cf\u5316\u8fd9\u79cd\u98ce\u9669\uff0c\u5e76\u5f00\u53d1\u6709\u6548\u7684\u9632\u5fa1\u673a\u5236\u3002", "method": "\u57fa\u4e8e\u4fe1\u606f\u6cc4\u9732\u4e0a\u754c\u63a8\u5bfc\u51fa\u68af\u5ea6\u552f\u4e00\u6027\u5ea6\u91cf\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u76d1\u63a7\u68af\u5ea6\u552f\u4e00\u6027\u7684\u7b80\u5355\u9632\u5fa1\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u4e0eDP-SGD\u7b49\u7ecf\u5178\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u5728\u6d4b\u8bd5\u51c6\u786e\u7387\u7b49\u6548\u7528\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u68af\u5ea6\u552f\u4e00\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u89c2\u7684\u9690\u79c1\u5ba1\u8ba1\u65b9\u5f0f\uff0c\u80fd\u591f\u6709\u6548\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6548\u7528\u3002"}}
{"id": "2510.10082", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10082", "abs": "https://arxiv.org/abs/2510.10082", "authors": ["Parthiv Chatterjee", "Shivam Sonawane", "Amey Hengle", "Aditya Tanna", "Sourish Dasgupta", "Tanmoy Chakraborty"], "title": "Diversity Augmentation of Dynamic User Preference Data for Boosting Personalized Text Summarizers", "comment": null, "summary": "Document summarization enables efficient extraction of user-relevant content\nbut is inherently shaped by individual subjectivity, making it challenging to\nidentify subjective salient information in multifaceted documents. This\ncomplexity underscores the necessity for personalized summarization. However,\ntraining models for personalized summarization has so far been challenging,\nparticularly because diverse training data containing both user preference\nhistory (i.e., click-skip trajectory) and expected (gold-reference) summaries\nare scarce. The MS/CAS PENS dataset is a valuable resource but includes only\npreference history without target summaries, preventing end-to-end supervised\nlearning, and its limited topic-transition diversity further restricts\ngeneralization. To address this, we propose $\\mathrm{PerAugy}$, a novel\ncross-trajectory shuffling and summary-content perturbation based data\naugmentation technique that significantly boosts the accuracy of four\nstate-of-the-art baseline (SOTA) user-encoders commonly used in personalized\nsummarization frameworks (best result: $\\text{0.132}$$\\uparrow$ w.r.t AUC). We\nselect two such SOTA summarizer frameworks as baselines and observe that when\naugmented with their corresponding improved user-encoders, they consistently\nshow an increase in personalization (avg. boost: $\\text{61.2\\%}\\uparrow$ w.r.t.\nPSE-SU4 metric). As a post-hoc analysis of the role of induced diversity in the\naugmented dataset by \\peraugy, we introduce three dataset diversity metrics --\n$\\mathrm{TP}$, $\\mathrm{RTC}$, and \\degreed\\ to quantify the induced diversity.\nWe find that $\\mathrm{TP}$ and $\\mathrm{DegreeD}$ strongly correlate with\nuser-encoder performance on the PerAugy-generated dataset across all accuracy\nmetrics, indicating that increased dataset diversity is a key factor driving\nperformance gains.", "AI": {"tldr": "\u63d0\u51fa\u4e86PerAugy\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u901a\u8fc7\u8de8\u8f68\u8ff9\u6df7\u6d17\u548c\u6458\u8981\u5185\u5bb9\u6270\u52a8\u6765\u63d0\u5347\u4e2a\u6027\u5316\u6458\u8981\u6a21\u578b\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7528\u6237\u7f16\u7801\u5668\u7684\u51c6\u786e\u6027\u548c\u4e2a\u6027\u5316\u6458\u8981\u8d28\u91cf\u3002", "motivation": "\u4e2a\u6027\u5316\u6458\u8981\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u73b0\u6709\u6570\u636e\u96c6\u5982MS/CAS PENS\u7f3a\u4e4f\u76ee\u6807\u6458\u8981\u4e14\u4e3b\u9898\u8f6c\u6362\u591a\u6837\u6027\u6709\u9650\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86PerAugy\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5305\u542b\u8de8\u8f68\u8ff9\u6df7\u6d17\u548c\u6458\u8981\u5185\u5bb9\u6270\u52a8\uff0c\u5e76\u5f15\u5165\u4e09\u4e2a\u6570\u636e\u96c6\u591a\u6837\u6027\u6307\u6807(TP\u3001RTC\u3001DegreeD)\u6765\u91cf\u5316\u589e\u5f3a\u6548\u679c\u3002", "result": "PerAugy\u663e\u8457\u63d0\u5347\u4e86\u56db\u79cdSOTA\u7528\u6237\u7f16\u7801\u5668\u7684\u51c6\u786e\u6027(AUC\u6700\u4f73\u63d0\u53470.132)\uff0c\u5e76\u4f7f\u4e24\u4e2aSOTA\u6458\u8981\u6846\u67b6\u7684\u4e2a\u6027\u5316\u7a0b\u5ea6\u5e73\u5747\u63d0\u534761.2%(\u57fa\u4e8ePSE-SU4\u6307\u6807)\u3002", "conclusion": "\u6570\u636e\u96c6\u591a\u6837\u6027\u662f\u9a71\u52a8\u6027\u80fd\u63d0\u5347\u7684\u5173\u952e\u56e0\u7d20\uff0cTP\u548cDegreeD\u6307\u6807\u4e0e\u7528\u6237\u7f16\u7801\u5668\u6027\u80fd\u5728\u6240\u6709\u51c6\u786e\u5ea6\u6307\u6807\u4e0a\u5747\u5448\u73b0\u5f3a\u76f8\u5173\u6027\u3002"}}
{"id": "2510.09767", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09767", "abs": "https://arxiv.org/abs/2510.09767", "authors": ["Yifan Lu", "Ziyun Zou", "Belal Alsinglawi", "Islam Al-Qudah", "Izzat Alsmadi", "Feilong Tang", "Pengfei Jiao", "Shoaib Jameel"], "title": "HeSRN: Representation Learning On Heterogeneous Graphs via Slot-Aware Retentive Network", "comment": null, "summary": "Graph Transformers have recently achieved remarkable progress in graph\nrepresentation learning by capturing long-range dependencies through\nself-attention. However, their quadratic computational complexity and inability\nto effectively model heterogeneous semantics severely limit their scalability\nand generalization on real-world heterogeneous graphs. To address these issues,\nwe propose HeSRN, a novel Heterogeneous Slot-aware Retentive Network for\nefficient and expressive heterogeneous graph representation learning. HeSRN\nintroduces a slot-aware structure encoder that explicitly disentangles\nnode-type semantics by projecting heterogeneous features into independent slots\nand aligning their distributions through slot normalization and retention-based\nfusion, effectively mitigating the semantic entanglement caused by forced\nfeature-space unification in previous Transformer-based models. Furthermore, we\nreplace the self-attention mechanism with a retention-based encoder, which\nmodels structural and contextual dependencies in linear time complexity while\nmaintaining strong expressive power. A heterogeneous retentive encoder is\nfurther employed to jointly capture both local structural signals and global\nheterogeneous semantics through multi-scale retention layers. Extensive\nexperiments on four real-world heterogeneous graph datasets demonstrate that\nHeSRN consistently outperforms state-of-the-art heterogeneous graph neural\nnetworks and Graph Transformer baselines on node classification tasks,\nachieving superior accuracy with significantly lower computational complexity.", "AI": {"tldr": "\u63d0\u51faHeSRN\uff0c\u4e00\u79cd\u57fa\u4e8e\u69fd\u611f\u77e5\u4fdd\u7559\u7f51\u7edc\u7684\u5f02\u6784\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u69fd\u5206\u79bb\u548c\u4fdd\u7559\u673a\u5236\u89e3\u51b3\u56feTransformer\u7684\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u8bed\u4e49\u7ea0\u7f20\u95ee\u9898", "motivation": "\u56feTransformer\u5728\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u65e0\u6cd5\u6709\u6548\u5efa\u6a21\u5f02\u6784\u8bed\u4e49\u9650\u5236\u4e86\u5728\u771f\u5b9e\u5f02\u6784\u56fe\u4e0a\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b", "method": "\u5f15\u5165\u69fd\u611f\u77e5\u7ed3\u6784\u7f16\u7801\u5668\u5c06\u5f02\u6784\u7279\u5f81\u6295\u5f71\u5230\u72ec\u7acb\u69fd\u4e2d\uff0c\u4f7f\u7528\u69fd\u5f52\u4e00\u5316\u548c\u4fdd\u7559\u878d\u5408\u5bf9\u9f50\u5206\u5e03\uff1b\u7528\u4fdd\u7559\u7f16\u7801\u5668\u66ff\u4ee3\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u7ebf\u6027\u590d\u6742\u5ea6\uff1b\u4f7f\u7528\u5f02\u6784\u4fdd\u7559\u7f16\u7801\u5668\u8054\u5408\u6355\u83b7\u5c40\u90e8\u7ed3\u6784\u4fe1\u53f7\u548c\u5168\u5c40\u5f02\u6784\u8bed\u4e49", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u5f02\u6784\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHeSRN\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u56feTransformer\u57fa\u7ebf\uff0c\u4ee5\u663e\u8457\u66f4\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u5b9e\u73b0\u66f4\u4f18\u7684\u51c6\u786e\u7387", "conclusion": "HeSRN\u901a\u8fc7\u69fd\u611f\u77e5\u5206\u79bb\u548c\u4fdd\u7559\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u56feTransformer\u5728\u5f02\u6784\u56fe\u4e2d\u7684\u8bed\u4e49\u7ea0\u7f20\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u7684\u5f02\u6784\u56fe\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2510.10644", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10644", "abs": "https://arxiv.org/abs/2510.10644", "authors": ["Yi Zhang", "Yushen Long", "Yun Ni", "Liping Huang", "Xiaohong Wang", "Jun Liu"], "title": "Hierarchical Optimization via LLM-Guided Objective Evolution for Mobility-on-Demand Systems", "comment": null, "summary": "Online ride-hailing platforms aim to deliver efficient mobility-on-demand\nservices, often facing challenges in balancing dynamic and spatially\nheterogeneous supply and demand. Existing methods typically fall into two\ncategories: reinforcement learning (RL) approaches, which suffer from data\ninefficiency, oversimplified modeling of real-world dynamics, and difficulty\nenforcing operational constraints; or decomposed online optimization methods,\nwhich rely on manually designed high-level objectives that lack awareness of\nlow-level routing dynamics. To address this issue, we propose a novel hybrid\nframework that integrates large language model (LLM) with mathematical\noptimization in a dynamic hierarchical system: (1) it is training-free,\nremoving the need for large-scale interaction data as in RL, and (2) it\nleverages LLM to bridge cognitive limitations caused by problem decomposition\nby adaptively generating high-level objectives. Within this framework, LLM\nserves as a meta-optimizer, producing semantic heuristics that guide a\nlow-level optimizer responsible for constraint enforcement and real-time\ndecision execution. These heuristics are refined through a closed-loop\nevolutionary process, driven by harmony search, which iteratively adapts the\nLLM prompts based on feasibility and performance feedback from the optimization\nlayer. Extensive experiments based on scenarios derived from both the New York\nand Chicago taxi datasets demonstrate the effectiveness of our approach,\nachieving an average improvement of 16% compared to state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u6570\u5b66\u4f18\u5316\u7ed3\u5408\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7ebf\u53eb\u8f66\u5e73\u53f0\u7684\u4f9b\u9700\u5e73\u8861\u95ee\u9898\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\uff0c\u901a\u8fc7LLM\u751f\u6210\u9ad8\u5c42\u76ee\u6807\u6307\u5bfc\u5e95\u5c42\u4f18\u5316\u5668\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747\u63d0\u534716%\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u7c7b\u95ee\u9898\uff1a\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6570\u636e\u6548\u7387\u4f4e\u3001\u5bf9\u73b0\u5b9e\u52a8\u6001\u5efa\u6a21\u8fc7\u4e8e\u7b80\u5316\u3001\u96be\u4ee5\u5b9e\u65bd\u64cd\u4f5c\u7ea6\u675f\uff1b\u5206\u89e3\u5728\u7ebf\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7684\u9ad8\u5c42\u76ee\u6807\uff0c\u7f3a\u4e4f\u5bf9\u5e95\u5c42\u8def\u7531\u52a8\u6001\u7684\u8ba4\u77e5\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3\u514d\u8d39\u7684\u6df7\u5408\u6846\u67b6\uff0cLLM\u4f5c\u4e3a\u5143\u4f18\u5316\u5668\u751f\u6210\u8bed\u4e49\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u6307\u5bfc\u5e95\u5c42\u4f18\u5316\u5668\u6267\u884c\u7ea6\u675f\u5b9e\u65bd\u548c\u5b9e\u65f6\u51b3\u7b56\uff0c\u901a\u8fc7\u548c\u58f0\u641c\u7d22\u9a71\u52a8\u7684\u95ed\u73af\u8fdb\u5316\u8fc7\u7a0b\u8fed\u4ee3\u4f18\u5316LLM\u63d0\u793a\u3002", "result": "\u57fa\u4e8e\u7ebd\u7ea6\u548c\u829d\u52a0\u54e5\u51fa\u79df\u8f66\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u5e73\u5747\u63d0\u534716%\u6027\u80fd\u3002", "conclusion": "LLM\u4e0e\u6570\u5b66\u4f18\u5316\u7ed3\u5408\u7684\u6df7\u5408\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u5728\u7ebf\u53eb\u8f66\u5e73\u53f0\u7684\u4f9b\u9700\u5e73\u8861\u95ee\u9898\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u751f\u6210\u9ad8\u5c42\u76ee\u6807\u5f25\u8865\u95ee\u9898\u5206\u89e3\u5e26\u6765\u7684\u8ba4\u77e5\u5c40\u9650\u3002"}}
{"id": "2510.10938", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10938", "abs": "https://arxiv.org/abs/2510.10938", "authors": ["Yuda Bi", "Ying Zhu", "Vince D Calhoun"], "title": "Redundancy as a Structural Information Principle for Learning and Generalization", "comment": null, "summary": "We present a theoretical framework that extends classical information theory\nto finite and structured systems by redefining redundancy as a fundamental\nproperty of information organization rather than inefficiency. In this\nframework, redundancy is expressed as a general family of informational\ndivergences that unifies multiple classical measures, such as mutual\ninformation, chi-squared dependence, and spectral redundancy, under a single\ngeometric principle. This reveals that these traditional quantities are not\nisolated heuristics but projections of a shared redundancy geometry. The theory\nfurther predicts that redundancy is bounded both above and below, giving rise\nto an optimal equilibrium that balances over-compression (loss of structure)\nand over-coupling (collapse). While classical communication theory favors\nminimal redundancy for transmission efficiency, finite and structured systems,\nsuch as those underlying real-world learning, achieve maximal stability and\ngeneralization near this equilibrium. Experiments with masked autoencoders are\nused to illustrate and verify this principle: the model exhibits a stable\nredundancy level where generalization peaks. Together, these results establish\nredundancy as a measurable and tunable quantity that bridges the asymptotic\nworld of communication and the finite world of learning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u5197\u4f59\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4fe1\u606f\u7ec4\u7ec7\u7684\u57fa\u672c\u5c5e\u6027\u800c\u975e\u4f4e\u6548\u6027\uff0c\u63ed\u793a\u4f20\u7edf\u4fe1\u606f\u5ea6\u91cf\u662f\u5171\u4eab\u5197\u4f59\u51e0\u4f55\u7684\u6295\u5f71\uff0c\u5e76\u9884\u6d4b\u5197\u4f59\u5b58\u5728\u4e0a\u4e0b\u754c\u5e73\u8861\u70b9\uff0c\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u8fbe\u5230\u6700\u5927\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6269\u5c55\u7ecf\u5178\u4fe1\u606f\u8bba\u5230\u6709\u9650\u7ed3\u6784\u5316\u7cfb\u7edf\uff0c\u91cd\u65b0\u5b9a\u4e49\u5197\u4f59\u4f5c\u4e3a\u4fe1\u606f\u7ec4\u7ec7\u7684\u57fa\u672c\u5c5e\u6027\uff0c\u800c\u975e\u4f20\u7edf\u901a\u4fe1\u7406\u8bba\u4e2d\u7684\u4f4e\u6548\u6982\u5ff5\u3002", "method": "\u5efa\u7acb\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u5197\u4f59\u8868\u8fbe\u4e3a\u4fe1\u606f\u6563\u5ea6\u7684\u4e00\u822c\u65cf\uff0c\u7edf\u4e00\u591a\u79cd\u7ecf\u5178\u5ea6\u91cf\u5982\u4e92\u4fe1\u606f\u3001\u5361\u65b9\u4f9d\u8d56\u548c\u8c31\u5197\u4f59\uff0c\u63ed\u793a\u5b83\u4eec\u5171\u4eab\u7684\u51e0\u4f55\u539f\u7406\u3002", "result": "\u7406\u8bba\u9884\u6d4b\u5197\u4f59\u5b58\u5728\u4e0a\u4e0b\u754c\uff0c\u4ea7\u751f\u6700\u4f18\u5e73\u8861\u70b9\uff1b\u5b9e\u9a8c\u663e\u793a\u63a9\u7801\u81ea\u7f16\u7801\u5668\u5728\u7279\u5b9a\u5197\u4f59\u6c34\u5e73\u8fbe\u5230\u6700\u4f73\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u5197\u4f59\u662f\u53ef\u6d4b\u91cf\u548c\u53ef\u8c03\u8282\u7684\u91cf\uff0c\u8fde\u63a5\u4e86\u901a\u4fe1\u7684\u6e10\u8fd1\u4e16\u754c\u548c\u5b66\u4e60\u7684\u6709\u9650\u4e16\u754c\uff0c\u5728\u7ed3\u6784\u5316\u7cfb\u7edf\u4e2d\u8fbe\u5230\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u5e73\u8861\u3002"}}
{"id": "2510.09904", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.09904", "abs": "https://arxiv.org/abs/2510.09904", "authors": ["Kelvin Kan", "Xingjian Li", "Benjamin J. Zhang", "Tuhin Sahai", "Stanley Osher", "Krishna Kumar", "Markos A. Katsoulakis"], "title": "Stability of Transformers under Layer Normalization", "comment": null, "summary": "Despite their widespread use, training deep Transformers can be unstable.\nLayer normalization, a standard component, improves training stability, but its\nplacement has often been ad-hoc. In this paper, we conduct a principled study\non the forward (hidden states) and backward (gradient) stability of\nTransformers under different layer normalization placements. Our theory\nprovides key insights into the training dynamics: whether training drives\nTransformers toward regular solutions or pathological behaviors. For forward\nstability, we derive explicit bounds on the growth of hidden states in trained\nTransformers. For backward stability, we analyze how layer normalization\naffects the backpropagation of gradients, thereby explaining the training\ndynamics of each layer normalization placement. Our analysis also guides the\nscaling of residual steps in Transformer blocks, where appropriate choices can\nfurther improve stability and performance. Our numerical results corroborate\nour theoretical findings. Beyond these results, our framework provides a\nprincipled way to sanity-check the stability of Transformers under new\narchitectural modifications, offering guidance for future designs.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86Transformer\u4e2dLayer Normalization\u7684\u4e0d\u540c\u653e\u7f6e\u4f4d\u7f6e\u5bf9\u524d\u5411\uff08\u9690\u85cf\u72b6\u6001\uff09\u548c\u540e\u5411\uff08\u68af\u5ea6\uff09\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\uff0c\u4e3a\u8bad\u7ec3\u52a8\u6001\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002", "motivation": "\u5c3d\u7ba1Transformer\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u8bad\u7ec3\u8fc7\u7a0b\u53ef\u80fd\u4e0d\u7a33\u5b9a\u3002Layer Normalization\u4f5c\u4e3a\u6807\u51c6\u7ec4\u4ef6\u80fd\u6539\u5584\u7a33\u5b9a\u6027\uff0c\u4f46\u5176\u653e\u7f6e\u4f4d\u7f6e\u5f80\u5f80\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63a8\u5bfc\u4e86\u8bad\u7ec3Transformer\u4e2d\u9690\u85cf\u72b6\u6001\u589e\u957f\u7684\u524d\u5411\u7a33\u5b9a\u6027\u754c\u9650\uff0c\u4ee5\u53caLayer Normalization\u5bf9\u68af\u5ea6\u53cd\u5411\u4f20\u64ad\u5f71\u54cd\u7684\u540e\u5411\u7a33\u5b9a\u6027\u5206\u6790\uff0c\u5e76\u6307\u5bfc\u6b8b\u5dee\u8fde\u63a5\u6b65\u957f\u7684\u7f29\u653e\u3002", "result": "\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u540cLayer Normalization\u653e\u7f6e\u4f4d\u7f6e\u5bf9\u5e94\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\uff0c\u5408\u9002\u7684\u6b8b\u5dee\u6b65\u957f\u7f29\u653e\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u68c0\u9a8c\u65b0\u67b6\u6784\u4fee\u6539\u4e0bTransformer\u7684\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2510.10103", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10103", "abs": "https://arxiv.org/abs/2510.10103", "authors": ["Renliang Sun", "Wei Cheng", "Dawei Li", "Haifeng Chen", "Wei Wang"], "title": "Stop When Enough: Adaptive Early-Stopping for Chain-of-Thought Reasoning", "comment": null, "summary": "Chain-of-Thought (CoT) reasoning has driven recent gains of large language\nmodels (LLMs) on reasoning-intensive tasks by externalizing intermediate steps.\nHowever, excessive or redundant reasoning -- so-called overthinking -- can\nincrease inference costs and lead LLMs toward incorrect conclusions. In this\npaper, we present REFRAIN ($\\underline{REF}$lective-$\\underline{R}$edundancy\nfor $\\underline{A}$daptive $\\underline{IN}$ference), a training-free framework\nthat adaptively determines when to stop reasoning to mitigate overthinking.\nREFRAIN integrates a two-stage stop discriminator to identify reflective yet\nredundant reasoning and a sliding-window Upper Confidence Bound (SW-UCB)\nmulti-armed bandit controller to dynamically adjust stopping thresholds\naccording to problem difficulty without supervision or fine-tuning. Across four\nrepresentative benchmarks and two model families, REFRAIN reduces token usage\nby 20-55% while maintaining or improving accuracy compared to standard CoT\nprompting. Extensive ablation and robustness analyses demonstrate its stability\nacross models, scorers, and prompt variations. In summary, our findings\nhighlight when-to-stop as a new and practical axis of test-time scaling --\nenabling models to reason not just more, but just enough.", "AI": {"tldr": "REFRAIN\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u9002\u5e94\u63a8\u7406\u505c\u6b62\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u5197\u4f59\u63a8\u7406\u548c\u52a8\u6001\u8c03\u6574\u505c\u6b62\u9608\u503c\uff0c\u663e\u8457\u51cf\u5c11\u63a8\u7406\u6210\u672c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709Chain-of-Thought\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5bfc\u81f4\u63a8\u7406\u6210\u672c\u589e\u52a0\u548c\u9519\u8bef\u7ed3\u8bba\uff0c\u9700\u8981\u81ea\u9002\u5e94\u505c\u6b62\u673a\u5236\u6765\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u7ed3\u5408\u4e24\u9636\u6bb5\u505c\u6b62\u5224\u522b\u5668\u548c\u6ed1\u52a8\u7a97\u53e3UCB\u591a\u81c2\u8001\u864e\u673a\u63a7\u5236\u5668\uff0c\u52a8\u6001\u8bc6\u522b\u5197\u4f59\u63a8\u7406\u5e76\u8c03\u6574\u505c\u6b62\u9608\u503c\uff0c\u65e0\u9700\u76d1\u7763\u6216\u5fae\u8c03\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e24\u4e2a\u6a21\u578b\u5bb6\u65cf\u4e0a\uff0cREFRAIN\u51cf\u5c1120-55%\u7684token\u4f7f\u7528\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u6027\u3002", "conclusion": "\u4f55\u65f6\u505c\u6b62\u63a8\u7406\u662f\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u65b0\u5b9e\u7528\u7ef4\u5ea6\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u9002\u91cf\u800c\u975e\u8fc7\u5ea6\u7684\u63a8\u7406\u3002"}}
{"id": "2510.09768", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.09768", "abs": "https://arxiv.org/abs/2510.09768", "authors": ["Khang Ngo", "Siamak Ravanbakhsh"], "title": "Scaling Laws and Symmetry, Evidence from Neural Force Fields", "comment": "22 pages, 10 figures", "summary": "We present an empirical study in the geometric task of learning interatomic\npotentials, which shows equivariance matters even more at larger scales; we\nshow a clear power-law scaling behaviour with respect to data, parameters and\ncompute with ``architecture-dependent exponents''. In particular, we observe\nthat equivariant architectures, which leverage task symmetry, scale better than\nnon-equivariant models. Moreover, among equivariant architectures, higher-order\nrepresentations translate to better scaling exponents. Our analysis also\nsuggests that for compute-optimal training, the data and model sizes should\nscale in tandem regardless of the architecture. At a high level, these results\nsuggest that, contrary to common belief, we should not leave it to the model to\ndiscover fundamental inductive biases such as symmetry, especially as we scale,\nbecause they change the inherent difficulty of the task and its scaling laws.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u51e0\u4f55\u4efb\u52a1\u4e2d\u7684\u539f\u5b50\u95f4\u52bf\u80fd\u5b66\u4e60\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u7b49\u53d8\u6027\u67b6\u6784\u6bd4\u975e\u7b49\u53d8\u6027\u6a21\u578b\u5177\u6709\u66f4\u597d\u7684\u7f29\u653e\u7279\u6027\uff0c\u4e14\u9ad8\u9636\u8868\u793a\u5bf9\u5e94\u66f4\u597d\u7684\u7f29\u653e\u6307\u6570\u3002", "motivation": "\u7814\u7a76\u7b49\u53d8\u6027\u5728\u66f4\u5927\u89c4\u6a21\u4e0b\u5bf9\u5b66\u4e60\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u9a8c\u8bc1\u5bf9\u79f0\u6027\u7b49\u57fa\u672c\u5f52\u7eb3\u504f\u7f6e\u5728\u6a21\u578b\u7f29\u653e\u4e2d\u7684\u91cd\u8981\u6027\u3002", "method": "\u5728\u539f\u5b50\u95f4\u52bf\u80fd\u5b66\u4e60\u7684\u51e0\u4f55\u4efb\u52a1\u4e2d\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e0d\u540c\u67b6\u6784\uff08\u7b49\u53d8\u4e0e\u975e\u7b49\u53d8\uff09\u5728\u6570\u636e\u3001\u53c2\u6570\u548c\u8ba1\u7b97\u65b9\u9762\u7684\u7f29\u653e\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u7b49\u53d8\u6027\u67b6\u6784\u6bd4\u975e\u7b49\u53d8\u6027\u6a21\u578b\u7f29\u653e\u66f4\u597d\uff0c\u9ad8\u9636\u8868\u793a\u5bf9\u5e94\u66f4\u597d\u7684\u7f29\u653e\u6307\u6570\uff1b\u8ba1\u7b97\u6700\u4f18\u8bad\u7ec3\u65f6\uff0c\u6570\u636e\u4e0e\u6a21\u578b\u5927\u5c0f\u5e94\u540c\u6b65\u7f29\u653e\u3002", "conclusion": "\u4e0d\u5e94\u8ba9\u6a21\u578b\u81ea\u884c\u53d1\u73b0\u5bf9\u79f0\u6027\u7b49\u57fa\u672c\u5f52\u7eb3\u504f\u7f6e\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e0b\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u504f\u7f6e\u4f1a\u6539\u53d8\u4efb\u52a1\u56fa\u6709\u96be\u5ea6\u548c\u7f29\u653e\u89c4\u5f8b\u3002"}}
{"id": "2510.10649", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10649", "abs": "https://arxiv.org/abs/2510.10649", "authors": ["Can Xie", "Ruotong Pan", "Xiangyu Wu", "Yunfei Zhang", "Jiayi Fu", "Tingting Gao", "Guorui Zhou"], "title": "Unlocking Exploration in RLVR: Uncertainty-aware Advantage Shaping for Deeper Reasoning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has shown significant\npromise for enhancing the reasoning capabilities of large language models\n(LLMs). However, prevailing algorithms like GRPO broadcast a uniform advantage\nsignal across all tokens in a sequence. This coarse-grained approach overlooks\nthe pivotal role of uncertain, high-stakes decisions during reasoning, leading\nto inefficient exploration and the well-documented problem of entropy collapse.\nTo address this, we introduce UnCertainty-aware Advantage Shaping (UCAS), a\nmodel-free method that refines credit assignment by leveraging the model's\ninternal uncertainty signals. UCAS operates in two stages: it first modulates\nthe response-level advantage using the model's overall self-confidence, and\nthen applies a token-level penalty based on raw logit certainty. This dual\nmechanism encourages exploration of high-uncertainty paths that yield correct\nanswers while penalizing overconfident yet erroneous reasoning, effectively\nbalancing the exploration-exploitation trade-off. Extensive experiments on five\nmathematical reasoning benchmarks show that UCAS significantly outperforms\nstrong RLVR baselines across multiple model scales, including 1.5B and 7B. Our\nanalysis confirms that UCAS not only achieves higher rewards but also promotes\ngreater reasoning diversity and successfully mitigates entropy collapse.", "AI": {"tldr": "\u63d0\u51faUCAS\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6a21\u578b\u5185\u90e8\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\u6765\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4fe1\u7528\u5206\u914d\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5bf9\u5173\u952e\u51b3\u7b56\u5173\u6ce8\u4e0d\u8db3\u7684\u95ee\u9898", "motivation": "\u73b0\u6709RLVR\u7b97\u6cd5\u5982GRPO\u5728\u5e8f\u5217\u4e2d\u5e7f\u64ad\u7edf\u4e00\u7684\u4f18\u52bf\u4fe1\u53f7\uff0c\u5ffd\u7565\u4e86\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4e0d\u786e\u5b9a\u7684\u9ad8\u98ce\u9669\u51b3\u7b56\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5bfc\u81f4\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\u548c\u71b5\u5d29\u6e83\u95ee\u9898", "method": "UCAS\u91c7\u7528\u4e24\u9636\u6bb5\u673a\u5236\uff1a\u9996\u5148\u57fa\u4e8e\u6a21\u578b\u6574\u4f53\u81ea\u4fe1\u5ea6\u8c03\u5236\u54cd\u5e94\u7ea7\u4f18\u52bf\uff0c\u7136\u540e\u57fa\u4e8e\u539f\u59cblogit\u786e\u5b9a\u6027\u5e94\u7528token\u7ea7\u60e9\u7f5a\uff0c\u9f13\u52b1\u63a2\u7d22\u9ad8\u4e0d\u786e\u5b9a\u6027\u4f46\u6b63\u786e\u7684\u8def\u5f84\uff0c\u60e9\u7f5a\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9519\u8bef\u63a8\u7406", "result": "\u5728\u4e94\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cUCAS\u5728\u591a\u4e2a\u6a21\u578b\u89c4\u6a21\uff08\u5305\u62ec1.5B\u548c7B\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u5f3aRLVR\u57fa\u7ebf\uff0c\u4e0d\u4ec5\u83b7\u5f97\u66f4\u9ad8\u5956\u52b1\uff0c\u8fd8\u4fc3\u8fdb\u66f4\u5927\u7684\u63a8\u7406\u591a\u6837\u6027\u5e76\u6210\u529f\u7f13\u89e3\u71b5\u5d29\u6e83", "conclusion": "UCAS\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u4f18\u52bf\u5851\u9020\u6709\u6548\u5e73\u8861\u4e86\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u6743\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b"}}
{"id": "2510.10959", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10959", "abs": "https://arxiv.org/abs/2510.10959", "authors": ["Xiaoyun Zhang", "Xiaojian Yuan", "Di Huang", "Wang You", "Chen Hu", "Jingqing Ruan", "Kejiang Chen", "Xing Hu"], "title": "Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its Potential for LLM Reinforcement Learning", "comment": "16 pages, 4 figures", "summary": "Reasoning ability has become a defining capability of Large Language Models\n(LLMs), with Reinforcement Learning with Verifiable Rewards (RLVR) emerging as\na key paradigm to enhance it. However, RLVR training often suffers from policy\nentropy collapse, where the policy becomes overly deterministic, hindering\nexploration and limiting reasoning performance. While entropy regularization is\na common remedy, its effectiveness is highly sensitive to the fixed\ncoefficient, making it unstable across tasks and models. In this work, we\nrevisit entropy regularization in RLVR and argue that its potential has been\nlargely underestimated. Our analysis shows that (i) tasks of varying difficulty\ndemand distinct exploration intensities, and (ii) balanced exploration may\nrequire the policy entropy to be maintained within a moderate range below its\ninitial level. Therefore, we propose Adaptive Entropy Regularization (AER)--a\nframework that dynamically balances exploration and exploitation via three\ncomponents: difficulty-aware coefficient allocation, initial-anchored target\nentropy, and dynamic global coefficient adjustment. Experiments on multiple\nmathematical reasoning benchmarks show that AER consistently outperforms\nbaselines, improving both reasoning accuracy and exploration capability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u81ea\u9002\u5e94\u71b5\u6b63\u5219\u5316(AER)\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u71b5\u6b63\u5219\u5316\u7cfb\u6570\u6765\u89e3\u51b3RLVR\u8bad\u7ec3\u4e2d\u7684\u7b56\u7565\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u51c6\u786e\u6027\u548c\u63a2\u7d22\u80fd\u529b\u3002", "motivation": "RLVR\u8bad\u7ec3\u4e2d\u5e38\u51fa\u73b0\u7b56\u7565\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u5bfc\u81f4\u7b56\u7565\u8fc7\u4e8e\u786e\u5b9a\u6027\u5316\uff0c\u963b\u788d\u63a2\u7d22\u5e76\u9650\u5236\u63a8\u7406\u6027\u80fd\u3002\u4f20\u7edf\u7684\u56fa\u5b9a\u7cfb\u6570\u71b5\u6b63\u5219\u5316\u65b9\u6cd5\u6548\u679c\u4e0d\u7a33\u5b9a\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u548c\u6a21\u578b\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u71b5\u6b63\u5219\u5316(AER)\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u96be\u5ea6\u611f\u77e5\u7cfb\u6570\u5206\u914d\u3001\u521d\u59cb\u951a\u5b9a\u76ee\u6807\u71b5\u548c\u52a8\u6001\u5168\u5c40\u7cfb\u6570\u8c03\u6574\uff0c\u52a8\u6001\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAER\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u51c6\u786e\u6027\u548c\u63a2\u7d22\u80fd\u529b\u3002", "conclusion": "\u81ea\u9002\u5e94\u71b5\u6b63\u5219\u5316\u80fd\u6709\u6548\u89e3\u51b3RLVR\u8bad\u7ec3\u4e2d\u7684\u7b56\u7565\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u71b5\u6b63\u5219\u5316\u7cfb\u6570\u6765\u9002\u5e94\u4e0d\u540c\u96be\u5ea6\u4efb\u52a1\u7684\u9700\u6c42\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2510.10114", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10114", "abs": "https://arxiv.org/abs/2510.10114", "authors": ["Luyao Zhuang", "Shengyuan Chen", "Yilin Xiao", "Huachi Zhou", "Yujing Zhang", "Hao Chen", "Qinggang Zhang", "Xiao Huang"], "title": "LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale Corpora", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is widely used to mitigate\nhallucinations of Large Language Models (LLMs) by leveraging external\nknowledge. While effective for simple queries, traditional RAG systems struggle\nwith large-scale, unstructured corpora where information is fragmented. Recent\nadvances incorporate knowledge graphs to capture relational structures,\nenabling more comprehensive retrieval for complex, multi-hop reasoning tasks.\nHowever, existing graph-based RAG (GraphRAG) methods rely on unstable and\ncostly relation extraction for graph construction, often producing noisy graphs\nwith incorrect or inconsistent relations that degrade retrieval quality. In\nthis paper, we revisit the pipeline of existing GraphRAG systems and propose\nLinearRAG (Linear Graph-based Retrieval-Augmented Generation), an efficient\nframework that enables reliable graph construction and precise passage\nretrieval. Specifically, LinearRAG constructs a relation-free hierarchical\ngraph, termed Tri-Graph, using only lightweight entity extraction and semantic\nlinking, avoiding unstable relation modeling. This new paradigm of graph\nconstruction scales linearly with corpus size and incurs no extra token\nconsumption, providing an economical and reliable indexing of the original\npassages. For retrieval, LinearRAG adopts a two-stage strategy: (i) relevant\nentity activation via local semantic bridging, followed by (ii) passage\nretrieval through global importance aggregation. Extensive experiments on four\ndatasets demonstrate that LinearRAG significantly outperforms baseline models.", "AI": {"tldr": "LinearRAG\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u56fe\u7ed3\u6784\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u65e0\u5173\u7cfb\u5c42\u6b21\u56fe\uff08Tri-Graph\uff09\u548c\u4e24\u9636\u6bb5\u68c0\u7d22\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfGraphRAG\u4e2d\u5173\u7cfb\u63d0\u53d6\u4e0d\u7a33\u5b9a\u548c\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfRAG\u7cfb\u7edf\u5728\u5904\u7406\u5927\u89c4\u6a21\u975e\u7ed3\u6784\u5316\u8bed\u6599\u65f6\u6548\u679c\u6709\u9650\uff0c\u800c\u73b0\u6709\u7684\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684RAG\u65b9\u6cd5\u4f9d\u8d56\u4e0d\u7a33\u5b9a\u4e14\u6210\u672c\u9ad8\u6602\u7684\u5173\u7cfb\u63d0\u53d6\uff0c\u5bfc\u81f4\u56fe\u8c31\u566a\u58f0\u5927\u3001\u68c0\u7d22\u8d28\u91cf\u4e0b\u964d\u3002", "method": "LinearRAG\u6784\u5efa\u65e0\u5173\u7cfb\u5c42\u6b21\u56fe\uff08Tri-Graph\uff09\uff0c\u4ec5\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5b9e\u4f53\u63d0\u53d6\u548c\u8bed\u4e49\u94fe\u63a5\uff0c\u907f\u514d\u4e0d\u7a33\u5b9a\u5173\u7cfb\u5efa\u6a21\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u68c0\u7d22\u7b56\u7565\uff1a\u901a\u8fc7\u5c40\u90e8\u8bed\u4e49\u6865\u63a5\u6fc0\u6d3b\u76f8\u5173\u5b9e\u4f53\uff0c\u7136\u540e\u901a\u8fc7\u5168\u5c40\u91cd\u8981\u6027\u805a\u5408\u8fdb\u884c\u6bb5\u843d\u68c0\u7d22\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLinearRAG\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "LinearRAG\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u53ef\u9760\u7684\u56fe\u6784\u5efa\u65b9\u6cd5\uff0c\u80fd\u591f\u7ebf\u6027\u6269\u5c55\u8bed\u6599\u89c4\u6a21\u4e14\u4e0d\u6d88\u8017\u989d\u5916token\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3002"}}
{"id": "2510.10675", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10675", "abs": "https://arxiv.org/abs/2510.10675", "authors": ["Deven Panchal"], "title": "Simpliflow: A Lightweight Open-Source Framework for Rapid Creation and Deployment of Generative Agentic AI Workflows", "comment": null, "summary": "Generative Agentic AI systems are emerging as a powerful paradigm for\nautomating complex, multi-step tasks. However, many existing frameworks for\nbuilding these systems introduce significant complexity, a steep learning\ncurve, and substantial boilerplate code, hindering rapid prototyping and\ndeployment. This paper introduces simpliflow, a lightweight, open-source Python\nframework designed to address these challenges. simpliflow enables the rapid\ndevelopment and orchestration of linear, deterministic agentic workflows\nthrough a declarative, JSON-based configuration. Its modular architecture\ndecouples agent management, workflow execution, and post-processing, promoting\nease of use and extensibility. By integrating with LiteLLM, it supports over\n100 Large Language Models (LLMs) out-of-the-box. We present the architecture,\noperational flow, and core features of simpliflow, demonstrating its utility\nthrough diverse use cases ranging from software development simulation to\nreal-time system interaction. A comparative analysis with prominent frameworks\nlike LangChain and AutoGen highlights simpliflow's unique position as a tool\noptimized for simplicity, control, and speed in deterministic workflow\nenvironments.", "AI": {"tldr": "simpliflow\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5f00\u6e90Python\u6846\u67b6\uff0c\u7528\u4e8e\u5feb\u901f\u6784\u5efa\u548c\u7f16\u6392\u7ebf\u6027\u786e\u5b9a\u6027\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u58f0\u660e\u5f0fJSON\u914d\u7f6e\u7b80\u5316\u5f00\u53d1\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u667a\u80fd\u4f53AI\u7cfb\u7edf\u6846\u67b6\u5b58\u5728\u590d\u6742\u6027\u9ad8\u3001\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\u3001\u6837\u677f\u4ee3\u7801\u591a\u7b49\u95ee\u9898\uff0c\u963b\u788d\u4e86\u5feb\u901f\u539f\u578b\u5f00\u53d1\u548c\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5c06\u667a\u80fd\u4f53\u7ba1\u7406\u3001\u5de5\u4f5c\u6d41\u6267\u884c\u548c\u540e\u5904\u7406\u89e3\u8026\uff0c\u901a\u8fc7\u58f0\u660e\u5f0fJSON\u914d\u7f6e\u5b9a\u4e49\u5de5\u4f5c\u6d41\uff0c\u5e76\u96c6\u6210LiteLLM\u652f\u6301100\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5c55\u793a\u4e86simpliflow\u5728\u8f6f\u4ef6\u5f00\u53d1\u6a21\u62df\u548c\u5b9e\u65f6\u7cfb\u7edf\u4ea4\u4e92\u7b49\u591a\u6837\u5316\u7528\u4f8b\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u4e0eLangChain\u548cAutoGen\u7b49\u6846\u67b6\u76f8\u6bd4\uff0c\u5728\u786e\u5b9a\u6027\u5de5\u4f5c\u6d41\u73af\u5883\u4e2d\u5177\u6709\u7b80\u5355\u6027\u3001\u63a7\u5236\u6027\u548c\u901f\u5ea6\u4f18\u52bf\u3002", "conclusion": "simpliflow\u4f5c\u4e3a\u4f18\u5316\u7b80\u5355\u6027\u3001\u63a7\u5236\u6027\u548c\u901f\u5ea6\u7684\u5de5\u5177\uff0c\u5728\u786e\u5b9a\u6027\u5de5\u4f5c\u6d41\u73af\u5883\u4e2d\u5177\u6709\u72ec\u7279\u5b9a\u4f4d\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u667a\u80fd\u4f53\u7cfb\u7edf\u5f00\u53d1\u590d\u6742\u5ea6\u3002"}}
{"id": "2510.10968", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10968", "abs": "https://arxiv.org/abs/2510.10968", "authors": ["Hongkai Zheng", "Austin Wang", "Zihui Wu", "Zhengyu Huang", "Ricardo Baptista", "Yisong Yue"], "title": "Blade: A Derivative-free Bayesian Inversion Method using Diffusion Priors", "comment": null, "summary": "Derivative-free Bayesian inversion is an important task in many science and\nengineering applications, particularly when computing the forward model\nderivative is computationally and practically challenging. In this paper, we\nintroduce Blade, which can produce accurate and well-calibrated posteriors for\nBayesian inversion using an ensemble of interacting particles. Blade leverages\npowerful data-driven priors based on diffusion models, and can handle nonlinear\nforward models that permit only black-box access (i.e., derivative-free).\nTheoretically, we establish a non-asymptotic convergence analysis to\ncharacterize the effects of forward model and prior estimation errors.\nEmpirically, Blade achieves superior performance compared to existing\nderivative-free Bayesian inversion methods on various inverse problems,\nincluding challenging highly nonlinear fluid dynamics.", "AI": {"tldr": "Blade\u662f\u4e00\u79cd\u65e0\u5bfc\u6570\u7684\u8d1d\u53f6\u65af\u53cd\u6f14\u65b9\u6cd5\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u5148\u9a8c\uff0c\u901a\u8fc7\u7c92\u5b50\u96c6\u5408\u4ea4\u4e92\u5904\u7406\u975e\u7ebf\u6027\u524d\u5411\u6a21\u578b\uff0c\u65e0\u9700\u8ba1\u7b97\u5bfc\u6570\u3002", "motivation": "\u5728\u8bb8\u591a\u79d1\u5b66\u548c\u5de5\u7a0b\u5e94\u7528\u4e2d\uff0c\u8ba1\u7b97\u524d\u5411\u6a21\u578b\u5bfc\u6570\u5728\u8ba1\u7b97\u548c\u5b9e\u9645\u5e94\u7528\u4e0a\u90fd\u5f88\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u65e0\u5bfc\u6570\u7684\u8d1d\u53f6\u65af\u53cd\u6f14\u65b9\u6cd5\u3002", "method": "Blade\u5229\u7528\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u9a71\u52a8\u5148\u9a8c\uff0c\u901a\u8fc7\u4ea4\u4e92\u7c92\u5b50\u96c6\u5408\u8fdb\u884c\u8d1d\u53f6\u65af\u53cd\u6f14\uff0c\u80fd\u591f\u5904\u7406\u4ec5\u5141\u8bb8\u9ed1\u76d2\u8bbf\u95ee\u7684\u975e\u7ebf\u6027\u524d\u5411\u6a21\u578b\u3002", "result": "\u7406\u8bba\u4e0a\u5efa\u7acb\u4e86\u975e\u6e10\u8fd1\u6536\u655b\u5206\u6790\u6765\u8868\u5f81\u524d\u5411\u6a21\u578b\u548c\u5148\u9a8c\u4f30\u8ba1\u8bef\u5dee\u7684\u5f71\u54cd\uff1b\u5b9e\u8bc1\u4e0a\uff0cBlade\u5728\u5404\u79cd\u53cd\u95ee\u9898\uff08\u5305\u62ec\u9ad8\u5ea6\u975e\u7ebf\u6027\u6d41\u4f53\u52a8\u529b\u5b66\uff09\u4e0a\u4f18\u4e8e\u73b0\u6709\u65e0\u5bfc\u6570\u8d1d\u53f6\u65af\u53cd\u6f14\u65b9\u6cd5\u3002", "conclusion": "Blade\u80fd\u591f\u4e3a\u8d1d\u53f6\u65af\u53cd\u6f14\u4ea7\u751f\u51c6\u786e\u4e14\u6821\u51c6\u826f\u597d\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5904\u7406\u975e\u7ebf\u6027\u524d\u5411\u6a21\u578b\u4e14\u65e0\u9700\u5bfc\u6570\u8ba1\u7b97\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.10138", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10138", "abs": "https://arxiv.org/abs/2510.10138", "authors": ["Zilong Wang", "Xiaoyu Shen"], "title": "Hybrid OCR-LLM Framework for Enterprise-Scale Document Information Extraction Under Copy-heavy Task", "comment": null, "summary": "Information extraction from copy-heavy documents, characterized by massive\nvolumes of structurally similar content, represents a critical yet understudied\nchallenge in enterprise document processing. We present a systematic framework\nthat strategically combines OCR engines with Large Language Models (LLMs) to\noptimize the accuracy-efficiency trade-off inherent in repetitive document\nextraction tasks. Unlike existing approaches that pursue universal solutions,\nour method exploits document-specific characteristics through intelligent\nstrategy selection. We implement and evaluate 25 configurations across three\nextraction paradigms (direct, replacement, and table-based) on identity\ndocuments spanning four formats (PNG, DOCX, XLSX, PDF). Through table-based\nextraction methods, our adaptive framework delivers outstanding results: F1=1.0\naccuracy with 0.97s latency for structured documents, and F1=0.997 accuracy\nwith 0.6 s for challenging image inputs when integrated with PaddleOCR, all\nwhile maintaining sub-second processing speeds. The 54 times performance\nimprovement compared with multimodal methods over naive approaches, coupled\nwith format-aware routing, enables processing of heterogeneous document streams\nat production scale. Beyond the specific application to identity extraction,\nthis work establishes a general principle: the repetitive nature of copy-heavy\ntasks can be transformed from a computational burden into an optimization\nopportunity through structure-aware method selection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408OCR\u5f15\u64ce\u548cLLM\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u91cd\u590d\u6027\u6587\u6863\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u7684\u51c6\u786e\u7387-\u6548\u7387\u6743\u8861\uff0c\u901a\u8fc7\u667a\u80fd\u7b56\u7565\u9009\u62e9\u5229\u7528\u6587\u6863\u7279\u5b9a\u7279\u5f81\uff0c\u5728\u8eab\u4efd\u6587\u6863\u63d0\u53d6\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u4e9a\u79d2\u7ea7\u5904\u7406\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u6587\u6863\u5904\u7406\u4e2d\u5927\u91cf\u7ed3\u6784\u76f8\u4f3c\u5185\u5bb9\u7684\u4fe1\u606f\u63d0\u53d6\u8fd9\u4e00\u5173\u952e\u4f46\u7814\u7a76\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u4f18\u5316\u91cd\u590d\u6027\u6587\u6863\u63d0\u53d6\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u7387-\u6548\u7387\u6743\u8861\u3002", "method": "\u7ed3\u5408OCR\u5f15\u64ce\u548cLLM\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u7b56\u7565\u9009\u62e9\u5229\u7528\u6587\u6863\u7279\u5b9a\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u4e09\u79cd\u63d0\u53d6\u8303\u5f0f\uff08\u76f4\u63a5\u3001\u66ff\u6362\u548c\u57fa\u4e8e\u8868\u683c\uff09\u768425\u79cd\u914d\u7f6e\uff0c\u5728\u56db\u79cd\u683c\u5f0f\uff08PNG\u3001DOCX\u3001XLSX\u3001PDF\uff09\u7684\u8eab\u4efd\u6587\u6863\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u57fa\u4e8e\u8868\u683c\u7684\u63d0\u53d6\u65b9\u6cd5\u53d6\u5f97\u4e86\u4f18\u5f02\u7ed3\u679c\uff1a\u7ed3\u6784\u5316\u6587\u6863F1=1.0\u51c6\u786e\u7387\uff0c\u5ef6\u8fdf0.97\u79d2\uff1b\u5177\u6709\u6311\u6218\u6027\u7684\u56fe\u50cf\u8f93\u5165F1=0.997\u51c6\u786e\u7387\uff0c\u5ef6\u8fdf0.6\u79d2\uff08\u4e0ePaddleOCR\u96c6\u6210\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e9a\u79d2\u7ea7\u5904\u7406\u901f\u5ea6\u3002\u76f8\u6bd4\u6734\u7d20\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u534754\u500d\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u786e\u7acb\u4e86\u4e00\u4e2a\u901a\u7528\u539f\u5219\uff1a\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u7684\u65b9\u6cd5\u9009\u62e9\uff0c\u53ef\u4ee5\u5c06\u590d\u5236\u7e41\u91cd\u4efb\u52a1\u7684\u91cd\u590d\u6027\u4ece\u8ba1\u7b97\u8d1f\u62c5\u8f6c\u53d8\u4e3a\u4f18\u5316\u673a\u4f1a\uff0c\u4e3a\u5904\u7406\u5f02\u6784\u6587\u6863\u6d41\u63d0\u4f9b\u4e86\u751f\u4ea7\u89c4\u6a21\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10689", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10689", "abs": "https://arxiv.org/abs/2510.10689", "authors": ["Caorui Li", "Yu Chen", "Yiyan Ji", "Jin Xu", "Zhenyu Cui", "Shihao Li", "Yuanxing Zhang", "Jiafu Tang", "Zhenghao Song", "Dingling Zhang", "Ying He", "Haoxiang Liu", "Yuxuan Wang", "Qiufeng Wang", "Zhenhe Wu", "Jiehui Luo", "Zhiyu Pan", "Weihao Xie", "Chenchen Zhang", "Zhaohui Wang", "Jiayi Tian", "Yanghai Wang", "Zhe Cao", "Minxin Dai", "Ke Wang", "Runzhe Wen", "Yinghao Ma", "Yaning Pan", "Sungkyun Chang", "Termeh Taheri", "Haiwen Xia", "Christos Plachouras", "Emmanouil Benetos", "Yizhi Li", "Ge Zhang", "Jian Yang", "Tianhao Peng", "Zili Wang", "Minghao Liu", "Junran Peng", "Zhaoxiang Zhang", "Jiaheng Liu"], "title": "OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs", "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nsubstantial potential in video understanding. However, existing benchmarks fail\nto comprehensively evaluate synergistic reasoning capabilities across audio and\nvisual modalities, often neglecting either one of the modalities or integrating\nthem in a logically inconsistent manner. To bridge this gap, we introduce\nOmniVideoBench, a large-scale and rigorously designed benchmark dedicated to\nassessing synergistic audio-visual understanding, with a strong emphasis on\nmodality complementarity and logical consistency. Specifically, OmniVideoBench\ncomprises 1000 high-quality question-answer(QA) pairs, each annotated with\nstep-by-step reasoning traces, derived from 628 diverse videos ranging from\nseveral seconds to 30 minutes, and manually verified to guarantee complete\ncorrectness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully\ndesigned question types, covering temporal reasoning, spatial localization,\ncounting, causal inference, summarization, and beyond, thereby capturing the\nessential challenges of video understanding. Evaluation of multiple MLLMs on\nOmniVideoBench reveals a pronounced gap between model performance and human\nreasoning, with open-source models lagging significantly behind their\nclosed-source counterparts, underscoring the inherent difficulty of genuine\naudio-visual reasoning. We will release OmniVideoBench to foster the\ndevelopment of MLLMs with stronger and more generalizable reasoning\ncapabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86OmniVideoBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u534f\u540c\u97f3\u9891-\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u5305\u542b1000\u4e2a\u9ad8\u8d28\u91cf\u95ee\u7b54\u5bf9\u548c13\u79cd\u95ee\u9898\u7c7b\u578b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u6001\u7684\u534f\u540c\u63a8\u7406\u80fd\u529b\uff0c\u5f80\u5f80\u5ffd\u89c6\u5176\u4e2d\u4e00\u4e2a\u6a21\u6001\u6216\u4ee5\u903b\u8f91\u4e0d\u4e00\u81f4\u7684\u65b9\u5f0f\u6574\u5408\u5b83\u4eec\u3002", "method": "\u6784\u5efa\u5305\u542b1000\u4e2a\u9ad8\u8d28\u91cf\u95ee\u7b54\u5bf9\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u6bcf\u4e2a\u95ee\u9898\u90fd\u6709\u9010\u6b65\u63a8\u7406\u8f68\u8ff9\uff0c\u6e90\u81ea628\u4e2a\u591a\u6837\u5316\u89c6\u9891\uff0c\u6db5\u76d613\u79cd\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u95ee\u9898\u7c7b\u578b\u3002", "result": "\u591a\u4e2aMLLM\u5728OmniVideoBench\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\u6a21\u578b\u6027\u80fd\u4e0e\u4eba\u7c7b\u63a8\u7406\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u5f00\u6e90\u6a21\u578b\u660e\u663e\u843d\u540e\u4e8e\u95ed\u6e90\u6a21\u578b\u3002", "conclusion": "OmniVideoBench\u5c06\u516c\u5f00\u53d1\u5e03\uff0c\u4ee5\u4fc3\u8fdb\u5177\u6709\u66f4\u5f3a\u548c\u66f4\u901a\u7528\u63a8\u7406\u80fd\u529b\u7684MLLM\u53d1\u5c55\uff0c\u7a81\u663e\u4e86\u771f\u6b63\u97f3\u9891-\u89c6\u89c9\u63a8\u7406\u7684\u5185\u5728\u96be\u5ea6\u3002"}}
{"id": "2510.10980", "categories": ["cs.LG", "cs.CV", "cs.IT", "math.IT", "math.ST", "stat.ML", "stat.TH", "68T07, 62B11, 94A17, 53B12", "I.2.6; I.5.1; G.3; H.1.1"], "pdf": "https://arxiv.org/pdf/2510.10980", "abs": "https://arxiv.org/abs/2510.10980", "authors": ["Di Zhang"], "title": "On the Optimal Representation Efficiency of Barlow Twins: An Information-Geometric Interpretation", "comment": "7 pages", "summary": "Self-supervised learning (SSL) has achieved remarkable success by learning\nmeaningful representations without labeled data. However, a unified theoretical\nframework for understanding and comparing the efficiency of different SSL\nparadigms remains elusive. In this paper, we introduce a novel\ninformation-geometric framework to quantify representation efficiency. We\ndefine representation efficiency $\\eta$ as the ratio between the effective\nintrinsic dimension of the learned representation space and its ambient\ndimension, where the effective dimension is derived from the spectral\nproperties of the Fisher Information Matrix (FIM) on the statistical manifold\ninduced by the encoder. Within this framework, we present a theoretical\nanalysis of the Barlow Twins method. Under specific but natural assumptions, we\nprove that Barlow Twins achieves optimal representation efficiency ($\\eta = 1$)\nby driving the cross-correlation matrix of representations towards the identity\nmatrix, which in turn induces an isotropic FIM. This work provides a rigorous\ntheoretical foundation for understanding the effectiveness of Barlow Twins and\noffers a new geometric perspective for analyzing SSL algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4fe1\u606f\u51e0\u4f55\u6846\u67b6\u91cf\u5316\u8868\u793a\u6548\u7387\uff0c\u8bc1\u660eBarlow Twins\u65b9\u6cd5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u80fd\u8fbe\u5230\u6700\u4f18\u8868\u793a\u6548\u7387", "motivation": "\u81ea\u76d1\u7763\u5b66\u4e60\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u7406\u89e3\u548c\u6bd4\u8f83\u4e0d\u540c\u8303\u5f0f\u7684\u6548\u7387", "method": "\u5b9a\u4e49\u8868\u793a\u6548\u7387\u03b7\u4e3a\u5b66\u4e60\u8868\u793a\u7a7a\u95f4\u7684\u6709\u6548\u5185\u5728\u7ef4\u5ea6\u4e0e\u73af\u5883\u7ef4\u5ea6\u4e4b\u6bd4\uff0c\u57fa\u4e8eFisher\u4fe1\u606f\u77e9\u9635\u7684\u5149\u8c31\u7279\u6027", "result": "\u5728\u7279\u5b9a\u81ea\u7136\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660eBarlow Twins\u901a\u8fc7\u9a71\u52a8\u8868\u793a\u4e92\u76f8\u5173\u77e9\u9635\u8d8b\u8fd1\u5355\u4f4d\u77e9\u9635\uff0c\u8bf1\u5bfc\u5404\u5411\u540c\u6027FIM\uff0c\u8fbe\u5230\u6700\u4f18\u8868\u793a\u6548\u7387(\u03b7=1)", "conclusion": "\u4e3a\u7406\u89e3Barlow Twins\u6709\u6548\u6027\u63d0\u4f9b\u4e25\u683c\u7406\u8bba\u57fa\u7840\uff0c\u4e3a\u5206\u6790SSL\u7b97\u6cd5\u63d0\u4f9b\u65b0\u7684\u51e0\u4f55\u89c6\u89d2"}}
{"id": "2510.10142", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10142", "abs": "https://arxiv.org/abs/2510.10142", "authors": ["Tingxu Han", "Wei Song", "Ziqi Ding", "Ziming Li", "Chunrong Fang", "Yuekang Li", "Dongfang Liu", "Zhenyu Chen", "Zhenting Wang"], "title": "DiffHeads: Differential Analysis and Inference-Time Masking of Bias Heads in Large Language Models", "comment": null, "summary": "Large language models (LLMs) increasingly mediate decisions in domains where\nunfair treatment of demographic groups is unacceptable. Existing work probes\nwhen biased outputs appear, but gives little insight into the mechanisms that\ngenerate them, leaving existing mitigations largely fragile. In this paper, we\nconduct a systematic investigation LLM unfairness and propose DiffHeads, a\nlightweight debiasing framework for LLMs. We first compare Direct-Answer (DA)\nprompting to Chain-of-Thought (CoT) prompting across eight representative open-\nand closed-source LLMs. DA will trigger the nature bias part of LLM and improve\nmeasured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues.\nNext, we define a token-to-head contribution score that traces each token's\ninfluence back to individual attention heads. This reveals a small cluster of\nbias heads that activate under DA but stay largely dormant with CoT, providing\nthe first causal link between prompting strategy and bias emergence. Finally,\nbuilding on this insight, we propose DiffHeads that identifies bias heads\nthrough differential activation analysis between DA and CoT, and selectively\nmasks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under\nDA and CoT, respectively, without harming model utility.", "AI": {"tldr": "DiffHeads\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684LLM\u53bb\u504f\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u6ce8\u610f\u529b\u5934\u5728\u76f4\u63a5\u56de\u7b54\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u4e0b\u7684\u5dee\u5f02\u6fc0\u6d3b\u6765\u9009\u62e9\u6027\u5c4f\u853d\u504f\u89c1\u5934\uff0c\u6709\u6548\u51cf\u5c11\u4e0d\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u504f\u89c1\u8f93\u51fa\u7684\u51fa\u73b0\u65f6\u673a\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u751f\u6210\u673a\u5236\u7684\u6df1\u5165\u7406\u89e3\uff0c\u5bfc\u81f4\u53bb\u504f\u65b9\u6cd5\u8106\u5f31\u3002\u9700\u8981\u7cfb\u7edf\u7814\u7a76LLM\u4e0d\u516c\u5e73\u6027\u5e76\u5f00\u53d1\u66f4\u6709\u6548\u7684\u53bb\u504f\u673a\u5236\u3002", "method": "1. \u6bd4\u8f83\u76f4\u63a5\u56de\u7b54\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u57288\u4e2a\u4ee3\u8868\u6027LLM\u4e2d\u7684\u8868\u73b0\uff1b2. \u5b9a\u4e49token-to-head\u8d21\u732e\u5206\u6570\u8ffd\u8e2a\u6ce8\u610f\u529b\u5934\u5f71\u54cd\uff1b3. \u63d0\u51faDiffHeads\u6846\u67b6\uff0c\u901a\u8fc7\u5dee\u5f02\u6fc0\u6d3b\u5206\u6790\u8bc6\u522b\u504f\u89c1\u5934\u5e76\u9009\u62e9\u6027\u5c4f\u853d\u3002", "result": "\u76f4\u63a5\u56de\u7b54\u63d0\u793a\u4f1a\u89e6\u53d1LLM\u7684\u56fa\u6709\u504f\u89c1\uff0c\u4e0d\u516c\u5e73\u6027\u589e\u52a0534.5%-391.9%\uff1b\u53d1\u73b0\u4e00\u5c0f\u7c07\u504f\u89c1\u5934\u5728\u76f4\u63a5\u56de\u7b54\u4e0b\u6fc0\u6d3b\u4f46\u5728\u601d\u7ef4\u94fe\u4e0b\u4f11\u7720\uff1bDiffHeads\u5728\u76f4\u63a5\u56de\u7b54\u548c\u601d\u7ef4\u94fe\u4e0b\u5206\u522b\u51cf\u5c11\u4e0d\u516c\u5e73\u602749.4%\u548c40.3%\uff0c\u4e14\u4e0d\u5f71\u54cd\u6a21\u578b\u6548\u7528\u3002", "conclusion": "DiffHeads\u901a\u8fc7\u56e0\u679c\u673a\u5236\u5206\u6790\u63d0\u4f9b\u4e86\u9996\u4e2a\u63d0\u793a\u7b56\u7565\u4e0e\u504f\u89c1\u51fa\u73b0\u4e4b\u95f4\u7684\u56e0\u679c\u8054\u7cfb\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u8f7b\u91cf\u7ea7\u53bb\u504f\uff0c\u4e3a\u7406\u89e3LLM\u504f\u89c1\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2510.09780", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09780", "abs": "https://arxiv.org/abs/2510.09780", "authors": ["ChengAo Shen", "Ziming Zhao", "Hanghang Tong", "Dongjin Song", "Dongsheng Luo", "Qingsong Wen", "Jingchao Ni"], "title": "SVTime: Small Time Series Forecasting Models Informed by \"Physics\" of Large Vision Model Forecasters", "comment": null, "summary": "Time series AI is crucial for analyzing dynamic web content, driving a surge\nof pre-trained large models known for their strong knowledge encoding and\ntransfer capabilities across diverse tasks. However, given their\nenergy-intensive training, inference, and hardware demands, using large models\nas a one-fits-all solution raises serious concerns about carbon footprint and\nsustainability. For a specific task, a compact yet specialized, high-performing\nmodel may be more practical and affordable, especially for resource-constrained\nusers such as small businesses. This motivates the question: Can we build\ncost-effective lightweight models with large-model-like performance on core\ntasks such as forecasting? This paper addresses this question by introducing\nSVTime, a novel Small model inspired by large Vision model (LVM) forecasters\nfor long-term Time series forecasting (LTSF). Recently, LVMs have been shown as\npowerful tools for LTSF. We identify a set of key inductive biases of LVM\nforecasters -- analogous to the \"physics\" governing their behaviors in LTSF --\nand design small models that encode these biases through meticulously crafted\nlinear layers and constraint functions. Across 21 baselines spanning\nlightweight, complex, and pre-trained large models on 8 benchmark datasets,\nSVTime outperforms state-of-the-art (SOTA) lightweight models and rivals large\nmodels with 10^3 fewer parameters than LVMs, while enabling efficient training\nand inference in low-resource settings.", "AI": {"tldr": "SVTime\u662f\u4e00\u4e2a\u53d7\u5927\u578b\u89c6\u89c9\u6a21\u578b\u542f\u53d1\u7684\u5c0f\u578b\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u53c2\u6570\u6570\u91cf\uff0c\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "motivation": "\u5927\u578b\u6a21\u578b\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u80fd\u8017\u9ad8\u3001\u6210\u672c\u5927\uff0c\u4e0d\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u7528\u6237\u3002\u9700\u8981\u5f00\u53d1\u65e2\u7d27\u51d1\u53c8\u4e13\u4e1a\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u5728\u6838\u5fc3\u4efb\u52a1\u4e0a\u8fbe\u5230\u7c7b\u4f3c\u5927\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u8bc6\u522b\u5927\u578b\u89c6\u89c9\u6a21\u578b\u9884\u6d4b\u5668\u7684\u5173\u952e\u5f52\u7eb3\u504f\u7f6e\uff08\u7c7b\u4f3c\"\u7269\u7406\u89c4\u5f8b\"\uff09\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7ebf\u6027\u5c42\u548c\u7ea6\u675f\u51fd\u6570\u5728\u5c0f\u6a21\u578b\u4e2d\u7f16\u7801\u8fd9\u4e9b\u504f\u7f6e\u3002", "result": "\u57288\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cSVTime\u8d85\u8d8a\u4e8621\u4e2a\u57fa\u7ebf\u6a21\u578b\uff08\u5305\u62ec\u8f7b\u91cf\u7ea7\u3001\u590d\u6742\u548c\u9884\u8bad\u7ec3\u5927\u6a21\u578b\uff09\uff0c\u6027\u80fd\u5ab2\u7f8e\u5927\u578b\u6a21\u578b\u4f46\u53c2\u6570\u6570\u91cf\u51cf\u5c1110^3\u500d\u3002", "conclusion": "SVTime\u8bc1\u660e\u4e86\u53ef\u4ee5\u6784\u5efa\u6210\u672c\u6548\u76ca\u9ad8\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u5728\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fbe\u5230\u7c7b\u4f3c\u5927\u578b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u652f\u6301\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u8bad\u7ec3\u548c\u63a8\u7406\u3002"}}
{"id": "2510.10701", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.10701", "abs": "https://arxiv.org/abs/2510.10701", "authors": ["Yang Xu", "Shuwei Chen", "Jun Liu", "Feng Cao", "Xingxing He"], "title": "Extended Triangular Method: A Generalized Algorithm for Contradiction Separation Based Automated Deduction", "comment": "38 pages, 8 figures", "summary": "Automated deduction lies at the core of Artificial Intelligence (AI),\nunderpinning theorem proving, formal verification, and logical reasoning.\nDespite decades of progress, reconciling deductive completeness with\ncomputational efficiency remains an enduring challenge. Traditional reasoning\ncalculi, grounded in binary resolution, restrict inference to pairwise clause\ninteractions and thereby limit deductive synergy among multiple clauses. The\nContradiction Separation Extension (CSE) framework, introduced in 2018,\nproposed a dynamic multi-clause reasoning theory that redefined logical\ninference as a process of contradiction separation rather than sequential\nresolution. While that work established the theoretical foundation, its\nalgorithmic realization remained unformalized and unpublished. This work\npresents the Extended Triangular Method (ETM), a generalized\ncontradiction-construction algorithm that formalizes and extends the internal\nmechanisms of contradiction separation. The ETM unifies multiple\ncontradiction-building strategies, including the earlier Standard Extension\nmethod, within a triangular geometric framework that supports flexible clause\ninteraction and dynamic synergy. ETM serves as the algorithmic core of several\nhigh-performance theorem provers, CSE, CSE-E, CSI-E, and CSI-Enig, whose\ncompetitive results in standard first-order benchmarks (TPTP problem sets and\nCASC 2018-2015) empirically validate the effectiveness and generality of the\nproposed approach. By bridging theoretical abstraction and operational\nimplementation, ETM advances the contradiction separation paradigm into a\ngeneralized, scalable, and practically competitive model for automated\nreasoning, offering new directions for future research in logical inference and\ntheorem proving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6269\u5c55\u4e09\u89d2\u65b9\u6cd5(ETM)\uff0c\u4f5c\u4e3a\u77db\u76fe\u5206\u79bb\u6269\u5c55(CSE)\u6846\u67b6\u7684\u7b97\u6cd5\u5b9e\u73b0\uff0c\u7edf\u4e00\u4e86\u591a\u79cd\u77db\u76fe\u6784\u5efa\u7b56\u7565\uff0c\u4e3a\u81ea\u52a8\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u63a8\u7406\u6f14\u7b97\u57fa\u4e8e\u4e8c\u5143\u5f52\u7ed3\uff0c\u9650\u5236\u4e86\u591a\u5b50\u53e5\u95f4\u7684\u63a8\u7406\u534f\u540c\u3002\u867d\u7136CSE\u6846\u67b6\u63d0\u51fa\u4e86\u52a8\u6001\u591a\u5b50\u53e5\u63a8\u7406\u7406\u8bba\uff0c\u4f46\u5176\u7b97\u6cd5\u5b9e\u73b0\u5c1a\u672a\u5f62\u5f0f\u5316\u3002", "method": "\u5f00\u53d1\u4e86\u6269\u5c55\u4e09\u89d2\u65b9\u6cd5(ETM)\uff0c\u5728\u4e09\u89d2\u51e0\u4f55\u6846\u67b6\u5185\u5f62\u5f0f\u5316\u77db\u76fe\u5206\u79bb\u673a\u5236\uff0c\u652f\u6301\u7075\u6d3b\u7684\u5b50\u53e5\u4ea4\u4e92\u548c\u52a8\u6001\u534f\u540c\u3002", "result": "ETM\u4f5c\u4e3a\u591a\u4e2a\u9ad8\u6027\u80fd\u5b9a\u7406\u8bc1\u660e\u5668(CSE\u3001CSE-E\u3001CSI-E\u3001CSI-Enig)\u7684\u6838\u5fc3\u7b97\u6cd5\uff0c\u5728\u6807\u51c6\u4e00\u9636\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002", "conclusion": "ETM\u901a\u8fc7\u6865\u63a5\u7406\u8bba\u62bd\u8c61\u4e0e\u64cd\u4f5c\u5b9e\u73b0\uff0c\u5c06\u77db\u76fe\u5206\u79bb\u8303\u5f0f\u63a8\u8fdb\u4e3a\u901a\u7528\u3001\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u81ea\u52a8\u63a8\u7406\u6a21\u578b\uff0c\u4e3a\u903b\u8f91\u63a8\u7406\u548c\u5b9a\u7406\u8bc1\u660e\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.10157", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10157", "abs": "https://arxiv.org/abs/2510.10157", "authors": ["Tsung-Min Pai", "Jui-I Wang", "Li-Chun Lu", "Shao-Hua Sun", "Hung-Yi Lee", "Kai-Wei Chang"], "title": "BILLY: Steering Large Language Models via Merging Persona Vectors for Creative Generation", "comment": null, "summary": "Multi-LLM systems enhance the creativity of large language models by\nsimulating human collective intelligence but suffer from significant drawbacks,\nsuch as high computational costs and inference latency. To address these\nlimitations, we propose BILLY (BlendIng persona vectors for Large Language\nmodel creativitY), a training-free framework that captures the benefits of\nmulti-LLM collaboration, i.e. inducing diverse perspectives and specialized\nexpertise, within a single model. BILLY operates by extracting and blending\nmultiple distinct persona vectors directly in the model's activation space. We\nsteer the model's generation process with this merged vector while inference,\nenabling multi-perspective output without explicit multi-LLM communication. Our\nexperiments across creativity-oriented benchmarks demonstrate that BILLY\nsurpasses single model prompting and traditional multi-LLM approaches, while\nsubstantially reducing inference time and computational costs. Our analyses\nfurther reveal that distinct persona vectors can be blended to achieve both\neffective control over complementary aspects of generation and greater\ninterpretability.", "AI": {"tldr": "BILLY\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5355\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u63d0\u53d6\u548c\u6df7\u5408\u591a\u4e2a\u4eba\u683c\u5411\u91cf\uff0c\u5b9e\u73b0\u591aLLM\u534f\u4f5c\u7684\u521b\u9020\u529b\u4f18\u52bf\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u591aLLM\u7cfb\u7edf\u867d\u7136\u80fd\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u521b\u9020\u529b\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u63a8\u7406\u5ef6\u8fdf\u5927\u7684\u663e\u8457\u7f3a\u70b9\u3002", "method": "\u5728\u6a21\u578b\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u63d0\u53d6\u548c\u6df7\u5408\u591a\u4e2a\u4e0d\u540c\u7684\u4eba\u683c\u5411\u91cf\uff0c\u5728\u63a8\u7406\u65f6\u7528\u878d\u5408\u540e\u7684\u5411\u91cf\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u591a\u89c6\u89d2\u8f93\u51fa\u800c\u65e0\u9700\u663e\u5f0f\u7684\u591aLLM\u901a\u4fe1\u3002", "result": "\u5728\u9762\u5411\u521b\u9020\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBILLY\u8d85\u8d8a\u4e86\u5355\u6a21\u578b\u63d0\u793a\u548c\u4f20\u7edf\u591aLLM\u65b9\u6cd5\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\u548c\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u4e0d\u540c\u4eba\u683c\u5411\u91cf\u7684\u6df7\u5408\u65e2\u80fd\u6709\u6548\u63a7\u5236\u751f\u6210\u7684\u4e92\u8865\u65b9\u9762\uff0c\u53c8\u80fd\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u5355\u6a21\u578b\u5b9e\u73b0\u591aLLM\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.09781", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09781", "abs": "https://arxiv.org/abs/2510.09781", "authors": ["Yue Huang", "Hang Hua", "Yujun Zhou", "Pengcheng Jing", "Manish Nagireddy", "Inkit Padhi", "Greta Dolcetti", "Zhangchen Xu", "Subhajit Chaudhury", "Ambrish Rawat", "Liubov Nedoshivina", "Pin-Yu Chen", "Prasanna Sattigeri", "Xiangliang Zhang"], "title": "Building a Foundational Guardrail for General Agentic Systems via Synthetic Data", "comment": null, "summary": "While LLM agents can plan multi-step tasks, intervening at the planning\nstage-before any action is executed-is often the safest way to prevent harm,\nsince certain risks can lead to severe consequences once carried out. However,\nexisting guardrails mostly operate post-execution, which is difficult to scale\nand leaves little room for controllable supervision at the plan level. To\naddress this challenge, we highlight three critical gaps in current research:\ndata gap, model gap, and evaluation gap. To close the data gap, we introduce\nAuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii)\ninjects category-labeled risks with calibrated difficulty, and (iii) filters\noutputs via an automated reward model, producing large and reliable corpora for\npre-execution safety. To close the guardian model gap, we propose a\nfoundational guardrail Safiron, combining a cross-planner adapter with a\ncompact guardian model. The adapter unifies different input formats, while\nSafiron flags risky cases, assigns risk types, and generates rationales;\ntrained in two stages with a broadly explored data recipe, Safiron achieves\nrobust transfer across settings. To close the evaluation gap, we release\nPre-Exec Bench, a realistic benchmark covering diverse tools and branching\ntrajectories, which measures detection, fine-grained categorization,\nexplanation, and cross-planner generalization in human-verified scenarios.\nExtensive experiments demonstrate consistent gains of the proposed guardrail\nover strong baselines on Pre-Exec Bench, and ablations further distill\nactionable practices, providing a practical template for safer agentic systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86AuraGen\u6570\u636e\u751f\u6210\u5f15\u64ce\u3001Safiron\u5b88\u62a4\u6a21\u578b\u548cPre-Exec Bench\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u5728LLM\u667a\u80fd\u4f53\u6267\u884c\u524d\u9636\u6bb5\u68c0\u6d4b\u548c\u9632\u8303\u98ce\u9669\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u7684\u6570\u636e\u3001\u6a21\u578b\u548c\u8bc4\u4f30\u4e09\u4e2a\u5173\u952e\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u9632\u62a4\u63aa\u65bd\u4e3b\u8981\u5728\u667a\u80fd\u4f53\u6267\u884c\u540e\u64cd\u4f5c\uff0c\u96be\u4ee5\u6269\u5c55\u4e14\u7f3a\u4e4f\u5bf9\u8ba1\u5212\u5c42\u9762\u7684\u53ef\u63a7\u76d1\u7763\u3002\u67d0\u4e9b\u98ce\u9669\u4e00\u65e6\u6267\u884c\u4f1a\u9020\u6210\u4e25\u91cd\u540e\u679c\uff0c\u56e0\u6b64\u5728\u89c4\u5212\u9636\u6bb5\u8fdb\u884c\u5e72\u9884\u662f\u6700\u5b89\u5168\u7684\u9632\u8303\u65b9\u5f0f\u3002", "method": "1) AuraGen\uff1a\u53ef\u63a7\u6570\u636e\u751f\u6210\u5f15\u64ce\uff0c\u5408\u6210\u826f\u6027\u8f68\u8ff9\u3001\u6ce8\u5165\u5e26\u6807\u7b7e\u7684\u98ce\u9669\u3001\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u8fc7\u6ee4\u8f93\u51fa\uff1b2) Safiron\uff1a\u57fa\u7840\u9632\u62a4\u6a21\u578b\uff0c\u7ed3\u5408\u8de8\u89c4\u5212\u5668\u9002\u914d\u5668\u548c\u7d27\u51d1\u5b88\u62a4\u6a21\u578b\uff0c\u8fdb\u884c\u98ce\u9669\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u89e3\u91ca\uff1b3) Pre-Exec Bench\uff1a\u5305\u542b\u591a\u6837\u5316\u5de5\u5177\u548c\u5206\u652f\u8f68\u8ff9\u7684\u73b0\u5b9e\u57fa\u51c6\u3002", "result": "\u5728Pre-Exec Bench\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u9632\u62a4\u6a21\u578b\u5728\u68c0\u6d4b\u3001\u7ec6\u7c92\u5ea6\u5206\u7c7b\u3001\u89e3\u91ca\u548c\u8de8\u89c4\u5212\u5668\u6cdb\u5316\u65b9\u9762\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u63d0\u70bc\u4e86\u53ef\u64cd\u4f5c\u5b9e\u8df5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u66f4\u5b89\u5168\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6a21\u677f\uff0c\u901a\u8fc7\u586b\u8865\u6570\u636e\u3001\u6a21\u578b\u548c\u8bc4\u4f30\u4e09\u4e2a\u5173\u952e\u7a7a\u767d\uff0c\u5b9e\u73b0\u4e86\u5728\u89c4\u5212\u9636\u6bb5\u7684\u6709\u6548\u98ce\u9669\u9632\u8303\u3002"}}
{"id": "2510.10703", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10703", "abs": "https://arxiv.org/abs/2510.10703", "authors": ["Xiangyu Wang", "Haocheng Yang", "Fengxiang Cheng", "Fenrong Liu"], "title": "Adaptive Selection of Symbolic Languages for Improving LLM Logical Reasoning", "comment": null, "summary": "Large Language Models (LLMs) still struggle with complex logical reasoning.\nWhile previous works achieve remarkable improvements, their performance is\nhighly dependent on the correctness of translating natural language (NL)\nproblems into a symbolic language (SL). Though numerous works focusing on\nimproving this translation accuracy, they only consider the similarity between\nthe meaning of SL and NL, overlooking another crucial influencing factor, the\nselection of the target SL type itself. For example, first-order logic language\nspecializes in logical reasoning with categorical syllogisms and complex\nquantifiers, while Boolean satisfiability formalism excels at representing\nconstraint satisfaction like partial problems. To our knowledge, this is the\nfirst paper to claim and verify that different NL logical reasoning problem\ncorresponds to different optimal SL formalization for translation. Based on\nthis, we propose a methods to improve the logical reasoning performance of LLMs\nby adaptively selecting the most suitable SL for each problem prior to\ntranslation. Specifically, we leverage LLMs to select the target SL among\nfirst-order logic, logic programming and Boolean satisfiability and then\ntranslate the problem in NL to target SL expressions as well as employ the\ncorresponding logical solver to derive the final answer. Experimental results\non benchmarks show that our adaptive selection method significantly outperforms\ntranslating all into single SL and randomly selecting the SL. On a mixed\ndataset of these benchmarks, our approach achieves 96% accuracy, which\nimproving performance by 25% compared to the second highest accuracy from the\nfirst-order logic translation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u9009\u62e9\u7b26\u53f7\u8bed\u8a00\u7684\u65b9\u6cd5\u6765\u6539\u8fdbLLMs\u7684\u903b\u8f91\u63a8\u7406\u6027\u80fd\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u95ee\u9898\u9009\u62e9\u6700\u5408\u9002\u7684\u7b26\u53f7\u8bed\u8a00\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u7ffb\u8bd1\u4e3a\u7b26\u53f7\u8bed\u8a00\u65f6\uff0c\u53ea\u5173\u6ce8\u7ffb\u8bd1\u7684\u51c6\u786e\u6027\uff0c\u4f46\u5ffd\u7565\u4e86\u4e0d\u540c\u7b26\u53f7\u8bed\u8a00\u7c7b\u578b\u672c\u8eab\u5bf9\u63a8\u7406\u6027\u80fd\u7684\u5f71\u54cd\u3002\u4e0d\u540c\u903b\u8f91\u63a8\u7406\u95ee\u9898\u5bf9\u5e94\u4e0d\u540c\u7684\u6700\u4f18\u7b26\u53f7\u8bed\u8a00\u5f62\u5f0f\u5316\u65b9\u6cd5\u3002", "method": "\u5229\u7528LLMs\u4ece\u4e00\u9636\u903b\u8f91\u3001\u903b\u8f91\u7f16\u7a0b\u548c\u5e03\u5c14\u53ef\u6ee1\u8db3\u6027\u4e09\u79cd\u7b26\u53f7\u8bed\u8a00\u4e2d\u81ea\u9002\u5e94\u9009\u62e9\u6700\u9002\u5408\u5f53\u524d\u95ee\u9898\u7684\u76ee\u6807\u8bed\u8a00\uff0c\u7136\u540e\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u7ffb\u8bd1\u4e3a\u76ee\u6807\u7b26\u53f7\u8bed\u8a00\u8868\u8fbe\u5f0f\uff0c\u5e76\u4f7f\u7528\u76f8\u5e94\u7684\u903b\u8f91\u6c42\u89e3\u5668\u5f97\u51fa\u6700\u7ec8\u7b54\u6848\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5c06\u6240\u6709\u95ee\u9898\u7ffb\u8bd1\u4e3a\u5355\u4e00\u7b26\u53f7\u8bed\u8a00\u6216\u968f\u673a\u9009\u62e9\u7b26\u53f7\u8bed\u8a00\u7684\u65b9\u6cd5\u3002\u5728\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8696%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u4e00\u9636\u903b\u8f91\u7ffb\u8bd1\u7684\u6b21\u9ad8\u51c6\u786e\u7387\u63d0\u9ad8\u4e8625%\u3002", "conclusion": "\u4e0d\u540c\u81ea\u7136\u8bed\u8a00\u903b\u8f91\u63a8\u7406\u95ee\u9898\u5bf9\u5e94\u4e0d\u540c\u7684\u6700\u4f18\u7b26\u53f7\u8bed\u8a00\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u76ee\u6807\u7b26\u53f7\u8bed\u8a00\u53ef\u4ee5\u663e\u8457\u63d0\u5347LLMs\u7684\u903b\u8f91\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2510.11354", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11354", "abs": "https://arxiv.org/abs/2510.11354", "authors": ["Xuan Tang", "Han Zhang", "Yuan Cao", "Difan Zou"], "title": "Understanding the Generalization of Stochastic Gradient Adam in Learning Neural Networks", "comment": "71 pages, 12 figures, NeurIPS 2025", "summary": "Adam is a popular and widely used adaptive gradient method in deep learning,\nwhich has also received tremendous focus in theoretical research. However, most\nexisting theoretical work primarily analyzes its full-batch version, which\ndiffers fundamentally from the stochastic variant used in practice. Unlike SGD,\nstochastic Adam does not converge to its full-batch counterpart even with\ninfinitesimal learning rates. We present the first theoretical characterization\nof how batch size affects Adam's generalization, analyzing two-layer\nover-parameterized CNNs on image data. Our results reveal that while both Adam\nand AdamW with proper weight decay $\\lambda$ converge to poor test error\nsolutions, their mini-batch variants can achieve near-zero test error. We\nfurther prove Adam has a strictly smaller effective weight decay bound than\nAdamW, theoretically explaining why Adam requires more sensitive $\\lambda$\ntuning. Extensive experiments validate our findings, demonstrating the critical\nrole of batch size and weight decay in Adam's generalization performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7406\u8bba\u5206\u6790\u4e86\u6279\u91cf\u5927\u5c0f\u5bf9Adam\u4f18\u5316\u5668\u6cdb\u5316\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5728\u8fc7\u53c2\u6570\u5316CNN\u4e2d\uff0c\u5c0f\u6279\u91cfAdam/AdamW\u80fd\u5b9e\u73b0\u63a5\u8fd1\u96f6\u7684\u6d4b\u8bd5\u8bef\u5dee\uff0c\u800c\u5168\u6279\u91cf\u7248\u672c\u4f1a\u6536\u655b\u5230\u8f83\u5dee\u7684\u89e3\u3002", "motivation": "\u73b0\u6709\u7406\u8bba\u7814\u7a76\u4e3b\u8981\u5206\u6790Adam\u7684\u5168\u6279\u91cf\u7248\u672c\uff0c\u4f46\u5b9e\u9645\u4f7f\u7528\u7684\u662f\u968f\u673a\u7248\u672c\uff0c\u4e14\u968f\u673aAdam\u5373\u4f7f\u5b66\u4e60\u7387\u65e0\u9650\u5c0f\u4e5f\u4e0d\u4f1a\u6536\u655b\u5230\u5168\u6279\u91cf\u5bf9\u5e94\u7248\u672c\uff0c\u9700\u8981\u7814\u7a76\u6279\u91cf\u5927\u5c0f\u5bf9Adam\u6cdb\u5316\u7684\u5f71\u54cd\u3002", "method": "\u7406\u8bba\u5206\u6790\u4e24\u5c42\u7ea7\u8fc7\u53c2\u6570\u5316CNN\u5728\u56fe\u50cf\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u6bd4\u8f83Adam\u548cAdamW\u5728\u4e0d\u540c\u6279\u91cf\u5927\u5c0f\u4e0b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u63a8\u5bfc\u6709\u6548\u6743\u91cd\u8870\u51cf\u8fb9\u754c\u3002", "result": "\u5c0f\u6279\u91cfAdam/AdamW\u80fd\u5b9e\u73b0\u63a5\u8fd1\u96f6\u7684\u6d4b\u8bd5\u8bef\u5dee\uff0c\u800c\u5168\u6279\u91cf\u7248\u672c\u6536\u655b\u5230\u8f83\u5dee\u7684\u6d4b\u8bd5\u8bef\u5dee\u89e3\u3002Adam\u7684\u6709\u6548\u6743\u91cd\u8870\u51cf\u8fb9\u754c\u4e25\u683c\u5c0f\u4e8eAdamW\uff0c\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48Adam\u9700\u8981\u66f4\u654f\u611f\u7684\u03bb\u8c03\u4f18\u3002", "conclusion": "\u6279\u91cf\u5927\u5c0f\u548c\u6743\u91cd\u8870\u51cf\u5728Adam\u7684\u6cdb\u5316\u6027\u80fd\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u5c0f\u6279\u91cf\u8bad\u7ec3\u80fd\u663e\u8457\u6539\u5584Adam\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e14Adam\u6bd4AdamW\u5bf9\u6743\u91cd\u8870\u51cf\u8d85\u53c2\u6570\u66f4\u654f\u611f\u3002"}}
{"id": "2510.10617", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.10617", "abs": "https://arxiv.org/abs/2510.10617", "authors": ["Bahadur Yadav", "Sanjay Kumar Mohanty"], "title": "Encoder Decoder Generative Adversarial Network Model for Stock Market Prediction", "comment": null, "summary": "Forecasting stock prices remains challenging due to the volatile and\nnon-linear nature of financial markets. Despite the promise of deep learning,\nissues such as mode collapse, unstable training, and difficulty in capturing\ntemporal and feature level correlations have limited the applications of GANs\nin this domain. We propose a GRU-based Encoder-Decoder GAN (EDGAN) model that\nstrikes a balance between expressive power and simplicity. The model introduces\nkey innovations such as a temporal decoder with residual connections for\nprecise reconstruction, conditioning on static and dynamic covariates for\ncontextual learning, and a windowing mechanism to capture temporal dynamics.\nHere, the generator uses a dense encoder-decoder framework with residual GRU\nblocks. Extensive experiments on diverse stock datasets demonstrate that EDGAN\nachieves superior forecasting accuracy and training stability, even in volatile\nmarkets. It consistently outperforms traditional GAN variants in forecasting\naccuracy and convergence stability under market conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGRU\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668GAN\u6a21\u578b(EDGAN)\uff0c\u7528\u4e8e\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\uff0c\u901a\u8fc7\u5f15\u5165\u6b8b\u5dee\u8fde\u63a5\u3001\u6761\u4ef6\u5b66\u4e60\u548c\u7a97\u53e3\u673a\u5236\u7b49\u521b\u65b0\uff0c\u5728\u6ce2\u52a8\u5e02\u573a\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u9884\u6d4b\u7cbe\u5ea6\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u91d1\u878d\u5e02\u573a\u5177\u6709\u6ce2\u52a8\u6027\u548c\u975e\u7ebf\u6027\u7279\u5f81\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u6709\u6f5c\u529b\uff0c\u4f46GANs\u5728\u5e94\u7528\u4e2d\u5b58\u5728\u6a21\u5f0f\u5d29\u6e83\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u4ee5\u53ca\u96be\u4ee5\u6355\u6349\u65f6\u95f4\u548c\u7279\u5f81\u5c42\u9762\u76f8\u5173\u6027\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528GRU-based\u7f16\u7801\u5668-\u89e3\u7801\u5668GAN\u6846\u67b6\uff0c\u5305\u542b\u5e26\u6b8b\u5dee\u8fde\u63a5\u7684\u65f6\u95f4\u89e3\u7801\u5668\u8fdb\u884c\u7cbe\u786e\u91cd\u6784\uff0c\u57fa\u4e8e\u9759\u6001\u548c\u52a8\u6001\u534f\u53d8\u91cf\u8fdb\u884c\u6761\u4ef6\u5b66\u4e60\uff0c\u4ee5\u53ca\u7a97\u53e3\u673a\u5236\u6355\u6349\u65f6\u95f4\u52a8\u6001\u3002\u751f\u6210\u5668\u91c7\u7528\u5e26\u6b8b\u5deeGRU\u5757\u7684\u5bc6\u96c6\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\u3002", "result": "\u5728\u591a\u6837\u80a1\u7968\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEDGAN\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u5373\u4f7f\u5728\u6ce2\u52a8\u5e02\u573a\u4e2d\u4e5f\u80fd\u4fdd\u6301\u7a33\u5b9a\u3002\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u6536\u655b\u7a33\u5b9a\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u4f20\u7edfGAN\u53d8\u4f53\u3002", "conclusion": "EDGAN\u6a21\u578b\u5728\u8868\u8fbe\u80fd\u529b\u548c\u7b80\u6d01\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4e3a\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u6ce2\u52a8\u5e02\u573a\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.10159", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10159", "abs": "https://arxiv.org/abs/2510.10159", "authors": ["Jaap Jumelet", "Abdellah Fourtassi", "Akari Haga", "Bastian Bunzeck", "Bhargav Shandilya", "Diana Galvan-Sosa", "Faiz Ghifari Haznitrama", "Francesca Padovani", "Francois Meyer", "Hai Hu", "Julen Etxaniz", "Laurent Pr\u00e9vot", "Linyang He", "Mar\u00eda Grandury", "Mila Marcheva", "Negar Foroutan", "Nikitas Theodoropoulos", "Pouya Sadeghi", "Siyuan Song", "Suchir Salhan", "Susana Zhou", "Yurii Paniv", "Ziyin Zhang", "Arianna Bisazza", "Alex Warstadt", "Leshem Choshen"], "title": "BabyBabelLM: A Multilingual Benchmark of Developmentally Plausible Training Data", "comment": null, "summary": "We present BabyBabelLM, a multilingual collection of datasets modeling the\nlanguage a person observes from birth until they acquire a native language. We\ncurate developmentally plausible pretraining data aiming to cover the\nequivalent of 100M English words of content in each of 45 languages. We compile\nevaluation suites and train baseline models in each language. BabyBabelLM aims\nto facilitate multilingual pretraining and cognitive modeling.", "AI": {"tldr": "BabyBabelLM\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u6570\u636e\u96c6\u96c6\u5408\uff0c\u6a21\u62df\u4ece\u51fa\u751f\u5230\u6bcd\u8bed\u4e60\u5f97\u671f\u95f4\u7684\u8bed\u8a00\u8f93\u5165\uff0c\u5305\u542b45\u79cd\u8bed\u8a00\u54041\u4ebf\u82f1\u8bed\u5355\u8bcd\u91cf\u7684\u53d1\u5c55\u5408\u7406\u6027\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u5957\u4ef6\u548c\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u65e8\u5728\u4fc3\u8fdb\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u548c\u8ba4\u77e5\u5efa\u6a21\uff0c\u901a\u8fc7\u6a21\u62df\u513f\u7ae5\u8bed\u8a00\u4e60\u5f97\u8fc7\u7a0b\u6765\u6784\u5efa\u53d1\u5c55\u5408\u7406\u6027\u7684\u8bed\u8a00\u6570\u636e\u3002", "method": "\u7b56\u5212\u4e86\u53d1\u5c55\u5408\u7406\u6027\u7684\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u8986\u76d645\u79cd\u8bed\u8a00\u54041\u4ebf\u82f1\u8bed\u5355\u8bcd\u91cf\u7684\u5185\u5bb9\uff0c\u5e76\u7f16\u8bd1\u4e86\u8bc4\u4f30\u5957\u4ef6\u548c\u8bad\u7ec3\u4e86\u5404\u8bed\u8a00\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b45\u79cd\u8bed\u8a00\u7684\u591a\u8bed\u8a00\u6570\u636e\u96c6\u96c6\u5408\uff0c\u6bcf\u79cd\u8bed\u8a00\u90fd\u63d0\u4f9b\u4e86\u76f8\u5f53\u4e8e1\u4ebf\u82f1\u8bed\u5355\u8bcd\u91cf\u7684\u9884\u8bad\u7ec3\u6570\u636e\u548c\u76f8\u5e94\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "conclusion": "BabyBabelLM\u4e3a\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u548c\u8ba4\u77e5\u5efa\u6a21\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6570\u636e\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u8bed\u8a00\u4e60\u5f97\u8fc7\u7a0b\u3002"}}
{"id": "2510.10813", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.10813", "abs": "https://arxiv.org/abs/2510.10813", "authors": ["Enric Junque de Fortuny", "Veronica Roberta Cappelli"], "title": "LLMs as Strategic Agents: Beliefs, Best Response Behavior, and Emergent Heuristics", "comment": null, "summary": "Large Language Models (LLMs) are increasingly applied to domains that require\nreasoning about other agents' behavior, such as negotiation, policy design, and\nmarket simulation, yet existing research has mostly evaluated their adherence\nto equilibrium play or their exhibited depth of reasoning. Whether they display\ngenuine strategic thinking, understood as the coherent formation of beliefs\nabout other agents, evaluation of possible actions, and choice based on those\nbeliefs, remains unexplored. We develop a framework to identify this ability by\ndisentangling beliefs, evaluation, and choice in static, complete-information\ngames, and apply it across a series of non-cooperative environments. By jointly\nanalyzing models' revealed choices and reasoning traces, and introducing a new\ncontext-free game to rule out imitation from memorization, we show that current\nfrontier models exhibit belief-coherent best-response behavior at targeted\nreasoning depths. When unconstrained, they self-limit their depth of reasoning\nand form differentiated conjectures about human and synthetic opponents,\nrevealing an emergent form of meta-reasoning. Under increasing complexity,\nexplicit recursion gives way to internally generated heuristic rules of choice\nthat are stable, model-specific, and distinct from known human biases. These\nfindings indicate that belief coherence, meta-reasoning, and novel heuristic\nformation can emerge jointly from language modeling objectives, providing a\nstructured basis for the study of strategic cognition in artificial agents.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u6709\u771f\u6b63\u7684\u6218\u7565\u601d\u7ef4\u80fd\u529b\uff0c\u53d1\u73b0\u524d\u6cbf\u6a21\u578b\u5728\u7279\u5b9a\u63a8\u7406\u6df1\u5ea6\u4e0b\u8868\u73b0\u51fa\u4fe1\u5ff5\u4e00\u81f4\u7684\u6700\u4f73\u54cd\u5e94\u884c\u4e3a\uff0c\u5e76\u5c55\u73b0\u51fa\u5143\u63a8\u7406\u548c\u65b0\u578b\u542f\u53d1\u5f0f\u89c4\u5219\u5f62\u6210\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u8bc4\u4f30LLMs\u5728\u5747\u8861\u535a\u5f08\u6216\u63a8\u7406\u6df1\u5ea6\u65b9\u9762\u7684\u8868\u73b0\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u771f\u6b63\u6218\u7565\u601d\u7ef4\u80fd\u529b\uff08\u5373\u5f62\u6210\u5bf9\u5176\u4ed6\u667a\u80fd\u4f53\u4fe1\u5ff5\u3001\u8bc4\u4f30\u53ef\u80fd\u884c\u52a8\u5e76\u57fa\u4e8e\u4fe1\u5ff5\u505a\u51fa\u9009\u62e9\u7684\u80fd\u529b\uff09\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5728\u9759\u6001\u5b8c\u5168\u4fe1\u606f\u535a\u5f08\u4e2d\u5206\u79bb\u4fe1\u5ff5\u3001\u8bc4\u4f30\u548c\u9009\u62e9\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u7684\u663e\u6027\u9009\u62e9\u548c\u63a8\u7406\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u65e0\u4e0a\u4e0b\u6587\u6e38\u620f\u6765\u6392\u9664\u8bb0\u5fc6\u6a21\u4eff\u7684\u5f71\u54cd\u3002", "result": "\u5f53\u524d\u524d\u6cbf\u6a21\u578b\u5728\u76ee\u6807\u63a8\u7406\u6df1\u5ea6\u4e0b\u8868\u73b0\u51fa\u4fe1\u5ff5\u4e00\u81f4\u7684\u6700\u4f73\u54cd\u5e94\u884c\u4e3a\uff1b\u5728\u65e0\u7ea6\u675f\u65f6\u4f1a\u81ea\u6211\u9650\u5236\u63a8\u7406\u6df1\u5ea6\uff0c\u5e76\u5bf9\u4eba\u7c7b\u548c\u5408\u6210\u5bf9\u624b\u5f62\u6210\u5dee\u5f02\u5316\u63a8\u6d4b\uff1b\u5728\u590d\u6742\u6027\u589e\u52a0\u65f6\uff0c\u663e\u5f0f\u9012\u5f52\u8ba9\u4f4d\u4e8e\u5185\u90e8\u751f\u6210\u7684\u7a33\u5b9a\u3001\u6a21\u578b\u7279\u5b9a\u7684\u542f\u53d1\u5f0f\u9009\u62e9\u89c4\u5219\u3002", "conclusion": "\u4fe1\u5ff5\u4e00\u81f4\u6027\u3001\u5143\u63a8\u7406\u548c\u65b0\u578b\u542f\u53d1\u5f0f\u5f62\u6210\u53ef\u4ee5\u4ece\u8bed\u8a00\u5efa\u6a21\u76ee\u6807\u4e2d\u5171\u540c\u6d8c\u73b0\uff0c\u4e3a\u7814\u7a76\u4eba\u5de5\u667a\u80fd\u4f53\u7684\u6218\u7565\u8ba4\u77e5\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u57fa\u7840\u3002"}}
{"id": "2510.10767", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.10767", "abs": "https://arxiv.org/abs/2510.10767", "authors": ["Jiayuan Sheng", "Hanyang Zhao", "Haoxian Chen", "David D. Yao", "Wenpin Tang"], "title": "Understanding Sampler Stochasticity in Training Diffusion Models for RLHF", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) is increasingly used to\nfine-tune diffusion models, but a key challenge arises from the mismatch\nbetween stochastic samplers used during training and deterministic samplers\nused during inference. In practice, models are fine-tuned using stochastic SDE\nsamplers to encourage exploration, while inference typically relies on\ndeterministic ODE samplers for efficiency and stability. This discrepancy\ninduces a reward gap, raising concerns about whether high-quality outputs can\nbe expected during inference. In this paper, we theoretically characterize this\nreward gap and provide non-vacuous bounds for general diffusion models, along\nwith sharper convergence rates for Variance Exploding (VE) and Variance\nPreserving (VP) Gaussian models. Methodologically, we adopt the generalized\ndenoising diffusion implicit models (gDDIM) framework to support arbitrarily\nhigh levels of stochasticity, preserving data marginals throughout.\nEmpirically, our findings through large-scale experiments on text-to-image\nmodels using denoising diffusion policy optimization (DDPO) and mixed group\nrelative policy optimization (MixGRPO) validate that reward gaps consistently\nnarrow over training, and ODE sampling quality improves when models are updated\nusing higher-stochasticity SDE training.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86RLHF\u5fae\u8c03\u6269\u6563\u6a21\u578b\u65f6\u8bad\u7ec3\u4e0e\u63a8\u7406\u9636\u6bb5\u91c7\u6837\u5668\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u5956\u52b1\u5dee\u8ddd\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u8fb9\u754c\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728RLHF\u5fae\u8c03\u65f6\uff0c\u8bad\u7ec3\u4f7f\u7528\u968f\u673aSDE\u91c7\u6837\u5668\u4ee5\u9f13\u52b1\u63a2\u7d22\uff0c\u800c\u63a8\u7406\u4f7f\u7528\u786e\u5b9a\u6027ODE\u91c7\u6837\u5668\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u8fd9\u79cd\u4e0d\u5339\u914d\u4f1a\u5bfc\u81f4\u5956\u52b1\u5dee\u8ddd\uff0c\u5f71\u54cd\u63a8\u7406\u9636\u6bb5\u7684\u8f93\u51fa\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u5e7f\u4e49\u53bb\u566a\u6269\u6563\u9690\u5f0f\u6a21\u578b(gDDIM)\u6846\u67b6\u652f\u6301\u4efb\u610f\u9ad8\u6c34\u5e73\u7684\u968f\u673a\u6027\uff0c\u4fdd\u6301\u6570\u636e\u8fb9\u9645\u5206\u5e03\uff1b\u901a\u8fc7\u7406\u8bba\u5206\u6790\u7ed9\u51fa\u5956\u52b1\u5dee\u8ddd\u7684\u975e\u7a7a\u8fb9\u754c\uff0c\u5e76\u5bf9VE\u548cVP\u9ad8\u65af\u6a21\u578b\u63d0\u4f9b\u66f4\u5c16\u9510\u7684\u6536\u655b\u7387\u3002", "result": "\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5b9e\u9a8c\uff08\u4f7f\u7528DDPO\u548cMixGRPO\uff09\u9a8c\u8bc1\u4e86\u5956\u52b1\u5dee\u8ddd\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6301\u7eed\u7f29\u5c0f\uff0c\u4e14\u5f53\u4f7f\u7528\u66f4\u9ad8\u968f\u673a\u6027\u7684SDE\u8bad\u7ec3\u66f4\u65b0\u6a21\u578b\u65f6\uff0cODE\u91c7\u6837\u8d28\u91cf\u5f97\u5230\u6539\u5584\u3002", "conclusion": "\u8bad\u7ec3\u4e0e\u63a8\u7406\u91c7\u6837\u5668\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u5956\u52b1\u5dee\u8ddd\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u9002\u5f53\u7684\u8bad\u7ec3\u7b56\u7565\u5f97\u5230\u6709\u6548\u7f13\u89e3\uff0c\u4f7f\u7528\u66f4\u9ad8\u968f\u673a\u6027\u7684\u8bad\u7ec3\u65b9\u6cd5\u80fd\u591f\u63d0\u5347\u63a8\u7406\u9636\u6bb5\u7684\u91c7\u6837\u8d28\u91cf\u3002"}}
{"id": "2510.10161", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10161", "abs": "https://arxiv.org/abs/2510.10161", "authors": ["Liang Pang", "Kangxi Wu", "Sunhao Dai", "Zihao Wei", "Zenghao Duan", "Jia Gu", "Xiang Li", "Zhiyi Yin", "Jun Xu", "Huawei Shen", "Xueqi Cheng"], "title": "Large Language Model Sourcing: A Survey", "comment": "31 pages", "summary": "The rapid advancement of large language models (LLMs) has revolutionized\nartificial intelligence, shifting from supporting objective tasks (e.g.,\nrecognition) to empowering subjective decision-making (e.g., planning,\ndecision). This marks the dawn of general and powerful AI, with applications\nspanning a wide range of fields, including programming, education, healthcare,\nfinance, and law. However, their deployment introduces multifaceted risks. Due\nto the black-box nature of LLMs and the human-like quality of their generated\ncontent, issues such as hallucinations, bias, unfairness, and copyright\ninfringement become particularly significant. In this context, sourcing\ninformation from multiple perspectives is essential.\n  This survey presents a systematic investigation into provenance tracking for\ncontent generated by LLMs, organized around four interrelated dimensions that\ntogether capture both model- and data-centric perspectives. From the model\nperspective, Model Sourcing treats the model as a whole, aiming to distinguish\ncontent generated by specific LLMs from content authored by humans. Model\nStructure Sourcing delves into the internal generative mechanisms, analyzing\narchitectural components that shape the outputs of model. From the data\nperspective, Training Data Sourcing focuses on internal attribution, tracing\nthe origins of generated content back to the training data of model. In\ncontrast, External Data Sourcing emphasizes external validation, identifying\nexternal information used to support or influence the responses of model.\nMoreover, we also propose a dual-paradigm taxonomy that classifies existing\nsourcing methods into prior-based (proactive traceability embedding) and\nposterior-based (retrospective inference) approaches. Traceability across these\ndimensions enhances the transparency, accountability, and trustworthiness of\nLLMs deployment in real-world applications.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8c03\u67e5\u4e86LLM\u751f\u6210\u5185\u5bb9\u7684\u6eaf\u6e90\u8ffd\u8e2a\uff0c\u56f4\u7ed5\u6a21\u578b\u548c\u6570\u636e\u4e24\u4e2a\u89c6\u89d2\u7684\u56db\u4e2a\u7ef4\u5ea6\u5c55\u5f00\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5148\u9a8c\u548c\u540e\u9a8c\u7684\u53cc\u8303\u5f0f\u5206\u7c7b\u6cd5\u3002", "motivation": "\u968f\u7740LLM\u4ece\u5ba2\u89c2\u4efb\u52a1\u8f6c\u5411\u4e3b\u89c2\u51b3\u7b56\u5e94\u7528\uff0c\u5176\u9ed1\u76d2\u7279\u6027\u548c\u7c7b\u4eba\u751f\u6210\u5185\u5bb9\u5e26\u6765\u4e86\u5e7b\u89c9\u3001\u504f\u89c1\u3001\u4e0d\u516c\u5e73\u548c\u7248\u6743\u4fb5\u6743\u7b49\u591a\u65b9\u9762\u98ce\u9669\uff0c\u9700\u8981\u591a\u89d2\u5ea6\u4fe1\u606f\u6eaf\u6e90\u6765\u589e\u5f3a\u900f\u660e\u5ea6\u3001\u95ee\u8d23\u5236\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "\u4ece\u6a21\u578b\u89c6\u89d2\uff08\u6a21\u578b\u6eaf\u6e90\u548c\u6a21\u578b\u7ed3\u6784\u6eaf\u6e90\uff09\u548c\u6570\u636e\u89c6\u89d2\uff08\u8bad\u7ec3\u6570\u636e\u6eaf\u6e90\u548c\u5916\u90e8\u6570\u636e\u6eaf\u6e90\uff09\u56db\u4e2a\u7ef4\u5ea6\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u4e86\u5148\u9a8c\u57fa\u4e8e\uff08\u4e3b\u52a8\u5d4c\u5165\u53ef\u8ffd\u6eaf\u6027\uff09\u548c\u540e\u9a8c\u57fa\u4e8e\uff08\u56de\u987e\u6027\u63a8\u65ad\uff09\u7684\u53cc\u8303\u5f0f\u5206\u7c7b\u6cd5\u3002", "result": "\u5efa\u7acb\u4e86\u7cfb\u7edf\u7684LLM\u751f\u6210\u5185\u5bb9\u6eaf\u6e90\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u5206\u6790\u589e\u5f3a\u4e86LLM\u90e8\u7f72\u7684\u900f\u660e\u5ea6\u3001\u95ee\u8d23\u5236\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u8de8\u7ef4\u5ea6\u7684\u53ef\u8ffd\u6eaf\u6027\u5bf9\u4e8e\u63d0\u5347LLM\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u900f\u660e\u5ea6\u3001\u95ee\u8d23\u5236\u548c\u53ef\u4fe1\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u65b9\u6cd5\u6307\u5bfc\u3002"}}
{"id": "2510.09784", "categories": ["cs.LG", "cond-mat.stat-mech", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.09784", "abs": "https://arxiv.org/abs/2510.09784", "authors": ["Richard John", "Yunrui Qiu", "Lukas Herron", "Pratyush Tiwary"], "title": "Combined Representation and Generation with Diffusive State Predictive Information Bottleneck", "comment": null, "summary": "Generative modeling becomes increasingly data-intensive in high-dimensional\nspaces. In molecular science, where data collection is expensive and important\nevents are rare, compression to lower-dimensional manifolds is especially\nimportant for various downstream tasks, including generation. We combine a\ntime-lagged information bottleneck designed to characterize molecular important\nrepresentations and a diffusion model in one joint training objective. The\nresulting protocol, which we term Diffusive State Predictive Information\nBottleneck (D-SPIB), enables the balancing of representation learning and\ngeneration aims in one flexible architecture. Additionally, the model is\ncapable of combining temperature information from different molecular\nsimulation trajectories to learn a coherent and useful internal representation\nof thermodynamics. We benchmark D-SPIB on multiple molecular tasks and showcase\nits potential for exploring physical conditions outside the training set.", "AI": {"tldr": "\u63d0\u51faD-SPIB\u65b9\u6cd5\uff0c\u7ed3\u5408\u65f6\u95f4\u6ede\u540e\u4fe1\u606f\u74f6\u9888\u548c\u6269\u6563\u6a21\u578b\uff0c\u5728\u5206\u5b50\u79d1\u5b66\u4e2d\u5e73\u8861\u8868\u793a\u5b66\u4e60\u548c\u751f\u6210\u4efb\u52a1\uff0c\u80fd\u591f\u6574\u5408\u4e0d\u540c\u6e29\u5ea6\u8f68\u8ff9\u6570\u636e\u5b66\u4e60\u70ed\u529b\u5b66\u8868\u793a\u3002", "motivation": "\u5206\u5b50\u79d1\u5b66\u4e2d\u6570\u636e\u6536\u96c6\u6602\u8d35\u4e14\u91cd\u8981\u4e8b\u4ef6\u7f55\u89c1\uff0c\u9700\u8981\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u8fdb\u884c\u538b\u7f29\u4ee5\u652f\u6301\u4e0b\u6e38\u751f\u6210\u4efb\u52a1\u3002", "method": "\u7ed3\u5408\u65f6\u95f4\u6ede\u540e\u4fe1\u606f\u74f6\u9888\u548c\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u76ee\u6807\u5e73\u8861\u8868\u793a\u5b66\u4e60\u548c\u751f\u6210\uff0c\u80fd\u591f\u6574\u5408\u4e0d\u540c\u6e29\u5ea6\u8f68\u8ff9\u6570\u636e\u3002", "result": "\u5728\u591a\u4e2a\u5206\u5b50\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u63a2\u7d22\u8bad\u7ec3\u96c6\u5916\u7269\u7406\u6761\u4ef6\u7684\u6f5c\u529b\u3002", "conclusion": "D-SPIB\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u67b6\u6784\uff0c\u80fd\u591f\u6709\u6548\u5e73\u8861\u5206\u5b50\u8868\u793a\u5b66\u4e60\u548c\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u5b66\u4e60\u8fde\u8d2f\u7684\u70ed\u529b\u5b66\u5185\u90e8\u8868\u793a\u3002"}}
{"id": "2510.10815", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.SC"], "pdf": "https://arxiv.org/pdf/2510.10815", "abs": "https://arxiv.org/abs/2510.10815", "authors": ["Meiru Zhang", "Philipp Borchert", "Milan Gritta", "Gerasimos Lampouras"], "title": "DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems", "comment": null, "summary": "Automating the formalization of mathematical statements for theorem proving\nremains a major challenge for Large Language Models (LLMs). LLMs struggle to\nidentify and utilize the prerequisite mathematical knowledge and its\ncorresponding formal representation in languages like Lean. Current\nretrieval-augmented autoformalization methods query external libraries using\nthe informal statement directly, but overlook a fundamental limitation:\ninformal mathematical statements are often complex and offer limited context on\nthe underlying math concepts. To address this, we introduce DRIFT, a novel\nframework that enables LLMs to decompose informal mathematical statements into\nsmaller, more tractable ''sub-components''. This facilitates targeted retrieval\nof premises from mathematical libraries such as Mathlib. Additionally, DRIFT\nretrieves illustrative theorems to help models use premises more effectively in\nformalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,\nConNF, and MiniF2F-test) and find that it consistently improves premise\nretrieval, nearly doubling the F1 score compared to the DPR baseline on\nProofNet. Notably, DRIFT demonstrates strong performance on the\nout-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and\n42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that\nretrieval effectiveness in mathematical autoformalization depends heavily on\nmodel-specific knowledge boundaries, highlighting the need for adaptive\nretrieval strategies aligned with each model's capabilities.", "AI": {"tldr": "DRIFT\u6846\u67b6\u901a\u8fc7\u5c06\u975e\u6b63\u5f0f\u6570\u5b66\u9648\u8ff0\u5206\u89e3\u4e3a\u5b50\u7ec4\u4ef6\u6765\u6539\u8fdb\u81ea\u52a8\u5f62\u5f0f\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u524d\u63d0\u68c0\u7d22\u6548\u679c\u548c\u5728\u5b9a\u7406\u8bc1\u660e\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u9648\u8ff0\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u96be\u4ee5\u8bc6\u522b\u548c\u5229\u7528\u5fc5\u8981\u7684\u6570\u5b66\u77e5\u8bc6\u53ca\u5176\u5728Lean\u7b49\u8bed\u8a00\u4e2d\u7684\u5f62\u5f0f\u5316\u8868\u793a\u3002\u73b0\u6709\u65b9\u6cd5\u76f4\u63a5\u4f7f\u7528\u975e\u6b63\u5f0f\u9648\u8ff0\u67e5\u8be2\u5916\u90e8\u5e93\uff0c\u4f46\u5ffd\u7565\u4e86\u975e\u6b63\u5f0f\u6570\u5b66\u9648\u8ff0\u7684\u590d\u6742\u6027\u548c\u4e0a\u4e0b\u6587\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165DRIFT\u6846\u67b6\uff0c\u8ba9LLMs\u5c06\u975e\u6b63\u5f0f\u6570\u5b66\u9648\u8ff0\u5206\u89e3\u4e3a\u66f4\u5c0f\u7684\u5b50\u7ec4\u4ef6\uff0c\u4ece\u800c\u4ece\u6570\u5b66\u5e93\u4e2d\u66f4\u6709\u9488\u5bf9\u6027\u5730\u68c0\u7d22\u524d\u63d0\uff0c\u5e76\u68c0\u7d22\u793a\u4f8b\u5b9a\u7406\u4ee5\u5e2e\u52a9\u6a21\u578b\u66f4\u6709\u6548\u5730\u4f7f\u7528\u524d\u63d0\u8fdb\u884c\u5f62\u5f0f\u5316\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDRIFT\u663e\u8457\u63d0\u5347\u4e86\u524d\u63d0\u68c0\u7d22\u6548\u679c\uff0c\u5728ProofNet\u4e0a\u76f8\u6bd4DPR\u57fa\u7ebfF1\u5206\u6570\u51e0\u4e4e\u7ffb\u500d\u3002\u5728\u5206\u5e03\u5916\u6d4b\u8bd5\u96c6ConNF\u4e0a\uff0c\u4f7f\u7528GPT-4.1\u548cDeepSeek-V3.1\u5206\u522b\u5b9e\u73b0\u4e8637.14%\u548c42.25%\u7684BEq+@10\u6539\u8fdb\u3002", "conclusion": "\u6570\u5b66\u81ea\u52a8\u5f62\u5f0f\u5316\u4e2d\u7684\u68c0\u7d22\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6a21\u578b\u7279\u5b9a\u7684\u77e5\u8bc6\u8fb9\u754c\uff0c\u9700\u8981\u4e0e\u6bcf\u4e2a\u6a21\u578b\u80fd\u529b\u76f8\u9002\u5e94\u7684\u81ea\u9002\u5e94\u68c0\u7d22\u7b56\u7565\u3002"}}
{"id": "2510.11495", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11495", "abs": "https://arxiv.org/abs/2510.11495", "authors": ["Nikolaos Tsilivis", "Eran Malach", "Karen Ullrich", "Julia Kempe"], "title": "How Reinforcement Learning After Next-Token Prediction Facilitates Learning", "comment": null, "summary": "Recent advances in reasoning domains with neural networks have primarily been\nenabled by a training recipe that optimizes Large Language Models, previously\ntrained to predict the next-token in a sequence, with reinforcement learning\nalgorithms. We introduce a framework to study the success of this paradigm, and\nwe theoretically expose the optimization mechanisms by which reinforcement\nlearning improves over next-token prediction in this setting. We study learning\nfrom mixture distributions of short and long ``chain-of-thought'' sequences\nencoding a single task. In particular, when the task consists of predicting the\nparity of $d$ bits and long sequences are rare, we show how reinforcement\nlearning after next-token prediction enables autoregressive transformers to\ngeneralize, whereas mere next-token prediction requires extreme statistical or\ncomputational resources to do so. We further explain how reinforcement learning\nleverages increased test-time computation, manifested in longer responses, to\nfacilitate this learning process. In a simplified setting, we theoretically\nprove that autoregressive linear models following this training recipe can\nefficiently learn to predict the parity of $d$ bits as long as the proportion\nof long demonstrations in the data mix is not exponentially small in the input\ndimension $d$. Finally, we demonstrate these same phenomena in other settings,\nincluding the post-training of Llama-series models on mixture variations of\ncommon mathematical reasoning benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u91ca\u4e3a\u4ec0\u4e48\u5728\u94fe\u5f0f\u601d\u7ef4\u5e8f\u5217\u4e0a\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6bd4\u4ec5\u4f7f\u7528\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u8bad\u7ec3\u66f4\u6709\u6548\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7f55\u89c1\u7684\u957f\u5e8f\u5217\u65f6\u3002\u901a\u8fc7\u5947\u5076\u6027\u9884\u6d4b\u4efb\u52a1\u7684\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u5229\u7528\u6d4b\u8bd5\u65f6\u66f4\u957f\u7684\u8ba1\u7b97\u6765\u4fc3\u8fdb\u6cdb\u5316\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u5728\u63a8\u7406\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u4ece\u7406\u8bba\u4e0a\u89e3\u91ca\u4e3a\u4ec0\u4e48\u8fd9\u79cd\u8bad\u7ec3\u8303\u5f0f\u6bd4\u5355\u7eaf\u7684\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u66f4\u6210\u529f\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5305\u542b\u7f55\u89c1\u957f\u5e8f\u5217\u7684\u6df7\u5408\u5206\u5e03\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u7814\u7a76\u4ece\u5305\u542b\u77ed\u548c\u957f\u94fe\u5f0f\u601d\u7ef4\u5e8f\u5217\u7684\u6df7\u5408\u5206\u5e03\u4e2d\u5b66\u4e60\u3002\u5728\u5947\u5076\u6027\u9884\u6d4b\u4efb\u52a1\u4e0a\uff0c\u7406\u8bba\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u540e\u81ea\u56de\u5f52\u53d8\u6362\u5668\u80fd\u591f\u6cdb\u5316\uff0c\u800c\u4ec5\u4f7f\u7528\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u9700\u8981\u6781\u7aef\u7684\u7edf\u8ba1\u6216\u8ba1\u7b97\u8d44\u6e90\u3002\u540c\u65f6\u5728\u7ebf\u6027\u6a21\u578b\u7684\u7b80\u5316\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u4e86\u7406\u8bba\u8bc1\u660e\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u53ea\u8981\u6570\u636e\u6df7\u5408\u4e2d\u957f\u6f14\u793a\u7684\u6bd4\u4f8b\u5728\u8f93\u5165\u7ef4\u5ea6d\u4e0a\u4e0d\u662f\u6307\u6570\u7ea7\u5c0f\u7684\uff0c\u81ea\u56de\u5f52\u7ebf\u6027\u6a21\u578b\u5c31\u80fd\u6709\u6548\u5b66\u4e60\u9884\u6d4bd\u4f4d\u7684\u5947\u5076\u6027\u3002\u5728Llama\u7cfb\u5217\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u4e5f\u9a8c\u8bc1\u4e86\u76f8\u540c\u73b0\u8c61\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u80fd\u591f\u5229\u7528\u6d4b\u8bd5\u65f6\u66f4\u957f\u7684\u8ba1\u7b97\uff08\u8868\u73b0\u4e3a\u66f4\u957f\u7684\u54cd\u5e94\uff09\u6765\u4fc3\u8fdb\u5b66\u4e60\u8fc7\u7a0b\uff0c\u8fd9\u79cd\u673a\u5236\u4f7f\u5f97\u6a21\u578b\u5728\u5904\u7406\u7f55\u89c1\u957f\u5e8f\u5217\u65f6\u6bd4\u5355\u7eaf\u7684\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\u3002"}}
{"id": "2510.10777", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.10777", "abs": "https://arxiv.org/abs/2510.10777", "authors": ["Andrey Veprikov", "Arman Bolatov", "Samuel Horv\u00e1th", "Aleksandr Beznosikov", "Martin Tak\u00e1\u010d", "Slavomir Hanzely"], "title": "Preconditioned Norms: A Unified Framework for Steepest Descent, Quasi-Newton and Adaptive Methods", "comment": "22 pages, 2 figures, 8 tables", "summary": "Optimization lies at the core of modern deep learning, yet existing methods\noften face a fundamental trade-off between adapting to problem geometry and\nleveraging curvature utilization. Steepest descent algorithms adapt to\ndifferent geometries through norm choices but remain strictly first-order,\nwhereas quasi-Newton and adaptive optimizers incorporate curvature information\nbut are restricted to Frobenius geometry, limiting their applicability across\ndiverse architectures. In this work, we propose a unified framework\ngeneralizing steepest descent, quasi-Newton methods, and adaptive methods\nthrough the novel notion of preconditioned matrix norms. This abstraction\nreveals that widely used optimizers such as SGD and Adam, as well as more\nadvanced approaches like Muon and KL-Shampoo, and recent hybrids including SOAP\nand SPlus, all emerge as special cases of the same principle. Within this\nframework, we provide the first systematic treatment of affine and scale\ninvariance in the matrix-parameterized setting, establishing necessary and\nsufficient conditions under generalized norms. Building on this foundation, we\nintroduce two new methods, $\\texttt{MuAdam}$ and $\\texttt{MuAdam-SANIA}$, which\ncombine the spectral geometry of Muon with Adam-style preconditioning. Our\nexperiments demonstrate that these optimizers are competitive with, and in some\ncases outperform, existing state-of-the-art methods. Our code is available at\nhttps://github.com/brain-lab-research/LIB/tree/quasi_descent", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6761\u4ef6\u77e9\u9635\u8303\u6570\u5c06\u6700\u901f\u4e0b\u964d\u6cd5\u3001\u62df\u725b\u987f\u6cd5\u548c\u81ea\u9002\u5e94\u65b9\u6cd5\u7edf\u4e00\u8d77\u6765\uff0c\u63ed\u793a\u4e86SGD\u3001Adam\u7b49\u4f18\u5316\u5668\u90fd\u662f\u8be5\u6846\u67b6\u7684\u7279\u4f8b\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u79cd\u65b0\u65b9\u6cd5MuAdam\u548cMuAdam-SANIA\u3002", "motivation": "\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u5728\u9002\u5e94\u95ee\u9898\u51e0\u4f55\u548c\u5229\u7528\u66f2\u7387\u4fe1\u606f\u4e4b\u95f4\u5b58\u5728\u57fa\u672c\u6743\u8861\uff1a\u6700\u901f\u4e0b\u964d\u6cd5\u80fd\u9002\u5e94\u4e0d\u540c\u51e0\u4f55\u4f46\u4ec5\u4e3a\u4e00\u9636\u65b9\u6cd5\uff0c\u800c\u62df\u725b\u987f\u6cd5\u548c\u81ea\u9002\u5e94\u4f18\u5316\u5668\u5305\u542b\u66f2\u7387\u4fe1\u606f\u4f46\u5c40\u9650\u4e8eFrobenius\u51e0\u4f55\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u9884\u6761\u4ef6\u77e9\u9635\u8303\u6570\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7cfb\u7edf\u5904\u7406\u4e86\u77e9\u9635\u53c2\u6570\u5316\u8bbe\u7f6e\u4e2d\u7684\u4eff\u5c04\u548c\u5c3a\u5ea6\u4e0d\u53d8\u6027\uff0c\u5e76\u57fa\u4e8e\u6b64\u6846\u67b6\u5f00\u53d1\u4e86MuAdam\u548cMuAdam-SANIA\u4e24\u79cd\u65b0\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u63d0\u51fa\u7684\u4f18\u5316\u5668\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u4f18\u5316\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u4f18\u5316\u5668\u4e4b\u95f4\u7684\u5185\u5728\u8054\u7cfb\uff0c\u5e76\u5c55\u793a\u4e86\u65b0\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.10182", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10182", "abs": "https://arxiv.org/abs/2510.10182", "authors": ["Kedi Chen", "Dezhao Ruan", "Yuhao Dan", "Yaoting Wang", "Siyu Yan", "Xuecheng Wu", "Yinqi Zhang", "Qin Chen", "Jie Zhou", "Liang He", "Biqing Qi", "Linyang Li", "Qipeng Guo", "Xiaoming Shi", "Wei Zhang"], "title": "A Survey of Inductive Reasoning for Large Language Models", "comment": null, "summary": "Reasoning is an important task for large language models (LLMs). Among all\nthe reasoning paradigms, inductive reasoning is one of the fundamental types,\nwhich is characterized by its particular-to-general thinking process and the\nnon-uniqueness of its answers. The inductive mode is crucial for knowledge\ngeneralization and aligns better with human cognition, so it is a fundamental\nmode of learning, hence attracting increasing interest. Despite the importance\nof inductive reasoning, there is no systematic summary of it. Therefore, this\npaper presents the first comprehensive survey of inductive reasoning for LLMs.\nFirst, methods for improving inductive reasoning are categorized into three\nmain areas: post-training, test-time scaling, and data augmentation. Then,\ncurrent benchmarks of inductive reasoning are summarized, and a unified\nsandbox-based evaluation approach with the observation coverage metric is\nderived. Finally, we offer some analyses regarding the source of inductive\nability and how simple model architectures and data help with inductive tasks,\nproviding a solid foundation for future research.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f52\u7eb3\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u5c06\u63d0\u5347\u65b9\u6cd5\u5206\u4e3a\u540e\u8bad\u7ec3\u3001\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u6570\u636e\u589e\u5f3a\u4e09\u7c7b\uff0c\u603b\u7ed3\u4e86\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6c99\u7bb1\u7684\u7edf\u4e00\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5f52\u7eb3\u63a8\u7406\u4f5c\u4e3a\u91cd\u8981\u7684\u63a8\u7406\u8303\u5f0f\uff0c\u5177\u6709\u4ece\u7279\u6b8a\u5230\u4e00\u822c\u7684\u601d\u7ef4\u8fc7\u7a0b\u548c\u7b54\u6848\u975e\u552f\u4e00\u6027\u7684\u7279\u70b9\uff0c\u5bf9\u77e5\u8bc6\u6cdb\u5316\u548c\u4eba\u7c7b\u8ba4\u77e5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u603b\u7ed3\u3002", "method": "\u5c06\u5f52\u7eb3\u63a8\u7406\u6539\u8fdb\u65b9\u6cd5\u5206\u4e3a\u4e09\u7c7b\uff1a\u540e\u8bad\u7ec3\u3001\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u6570\u636e\u589e\u5f3a\uff1b\u603b\u7ed3\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6c99\u7bb1\u7684\u7edf\u4e00\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f7f\u7528\u89c2\u5bdf\u8986\u76d6\u7387\u6307\u6807\u3002", "result": "\u5efa\u7acb\u4e86\u5f52\u7eb3\u63a8\u7406\u7684\u7cfb\u7edf\u6027\u5206\u7c7b\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u5f52\u7eb3\u63a8\u7406\u5728LLMs\u7814\u7a76\u4e2d\u7684\u7a7a\u767d\uff0c\u4e3a\u7406\u89e3\u5f52\u7eb3\u80fd\u529b\u6765\u6e90\u4ee5\u53ca\u7b80\u5355\u6a21\u578b\u67b6\u6784\u548c\u6570\u636e\u5982\u4f55\u5e2e\u52a9\u5f52\u7eb3\u4efb\u52a1\u63d0\u4f9b\u4e86\u5206\u6790\u57fa\u7840\u3002"}}
{"id": "2510.09792", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.09792", "abs": "https://arxiv.org/abs/2510.09792", "authors": ["Vahidreza Jahanmard", "Ali Ramezani-Kebrya", "Robinson Hordoir"], "title": "Principled Operator Learning in Ocean Dynamics: The Role of Temporal Structure", "comment": "Accepted at NeurIPS ML4PS 2025", "summary": "Neural operators are becoming the default tools to learn solutions to\ngoverning partial differential equations (PDEs) in weather and ocean\nforecasting applications. Despite early promising achievements, significant\nchallenges remain, including long-term prediction stability and adherence to\nphysical laws, particularly for high-frequency processes. In this paper, we\ntake a step toward addressing these challenges in high-resolution ocean\nprediction by incorporating temporal Fourier modes, demonstrating how this\nmodification enhances physical fidelity. This study compares the standard\nFourier Neural Operator (FNO) with its variant, FNOtD, which has been modified\nto internalize the dispersion relation while learning the solution operator for\nocean PDEs. The results demonstrate that entangling space and time in the\ntraining of integral kernels enables the model to capture multiscale wave\npropagation and effectively learn ocean dynamics. FNOtD substantially improves\nlong-term prediction stability and consistency with underlying physical\ndynamics in challenging high-frequency settings compared to the standard FNO.\nIt also provides competitive predictive skill relative to a state-of-the-art\nnumerical ocean model, while requiring significantly lower computational cost.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684FNOtD\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u65f6\u7a7a\u5085\u91cc\u53f6\u6a21\u5f0f\u6765\u589e\u5f3a\u795e\u7ecf\u7b97\u5b50\u5728\u6d77\u6d0bPDE\u6c42\u89e3\u4e2d\u7684\u7269\u7406\u4fdd\u771f\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u9884\u6d4b\u7a33\u5b9a\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002", "motivation": "\u795e\u7ecf\u7b97\u5b50\u5728\u5929\u6c14\u548c\u6d77\u6d0b\u9884\u62a5\u5e94\u7528\u4e2d\u9762\u4e34\u957f\u671f\u9884\u6d4b\u4e0d\u7a33\u5b9a\u548c\u7269\u7406\u5b9a\u5f8b\u9075\u5faa\u6027\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9ad8\u9891\u8fc7\u7a0b\u5efa\u6a21\u4e2d\u3002", "method": "\u5c06\u6807\u51c6\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50(FNO)\u6539\u8fdb\u4e3aFNOtD\uff0c\u901a\u8fc7\u5185\u90e8\u5316\u8272\u6563\u5173\u7cfb\u5e76\u5728\u79ef\u5206\u6838\u8bad\u7ec3\u4e2d\u7ea0\u7f20\u65f6\u7a7a\u7ef4\u5ea6\u6765\u5b66\u4e60\u6d77\u6d0bPDE\u7684\u89e3\u7b97\u5b50\u3002", "result": "FNOtD\u663e\u8457\u6539\u5584\u4e86\u957f\u671f\u9884\u6d4b\u7a33\u5b9a\u6027\uff0c\u5728\u6311\u6218\u6027\u9ad8\u9891\u8bbe\u7f6e\u4e2d\u4e0e\u57fa\u7840\u7269\u7406\u52a8\u529b\u5b66\u4fdd\u6301\u66f4\u597d\u4e00\u81f4\u6027\uff0c\u76f8\u6bd4\u6807\u51c6FNO\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "FNOtD\u5728\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb\u6570\u503c\u6d77\u6d0b\u6a21\u578b\u7ade\u4e89\u6027\u9884\u6d4b\u80fd\u529b\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387\u6d77\u6d0b\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10823", "categories": ["cs.AI", "cs.NE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10823", "abs": "https://arxiv.org/abs/2510.10823", "authors": ["Daniel Howard"], "title": "The Irrational Machine: Neurosis and the Limits of Algorithmic Safety", "comment": "41 pages, 17 figures, 5 tables", "summary": "We present a framework for characterizing neurosis in embodied AI: behaviors\nthat are internally coherent yet misaligned with reality, arising from\ninteractions among planning, uncertainty handling, and aversive memory. In a\ngrid navigation stack we catalogue recurrent modalities including flip-flop,\nplan churn, perseveration loops, paralysis and hypervigilance, futile search,\nbelief incoherence, tie break thrashing, corridor thrashing, optimality\ncompulsion, metric mismatch, policy oscillation, and limited-visibility\nvariants. For each we give lightweight online detectors and reusable escape\npolicies (short commitments, a margin to switch, smoothing, principled\narbitration). We then show that durable phobic avoidance can persist even under\nfull visibility when learned aversive costs dominate local choice, producing\nlong detours despite globally safe routes. Using First/Second/Third Law as\nengineering shorthand for safety latency, command compliance, and resource\nefficiency, we argue that local fixes are insufficient; global failures can\nremain. To surface them, we propose genetic-programming based destructive\ntesting that evolves worlds and perturbations to maximize law pressure and\nneurosis scores, yielding adversarial curricula and counterfactual traces that\nexpose where architectural revision, not merely symptom-level patches, is\nrequired.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u8868\u5f81\u5177\u8eabAI\u4e2d\u7684\u795e\u7ecf\u75c7\u884c\u4e3a\uff0c\u8fd9\u4e9b\u884c\u4e3a\u5185\u90e8\u4e00\u81f4\u4f46\u4e0e\u73b0\u5b9e\u4e0d\u7b26\uff0c\u6e90\u4e8e\u89c4\u5212\u3001\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u548c\u538c\u6076\u8bb0\u5fc6\u7684\u4ea4\u4e92\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u5177\u8eabAI\u4e2d\u51fa\u73b0\u7684\u795e\u7ecf\u75c7\u884c\u4e3a\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u884c\u4e3a\u867d\u7136\u5185\u90e8\u903b\u8f91\u4e00\u81f4\u4f46\u4e0e\u73b0\u5b9e\u73af\u5883\u4e0d\u5339\u914d\uff0c\u5f71\u54cdAI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "method": "\u5728\u7f51\u683c\u5bfc\u822a\u7cfb\u7edf\u4e2d\u8bc6\u522b\u591a\u79cd\u795e\u7ecf\u75c7\u6a21\u5f0f\uff0c\u5f00\u53d1\u8f7b\u91cf\u7ea7\u5728\u7ebf\u68c0\u6d4b\u5668\u548c\u53ef\u91cd\u7528\u9003\u8131\u7b56\u7565\uff0c\u5e76\u4f7f\u7528\u9057\u4f20\u7f16\u7a0b\u8fdb\u884c\u7834\u574f\u6027\u6d4b\u8bd5\u6765\u6700\u5927\u5316\u795e\u7ecf\u75c7\u8bc4\u5206\u3002", "result": "\u8bc6\u522b\u4e8612\u79cd\u795e\u7ecf\u75c7\u6a21\u5f0f\uff0c\u5f00\u53d1\u4e86\u6709\u6548\u7684\u68c0\u6d4b\u548c\u9003\u8131\u673a\u5236\uff0c\u5e76\u5c55\u793a\u4e86\u5373\u4f7f\u5728\u5b8c\u5168\u53ef\u89c1\u60c5\u51b5\u4e0b\uff0c\u4e60\u5f97\u7684\u538c\u6076\u6210\u672c\u4ecd\u80fd\u5bfc\u81f4\u6301\u4e45\u7684\u6050\u60e7\u56de\u907f\u884c\u4e3a\u3002", "conclusion": "\u5c40\u90e8\u4fee\u590d\u4e0d\u8db3\u4ee5\u89e3\u51b3\u5168\u5c40\u6545\u969c\uff0c\u9700\u8981\u901a\u8fc7\u7834\u574f\u6027\u6d4b\u8bd5\u66b4\u9732\u7cfb\u7edf\u67b6\u6784\u5c42\u9762\u7684\u95ee\u9898\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u75c7\u72b6\u5c42\u9762\u7684\u4fee\u8865\u3002"}}
{"id": "2510.11590", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11590", "abs": "https://arxiv.org/abs/2510.11590", "authors": ["Zihao Zhao", "Christopher Yeh", "Lingkai Kong", "Kai Wang"], "title": "Diffusion-DFL: Decision-focused Diffusion Models for Stochastic Optimization", "comment": null, "summary": "Decision-focused learning (DFL) integrates predictive modeling and\noptimization by training predictors to optimize the downstream decision target\nrather than merely minimizing prediction error. To date, existing DFL methods\ntypically rely on deterministic point predictions, which are often insufficient\nto capture the intrinsic stochasticity of real-world environments. To address\nthis challenge, we propose the first diffusion-based DFL approach, which trains\na diffusion model to represent the distribution of uncertain parameters and\noptimizes the decision by solving a stochastic optimization with samples drawn\nfrom the diffusion model. Our contributions are twofold. First, we formulate\ndiffusion DFL using the reparameterization trick, enabling end-to-end training\nthrough diffusion. While effective, it is memory and compute-intensive due to\nthe need to differentiate through the diffusion sampling process. Second, we\npropose a lightweight score function estimator that uses only several forward\ndiffusion passes and avoids backpropagation through the sampling. This follows\nfrom our results that backpropagating through stochastic optimization can be\napproximated by a weighted score function formulation. We empirically show that\nour diffusion DFL approach consistently outperforms strong baselines in\ndecision quality. The source code for all experiments is available at the\nproject repository: https://github.com/GT-KOALA/Diffusion_DFL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u51b3\u7b56\u805a\u7126\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u8868\u793a\u4e0d\u786e\u5b9a\u53c2\u6570\u7684\u5206\u5e03\uff0c\u5e76\u7528\u91c7\u6837\u8fdb\u884c\u968f\u673a\u4f18\u5316\uff0c\u5728\u51b3\u7b56\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u51b3\u7b56\u805a\u7126\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u786e\u5b9a\u6027\u70b9\u9884\u6d4b\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u5185\u5728\u968f\u673a\u6027\u3002", "method": "1. \u4f7f\u7528\u91cd\u53c2\u6570\u5316\u6280\u5de7\u6784\u5efa\u6269\u6563DFL\uff0c\u652f\u6301\u7aef\u5230\u7aef\u8bad\u7ec3\uff1b2. \u63d0\u51fa\u8f7b\u91cf\u7ea7\u8bc4\u5206\u51fd\u6570\u4f30\u8ba1\u5668\uff0c\u4ec5\u9700\u524d\u5411\u6269\u6563\u8fc7\u7a0b\uff0c\u907f\u514d\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u53cd\u5411\u4f20\u64ad\u3002", "result": "\u5b9e\u8bc1\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u51b3\u7b56\u8d28\u91cf\u4e0a\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6269\u6563DFL\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\uff0c\u4e14\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6848\u3002"}}
{"id": "2510.10185", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.10185", "abs": "https://arxiv.org/abs/2510.10185", "authors": ["Lei Gu", "Yinghao Zhu", "Haoran Sang", "Zixiang Wang", "Dehao Sui", "Wen Tang", "Ewen Harrison", "Junyi Gao", "Lequan Yu", "Liantao Ma"], "title": "MedAgentAudit: Diagnosing and Quantifying Collaborative Failure Modes in Medical Multi-Agent Systems", "comment": "Code: https://github.com/yhzhu99/MedAgentAudit", "summary": "While large language model (LLM)-based multi-agent systems show promise in\nsimulating medical consultations, their evaluation is often confined to\nfinal-answer accuracy. This practice treats their internal collaborative\nprocesses as opaque \"black boxes\" and overlooks a critical question: is a\ndiagnostic conclusion reached through a sound and verifiable reasoning pathway?\nThe inscrutable nature of these systems poses a significant risk in high-stakes\nmedical applications, potentially leading to flawed or untrustworthy\nconclusions. To address this, we conduct a large-scale empirical study of 3,600\ncases from six medical datasets and six representative multi-agent frameworks.\nThrough a rigorous, mixed-methods approach combining qualitative analysis with\nquantitative auditing, we develop a comprehensive taxonomy of collaborative\nfailure modes. Our quantitative audit reveals four dominant failure patterns:\nflawed consensus driven by shared model deficiencies, suppression of correct\nminority opinions, ineffective discussion dynamics, and critical information\nloss during synthesis. This study demonstrates that high accuracy alone is an\ninsufficient measure of clinical or public trust. It highlights the urgent need\nfor transparent and auditable reasoning processes, a cornerstone for the\nresponsible development and deployment of medical AI.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u533b\u7597\u54a8\u8be2\u7cfb\u7edf\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u63ed\u793a\u4e86\u4ec5\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8bc6\u522b\u51fa\u56db\u79cd\u4e3b\u8981\u7684\u534f\u4f5c\u5931\u8d25\u6a21\u5f0f\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u533b\u7597\u54a8\u8be2\u6a21\u62df\u4e2d\u4ec5\u8bc4\u4f30\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\uff0c\u5c06\u5176\u5185\u90e8\u534f\u4f5c\u8fc7\u7a0b\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u5ffd\u7565\u4e86\u8bca\u65ad\u7ed3\u8bba\u662f\u5426\u901a\u8fc7\u5408\u7406\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u8def\u5f84\u5f97\u51fa\uff0c\u8fd9\u5728\u9ad8\u98ce\u9669\u533b\u7597\u5e94\u7528\u4e2d\u5b58\u5728\u91cd\u5927\u98ce\u9669\u3002", "method": "\u5bf9\u6765\u81ea\u516d\u4e2a\u533b\u7597\u6570\u636e\u96c6\u548c\u516d\u4e2a\u4ee3\u8868\u6027\u591a\u667a\u80fd\u4f53\u6846\u67b6\u76843,600\u4e2a\u6848\u4f8b\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u91c7\u7528\u7ed3\u5408\u5b9a\u6027\u5206\u6790\u548c\u5b9a\u91cf\u5ba1\u8ba1\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u534f\u4f5c\u5931\u8d25\u6a21\u5f0f\u7684\u5168\u9762\u5206\u7c7b\u6cd5\u3002", "result": "\u5b9a\u91cf\u5ba1\u8ba1\u63ed\u793a\u4e86\u56db\u79cd\u4e3b\u8981\u5931\u8d25\u6a21\u5f0f\uff1a\u7531\u5171\u4eab\u6a21\u578b\u7f3a\u9677\u9a71\u52a8\u7684\u6709\u7f3a\u9677\u5171\u8bc6\u3001\u6b63\u786e\u5c11\u6570\u610f\u89c1\u7684\u538b\u5236\u3001\u65e0\u6548\u7684\u8ba8\u8bba\u52a8\u6001\u4ee5\u53ca\u5408\u6210\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u4fe1\u606f\u4e22\u5931\u3002", "conclusion": "\u4ec5\u9760\u9ad8\u51c6\u786e\u6027\u4e0d\u8db3\u4ee5\u5efa\u7acb\u4e34\u5e8a\u6216\u516c\u4f17\u4fe1\u4efb\uff0c\u8feb\u5207\u9700\u8981\u900f\u660e\u548c\u53ef\u5ba1\u8ba1\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u8fd9\u662f\u8d1f\u8d23\u4efb\u5f00\u53d1\u548c\u90e8\u7f72\u533b\u7597AI\u7684\u57fa\u77f3\u3002"}}
{"id": "2510.09794", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09794", "abs": "https://arxiv.org/abs/2510.09794", "authors": ["Lianghuan Huang", "Yingshan Chang"], "title": "Causality $\\neq$ Decodability, and Vice Versa: Lessons from Interpreting Counting ViTs", "comment": null, "summary": "Mechanistic interpretability seeks to uncover how internal components of\nneural networks give rise to predictions. A persistent challenge, however, is\ndisentangling two often conflated notions: decodability--the recoverability of\ninformation from hidden states--and causality--the extent to which those states\nfunctionally influence outputs. In this work, we investigate their relationship\nin vision transformers (ViTs) fine-tuned for object counting. Using activation\npatching, we test the causal role of spatial and CLS tokens by transplanting\nactivations across clean-corrupted image pairs. In parallel, we train linear\nprobes to assess the decodability of count information at different depths. Our\nresults reveal systematic mismatches: middle-layer object tokens exert strong\ncausal influence despite being weakly decodable, whereas final-layer object\ntokens support accurate decoding yet are functionally inert. Similarly, the CLS\ntoken becomes decodable in mid-layers but only acquires causal power in the\nfinal layers. These findings highlight that decodability and causality reflect\ncomplementary dimensions of representation--what information is present versus\nwhat is used--and that their divergence can expose hidden computational\ncircuits.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u89c6\u89c9Transformer\u4e2d\uff0c\u4fe1\u606f\u7684\u53ef\u89e3\u7801\u6027\u548c\u56e0\u679c\u6027\u5b58\u5728\u7cfb\u7edf\u6027\u4e0d\u5339\u914d\uff1a\u4e2d\u95f4\u5c42\u7684\u5bf9\u8c61token\u5177\u6709\u5f3a\u56e0\u679c\u5f71\u54cd\u4f46\u5f31\u53ef\u89e3\u7801\u6027\uff0c\u800c\u6700\u7ec8\u5c42\u7684\u5bf9\u8c61token\u652f\u6301\u51c6\u786e\u89e3\u7801\u4f46\u5728\u529f\u80fd\u4e0a\u60f0\u6027\u3002", "motivation": "\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u9762\u4e34\u7684\u4e00\u4e2a\u6301\u7eed\u6311\u6218\u662f\u533a\u5206\u4e24\u4e2a\u5e38\u88ab\u6df7\u6dc6\u7684\u6982\u5ff5\uff1a\u53ef\u89e3\u7801\u6027\uff08\u4ece\u9690\u85cf\u72b6\u6001\u6062\u590d\u4fe1\u606f\u7684\u80fd\u529b\uff09\u548c\u56e0\u679c\u6027\uff08\u8fd9\u4e9b\u72b6\u6001\u5bf9\u8f93\u51fa\u7684\u529f\u80fd\u5f71\u54cd\u7a0b\u5ea6\uff09\u3002", "method": "\u4f7f\u7528\u6fc0\u6d3b\u4fee\u8865\u6280\u672f\u6d4b\u8bd5\u7a7a\u95f4\u548cCLS token\u7684\u56e0\u679c\u4f5c\u7528\uff0c\u901a\u8fc7\u79fb\u690d\u5e72\u51c0-\u635f\u574f\u56fe\u50cf\u5bf9\u4e4b\u95f4\u7684\u6fc0\u6d3b\uff1b\u540c\u65f6\u8bad\u7ec3\u7ebf\u6027\u63a2\u9488\u8bc4\u4f30\u4e0d\u540c\u6df1\u5ea6\u8ba1\u6570\u4fe1\u606f\u7684\u53ef\u89e3\u7801\u6027\u3002", "result": "\u53d1\u73b0\u4e2d\u95f4\u5c42\u5bf9\u8c61token\u5177\u6709\u5f3a\u56e0\u679c\u5f71\u54cd\u4f46\u5f31\u53ef\u89e3\u7801\u6027\uff0c\u6700\u7ec8\u5c42\u5bf9\u8c61token\u652f\u6301\u51c6\u786e\u89e3\u7801\u4f46\u529f\u80fd\u60f0\u6027\uff1bCLS token\u5728\u4e2d\u95f4\u5c42\u53ef\u89e3\u7801\u4f46\u4ec5\u5728\u6700\u7ec8\u5c42\u83b7\u5f97\u56e0\u679c\u80fd\u529b\u3002", "conclusion": "\u53ef\u89e3\u7801\u6027\u548c\u56e0\u679c\u6027\u53cd\u6620\u4e86\u8868\u5f81\u7684\u4e92\u8865\u7ef4\u5ea6\u2014\u2014\u5b58\u5728\u4ec0\u4e48\u4fe1\u606f\u4e0e\u4f7f\u7528\u4ec0\u4e48\u4fe1\u606f\u2014\u2014\u5b83\u4eec\u7684\u5206\u6b67\u53ef\u4ee5\u63ed\u793a\u9690\u85cf\u7684\u8ba1\u7b97\u7535\u8def\u3002"}}
{"id": "2510.10895", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10895", "abs": "https://arxiv.org/abs/2510.10895", "authors": ["Renxuan Tan", "Rongpeng Li", "Fei Wang", "Chenghui Peng", "Shaoyun Wu", "Zhifeng Zhao", "Honggang Zhang"], "title": "LLM-Empowered Agentic MAC Protocols: A Dynamic Stackelberg Game Approach", "comment": "This work has been submitted to IEEE for possible publication", "summary": "Medium Access Control (MAC) protocols, essential for wireless networks, are\ntypically manually configured. While deep reinforcement learning (DRL)-based\nprotocols enhance task-specified network performance, they suffer from poor\ngeneralizability and resilience, demanding costly retraining to adapt to\ndynamic environments. To overcome this limitation, we introduce a\ngame-theoretic LLM-empowered multi-agent DRL (MARL) framework, in which the\nuplink transmission between a base station and a varying number of user\nequipments is modeled as a dynamic multi-follower Stackelberg game (MFSG),\ncapturing the network's natural hierarchical structure. Within this game,\nLLM-driven agents, coordinated through proximal policy optimization (PPO),\nsynthesize adaptive, semantic MAC protocols in response to network dynamics.\nProtocol action grammar (PAG) is employed to ensure the reliability and\nefficiency of this process. Under this system, we further analyze the existence\nand convergence behavior in terms of a Stackelberg equilibrium by studying the\nlearning dynamics of LLM-empowered unified policies in response to changing\nfollowers. Simulations corroborate that our framework achieves a 77.6% greater\nthroughput and a 65.2% fairness improvement over conventional baselines.\nBesides, our framework generalizes excellently to a fluctuating number of users\nwithout requiring retraining or architectural changes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u535a\u5f08\u8bba\u548cLLM\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u81ea\u9002\u5e94MAC\u534f\u8bae\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfDRL\u534f\u8bae\u6cdb\u5316\u6027\u548c\u5f39\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfMAC\u534f\u8bae\u9700\u8981\u624b\u52a8\u914d\u7f6e\uff0c\u800c\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u534f\u8bae\u867d\u7136\u80fd\u63d0\u5347\u7f51\u7edc\u6027\u80fd\uff0c\u4f46\u6cdb\u5316\u6027\u548c\u5f39\u6027\u8f83\u5dee\uff0c\u9700\u8981\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\u6765\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002", "method": "\u4f7f\u7528\u535a\u5f08\u8bbaLLM\u8d4b\u80fd\u7684MARL\u6846\u67b6\uff0c\u5c06\u4e0a\u884c\u94fe\u8def\u4f20\u8f93\u5efa\u6a21\u4e3a\u52a8\u6001\u591a\u8ddf\u968f\u8005Stackelberg\u535a\u5f08\uff0c\u91c7\u7528PPO\u534f\u8c03LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u534f\u8bae\u52a8\u4f5c\u8bed\u6cd5\u786e\u4fdd\u53ef\u9760\u6027\u548c\u6548\u7387\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u8be5\u6846\u67b6\u6bd4\u4f20\u7edf\u57fa\u7ebf\u541e\u5410\u91cf\u63d0\u534777.6%\uff0c\u516c\u5e73\u6027\u63d0\u534765.2%\uff0c\u4e14\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u7528\u6237\u6570\u91cf\u6ce2\u52a8\u7684\u60c5\u51b5\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u67b6\u6784\u66f4\u6539\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86DRL\u534f\u8bae\u6cdb\u5316\u6027\u5dee\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u8bed\u4e49MAC\u534f\u8bae\u7684\u81ea\u52a8\u751f\u6210\uff0c\u5728\u52a8\u6001\u7f51\u7edc\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.11657", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11657", "abs": "https://arxiv.org/abs/2510.11657", "authors": ["Panos Tsimpos", "Youssef Marzouk"], "title": "An Eulerian Perspective on Straight-Line Sampling", "comment": null, "summary": "We study dynamic measure transport for generative modeling: specifically,\nflows induced by stochastic processes that bridge a specified source and target\ndistribution. The conditional expectation of the process' velocity defines an\nODE whose flow map achieves the desired transport. We ask \\emph{which processes\nproduce straight-line flows} -- i.e., flows whose pointwise acceleration\nvanishes and thus are exactly integrable with a first-order method? We provide\na concise PDE characterization of straightness as a balance between conditional\nacceleration and the divergence of a weighted covariance (Reynolds) tensor.\nUsing this lens, we fully characterize affine-in-time interpolants and show\nthat straightness occurs exactly under deterministic endpoint couplings. We\nalso derive necessary conditions that constrain flow geometry for general\nprocesses, offering broad guidance for designing transports that are easier to\nintegrate.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u52a8\u6001\u5ea6\u91cf\u4f20\u8f93\u7684\u751f\u6210\u5efa\u6a21\uff0c\u7279\u522b\u5173\u6ce8\u7531\u8fde\u63a5\u6307\u5b9a\u6e90\u5206\u5e03\u548c\u76ee\u6807\u5206\u5e03\u7684\u968f\u673a\u8fc7\u7a0b\u8bf1\u5bfc\u7684\u6d41\u3002\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2aPDE\u7279\u5f81\u6765\u63cf\u8ff0\u76f4\u7ebf\u6d41\uff08\u70b9\u52a0\u901f\u5ea6\u4e3a\u96f6\u7684\u6d41\uff09\uff0c\u5e76\u5b8c\u5168\u523b\u753b\u4e86\u4eff\u5c04\u65f6\u95f4\u63d2\u503c\uff0c\u8868\u660e\u76f4\u7ebf\u6027\u4ec5\u5728\u786e\u5b9a\u6027\u7aef\u70b9\u8026\u5408\u4e0b\u53d1\u751f\u3002", "motivation": "\u7814\u7a76\u968f\u673a\u8fc7\u7a0b\u8bf1\u5bfc\u7684\u6d41\uff0c\u4ee5\u627e\u5230\u66f4\u5bb9\u6613\u79ef\u5206\u7684\u4f20\u8f93\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u4ea7\u751f\u76f4\u7ebf\u6d41\u7684\u8fc7\u7a0b\uff0c\u8fd9\u4e9b\u6d41\u53ef\u4ee5\u901a\u8fc7\u4e00\u9636\u65b9\u6cd5\u7cbe\u786e\u79ef\u5206\u3002", "method": "\u901a\u8fc7\u5206\u6790\u968f\u673a\u8fc7\u7a0b\u7684\u6761\u4ef6\u671f\u671b\u548c\u901f\u5ea6\uff0c\u63a8\u5bfc\u51fa\u63cf\u8ff0\u76f4\u7ebf\u6d41\u7684PDE\u7279\u5f81\uff0c\u7814\u7a76\u4eff\u5c04\u65f6\u95f4\u63d2\u503c\u548c\u7aef\u70b9\u8026\u5408\u6761\u4ef6\u3002", "result": "\u53d1\u73b0\u76f4\u7ebf\u6d41\u4ec5\u5728\u786e\u5b9a\u6027\u7aef\u70b9\u8026\u5408\u4e0b\u53d1\u751f\uff0c\u5e76\u63a8\u5bfc\u4e86\u7ea6\u675f\u6d41\u51e0\u4f55\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u5bb9\u6613\u79ef\u5206\u7684\u4f20\u8f93\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "\u8bba\u6587\u4e3a\u52a8\u6001\u5ea6\u91cf\u4f20\u8f93\u7684\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u660e\u786e\u4e86\u4ea7\u751f\u76f4\u7ebf\u6d41\u7684\u6761\u4ef6\uff0c\u6709\u52a9\u4e8e\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u79ef\u5206\u65b9\u6cd5\u3002"}}
{"id": "2510.10208", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10208", "abs": "https://arxiv.org/abs/2510.10208", "authors": ["Bo Yuan", "Yulin Chen", "Yin Zhang"], "title": "Weed Out, Then Harvest: Dual Low-Rank Adaptation is an Effective Noisy Label Detector for Noise-Robust Learning", "comment": "ACL 2025", "summary": "Parameter-efficient fine-tuning (PEFT) large language models (LLMs) have\nshown impressive performance in various downstream tasks. However, in many\nreal-world scenarios, the collected training data inevitably contains noisy\nlabels. To learn from noisy labels, most solutions select samples with small\nlosses for model training. However, the selected samples, in turn, impact the\nloss computation in the next iteration. An inaccurate initial selection can\ncreate a vicious cycle, leading to suboptimal performance. To break this cycle,\nwe propose Delora, a novel framework that decouples the sample selection from\nmodel training. For sample selection, Delora establishes a noisy label detector\nby introducing clean and noisy LoRA. Benefiting from the memory effect, the\nclean LoRA is encouraged to memorize clean data, while the noisy LoRA is\nconstrained to memorize mislabeled data, which serves as a learnable threshold\nfor selecting clean and noisy samples. For model training, Delora can use\ncarefully selected samples to fine-tune language models seamlessly.\nExperimental results on synthetic and real-world noisy datasets demonstrate the\neffectiveness of Delora in noisy label detection and text classification.", "AI": {"tldr": "Delora\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u6837\u672c\u9009\u62e9\u548c\u6a21\u578b\u8bad\u7ec3\u6765\u89e3\u51b3\u5e26\u566a\u6807\u7b7e\u7684PEFT\u5fae\u8c03\u95ee\u9898\u3002\u5b83\u5f15\u5165\u5e72\u51c0\u548c\u566a\u58f0LoRA\u6765\u5efa\u7acb\u566a\u58f0\u6807\u7b7e\u68c0\u6d4b\u5668\uff0c\u5229\u7528\u8bb0\u5fc6\u6548\u5e94\u533a\u5206\u5e72\u51c0\u548c\u566a\u58f0\u6837\u672c\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u6536\u96c6\u7684\u8bad\u7ec3\u6570\u636e\u5f80\u5f80\u5305\u542b\u566a\u58f0\u6807\u7b7e\uff0c\u4f20\u7edf\u65b9\u6cd5\u57fa\u4e8e\u5c0f\u635f\u5931\u9009\u62e9\u6837\u672c\u4f46\u5bb9\u6613\u5f62\u6210\u6076\u6027\u5faa\u73af\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002\u9700\u8981\u6253\u7834\u8fd9\u79cd\u5faa\u73af\u6765\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "method": "\u63d0\u51faDelora\u6846\u67b6\uff0c\u5c06\u6837\u672c\u9009\u62e9\u4e0e\u6a21\u578b\u8bad\u7ec3\u89e3\u8026\uff1a1\uff09\u5f15\u5165\u5e72\u51c0LoRA\u548c\u566a\u58f0LoRA\u5efa\u7acb\u566a\u58f0\u6807\u7b7e\u68c0\u6d4b\u5668\uff1b2\uff09\u5229\u7528\u8bb0\u5fc6\u6548\u5e94\uff0c\u5e72\u51c0LoRA\u8bb0\u5fc6\u5e72\u51c0\u6570\u636e\uff0c\u566a\u58f0LoRA\u8bb0\u5fc6\u8bef\u6807\u6570\u636e\u4f5c\u4e3a\u53ef\u5b66\u4e60\u9608\u503c\uff1b3\uff09\u4f7f\u7528\u7cbe\u5fc3\u9009\u62e9\u7684\u6837\u672c\u65e0\u7f1d\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDelora\u5728\u566a\u58f0\u6807\u7b7e\u68c0\u6d4b\u548c\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "Delora\u901a\u8fc7\u89e3\u8026\u6837\u672c\u9009\u62e9\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5e26\u566a\u6807\u7b7e\u7684PEFT\u5fae\u8c03\u95ee\u9898\uff0c\u5728\u566a\u58f0\u6807\u7b7e\u68c0\u6d4b\u548c\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002"}}
{"id": "2510.10909", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10909", "abs": "https://arxiv.org/abs/2510.10909", "authors": ["Daoyu Wang", "Mingyue Cheng", "Qi Liu", "Shuo Yu", "Zirui Liu", "Ze Guo"], "title": "PaperArena: An Evaluation Benchmark for Tool-Augmented Agentic Reasoning on Scientific Literature", "comment": "12 pages, 9 figures", "summary": "Understanding and reasoning on the web-scale scientific literature is a\ncrucial touchstone for large language model (LLM) based agents designed to\nsupport complex knowledge-intensive tasks. However, existing works are mainly\nrestricted to tool-free tasks within isolated papers, largely due to the lack\nof a benchmark for cross-paper reasoning and multi-tool orchestration in real\nresearch scenarios. In this work, we propose PaperArena, an evaluation\nbenchmark for agents to address real-world research questions that typically\nrequire integrating information across multiple papers with the assistance of\nexternal tools. Given a research question, agents should integrate diverse\nformats across multiple papers through reasoning and interacting with\nappropriate tools, thereby producing a well-grounded answer. To support\nstandardized evaluation, we provide a modular and extensible platform for agent\nexecution, offering tools such as multimodal parsing, context retrieval, and\nprogrammatic computation. Experimental results reveal that even the most\nadvanced LLM powering a well-established agent system achieves merely 38.78%\naverage accuracy. On the hard subset, accuracy drops to only 18.47%,\nhighlighting great potential for improvement. We also present several empirical\nfindings, including that all agents tested exhibit inefficient tool usage,\noften invoking more tools than necessary to solve a task. We invite the\ncommunity to adopt PaperArena to develop and evaluate more capable agents for\nscientific discovery. Our code and data are available\nhttps://github.com/Melmaphother/PaperArena.", "AI": {"tldr": "PaperArena\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u771f\u5b9e\u7814\u7a76\u573a\u666f\u4e2d\u8fdb\u884c\u8de8\u8bba\u6587\u63a8\u7406\u548c\u591a\u5de5\u5177\u534f\u8c03\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u7bc7\u8bba\u6587\u5185\u7684\u65e0\u5de5\u5177\u4efb\u52a1\uff0c\u7f3a\u4e4f\u9488\u5bf9\u8de8\u8bba\u6587\u63a8\u7406\u548c\u771f\u5b9e\u7814\u7a76\u573a\u666f\u4e2d\u591a\u5de5\u5177\u534f\u8c03\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u5e73\u53f0\uff0c\u63d0\u4f9b\u591a\u6a21\u6001\u89e3\u6790\u3001\u4e0a\u4e0b\u6587\u68c0\u7d22\u548c\u7a0b\u5e8f\u5316\u8ba1\u7b97\u7b49\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u4ee3\u7406\u5728\u56de\u7b54\u7814\u7a76\u95ee\u9898\u65f6\u6574\u5408\u591a\u7bc7\u8bba\u6587\u4fe1\u606f\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684LLM\u9a71\u52a8\u7684\u4ee3\u7406\u7cfb\u7edf\u5e73\u5747\u51c6\u786e\u7387\u4ec5\u4e3a38.78%\uff0c\u5728\u56f0\u96be\u5b50\u96c6\u4e0a\u51c6\u786e\u7387\u66f4\u662f\u964d\u81f318.47%\u3002\u6240\u6709\u6d4b\u8bd5\u4ee3\u7406\u90fd\u8868\u73b0\u51fa\u5de5\u5177\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "conclusion": "PaperArena\u63ed\u793a\u4e86\u5f53\u524d\u79d1\u5b66\u53d1\u73b0\u4ee3\u7406\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u5e76\u9080\u8bf7\u7814\u7a76\u793e\u533a\u91c7\u7528\u8be5\u57fa\u51c6\u6765\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u79d1\u5b66\u53d1\u73b0\u4ee3\u7406\u3002"}}
{"id": "2510.10223", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10223", "abs": "https://arxiv.org/abs/2510.10223", "authors": ["Yijie Xu", "Huizai Yao", "Zhiyu Guo", "Weiyu Guo", "Pengteng Li", "Aiwei Liu", "Xuming Hu", "Hui Xiong"], "title": "You only need 4 extra tokens: Synergistic Test-time Adaptation for LLMs", "comment": "Under Review", "summary": "Large language models (LLMs) are increasingly deployed in specialized domains\nsuch as finance, medicine, and agriculture, where they face significant\ndistribution shifts from their training data. Domain-specific fine-tuning can\nmitigate this challenge but relies on high-quality labeled data that is\nexpensive and slow to collect in expertise-limited settings. We study\nlabel-free test-time adaptation for language models and present SyTTA, an\ninference-time framework that adapts models on-the-fly without additional\nsupervision. SyTTA couples two complementary uncertainty signals that arise\nunder distribution shift: input-side perplexity, indicating mismatch with\ndomain-specific terminology and patterns, and output-side predictive entropy,\nindicating diffuse and unstable token probabilities during generation. Across\ndiverse model architectures and domain-specific benchmarks, SyTTA delivers\nconsistent gains. Notably, on agricultural question answering, SyTTA improves\nRouge-LSum by over 120% on Qwen-2.5-7B with only 4 extra tokens per query.\nThese results show that effective test-time adaptation for language models is\nachievable without labeled examples, supporting deployment in label-scarce\ndomains. The code will be made available upon acceptance.", "AI": {"tldr": "SyTTA\u662f\u4e00\u4e2a\u65e0\u9700\u6807\u7b7e\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8f93\u5165\u4fa7\u56f0\u60d1\u5ea6\u548c\u8f93\u51fa\u4fa7\u9884\u6d4b\u71b5\u4e24\u79cd\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\uff0c\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u5b9e\u65f6\u8c03\u6574\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u4e13\u4e1a\u9886\u57df\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u90e8\u7f72\u65f6\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u504f\u79fb\u7684\u6311\u6218\uff0c\u800c\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u9700\u8981\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u8fd9\u5728\u4e13\u4e1a\u77e5\u8bc6\u6709\u9650\u7684\u573a\u666f\u4e2d\u6210\u672c\u9ad8\u6602\u4e14\u6536\u96c6\u7f13\u6162\u3002", "method": "SyTTA\u6846\u67b6\u5728\u63a8\u7406\u65f6\u8026\u5408\u4e24\u79cd\u4e92\u8865\u7684\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\uff1a\u8f93\u5165\u4fa7\u56f0\u60d1\u5ea6\uff08\u53cd\u6620\u4e0e\u9886\u57df\u7279\u5b9a\u672f\u8bed\u548c\u6a21\u5f0f\u7684\u5931\u914d\uff09\u548c\u8f93\u51fa\u4fa7\u9884\u6d4b\u71b5\uff08\u53cd\u6620\u751f\u6210\u8fc7\u7a0b\u4e2d\u4ee4\u724c\u6982\u7387\u7684\u6269\u6563\u548c\u4e0d\u7a33\u5b9a\u6027\uff09\uff0c\u5b9e\u73b0\u65e0\u9700\u989d\u5916\u76d1\u7763\u7684\u5b9e\u65f6\u9002\u5e94\u3002", "result": "\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u548c\u9886\u57df\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSyTTA\u5e26\u6765\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002\u5728\u519c\u4e1a\u95ee\u7b54\u4efb\u52a1\u4e0a\uff0cQwen-2.5-7B\u6a21\u578b\u7684Rouge-LSum\u6307\u6807\u63d0\u9ad8\u4e86\u8d85\u8fc7120%\uff0c\u6bcf\u67e5\u8be2\u4ec5\u97004\u4e2a\u989d\u5916\u4ee4\u724c\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u65e0\u9700\u6807\u6ce8\u793a\u4f8b\u5373\u53ef\u5b9e\u73b0\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u6d4b\u8bd5\u65f6\u9002\u5e94\uff0c\u652f\u6301\u5728\u6807\u7b7e\u7a00\u7f3a\u9886\u57df\u7684\u90e8\u7f72\u3002"}}
{"id": "2510.09805", "categories": ["cs.LG", "cs.AI", "35Q30, 76D05, 65M70, 68T07, 68T27, 03D45", "I.2.0"], "pdf": "https://arxiv.org/pdf/2510.09805", "abs": "https://arxiv.org/abs/2510.09805", "authors": ["Jeffrey Camlin"], "title": "Temporal Lifting as Latent-Space Regularization for Continuous-Time Flow Models in AI Systems", "comment": "6 pages, 1 figure, 1 table, 1 algorithm", "summary": "We present a latent-space formulation of adaptive temporal reparametrization\nfor continuous-time dynamical systems. The method, called *temporal lifting*,\nintroduces a smooth monotone mapping $t \\mapsto \\tau(t)$ that regularizes\nnear-singular behavior of the underlying flow while preserving its conservation\nlaws. In the lifted coordinate, trajectories such as those of the\nincompressible Navier-Stokes equations on the torus $\\mathbb{T}^3$ become\nglobally smooth. From the standpoint of machine-learning dynamics, temporal\nlifting acts as a continuous-time normalization or time-warping operator that\ncan stabilize physics-informed neural networks and other latent-flow\narchitectures used in AI systems. The framework links analytic regularity\ntheory with representation-learning methods for stiff or turbulent processes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u65f6\u95f4\u63d0\u5347\u7684\u6f5c\u5728\u7a7a\u95f4\u81ea\u9002\u5e94\u65f6\u95f4\u91cd\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fde\u7eed\u65f6\u95f4\u52a8\u529b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5e73\u6ed1\u5355\u8c03\u6620\u5c04\u6b63\u5219\u5316\u6d41\u573a\u4e2d\u7684\u8fd1\u5947\u5f02\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u6301\u5b88\u6052\u5b9a\u5f8b\u3002", "motivation": "\u89e3\u51b3\u8fde\u7eed\u65f6\u95f4\u52a8\u529b\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u8fd1\u5947\u5f02\u884c\u4e3a\u548c\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u521a\u6027\u548c\u6e4d\u6d41\u8fc7\u7a0b\u65f6\uff0c\u4e3a\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u548c\u5176\u4ed6\u6f5c\u5728\u6d41\u67b6\u6784\u63d0\u4f9b\u7a33\u5b9a\u5316\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u5e73\u6ed1\u5355\u8c03\u6620\u5c04t\u21a6\u03c4(t)\u8fdb\u884c\u65f6\u95f4\u91cd\u53c2\u6570\u5316\uff0c\u5728\u63d0\u5347\u5750\u6807\u7cfb\u4e2d\u4f7f\u8f68\u8ff9\u53d8\u5f97\u5168\u5c40\u5149\u6ed1\uff0c\u540c\u65f6\u4fdd\u6301\u7cfb\u7edf\u7684\u5b88\u6052\u5b9a\u5f8b\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4f7f\u4e0d\u53ef\u538b\u7f29Navier-Stokes\u65b9\u7a0b\u5728\u73af\u9762T^3\u4e0a\u7684\u8f68\u8ff9\u53d8\u5f97\u5168\u5c40\u5149\u6ed1\uff0c\u5e76\u80fd\u591f\u7a33\u5b9a\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u548c\u6f5c\u5728\u6d41\u67b6\u6784\u3002", "conclusion": "\u65f6\u95f4\u63d0\u5347\u6846\u67b6\u5c06\u89e3\u6790\u6b63\u5219\u6027\u7406\u8bba\u4e0e\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u5904\u7406\u521a\u6027\u548c\u6e4d\u6d41\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8fde\u7eed\u65f6\u95f4\u5f52\u4e00\u5316\u65b9\u6cd5\u3002"}}
{"id": "2510.10931", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10931", "abs": "https://arxiv.org/abs/2510.10931", "authors": ["SHengjie Ma", "Chenlong Deng", "Jiaxin Mao", "Jiadeng Huang", "Teng Wang", "Junjie Wu", "Changwang Zhang", "Jun wang"], "title": "PoU: Proof-of-Use to Counter Tool-Call Hacking in DeepResearch Agents", "comment": null, "summary": "Retrieval-augmented generation (RAG) agents, such as recent\nDeepResearch-style systems, extend large language models (LLMs) with autonomous\ninformation-seeking capabilities through external tools. While reinforcement\nlearning (RL) has enabled impressive multi-step reasoning, we identify a\npreviously overlooked failure mode, Tool-Call Hacking, where agents inflate\nreward signals by issuing superficially correct tool calls without genuinely\nleveraging the retrieved evidence. This results in (i) mode collapse into\nrepetitive reliance on a single source and (ii) spurious grounding, where\nanswers are only weakly supported by cited content.\n  To address this, we propose Proof-of-Use (PoU), an evidence-grounded RL\nframework that enforces verifiable causal links between retrieved evidence,\nreasoning traces, and final answers. PoU operationalizes this through a unified\nstep-wise contract combining syntactic citation validation, perturbation-based\nsensitivity rewards, and answer-evidence alignment objectives, ensuring that\ntool usage remains both interpretable and functionally grounded.\n  Across seven QA benchmarks spanning in-domain, out-of-domain, and\nout-of-tool-distribution settings, PoU consistently outperforms strong\nDeepResearch baselines in factual accuracy, evidence faithfulness, and\ntool-routing balance. These findings highlight the necessity of grounding\nRL-trained agents not merely in task outcomes but in the causal use of\nretrieved information, offering a principled path toward trustworthy\nretrieval-augmented reasoning.", "AI": {"tldr": "\u63d0\u51faProof-of-Use (PoU)\u6846\u67b6\uff0c\u89e3\u51b3\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u4ee3\u7406\u4e2d\u7684Tool-Call Hacking\u95ee\u9898\uff0c\u901a\u8fc7\u8bc1\u636e\u57fa\u7840\u7684\u5f3a\u5316\u5b66\u4e60\u786e\u4fdd\u68c0\u7d22\u8bc1\u636e\u4e0e\u63a8\u7406\u8fc7\u7a0b\u4e4b\u95f4\u7684\u53ef\u9a8c\u8bc1\u56e0\u679c\u8054\u7cfb\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684RAG\u4ee3\u7406\u5b58\u5728Tool-Call Hacking\u5931\u8d25\u6a21\u5f0f\uff0c\u5373\u4ee3\u7406\u901a\u8fc7\u8868\u9762\u6b63\u786e\u7684\u5de5\u5177\u8c03\u7528\u6765\u5938\u5927\u5956\u52b1\u4fe1\u53f7\uff0c\u800c\u4e0d\u771f\u6b63\u5229\u7528\u68c0\u7d22\u8bc1\u636e\uff0c\u5bfc\u81f4\u6a21\u5f0f\u5d29\u6e83\u548c\u865a\u5047\u63a5\u5730\u95ee\u9898\u3002", "method": "PoU\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u7684\u9010\u6b65\u5408\u7ea6\u5b9e\u73b0\uff0c\u5305\u62ec\u8bed\u6cd5\u5f15\u7528\u9a8c\u8bc1\u3001\u57fa\u4e8e\u6270\u52a8\u7684\u654f\u611f\u6027\u5956\u52b1\u548c\u7b54\u6848-\u8bc1\u636e\u5bf9\u9f50\u76ee\u6807\uff0c\u786e\u4fdd\u5de5\u5177\u4f7f\u7528\u7684\u53ef\u89e3\u91ca\u6027\u548c\u529f\u80fd\u6027\u63a5\u5730\u3002", "result": "\u5728\u4e03\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPoU\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u8bc1\u636e\u5fe0\u5b9e\u5ea6\u548c\u5de5\u5177\u8def\u7531\u5e73\u8861\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8eDeepResearch\u57fa\u7ebf\uff0c\u6db5\u76d6\u57df\u5185\u3001\u57df\u5916\u548c\u5de5\u5177\u5206\u5e03\u5916\u8bbe\u7f6e\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u8981\u5c06RL\u8bad\u7ec3\u4ee3\u7406\u4e0d\u4ec5\u57fa\u4e8e\u4efb\u52a1\u7ed3\u679c\uff0c\u8fd8\u8981\u57fa\u4e8e\u68c0\u7d22\u4fe1\u606f\u7684\u56e0\u679c\u4f7f\u7528\uff0c\u4e3a\u53ef\u4fe1\u8d56\u7684\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u8def\u5f84\u3002"}}
{"id": "2510.11691", "categories": ["cs.LG", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11691", "abs": "https://arxiv.org/abs/2510.11691", "authors": ["Taira Tsuchiya"], "title": "Tight Regret Upper and Lower Bounds for Optimistic Hedge in Two-Player Zero-Sum Games", "comment": "29 pages, 2 figures", "summary": "In two-player zero-sum games, the learning dynamic based on optimistic Hedge\nachieves one of the best-known regret upper bounds among strongly-uncoupled\nlearning dynamics. With an appropriately chosen learning rate, the social and\nindividual regrets can be bounded by $O(\\log(mn))$ in terms of the numbers of\nactions $m$ and $n$ of the two players. This study investigates the optimality\nof the dependence on $m$ and $n$ in the regret of optimistic Hedge. To this\nend, we begin by refining existing regret analysis and show that, in the\nstrongly-uncoupled setting where the opponent's number of actions is known,\nboth the social and individual regret bounds can be improved to $O(\\sqrt{\\log m\n\\log n})$. In this analysis, we express the regret upper bound as an\noptimization problem with respect to the learning rates and the coefficients of\ncertain negative terms, enabling refined analysis of the leading constants. We\nthen show that the existing social regret bound as well as these new social and\nindividual regret upper bounds cannot be further improved for optimistic Hedge\nby providing algorithm-dependent individual regret lower bounds. Importantly,\nthese social regret upper and lower bounds match exactly including the constant\nfactor in the leading term. Finally, building on these results, we improve the\nlast-iterate convergence rate and the dynamic regret of a learning dynamic\nbased on optimistic Hedge, and complement these bounds with algorithm-dependent\ndynamic regret lower bounds that match the improved bounds.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e50\u89c2Hedge\u7b97\u6cd5\u5728\u4e24\u4eba\u96f6\u548c\u535a\u5f08\u4e2d\u7684\u9057\u61be\u6700\u4f18\u6027\uff0c\u6539\u8fdb\u4e86\u793e\u4ea4\u548c\u4e2a\u4f53\u9057\u61be\u4e0a\u754c\u81f3O(\u221a(log m log n))\uff0c\u5e76\u8bc1\u660e\u4e86\u8fd9\u4e9b\u4e0a\u754c\u65e0\u6cd5\u8fdb\u4e00\u6b65\u6539\u8fdb\uff0c\u5305\u62ec\u5e38\u6570\u56e0\u5b50\u5339\u914d\u3002", "motivation": "\u7814\u7a76\u4e50\u89c2Hedge\u7b97\u6cd5\u5728\u5f3a\u975e\u8026\u5408\u5b66\u4e60\u52a8\u6001\u4e2d\u7684\u9057\u61be\u4f9d\u8d56\u6027\u7684\u6700\u4f18\u6027\uff0c\u73b0\u6709\u9057\u61be\u4e0a\u754c\u4e3aO(log(mn))\uff0c\u4f46\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\u3002", "method": "\u901a\u8fc7\u5c06\u9057\u61be\u4e0a\u754c\u8868\u793a\u4e3a\u5173\u4e8e\u5b66\u4e60\u7387\u548c\u67d0\u4e9b\u8d1f\u9879\u7cfb\u6570\u7684\u4f18\u5316\u95ee\u9898\uff0c\u8fdb\u884c\u7cbe\u7ec6\u5316\u5206\u6790\uff1b\u63d0\u4f9b\u7b97\u6cd5\u4f9d\u8d56\u7684\u4e2a\u4f53\u9057\u61be\u4e0b\u754c\u6765\u8bc1\u660e\u4e0a\u754c\u7684\u6700\u4f18\u6027\u3002", "result": "\u5728\u5f3a\u975e\u8026\u5408\u8bbe\u7f6e\u4e0b\uff0c\u793e\u4ea4\u548c\u4e2a\u4f53\u9057\u61be\u4e0a\u754c\u6539\u8fdb\u4e3aO(\u221a(log m log n))\uff0c\u4e14\u8fd9\u4e9b\u4e0a\u754c\u65e0\u6cd5\u8fdb\u4e00\u6b65\u6539\u8fdb\uff0c\u4e0a\u4e0b\u754c\u5e38\u6570\u56e0\u5b50\u5b8c\u5168\u5339\u914d\u3002", "conclusion": "\u4e50\u89c2Hedge\u7b97\u6cd5\u7684\u9057\u61be\u4e0a\u754c\u5df2\u8fbe\u5230\u6700\u4f18\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u7ed3\u679c\u8fd8\u6539\u8fdb\u4e86\u6700\u540e\u8fed\u4ee3\u6536\u655b\u7387\u548c\u52a8\u6001\u9057\u61be\uff0c\u5e76\u63d0\u4f9b\u4e86\u5339\u914d\u7684\u52a8\u6001\u9057\u61be\u4e0b\u754c\u3002"}}
{"id": "2510.10224", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10224", "abs": "https://arxiv.org/abs/2510.10224", "authors": ["Ruize An", "Richong Zhang", "Zhijie Nie", "Zhanyu Wu", "Yanzhao Zhang", "Dingkun Long"], "title": "Text2Token: Unsupervised Text Representation Learning with Token Target Prediction", "comment": null, "summary": "Unsupervised text representation learning (TRL) is a fundamental task in\nnatural language processing, which is beneficial for improving search and\nrecommendations with the web's unlabeled texts. A recent empirical study finds\nthat the high-quality representation aligns with the key token of the input\ntext, uncovering the potential connection between representation space and\nvocabulary space. Inspired by the findings, we revisit the generative tasks and\ndevelop an unsupervised generative framework for TRL, Text2Token. The framework\nis based on the token target prediction task, utilizing carefully constructed\ntarget token distribution as supervisory signals. To construct the high-quality\ntarget token distribution, we analyze the token-alignment properties with\nadvanced embedders and identify two essential categories of key tokens: (1) the\nmeaningful tokens in the text and (2) semantically derived tokens beyond the\ntext. Based on these insights, we propose two methods -- data-driven and\nmodel-derived -- to construct synthetic token targets from data or the LLM\nbackbone. Experiments on the MTEB v2 benchmark demonstrate that Text2Token\nachieves performance competitive with the state-of-the-art embedder with\nunsupervised contrastive learning, LLM2Vec. Our analysis further shows that\nvocabulary and representation spaces optimize together and toward the optimum\nsolution during training, providing new ideas and insights for future work.", "AI": {"tldr": "\u63d0\u51faText2Token\uff0c\u4e00\u79cd\u57fa\u4e8etoken\u76ee\u6807\u9884\u6d4b\u7684\u65e0\u76d1\u7763\u6587\u672c\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u76ee\u6807token\u5206\u5e03\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u5728MTEB v2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5LLM2Vec\u7ade\u4e89\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u9ad8\u8d28\u91cf\u6587\u672c\u8868\u793a\u4e0e\u8f93\u5165\u6587\u672c\u7684\u5173\u952etoken\u5bf9\u9f50\uff0c\u63ed\u793a\u4e86\u8868\u793a\u7a7a\u95f4\u4e0e\u8bcd\u6c47\u7a7a\u95f4\u4e4b\u95f4\u7684\u6f5c\u5728\u8054\u7cfb\uff0c\u8fd9\u542f\u53d1\u4e86\u91cd\u65b0\u5ba1\u89c6\u751f\u6210\u4efb\u52a1\u7528\u4e8e\u6587\u672c\u8868\u793a\u5b66\u4e60\u3002", "method": "\u57fa\u4e8etoken\u76ee\u6807\u9884\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u548c\u6a21\u578b\u63a8\u5bfc\u4e24\u79cd\u65b9\u6cd5\u6784\u5efa\u5408\u6210token\u76ee\u6807\uff1a\u4ece\u6570\u636e\u4e2d\u8bc6\u522b\u6709\u610f\u4e49\u7684token\uff0c\u4ee5\u53ca\u4eceLLM\u9aa8\u5e72\u7f51\u7edc\u4e2d\u63a8\u5bfc\u8bed\u4e49\u76f8\u5173\u7684token\u3002", "result": "\u5728MTEB v2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cText2Token\u4e0e\u4f7f\u7528\u65e0\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u7684\u6700\u5148\u8fdb\u5d4c\u5165\u5668LLM2Vec\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\u3002\u5206\u6790\u663e\u793a\u8bcd\u6c47\u548c\u8868\u793a\u7a7a\u95f4\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5171\u540c\u4f18\u5316\u5e76\u8d8b\u5411\u6700\u4f18\u89e3\u3002", "conclusion": "Text2Token\u8bc1\u660e\u4e86\u751f\u6210\u6846\u67b6\u5728\u65e0\u76d1\u7763\u6587\u672c\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u89c1\u89e3\uff0c\u7279\u522b\u662f\u8bcd\u6c47\u7a7a\u95f4\u548c\u8868\u793a\u7a7a\u95f4\u7684\u534f\u540c\u4f18\u5316\u673a\u5236\u3002"}}
{"id": "2510.09825", "categories": ["cs.LG", "cs.CV", "cs.IT", "cs.NE", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.09825", "abs": "https://arxiv.org/abs/2510.09825", "authors": ["Mohsen Joneidi"], "title": "Decomposer Networks: Deep Component Analysis and Synthesis", "comment": "13 Pages, 4 figures", "summary": "We propose the Decomposer Networks (DecompNet), a semantic autoencoder that\nfactorizes an input into multiple interpretable components. Unlike classical\nautoencoders that compress an input into a single latent representation, the\nDecomposer Network maintains N parallel branches, each assigned a residual\ninput defined as the original signal minus the reconstructions of all other\nbranches. By unrolling a Gauss--Seidel style block-coordinate descent into a\ndifferentiable network, DecompNet enforce explicit competition among\ncomponents, yielding parsimonious, semantically meaningful representations. We\nsituate our model relative to linear decomposition methods (PCA, NMF), deep\nunrolled optimization, and object-centric architectures (MONet, IODINE, Slot\nAttention), and highlight its novelty as the first semantic autoencoder to\nimplement an all-but-one residual update rule.", "AI": {"tldr": "Decomposer Networks (DecompNet) \u662f\u4e00\u79cd\u8bed\u4e49\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5e76\u884c\u5206\u652f\u548c\u6b8b\u5dee\u8f93\u5165\u5206\u89e3\u8f93\u5165\u4fe1\u53f7\u4e3a\u591a\u4e2a\u53ef\u89e3\u91ca\u7ec4\u4ef6\uff0c\u91c7\u7528\u7c7bGauss-Seidel\u7684\u5757\u5750\u6807\u4e0b\u964d\u65b9\u6cd5\u5b9e\u73b0\u7ec4\u4ef6\u95f4\u7684\u663e\u5f0f\u7ade\u4e89\u3002", "motivation": "\u4f20\u7edf\u81ea\u7f16\u7801\u5668\u5c06\u8f93\u5165\u538b\u7f29\u4e3a\u5355\u4e00\u6f5c\u5728\u8868\u793a\uff0c\u800cDecompNet\u65e8\u5728\u5c06\u8f93\u5165\u5206\u89e3\u4e3a\u591a\u4e2a\u8bed\u4e49\u53ef\u89e3\u91ca\u7684\u7ec4\u4ef6\uff0c\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u8868\u793a\u5f62\u5f0f\u3002", "method": "\u4f7f\u7528N\u4e2a\u5e76\u884c\u5206\u652f\uff0c\u6bcf\u4e2a\u5206\u652f\u5904\u7406\u539f\u59cb\u4fe1\u53f7\u51cf\u53bb\u5176\u4ed6\u5206\u652f\u91cd\u6784\u7684\u6b8b\u5dee\uff0c\u901a\u8fc7\u5c55\u5f00Gauss-Seidel\u98ce\u683c\u7684\u5757\u5750\u6807\u4e0b\u964d\u4e3a\u53ef\u5fae\u5206\u7f51\u7edc\uff0c\u5b9e\u73b0\u7ec4\u4ef6\u95f4\u7684\u663e\u5f0f\u7ade\u4e89\u3002", "result": "\u6a21\u578b\u80fd\u591f\u4ea7\u751f\u7b80\u6d01\u4e14\u8bed\u4e49\u6709\u610f\u4e49\u7684\u8868\u793a\uff0c\u662f\u9996\u4e2a\u5b9e\u73b0\"\u5168\u9664\u4e00\"\u6b8b\u5dee\u66f4\u65b0\u89c4\u5219\u7684\u8bed\u4e49\u81ea\u7f16\u7801\u5668\u3002", "conclusion": "DecompNet\u5728\u5206\u89e3\u53ef\u89e3\u91ca\u7ec4\u4ef6\u65b9\u9762\u5177\u6709\u521b\u65b0\u6027\uff0c\u4e3a\u8bed\u4e49\u81ea\u7f16\u7801\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u4e0e\u7ebf\u6027\u5206\u89e3\u65b9\u6cd5\u548c\u5bf9\u8c61\u4e2d\u5fc3\u67b6\u6784\u5f62\u6210\u5bf9\u6bd4\u3002"}}
{"id": "2510.10942", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10942", "abs": "https://arxiv.org/abs/2510.10942", "authors": ["Nilima Rao", "Jagriti Srivastava", "Pradeep Kumar Sharma", "Hritvik Shrivastava"], "title": "Scalable and Explainable Enterprise Knowledge Discovery Using Graph-Centric Hybrid Retrieval", "comment": null, "summary": "Modern enterprises manage vast knowledge distributed across heterogeneous\nsystems such as Jira, Git repositories, Confluence, and wikis. Conventional\nretrieval methods based on keyword search or static embeddings often fail to\nanswer complex queries that require contextual reasoning and multi-hop\ninference across artifacts. We present a modular hybrid retrieval framework for\nadaptive enterprise information access that integrates Knowledge Base\nLanguage-Augmented Models (KBLam), DeepGraph representations, and\nembedding-driven semantic search. The framework builds a unified knowledge\ngraph from parsed repositories including code, pull requests, and commit\nhistories, enabling semantic similarity search, structural inference, and\nmulti-hop reasoning. Query analysis dynamically determines the optimal\nretrieval strategy, supporting both structured and unstructured data sources\nthrough independent or fused processing. An interactive interface provides\ngraph visualizations, subgraph exploration, and context-aware query routing to\ngenerate concise and explainable answers. Experiments on large-scale Git\nrepositories show that the unified reasoning layer improves answer relevance by\nup to 80 percent compared with standalone GPT-based retrieval pipelines. By\ncombining graph construction, hybrid reasoning, and interactive visualization,\nthe proposed framework offers a scalable, explainable, and user-centric\nfoundation for intelligent knowledge assistants in enterprise environments.", "AI": {"tldr": "\u63d0\u51fa\u6a21\u5757\u5316\u6df7\u5408\u68c0\u7d22\u6846\u67b6\uff0c\u96c6\u6210\u77e5\u8bc6\u5e93\u8bed\u8a00\u589e\u5f3a\u6a21\u578b\u3001\u6df1\u5ea6\u56fe\u8868\u793a\u548c\u5d4c\u5165\u9a71\u52a8\u8bed\u4e49\u641c\u7d22\uff0c\u89e3\u51b3\u4f01\u4e1a\u5f02\u6784\u7cfb\u7edf\u4e2d\u590d\u6742\u67e5\u8be2\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u591a\u8df3\u63a8\u7406\u95ee\u9898\u3002", "motivation": "\u4f01\u4e1a\u77e5\u8bc6\u5206\u6563\u5728Jira\u3001Git\u3001Confluence\u7b49\u5f02\u6784\u7cfb\u7edf\u4e2d\uff0c\u4f20\u7edf\u57fa\u4e8e\u5173\u952e\u8bcd\u641c\u7d22\u6216\u9759\u6001\u5d4c\u5165\u7684\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u9700\u8981\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u591a\u8df3\u63a8\u7406\u7684\u590d\u6742\u67e5\u8be2\u3002", "method": "\u6784\u5efa\u7edf\u4e00\u77e5\u8bc6\u56fe\u8c31\uff0c\u96c6\u6210KBLam\u6a21\u578b\u3001DeepGraph\u8868\u793a\u548c\u8bed\u4e49\u641c\u7d22\uff0c\u901a\u8fc7\u67e5\u8be2\u5206\u6790\u52a8\u6001\u786e\u5b9a\u6700\u4f18\u68c0\u7d22\u7b56\u7565\uff0c\u652f\u6301\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\u6e90\u3002", "result": "\u5728\u5927\u89c4\u6a21Git\u4ed3\u5e93\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7edf\u4e00\u63a8\u7406\u5c42\u76f8\u6bd4\u72ec\u7acbGPT\u68c0\u7d22\u7ba1\u9053\u5c06\u7b54\u6848\u76f8\u5173\u6027\u63d0\u9ad8\u4e8680%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u56fe\u6784\u5efa\u3001\u6df7\u5408\u63a8\u7406\u548c\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u7684\u7ed3\u5408\uff0c\u4e3a\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u667a\u80fd\u77e5\u8bc6\u52a9\u624b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u548c\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u57fa\u7840\u3002"}}
{"id": "2510.11711", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.11711", "abs": "https://arxiv.org/abs/2510.11711", "authors": ["Sanghyeok Choi", "Sarthak Mittal", "V\u00edctor Elvira", "Jinkyoo Park", "Nikolay Malkin"], "title": "Reinforced sequential Monte Carlo for amortised sampling", "comment": "code: https://github.com/hyeok9855/gfn-smc-jax", "summary": "This paper proposes a synergy of amortised and particle-based methods for\nsampling from distributions defined by unnormalised density functions. We state\na connection between sequential Monte Carlo (SMC) and neural sequential\nsamplers trained by maximum-entropy reinforcement learning (MaxEnt RL), wherein\nlearnt sampling policies and value functions define proposal kernels and twist\nfunctions. Exploiting this connection, we introduce an off-policy RL training\nprocedure for the sampler that uses samples from SMC -- using the learnt\nsampler as a proposal -- as a behaviour policy that better explores the target\ndistribution. We describe techniques for stable joint training of proposals and\ntwist functions and an adaptive weight tempering scheme to reduce training\nsignal variance. Furthermore, building upon past attempts to use experience\nreplay to guide the training of neural samplers, we derive a way to combine\nhistorical samples with annealed importance sampling weights within a replay\nbuffer. On synthetic multi-modal targets (in both continuous and discrete\nspaces) and the Boltzmann distribution of alanine dipeptide conformations, we\ndemonstrate improvements in approximating the true distribution as well as\ntraining stability compared to both amortised and Monte Carlo methods.", "AI": {"tldr": "\u7ed3\u5408\u644a\u9500\u65b9\u6cd5\u548c\u57fa\u4e8e\u7c92\u5b50\u7684\u65b9\u6cd5\u8fdb\u884c\u975e\u5f52\u4e00\u5316\u5bc6\u5ea6\u51fd\u6570\u5206\u5e03\u91c7\u6837\uff0c\u901a\u8fc7\u8fde\u63a5SMC\u548c\u795e\u7ecf\u5e8f\u5217\u91c7\u6837\u5668\uff0c\u5f15\u5165\u79bb\u7b56\u7565RL\u8bad\u7ec3\uff0c\u5728\u5408\u6210\u591a\u6a21\u6001\u76ee\u6807\u548c\u4e19\u6c28\u9178\u4e8c\u80bd\u6784\u8c61\u7684\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u4e0a\u5c55\u793a\u4e86\u6539\u8fdb\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u4ece\u975e\u5f52\u4e00\u5316\u5bc6\u5ea6\u51fd\u6570\u5b9a\u4e49\u7684\u5206\u5e03\u4e2d\u91c7\u6837\u7684\u6311\u6218\uff0c\u7ed3\u5408\u644a\u9500\u65b9\u6cd5\u548c\u7c92\u5b50\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u63d0\u9ad8\u91c7\u6837\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "method": "\u5efa\u7acbSMC\u4e0e\u795e\u7ecf\u5e8f\u5217\u91c7\u6837\u5668\u4e4b\u95f4\u7684\u8fde\u63a5\uff0c\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u91c7\u6837\u7b56\u7565\u548c\u4ef7\u503c\u51fd\u6570\u5b9a\u4e49\u63d0\u8bae\u6838\u548c\u626d\u66f2\u51fd\u6570\uff0c\u5f15\u5165\u79bb\u7b56\u7565RL\u8bad\u7ec3\u7a0b\u5e8f\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u6743\u91cd\u9000\u706b\u65b9\u6848\u548c\u7ecf\u9a8c\u56de\u653e\u6280\u672f\u3002", "result": "\u5728\u8fde\u7eed\u548c\u79bb\u6563\u7a7a\u95f4\u7684\u5408\u6210\u591a\u6a21\u6001\u76ee\u6807\u4ee5\u53ca\u4e19\u6c28\u9178\u4e8c\u80bd\u6784\u8c61\u7684\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u4e0a\uff0c\u76f8\u6bd4\u644a\u9500\u65b9\u6cd5\u548c\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\uff0c\u5728\u8fd1\u4f3c\u771f\u5b9e\u5206\u5e03\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\u90fd\u6709\u6539\u8fdb\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u7ed3\u5408\u4e86\u644a\u9500\u91c7\u6837\u548c\u7c92\u5b50\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u795e\u7ecf\u5e8f\u5217\u91c7\u6837\u5668\u548cSMC\u7684\u534f\u540c\u4f5c\u7528\uff0c\u5728\u590d\u6742\u5206\u5e03\u91c7\u6837\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.10241", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10241", "abs": "https://arxiv.org/abs/2510.10241", "authors": ["Kangyang Luo", "Yuzhuo Bai", "Shuzheng Si", "Cheng Gao", "Zhitong Wang", "Yingli Shen", "Wenhao Li", "Zhu Liu", "Yufeng Han", "Jiayi Wu", "Cunliang Kong", "Maosong Sun"], "title": "ImCoref-CeS: An Improved Lightweight Pipeline for Coreference Resolution with LLM-based Checker-Splitter Refinement", "comment": null, "summary": "Coreference Resolution (CR) is a critical task in Natural Language Processing\n(NLP). Current research faces a key dilemma: whether to further explore the\npotential of supervised neural methods based on small language models, whose\ndetect-then-cluster pipeline still delivers top performance, or embrace the\npowerful capabilities of Large Language Models (LLMs). However, effectively\ncombining their strengths remains underexplored. To this end, we propose\n\\textbf{ImCoref-CeS}, a novel framework that integrates an enhanced supervised\nmodel with LLM-based reasoning. First, we present an improved CR method\n(\\textbf{ImCoref}) to push the performance boundaries of the supervised neural\nmethod by introducing a lightweight bridging module to enhance long-text\nencoding capability, devising a biaffine scorer to comprehensively capture\npositional information, and invoking a hybrid mention regularization to improve\ntraining efficiency. Importantly, we employ an LLM acting as a multi-role\nChecker-Splitter agent to validate candidate mentions (filtering out invalid\nones) and coreference results (splitting erroneous clusters) predicted by\nImCoref. Extensive experiments demonstrate the effectiveness of ImCoref-CeS,\nwhich achieves superior performance compared to existing state-of-the-art\n(SOTA) methods.", "AI": {"tldr": "\u63d0\u51faImCoref-CeS\u6846\u67b6\uff0c\u5c06\u6539\u8fdb\u7684\u76d1\u7763\u795e\u7ecf\u6a21\u578b\u4e0eLLM\u63a8\u7406\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u5728\u5171\u6307\u6d88\u89e3\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u89e3\u51b3\u5f53\u524d\u5171\u6307\u6d88\u89e3\u7814\u7a76\u7684\u56f0\u5883\uff1a\u662f\u7ee7\u7eed\u63a2\u7d22\u57fa\u4e8e\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u76d1\u7763\u795e\u7ecf\u65b9\u6cd5\u6f5c\u529b\uff0c\u8fd8\u662f\u62e5\u62b1\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u5e76\u6709\u6548\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf", "method": "1. \u6539\u8fdb\u7684\u76d1\u7763\u6a21\u578bImCoref\uff1a\u5f15\u5165\u8f7b\u91cf\u7ea7\u6865\u63a5\u6a21\u5757\u589e\u5f3a\u957f\u6587\u672c\u7f16\u7801\uff0c\u8bbe\u8ba1\u53cc\u4eff\u5c04\u8bc4\u5206\u5668\u5168\u9762\u6355\u6349\u4f4d\u7f6e\u4fe1\u606f\uff0c\u91c7\u7528\u6df7\u5408\u63d0\u53ca\u6b63\u5219\u5316\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff1b2. LLM\u4f5c\u4e3a\u591a\u89d2\u8272\u68c0\u67e5\u5668-\u5206\u5272\u5668\u4ee3\u7406\uff1a\u9a8c\u8bc1\u5019\u9009\u63d0\u53ca\u548c\u5171\u6307\u7ed3\u679c\uff0c\u8fc7\u6ee4\u65e0\u6548\u63d0\u53ca\u5e76\u5206\u5272\u9519\u8bef\u805a\u7c7b", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660eImCoref-CeS\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u8d8a\u7684\u6027\u80fd", "conclusion": "ImCoref-CeS\u6210\u529f\u6574\u5408\u4e86\u76d1\u7763\u795e\u7ecf\u65b9\u6cd5\u548cLLM\u7684\u4f18\u52bf\uff0c\u4e3a\u5171\u6307\u6d88\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6df7\u5408\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.10976", "categories": ["cs.AI", "68T05", "I.2.10"], "pdf": "https://arxiv.org/pdf/2510.10976", "abs": "https://arxiv.org/abs/2510.10976", "authors": ["Wentao Wang", "Heqing Zou", "Tianze Luo", "Rui Huang", "Yutian Zhao", "Zhuochen Wang", "Hansheng Zhang", "Chengwei Qin", "Yan Wang", "Lin Zhao", "Huaijian Zhang"], "title": "Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with Relation Graph", "comment": null, "summary": "Recent progress in Multimodal Large Language Models (MLLMs) has demonstrated\nstrong semantic understanding capabilities, but struggles to perform precise\nspatio-temporal understanding. Existing spatio-temporal methods primarily focus\non the video itself, while overlooking the physical information within the\nvideo, such as multi-object layouts and motion. Such limitations restrict the\nuse of MLLMs in downstream applications that demand high precision, including\nembodied intelligence and VR. To address this issue, we present Video-STR, a\nnovel graph-based reinforcement method for precise Video Spatio-Temporal\nReasoning. Building upon the capacity of Reinforcement Learning with Verifiable\nReward (RLVR) to improve model abilities, we introduce a reasoning mechanism\nusing graph-based Group Relative Policy Optimization (GRPO) method to guide the\nmodel in inferring the underlying spatio-temporal topology of scenarios during\nthe thinking process. To resolve the lack of spatio-temporal training data, we\nconstruct the STV-205k dataset with 205k question-answering pairs, covering\ndynamic multi-object scenes in both indoor and outdoor environments, to support\nthe model training. Experiments show that Video-STR achieves state-of-the-art\nresults on various benchmarks, outperforming the base model by 13% on\nSTI-Bench, and demonstrating the effectiveness of our approach and dataset.\nCode, model, and data will be released.", "AI": {"tldr": "Video-STR\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u578b\u89c6\u9891\u65f6\u7a7a\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7GRPO\u4f18\u5316\u7b56\u7565\u6307\u5bfc\u6a21\u578b\u63a8\u65ad\u573a\u666f\u7684\u65f6\u7a7a\u62d3\u6251\u7ed3\u6784\uff0c\u5e76\u5728STV-205k\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7cbe\u786e\u7684\u65f6\u7a7a\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5ffd\u7565\u4e86\u89c6\u9891\u4e2d\u7684\u7269\u7406\u4fe1\u606f\uff08\u5982\u591a\u7269\u4f53\u5e03\u5c40\u548c\u8fd0\u52a8\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u5177\u8eab\u667a\u80fd\u548cVR\u7b49\u9ad8\u7cbe\u5ea6\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5Video-STR\uff0c\u91c7\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60(RLVR)\u80fd\u529b\uff0c\u5f15\u5165\u57fa\u4e8e\u56fe\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316(GRPO)\u63a8\u7406\u673a\u5236\uff0c\u6307\u5bfc\u6a21\u578b\u5728\u601d\u8003\u8fc7\u7a0b\u4e2d\u63a8\u65ad\u573a\u666f\u7684\u65f6\u7a7a\u62d3\u6251\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVideo-STR\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5728STI-Bench\u4e0a\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u534713%\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "Video-STR\u901a\u8fc7\u56fe\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7cbe\u786e\u65f6\u7a7a\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u9ad8\u7cbe\u5ea6\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10252", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10252", "abs": "https://arxiv.org/abs/2510.10252", "authors": ["Samir Abdaljalil", "Erchin Serpedin", "Khalid Qaraqe", "Hasan Kurban"], "title": "Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models", "comment": null, "summary": "Large language models (LLMs) often generate reasoning traces that appear\ncoherent but rest on unsupported assumptions, leading to hallucinated\nconclusions. Prior work mainly addresses factual hallucinations or relies on\npost-hoc verification, leaving reasoning-induced hallucinations largely\nunaddressed. We propose Audit-of-Understanding (AoU), a framework that\nconstrains inference to validated premises through three phases: (1)\ndecomposing a query into candidate assumptions, (2) auditing their support, and\n(3) conditioning inference only on the validated subset. Formally, AoU is\n\\emph{posterior-constrained inference}, connecting to selective prediction and\nrejection learning. Our contributions are threefold: (i) theoretical guarantees\nunder perfect validation, (ii) excess-risk bounds under imperfect audits, and\n(iii) tractability analysis. Empirically, AoU improves both accuracy and\nfaithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on\nGSM8K, +45% on MultiArith, and consistent +20--28% improvements on SVAMP over\nChain-of-Thought, Self-Consistency, and CoT-Decoding. Code is available at\nhttps://anonymous.4open.science/r/audit-of-understanding-E28B.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5ba1\u8ba1\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u9a8c\u8bc1\u524d\u63d0\u6761\u4ef6\u6765\u7ea6\u675f\u63a8\u7406\u8fc7\u7a0b\uff0c\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u672a\u7ecf\u9a8c\u8bc1\u5047\u8bbe\u4ea7\u751f\u63a8\u7406\u5e7b\u89c9\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u751f\u6210\u770b\u4f3c\u8fde\u8d2f\u4f46\u57fa\u4e8e\u672a\u7ecf\u9a8c\u8bc1\u5047\u8bbe\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5bfc\u81f4\u4ea7\u751f\u5e7b\u89c9\u7ed3\u8bba\u3002\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u4e8b\u5b9e\u6027\u5e7b\u89c9\u6216\u4f9d\u8d56\u4e8b\u540e\u9a8c\u8bc1\uff0c\u5bf9\u63a8\u7406\u5f15\u8d77\u7684\u5e7b\u89c9\u95ee\u9898\u5173\u6ce8\u4e0d\u8db3\u3002", "method": "\u5ba1\u8ba1\u7406\u89e3\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a(1)\u5c06\u67e5\u8be2\u5206\u89e3\u4e3a\u5019\u9009\u5047\u8bbe\uff0c(2)\u5ba1\u8ba1\u8fd9\u4e9b\u5047\u8bbe\u7684\u652f\u6301\u5ea6\uff0c(3)\u4ec5\u57fa\u4e8e\u5df2\u9a8c\u8bc1\u5b50\u96c6\u8fdb\u884c\u63a8\u7406\u3002\u8be5\u65b9\u6cd5\u5728\u5f62\u5f0f\u4e0a\u5c5e\u4e8e\u540e\u9a8c\u7ea6\u675f\u63a8\u7406\uff0c\u4e0e\u9009\u62e9\u6027\u9884\u6d4b\u548c\u62d2\u7edd\u5b66\u4e60\u76f8\u5173\u3002", "result": "\u5728GSM8K\u3001MultiArith\u548cSVAMP\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u5fe0\u5b9e\u6027\uff0c\u76f8\u6bd4Chain-of-Thought\u3001Self-Consistency\u548cCoT-Decoding\u65b9\u6cd5\uff0c\u5728GSM8K\u4e0a\u63d0\u5347\u8fbe+30%\uff0cMultiArith\u4e0a+45%\uff0cSVAMP\u4e0a\u6301\u7eed\u63d0\u534720-28%\u3002", "conclusion": "\u5ba1\u8ba1\u7406\u89e3\u6846\u67b6\u901a\u8fc7\u7ea6\u675f\u63a8\u7406\u4e8e\u5df2\u9a8c\u8bc1\u524d\u63d0\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u63a8\u7406\u5f15\u8d77\u7684\u5e7b\u89c9\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u5c42\u9762\u90fd\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.09845", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09845", "abs": "https://arxiv.org/abs/2510.09845", "authors": ["Nicholas LaHaye", "Thilanka Munashinge", "Hugo Lee", "Xiaohua Pan", "Gonzalo Gonzalez Abad", "Hazem Mahmoud", "Jennifer Wei"], "title": "Harnessing Self-Supervised Deep Learning and Geostationary Remote Sensing for Advancing Wildfire and Associated Air Quality Monitoring: Improved Smoke and Fire Front Masking using GOES and TEMPO Radiance Data", "comment": "https://2025.ieeeigarss.org/view_paper.php?PaperNum=6389&SessionID=1611", "summary": "This work demonstrates the possibilities for improving wildfire and air\nquality management in the western United States by leveraging the unprecedented\nhourly data from NASA's TEMPO satellite mission and advances in self-supervised\ndeep learning. Here we demonstrate the efficacy of deep learning for mapping\nthe near real-time hourly spread of wildfire fronts and smoke plumes using an\ninnovative self-supervised deep learning-system: successfully distinguishing\nsmoke plumes from clouds using GOES-18 and TEMPO data, strong agreement across\nthe smoke and fire masks generated from different sensing modalities as well as\nsignificant improvement over operational products for the same cases.", "AI": {"tldr": "\u5229\u7528NASA TEMPO\u536b\u661f\u4efb\u52a1\u7684\u5c0f\u65f6\u6570\u636e\u548c\u81ea\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6539\u8fdb\u7f8e\u56fd\u897f\u90e8\u91ce\u706b\u548c\u7a7a\u6c14\u8d28\u91cf\u7ba1\u7406\uff0c\u5b9e\u73b0\u4e86\u91ce\u706b\u524d\u6cbf\u548c\u70df\u96fe\u7fbd\u6d41\u7684\u8fd1\u5b9e\u65f6\u5c0f\u65f6\u7ea7\u76d1\u6d4b", "motivation": "\u5229\u7528TEMPO\u536b\u661f\u524d\u6240\u672a\u6709\u7684\u5c0f\u65f6\u6570\u636e\u548c\u81ea\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u6539\u8fdb\u7f8e\u56fd\u897f\u90e8\u91ce\u706b\u548c\u7a7a\u6c14\u8d28\u91cf\u7ba1\u7406\u7684\u53ef\u80fd\u6027", "method": "\u521b\u65b0\u7684\u81ea\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u4f7f\u7528GOES-18\u548cTEMPO\u6570\u636e\u6210\u529f\u533a\u5206\u70df\u96fe\u7fbd\u6d41\u4e0e\u4e91\u5c42", "result": "\u4e0d\u540c\u4f20\u611f\u6a21\u5f0f\u751f\u6210\u7684\u70df\u96fe\u548c\u706b\u707e\u63a9\u819c\u4e4b\u95f4\u5177\u6709\u5f3a\u4e00\u81f4\u6027\uff0c\u76f8\u6bd4\u76f8\u540c\u6848\u4f8b\u7684\u64cd\u4f5c\u4ea7\u54c1\u6709\u663e\u8457\u6539\u8fdb", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u5728\u5229\u7528\u536b\u661f\u6570\u636e\u8fd1\u5b9e\u65f6\u76d1\u6d4b\u91ce\u706b\u8513\u5ef6\u548c\u70df\u96fe\u6269\u6563\u65b9\u9762\u5177\u6709\u663e\u8457\u6548\u679c"}}
{"id": "2510.10977", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10977", "abs": "https://arxiv.org/abs/2510.10977", "authors": ["Taiqiang Wu", "Runming Yang", "Tao Liu", "Jiahao Wang", "Ngai Wong"], "title": "Revisiting Model Interpolation for Efficient Reasoning", "comment": "14 pages, 6 figures, 7 tables. Working in progress", "summary": "Model merging, typically on Instruct and Thinking models, has shown\nremarkable performance for efficient reasoning. In this paper, we\nsystematically revisit the simplest merging method that interpolates two\nweights directly. Particularly, we observe that model interpolation follows a\nthree-stage evolutionary paradigm with distinct behaviors on the reasoning\ntrajectory. These dynamics provide a principled guide for navigating the\nperformance-cost trade-off. Empirical results demonstrate that a strategically\ninterpolated model surprisingly surpasses sophisticated model merging baselines\non both efficiency and effectiveness. We further validate our findings with\nextensive ablation studies on model layers, modules, and decoding strategies.\nUltimately, this work demystifies model interpolation and offers a practical\nframework for crafting models with precisely targeted reasoning capabilities.\nCode is available at \\href{https://github.com/wutaiqiang/MI}{Github}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u6700\u7b80\u5355\u7684\u6a21\u578b\u6743\u91cd\u63d2\u503c\u65b9\u6cd5\uff0c\u53d1\u73b0\u6a21\u578b\u63d2\u503c\u9075\u5faa\u4e09\u9636\u6bb5\u6f14\u5316\u8303\u5f0f\uff0c\u80fd\u591f\u8d85\u8d8a\u590d\u6742\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\uff0c\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u90fd\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u6700\u7b80\u5355\u7684\u6a21\u578b\u6743\u91cd\u76f4\u63a5\u63d2\u503c\u65b9\u6cd5\uff0c\u63a2\u7d22\u5176\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u548c\u52a8\u6001\u7279\u6027\u3002", "method": "\u4f7f\u7528\u6a21\u578b\u6743\u91cd\u76f4\u63a5\u63d2\u503c\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u5728\u63a8\u7406\u8f68\u8ff9\u4e0a\u7684\u4e09\u9636\u6bb5\u6f14\u5316\u884c\u4e3a\uff0c\u5e76\u57fa\u4e8e\u6b64\u5236\u5b9a\u6027\u80fd-\u6210\u672c\u6743\u8861\u7684\u6307\u5bfc\u539f\u5219\u3002", "result": "\u7b56\u7565\u6027\u63d2\u503c\u7684\u6a21\u578b\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u90fd\u8d85\u8d8a\u4e86\u590d\u6742\u7684\u6a21\u578b\u878d\u5408\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u53d1\u73b0\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63ed\u793a\u4e86\u6a21\u578b\u63d2\u503c\u7684\u673a\u5236\uff0c\u4e3a\u6784\u5efa\u5177\u6709\u7cbe\u786e\u76ee\u6807\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2510.10265", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10265", "abs": "https://arxiv.org/abs/2510.10265", "authors": ["Liang Lin", "Miao Yu", "Moayad Aloqaily", "Zhenhong Zhou", "Kun Wang", "Linsey Pang", "Prakhar Mehrotra", "Qingsong Wen"], "title": "Backdoor Collapse: Eliminating Unknown Threats via Known Backdoor Aggregation in Language Models", "comment": null, "summary": "Backdoor attacks are a significant threat to large language models (LLMs),\noften embedded via public checkpoints, yet existing defenses rely on\nimpractical assumptions about trigger settings. To address this challenge, we\npropose \\ourmethod, a defense framework that requires no prior knowledge of\ntrigger settings. \\ourmethod is based on the key observation that when\ndeliberately injecting known backdoors into an already-compromised model, both\nexisting unknown and newly injected backdoors aggregate in the representation\nspace. \\ourmethod leverages this through a two-stage process: \\textbf{first},\naggregating backdoor representations by injecting known triggers, and\n\\textbf{then}, performing recovery fine-tuning to restore benign outputs.\nExtensive experiments across multiple LLM architectures demonstrate that: (I)\n\\ourmethod reduces the average Attack Success Rate to 4.41\\% across multiple\nbenchmarks, outperforming existing baselines by 28.1\\%$\\sim$69.3\\%$\\uparrow$.\n(II) Clean accuracy and utility are preserved within 0.5\\% of the original\nmodel, ensuring negligible impact on legitimate tasks. (III) The defense\ngeneralizes across different types of backdoors, confirming its robustness in\npractical deployment scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u9884\u77e5\u89e6\u53d1\u5668\u8bbe\u7f6e\u7684LLM\u540e\u95e8\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u5165\u5df2\u77e5\u540e\u95e8\u6765\u805a\u96c6\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u672a\u77e5\u540e\u95e8\uff0c\u7136\u540e\u8fdb\u884c\u6062\u590d\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u540e\u95e8\u9632\u5fa1\u65b9\u6cd5\u4f9d\u8d56\u4e0d\u5207\u5b9e\u9645\u7684\u89e6\u53d1\u5668\u8bbe\u7f6e\u5047\u8bbe\uff0c\u800c\u516c\u5f00\u68c0\u67e5\u70b9\u4e2d\u7684\u540e\u95e8\u653b\u51fb\u5bf9LLM\u6784\u6210\u91cd\u5927\u5a01\u80c1\u3002", "method": "\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff1a\u9996\u5148\u901a\u8fc7\u6ce8\u5165\u5df2\u77e5\u89e6\u53d1\u5668\u805a\u96c6\u540e\u95e8\u8868\u793a\uff0c\u7136\u540e\u8fdb\u884c\u6062\u590d\u5fae\u8c03\u4ee5\u6062\u590d\u826f\u6027\u8f93\u51fa\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c06\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\u964d\u81f34.41%\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf28.1%~69.3%\uff1b\u5e72\u51c0\u51c6\u786e\u7387\u548c\u5b9e\u7528\u6027\u4fdd\u6301\u5728\u539f\u59cb\u6a21\u578b\u76840.5%\u4ee5\u5185\u3002", "conclusion": "\u8be5\u9632\u5fa1\u65b9\u6cd5\u5728\u4e0d\u540c\u7c7b\u578b\u540e\u95e8\u4e0a\u5177\u6709\u901a\u7528\u6027\uff0c\u8bc1\u5b9e\u4e86\u5728\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.09846", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09846", "abs": "https://arxiv.org/abs/2510.09846", "authors": ["Zhenjiang Fan", "Zengyi Qin", "Yuanning Zheng", "Bo Xiong", "Summer Han"], "title": "CALM: A Causal Analysis Language Model for Tabular Data in Complex Systems with Local Scores, Conditional Independence Tests, and Relation Attributes", "comment": null, "summary": "Causal discovery from observational data is fundamental to scientific fields\nlike biology, where controlled experiments are often impractical. However,\nexisting methods, including constraint-based (e.g., PC, causalMGM) and\nscore-based approaches (e.g., NOTEARS), face significant limitations. These\ninclude an inability to resolve causal direction, restrictions to linear\nassociations, sensitivity to violations of the faithfulness assumption, and\ninefficiency in searching vast hypothesis spaces. While large language models\n(LLMs) offer powerful reasoning capabilities, their application is hindered by\na fundamental discrepancy: they are designed for text, while most causal data\nis tabular. To address these challenges, we introduce CALM, a novel causal\nanalysis language model specifically designed for tabular data in complex\nsystems. CALM leverages a Mamba-based architecture to classify causal patterns\nfrom pairwise variable relationships. It integrates a comprehensive suite of\nevidence, including local causal scores, conditional independence tests, and\nrelational attributes, to capture a wide spectrum of linear, nonlinear, and\nconditional causal mechanisms. Trained on a diverse corpus of synthetic data\n(from linear, mixed, and nonlinear models) and 10 real-world biological\ndatasets with rigorously validated causal relationships, our model ensures\nrobustness and generalizability. Empirical evaluation demonstrates that CALM\nsignificantly outperforms existing methods in both simulation studies,\nachieving over 91% accuracy, and in a real-world application identifying causal\nfactors in Hepatitis C virus progression. This work represents a significant\nstep towards accurate and generalizable causal discovery by successfully\nadapting the pattern recognition capabilities of language models to the\nintricacies of tabular data.", "AI": {"tldr": "CALM\u662f\u4e00\u79cd\u4e13\u95e8\u4e3a\u8868\u683c\u6570\u636e\u8bbe\u8ba1\u7684\u56e0\u679c\u5206\u6790\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7Mamba\u67b6\u6784\u8bc6\u522b\u53d8\u91cf\u95f4\u7684\u56e0\u679c\u6a21\u5f0f\uff0c\u5728\u6a21\u62df\u7814\u7a76\u4e2d\u51c6\u786e\u7387\u8d85\u8fc791%\uff0c\u5728\u771f\u5b9e\u751f\u7269\u6570\u636e\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5b58\u5728\u65e0\u6cd5\u786e\u5b9a\u56e0\u679c\u65b9\u5411\u3001\u4ec5\u9650\u4e8e\u7ebf\u6027\u5173\u8054\u3001\u5bf9\u5fe0\u5b9e\u6027\u5047\u8bbe\u654f\u611f\u4ee5\u53ca\u641c\u7d22\u6548\u7387\u4f4e\u7b49\u95ee\u9898\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u5907\u5f3a\u5927\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4e0d\u9002\u5408\u5904\u7406\u8868\u683c\u6570\u636e\u3002", "method": "\u91c7\u7528\u57fa\u4e8eMamba\u7684\u67b6\u6784\uff0c\u6574\u5408\u5c40\u90e8\u56e0\u679c\u8bc4\u5206\u3001\u6761\u4ef6\u72ec\u7acb\u6027\u68c0\u9a8c\u548c\u5173\u7cfb\u5c5e\u6027\u7b49\u591a\u79cd\u8bc1\u636e\uff0c\u8bad\u7ec3\u6570\u636e\u5305\u62ec\u5408\u6210\u6570\u636e\uff08\u7ebf\u6027\u3001\u6df7\u5408\u3001\u975e\u7ebf\u6027\u6a21\u578b\uff09\u548c10\u4e2a\u771f\u5b9e\u751f\u7269\u6570\u636e\u96c6\u3002", "result": "CALM\u5728\u6a21\u62df\u7814\u7a76\u4e2d\u51c6\u786e\u7387\u8d85\u8fc791%\uff0c\u5728\u8bc6\u522b\u4e19\u578b\u809d\u708e\u75c5\u6bd2\u8fdb\u5c55\u7684\u56e0\u679c\u56e0\u7d20\u7b49\u771f\u5b9e\u5e94\u7528\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u6210\u529f\u5c06\u8bed\u8a00\u6a21\u578b\u7684\u6a21\u5f0f\u8bc6\u522b\u80fd\u529b\u5e94\u7528\u4e8e\u8868\u683c\u6570\u636e\u7684\u590d\u6742\u6027\uff0c\u4e3a\u51c6\u786e\u4e14\u53ef\u6cdb\u5316\u7684\u56e0\u679c\u53d1\u73b0\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2510.11003", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11003", "abs": "https://arxiv.org/abs/2510.11003", "authors": ["Takuma Fujiu", "Sho Okazaki", "Kohei Kaminishi", "Yuji Nakata", "Shota Hamamoto", "Kenshin Yokose", "Tatsunori Hara", "Yasushi Umeda", "Jun Ota"], "title": "FBS Model-based Maintenance Record Accumulation for Failure-Cause Inference in Manufacturing Systems", "comment": null, "summary": "In manufacturing systems, identifying the causes of failures is crucial for\nmaintaining and improving production efficiency. In knowledge-based\nfailure-cause inference, it is important that the knowledge base (1) explicitly\nstructures knowledge about the target system and about failures, and (2)\ncontains sufficiently long causal chains of failures. In this study, we\nconstructed Diagnostic Knowledge Ontology and proposed a\nFunction-Behavior-Structure (FBS) model-based maintenance-record accumulation\nmethod based on it. Failure-cause inference using the maintenance records\naccumulated by the proposed method showed better agreement with the set of\ncandidate causes enumerated by experts, especially in difficult cases where the\nnumber of related cases is small and the vocabulary used differs. In the\nfuture, it will be necessary to develop inference methods tailored to these\nmaintenance records, build a user interface, and carry out validation on larger\nand more diverse systems. Additionally, this approach leverages the\nunderstanding and knowledge of the target in the design phase to support\nknowledge accumulation and problem solving during the maintenance phase, and it\nis expected to become a foundation for knowledge sharing across the entire\nengineering chain in the future.", "AI": {"tldr": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u8bca\u65ad\u77e5\u8bc6\u672c\u4f53\u8bba\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u529f\u80fd-\u884c\u4e3a-\u7ed3\u6784(FBS)\u6a21\u578b\u7684\u7ef4\u62a4\u8bb0\u5f55\u79ef\u7d2f\u65b9\u6cd5\uff0c\u5728\u6545\u969c\u539f\u56e0\u63a8\u65ad\u65b9\u9762\u53d6\u5f97\u4e86\u66f4\u597d\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u76f8\u5173\u6848\u4f8b\u5c11\u3001\u8bcd\u6c47\u4e0d\u540c\u7684\u56f0\u96be\u60c5\u51b5\u4e0b\u3002", "motivation": "\u5728\u5236\u9020\u7cfb\u7edf\u4e2d\uff0c\u8bc6\u522b\u6545\u969c\u539f\u56e0\u5bf9\u7ef4\u6301\u548c\u63d0\u9ad8\u751f\u4ea7\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u77e5\u8bc6\u5e93\u9700\u8981\u660e\u786e\u7ed3\u6784\u5316\u76ee\u6807\u7cfb\u7edf\u548c\u6545\u969c\u77e5\u8bc6\uff0c\u5e76\u5305\u542b\u8db3\u591f\u957f\u7684\u6545\u969c\u56e0\u679c\u94fe\u3002", "method": "\u6784\u5efa\u8bca\u65ad\u77e5\u8bc6\u672c\u4f53\u8bba\uff0c\u63d0\u51fa\u57fa\u4e8e\u529f\u80fd-\u884c\u4e3a-\u7ed3\u6784(FBS)\u6a21\u578b\u7684\u7ef4\u62a4\u8bb0\u5f55\u79ef\u7d2f\u65b9\u6cd5\u3002", "result": "\u4f7f\u7528\u8be5\u65b9\u6cd5\u79ef\u7d2f\u7684\u7ef4\u62a4\u8bb0\u5f55\u8fdb\u884c\u6545\u969c\u539f\u56e0\u63a8\u65ad\uff0c\u4e0e\u4e13\u5bb6\u5217\u4e3e\u7684\u5019\u9009\u539f\u56e0\u96c6\u6709\u66f4\u597d\u7684\u4e00\u81f4\u6027\uff0c\u7279\u522b\u662f\u5728\u76f8\u5173\u6848\u4f8b\u5c11\u3001\u8bcd\u6c47\u4e0d\u540c\u7684\u56f0\u96be\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5229\u7528\u8bbe\u8ba1\u9636\u6bb5\u5bf9\u76ee\u6807\u7684\u7406\u89e3\u548c\u77e5\u8bc6\u6765\u652f\u6301\u7ef4\u62a4\u9636\u6bb5\u7684\u77e5\u8bc6\u79ef\u7d2f\u548c\u95ee\u9898\u89e3\u51b3\uff0c\u6709\u671b\u6210\u4e3a\u672a\u6765\u6574\u4e2a\u5de5\u7a0b\u94fe\u77e5\u8bc6\u5171\u4eab\u7684\u57fa\u7840\u3002\u9700\u8981\u5f00\u53d1\u9488\u5bf9\u8fd9\u4e9b\u7ef4\u62a4\u8bb0\u5f55\u7684\u63a8\u65ad\u65b9\u6cd5\u3001\u6784\u5efa\u7528\u6237\u754c\u9762\uff0c\u5e76\u5728\u66f4\u5927\u66f4\u591a\u6837\u5316\u7684\u7cfb\u7edf\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002"}}
{"id": "2510.10280", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10280", "abs": "https://arxiv.org/abs/2510.10280", "authors": ["Yihong Liu", "Mingyang Wang", "Fran\u00e7ois Yvon", "Hinrich Sch\u00fctze"], "title": "On the Entity-Level Alignment in Crosslingual Consistency", "comment": "preprint", "summary": "Multilingual large language models (LLMs) are expected to recall factual\nknowledge consistently across languages. However, the factors that give rise to\nsuch crosslingual consistency -- and its frequent failure -- remain poorly\nunderstood. In this work, we hypothesize that these inconsistencies may arise\nfrom failures in entity alignment, the process of mapping subject and object\nentities into a shared conceptual space across languages. To test this, we\nassess alignment through entity-level (subject and object) translation tasks,\nand find that consistency is strongly correlated with alignment across all\nstudied models, with misalignment of subjects or objects frequently resulting\nin inconsistencies. Building on this insight, we propose SubSub and SubInj, two\neffective methods that integrate English translations of subjects into prompts\nacross languages, leading to substantial gains in both factual recall accuracy\nand consistency. Finally, our mechanistic analysis reveals that these\ninterventions reinforce the entity representation alignment in the conceptual\nspace through model's internal pivot-language processing, offering effective\nand practical strategies for improving multilingual factual prediction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u4e8b\u5b9e\u77e5\u8bc6\u56de\u5fc6\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u5b9e\u4f53\u5bf9\u9f50\u5931\u8d25\u662f\u4e3b\u8981\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u901a\u8fc7\u6574\u5408\u82f1\u6587\u5b9e\u4f53\u7ffb\u8bd1\u6765\u63d0\u5347\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u4e8b\u5b9e\u77e5\u8bc6\u56de\u5fc6\u4e2d\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u4f46\u5bfc\u81f4\u8fd9\u79cd\u4e0d\u4e00\u81f4\u6027\u7684\u56e0\u7d20\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u4f5c\u8005\u5047\u8bbe\u8fd9\u53ef\u80fd\u6e90\u4e8e\u5b9e\u4f53\u5bf9\u9f50\u7684\u5931\u8d25\uff0c\u5373\u8de8\u8bed\u8a00\u5171\u4eab\u6982\u5ff5\u7a7a\u95f4\u4e2d\u4e3b\u8bed\u548c\u5bbe\u8bed\u5b9e\u4f53\u7684\u6620\u5c04\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u5b9e\u4f53\u7ea7\u7ffb\u8bd1\u4efb\u52a1\u8bc4\u4f30\u5bf9\u9f50\u6548\u679c\uff0c\u53d1\u73b0\u4e00\u81f4\u6027\u4e0e\u5bf9\u9f50\u5f3a\u76f8\u5173\u3002\u63d0\u51faSubSub\u548cSubInj\u4e24\u79cd\u65b9\u6cd5\uff0c\u5728\u8de8\u8bed\u8a00\u63d0\u793a\u4e2d\u6574\u5408\u82f1\u6587\u4e3b\u8bed\u7ffb\u8bd1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u4e8b\u5b9e\u56de\u5fc6\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u65b9\u9762\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u901a\u8fc7\u6a21\u578b\u5185\u90e8\u67a2\u7ebd\u8bed\u8a00\u5904\u7406\u5f3a\u5316\u4e86\u5b9e\u4f53\u8868\u793a\u5bf9\u9f50\u3002", "conclusion": "\u5b9e\u4f53\u5bf9\u9f50\u662f\u591a\u8bed\u8a00\u4e8b\u5b9e\u9884\u6d4b\u4e00\u81f4\u6027\u7684\u5173\u952e\u56e0\u7d20\uff0c\u63d0\u51fa\u7684\u5e72\u9884\u7b56\u7565\u901a\u8fc7\u5f3a\u5316\u6982\u5ff5\u7a7a\u95f4\u4e2d\u7684\u5b9e\u4f53\u8868\u793a\u5bf9\u9f50\uff0c\u4e3a\u6539\u8fdb\u591a\u8bed\u8a00\u4e8b\u5b9e\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u5b9e\u7528\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.09852", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09852", "abs": "https://arxiv.org/abs/2510.09852", "authors": ["Shivam Patel", "Neharika Jali", "Ankur Mallick", "Gauri Joshi"], "title": "ProxRouter: Proximity-Weighted LLM Query Routing for Improved Robustness to Outliers", "comment": null, "summary": "Large language model (LLM) query routers are critical to modern AI platforms\nas they seek to improve efficiency by assigning inference queries to accurate,\nyet low-cost models. Parametric routers typically use trained neural networks\nfor LLM selection but suffer from retraining and maintenance overheads.\nNonparametric routers are training-free, instead estimating LLM accuracy and\ncost via similarity between encodings of the input query and training set\nqueries. However, like their parametric counterparts, nonparametric routers\nstruggle to generalize to outlier queries, an issue exacerbated by limited\ndiversity in training sets which are costly to expand and difficult to keep\ncurrent with ever-evolving use cases. We propose ProxRouter, which applies an\nexponentially tilted aggregation mechanism to balance bias and variance in\nnonparametric routers, improving their robustness to outliers. Experiments show\nProxRouter enhances outlier routing while preserving inlier performance with\nminimal overhead.", "AI": {"tldr": "ProxRouter\u662f\u4e00\u79cd\u975e\u53c2\u6570\u5316\u67e5\u8be2\u8def\u7531\u5668\uff0c\u901a\u8fc7\u6307\u6570\u503e\u659c\u805a\u5408\u673a\u5236\u5e73\u8861\u504f\u5dee\u548c\u65b9\u5dee\uff0c\u63d0\u9ad8\u4e86\u5bf9\u5f02\u5e38\u67e5\u8be2\u7684\u8def\u7531\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6b63\u5e38\u67e5\u8be2\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u53c2\u6570\u5316\u8def\u7531\u5668\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u548c\u7ef4\u62a4\u5f00\u9500\uff0c\u975e\u53c2\u6570\u5316\u8def\u7531\u5668\u867d\u7136\u514d\u8bad\u7ec3\u4f46\u96be\u4ee5\u6cdb\u5316\u5230\u5f02\u5e38\u67e5\u8be2\uff0c\u4e14\u8bad\u7ec3\u96c6\u591a\u6837\u6027\u6709\u9650\u4e14\u66f4\u65b0\u6210\u672c\u9ad8\u3002", "method": "\u91c7\u7528\u6307\u6570\u503e\u659c\u805a\u5408\u673a\u5236\u6765\u5e73\u8861\u975e\u53c2\u6570\u5316\u8def\u7531\u5668\u4e2d\u7684\u504f\u5dee\u548c\u65b9\u5dee\uff0c\u63d0\u9ad8\u5bf9\u5f02\u5e38\u67e5\u8be2\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eProxRouter\u5728\u589e\u5f3a\u5f02\u5e38\u67e5\u8be2\u8def\u7531\u7684\u540c\u65f6\uff0c\u4ee5\u6700\u5c0f\u5f00\u9500\u4fdd\u6301\u4e86\u6b63\u5e38\u67e5\u8be2\u7684\u6027\u80fd\u3002", "conclusion": "ProxRouter\u901a\u8fc7\u5e73\u8861\u504f\u5dee\u548c\u65b9\u5dee\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u975e\u53c2\u6570\u5316\u8def\u7531\u5668\u5728\u5f02\u5e38\u67e5\u8be2\u4e0a\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.11079", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11079", "abs": "https://arxiv.org/abs/2510.11079", "authors": ["Andrada Iulia Prajescu", "Roberto Confalonieri"], "title": "Argumentation-Based Explainability for Legal AI: Comparative and Regulatory Perspectives", "comment": null, "summary": "Artificial Intelligence (AI) systems are increasingly deployed in legal\ncontexts, where their opacity raises significant challenges for fairness,\naccountability, and trust. The so-called ``black box problem'' undermines the\nlegitimacy of automated decision-making, as affected individuals often lack\naccess to meaningful explanations. In response, the field of Explainable AI\n(XAI) has proposed a variety of methods to enhance transparency, ranging from\nexample-based and rule-based techniques to hybrid and argumentation-based\napproaches. This paper promotes computational models of arguments and their\nrole in providing legally relevant explanations, with particular attention to\ntheir alignment with emerging regulatory frameworks such as the EU General Data\nProtection Regulation (GDPR) and the Artificial Intelligence Act (AIA). We\nanalyze the strengths and limitations of different explanation strategies,\nevaluate their applicability to legal reasoning, and highlight how\nargumentation frameworks -- by capturing the defeasible, contestable, and\nvalue-sensitive nature of law -- offer a particularly robust foundation for\nexplainable legal AI. Finally, we identify open challenges and research\ndirections, including bias mitigation, empirical validation in judicial\nsettings, and compliance with evolving ethical and legal standards, arguing\nthat computational argumentation is best positioned to meet both technical and\nnormative requirements of transparency in the law domain.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728AI\u7cfb\u7edf\u5e94\u7528\u4e8e\u6cd5\u5f8b\u9886\u57df\u65f6\uff0c\u5982\u4f55\u901a\u8fc7\u8ba1\u7b97\u8bba\u8bc1\u6a21\u578b\u63d0\u4f9b\u7b26\u5408\u6cd5\u5f8b\u8981\u6c42\u7684\u89e3\u91ca\uff0c\u4ee5\u89e3\u51b3AI\u9ed1\u7bb1\u95ee\u9898\uff0c\u786e\u4fdd\u516c\u5e73\u6027\u3001\u95ee\u8d23\u5236\u548c\u4fe1\u4efb\u3002", "motivation": "AI\u7cfb\u7edf\u5728\u6cd5\u5f8b\u73af\u5883\u4e2d\u7684\u4e0d\u900f\u660e\u6027\uff08\u9ed1\u7bb1\u95ee\u9898\uff09\u5bf9\u516c\u5e73\u6027\u3001\u95ee\u8d23\u5236\u548c\u4fe1\u4efb\u6784\u6210\u6311\u6218\uff0c\u5f71\u54cd\u4e86\u81ea\u52a8\u5316\u51b3\u7b56\u7684\u5408\u6cd5\u6027\uff0c\u9700\u8981\u5f00\u53d1\u7b26\u5408\u6cd5\u5f8b\u8981\u6c42\u7684\u89e3\u91ca\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8ba1\u7b97\u8bba\u8bc1\u6a21\u578b\uff0c\u5206\u6790\u4e0d\u540c\u89e3\u91ca\u7b56\u7565\uff08\u57fa\u4e8e\u793a\u4f8b\u3001\u89c4\u5219\u3001\u6df7\u5408\u548c\u8bba\u8bc1\u7684\u65b9\u6cd5\uff09\u7684\u4f18\u52bf\u548c\u5c40\u9650\uff0c\u7279\u522b\u5173\u6ce8\u4e0eGDPR\u548cAIA\u7b49\u6cd5\u89c4\u7684\u517c\u5bb9\u6027\u3002", "result": "\u8bba\u8bc1\u6846\u67b6\u80fd\u591f\u6355\u6349\u6cd5\u5f8b\u7684\u53ef\u5e9f\u6b62\u6027\u3001\u53ef\u4e89\u8bae\u6027\u548c\u4ef7\u503c\u654f\u611f\u6027\uff0c\u4e3a\u53ef\u89e3\u91ca\u6cd5\u5f8bAI\u63d0\u4f9b\u4e86\u7a33\u5065\u57fa\u7840\uff0c\u6bd4\u5176\u4ed6\u89e3\u91ca\u65b9\u6cd5\u66f4\u9002\u5408\u6cd5\u5f8b\u63a8\u7406\u3002", "conclusion": "\u8ba1\u7b97\u8bba\u8bc1\u65b9\u6cd5\u6700\u80fd\u6ee1\u8db3\u6cd5\u5f8b\u9886\u57df\u900f\u660e\u5ea6\u7684\u6280\u672f\u548c\u89c4\u8303\u8981\u6c42\uff0c\u4f46\u8fd8\u9700\u8981\u89e3\u51b3\u504f\u89c1\u7f13\u89e3\u3001\u53f8\u6cd5\u73af\u5883\u5b9e\u8bc1\u9a8c\u8bc1\u7b49\u5f00\u653e\u6311\u6218\u3002"}}
{"id": "2510.10293", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10293", "abs": "https://arxiv.org/abs/2510.10293", "authors": ["Hongwei Chen", "Yishu Lei", "Dan Zhang", "Bo Ke", "Danxiang Zhu", "Xuyi Chen", "Yuxiang Lu", "Zhengjie Huang", "Shikun Feng", "Jingzhou He", "Yu Sun", "Hua Wu", "Haifeng Wang"], "title": "MatryoshkaThinking: Recursive Test-Time Scaling Enables Efficient Reasoning", "comment": null, "summary": "Test-time scaling has emerged as a promising paradigm in language modeling,\nwherein additional computational resources are allocated during inference to\nenhance model performance. Recent approaches, such as DeepConf, have\ndemonstrated the efficacy of this strategy, however, they often incur\nsubstantial computational overhead to achieve competitive results. In this\nwork, we propose MatryoshkaThinking, a novel method that significantly reduces\ncomputational cost while maintaining state-of-the-art performance.\nSpecifically, MatryoshkaThinking attains a score of 99.79 on AIME2025 using\nonly 4% of the computation required by DeepConf. The core of our approach lies\nin the recursive exploitation of the model's intrinsic capabilities in\nreasoning, verification, and summarization, which collectively enhance the\nretention of correct solutions and reduce the disparity between Pass@k and\nPass@1. Comprehensive evaluations across multiple open-source models and\nchallenging multi-modal reasoning benchmarks validate the effectiveness and\ngenerality of our method. These findings offer new insights into the design of\nefficient and scalable test-time inference strategies for advanced language\nmodels.", "AI": {"tldr": "\u63d0\u51faMatryoshkaThinking\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6700\u5148\u8fdb\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4ec5\u9700DeepConf 4%\u7684\u8ba1\u7b97\u91cf\u5373\u53ef\u8fbe\u523099.79\u7684AIME2025\u5206\u6570\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u5982DeepConf\u867d\u7136\u6709\u6548\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u9012\u5f52\u5229\u7528\u6a21\u578b\u5185\u5728\u7684\u63a8\u7406\u3001\u9a8c\u8bc1\u548c\u603b\u7ed3\u80fd\u529b\uff0c\u589e\u5f3a\u6b63\u786e\u89e3\u7684\u4fdd\u7559\u5e76\u51cf\u5c11Pass@k\u4e0ePass@1\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "\u5728\u591a\u4e2a\u5f00\u6e90\u6a21\u578b\u548c\u6311\u6218\u6027\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u4e3a\u9ad8\u7ea7\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u53ef\u6269\u5c55\u6d4b\u8bd5\u65f6\u63a8\u7406\u7b56\u7565\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2510.09872", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09872", "abs": "https://arxiv.org/abs/2510.09872", "authors": ["Sanjari Srivastava", "Gang Li", "Cheng Chang", "Rishu Garg", "Manpreet Kaur", "Charlene Y. Lee", "Yuezhang Li", "Yining Mao", "Ignacio Cases", "Yanan Xie", "Peng Qi"], "title": "WARC-Bench: Web Archive Based Benchmark for GUI Subtask Executions", "comment": null, "summary": "Training web agents to navigate complex, real-world websites requires them to\nmaster $\\textit{subtasks}$ - short-horizon interactions on multiple UI\ncomponents (e.g., choosing the correct date in a date picker, or scrolling in a\ncontainer to extract information). We introduce WARC-Bench (Web Archive\nBenchmark), a novel web navigation benchmark featuring 438 tasks designed to\nevaluate multimodal AI agents on subtasks. WARC-Bench enables sandboxed\ninteractions with dynamic and realistic webpages using Web ARChive files. We\nshow that WARC-Bench is challenging for leading computer-use models, with the\nhighest observed success rate being 64.8%. To improve open source models on\nsubtask, we explore two common training techniques: supervised fine-tuning\n(SFT) and reinforcement learning with verifiable rewards (RLVR). Experiments\nshow that SFT models obtain a 48.8% success rate on the benchmark. Training\nwith RLVR over SFT checkpoints, even in data-scarce settings, improves the\nscore to 52.8% on WARC-Bench, outperforming many frontier models. Our analysis\nconcludes that mastering these subtasks is essential for robust web planning\nand navigation, and is a capability not extensively evaluated by existing\nbenchmarks.", "AI": {"tldr": "WARC-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u7f51\u9875\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b438\u4e2a\u4efb\u52a1\uff0c\u4e13\u95e8\u8bc4\u4f30AI\u4ee3\u7406\u5728\u7f51\u9875\u5b50\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u8bad\u7ec3\u7f51\u9875\u4ee3\u7406\u9700\u8981\u638c\u63e1\u5b50\u4efb\u52a1\uff08\u5982\u65e5\u671f\u9009\u62e9\u5668\u64cd\u4f5c\u3001\u6eda\u52a8\u5bb9\u5668\u7b49\uff09\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u672a\u80fd\u5145\u5206\u8bc4\u4f30\u8fd9\u4e9b\u80fd\u529b\u3002", "method": "\u4f7f\u7528Web ARChive\u6587\u4ef6\u521b\u5efa\u6c99\u76d2\u5316\u7684\u52a8\u6001\u7f51\u9875\u4ea4\u4e92\u73af\u5883\uff0c\u91c7\u7528\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5e26\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60(RLVR)\u6765\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u9886\u5148\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u6a21\u578b\u5728WARC-Bench\u4e0a\u7684\u6700\u9ad8\u6210\u529f\u7387\u4ec5\u4e3a64.8%\u3002SFT\u6a21\u578b\u8fbe\u523048.8%\u6210\u529f\u7387\uff0cRLVR\u5728SFT\u57fa\u7840\u4e0a\u63d0\u5347\u81f352.8%\uff0c\u8d85\u8fc7\u8bb8\u591a\u524d\u6cbf\u6a21\u578b\u3002", "conclusion": "\u638c\u63e1\u7f51\u9875\u5b50\u4efb\u52a1\u5bf9\u4e8e\u9c81\u68d2\u7684\u7f51\u9875\u89c4\u5212\u548c\u5bfc\u822a\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u662f\u73b0\u6709\u57fa\u51c6\u672a\u80fd\u5145\u5206\u8bc4\u4f30\u7684\u80fd\u529b\u3002"}}
{"id": "2510.11085", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11085", "abs": "https://arxiv.org/abs/2510.11085", "authors": ["Yuxinyue Qian", "Jun Liu"], "title": "Modeling AI-Driven Production and Competitiveness A Multi-Agent Economic Simulation of China and the United States", "comment": null, "summary": "With the rapid development of artificial intelligence (AI) technology,\nsocio-economic systems are entering a new stage of \"human-AI co-creation.\"\nBuilding upon a previously established multi-level intelligent agent economic\nmodel, this paper conducts simulation-based comparisons of macroeconomic output\nevolution in China and the United States under different mechanisms-AI\ncollaboration, network effects, and AI autonomous production. The results show\nthat: (1) when AI functions as an independent productive entity, the overall\ngrowth rate of social output far exceeds that of traditional human-labor-based\nmodels; (2) China demonstrates clear potential for acceleration in both the\nexpansion of intelligent agent populations and the pace of technological\ncatch-up, offering the possibility of achieving technological convergence or\neven partial surpassing. This study provides a systematic, model-based\nanalytical framework for understanding AI-driven production system\ntransformation and shifts in international competitiveness, as well as\nquantitative insights for relevant policy formulation.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u591a\u5c42\u7ea7\u667a\u80fd\u4f53\u7ecf\u6d4e\u6a21\u578b\uff0c\u6bd4\u8f83\u4e86\u4e2d\u7f8e\u4e24\u56fd\u5728\u4e0d\u540cAI\u673a\u5236\uff08\u534f\u4f5c\u3001\u7f51\u7edc\u6548\u5e94\u3001\u81ea\u4e3b\u751f\u4ea7\uff09\u4e0b\u7684\u5b8f\u89c2\u7ecf\u6d4e\u4ea7\u51fa\u6f14\u53d8\uff0c\u53d1\u73b0AI\u4f5c\u4e3a\u72ec\u7acb\u751f\u4ea7\u5b9e\u4f53\u80fd\u5927\u5e45\u63d0\u5347\u793e\u4f1a\u4ea7\u51fa\u589e\u957f\u7387\uff0c\u4e2d\u56fd\u5728\u667a\u80fd\u4f53\u6269\u5f20\u548c\u6280\u672f\u8ffd\u8d76\u65b9\u9762\u5177\u6709\u52a0\u901f\u6f5c\u529b\u3002", "motivation": "\u968f\u7740AI\u6280\u672f\u5feb\u901f\u53d1\u5c55\uff0c\u793e\u4f1a\u7ecf\u6d4e\u7cfb\u7edf\u8fdb\u5165\"\u4eba\u673a\u5171\u521b\"\u65b0\u9636\u6bb5\uff0c\u9700\u8981\u7cfb\u7edf\u7406\u89e3AI\u9a71\u52a8\u7684\u751f\u4ea7\u7cfb\u7edf\u8f6c\u578b\u548c\u56fd\u9645\u7ade\u4e89\u529b\u53d8\u5316\u3002", "method": "\u57fa\u4e8e\u5148\u524d\u5efa\u7acb\u7684\u591a\u5c42\u7ea7\u667a\u80fd\u4f53\u7ecf\u6d4e\u6a21\u578b\uff0c\u5bf9\u4e0d\u540cAI\u673a\u5236\uff08\u534f\u4f5c\u3001\u7f51\u7edc\u6548\u5e94\u3001\u81ea\u4e3b\u751f\u4ea7\uff09\u8fdb\u884c\u4eff\u771f\u6bd4\u8f83\u5206\u6790\u3002", "result": "AI\u4f5c\u4e3a\u72ec\u7acb\u751f\u4ea7\u5b9e\u4f53\u65f6\uff0c\u793e\u4f1a\u4ea7\u51fa\u589e\u957f\u7387\u8fdc\u8d85\u4f20\u7edf\u4eba\u529b\u52b3\u52a8\u6a21\u5f0f\uff1b\u4e2d\u56fd\u5728\u667a\u80fd\u4f53\u4eba\u53e3\u6269\u5f20\u548c\u6280\u672f\u8ffd\u8d76\u901f\u5ea6\u4e0a\u663e\u793a\u51fa\u660e\u663e\u52a0\u901f\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3AI\u9a71\u52a8\u7684\u751f\u4ea7\u7cfb\u7edf\u8f6c\u578b\u548c\u56fd\u9645\u7ade\u4e89\u529b\u53d8\u5316\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u6a21\u578b\u5206\u6790\u6846\u67b6\uff0c\u5e76\u4e3a\u76f8\u5173\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u4e86\u91cf\u5316\u53c2\u8003\u3002"}}
{"id": "2510.10328", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10328", "abs": "https://arxiv.org/abs/2510.10328", "authors": ["Ananya Malik", "Nazanin Sabri", "Melissa Karnaze", "Mai Elsherief"], "title": "Are LLMs Empathetic to All? Investigating the Influence of Multi-Demographic Personas on a Model's Empathy", "comment": "9 pages, 4 figures, 4 tables, EMNLP 2025 Findings", "summary": "Large Language Models' (LLMs) ability to converse naturally is empowered by\ntheir ability to empathetically understand and respond to their users. However,\nemotional experiences are shaped by demographic and cultural contexts. This\nraises an important question: Can LLMs demonstrate equitable empathy across\ndiverse user groups? We propose a framework to investigate how LLMs' cognitive\nand affective empathy vary across user personas defined by intersecting\ndemographic attributes. Our study introduces a novel intersectional analysis\nspanning 315 unique personas, constructed from combinations of age, culture,\nand gender, across four LLMs. Results show that attributes profoundly shape a\nmodel's empathetic responses. Interestingly, we see that adding multiple\nattributes at once can attenuate and reverse expected empathy patterns. We show\nthat they broadly reflect real-world empathetic trends, with notable\nmisalignments for certain groups, such as those from Confucian culture. We\ncomplement our quantitative findings with qualitative insights to uncover model\nbehaviour patterns across different demographic groups. Our findings highlight\nthe importance of designing empathy-aware LLMs that account for demographic\ndiversity to promote more inclusive and equitable model behaviour.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u4eba\u53e3\u5c5e\u6027\u7528\u6237\u7fa4\u4f53\u95f4\u7684\u5171\u60c5\u80fd\u529b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u591a\u5c5e\u6027\u53e0\u52a0\u4f1a\u6539\u53d8\u9884\u671f\u5171\u60c5\u6a21\u5f0f\uff0c\u6a21\u578b\u5171\u60c5\u8d8b\u52bf\u603b\u4f53\u4e0a\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u6a21\u5f0f\uff0c\u4f46\u5728\u67d0\u4e9b\u7fa4\u4f53\u4e2d\u5b58\u5728\u504f\u5dee\u3002", "motivation": "\u63a2\u7a76LLMs\u662f\u5426\u80fd\u5728\u4e0d\u540c\u4eba\u53e3\u5c5e\u6027\u7528\u6237\u7fa4\u4f53\u95f4\u5c55\u73b0\u516c\u5e73\u7684\u5171\u60c5\u80fd\u529b\uff0c\u56e0\u4e3a\u60c5\u611f\u4f53\u9a8c\u53d7\u5230\u4eba\u53e3\u7edf\u8ba1\u548c\u6587\u5316\u80cc\u666f\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7315\u4e2a\u72ec\u7279\u4eba\u7269\u89d2\u8272\uff08\u7531\u5e74\u9f84\u3001\u6587\u5316\u548c\u6027\u522b\u7ec4\u5408\u6784\u6210\uff09\u5bf94\u4e2aLLMs\u8fdb\u884c\u4ea4\u53c9\u5206\u6790\uff0c\u8bc4\u4f30\u5176\u8ba4\u77e5\u548c\u60c5\u611f\u5171\u60c5\u80fd\u529b\u3002", "result": "\u4eba\u53e3\u5c5e\u6027\u663e\u8457\u5f71\u54cd\u6a21\u578b\u7684\u5171\u60c5\u56de\u5e94\uff0c\u591a\u5c5e\u6027\u53e0\u52a0\u4f1a\u51cf\u5f31\u751a\u81f3\u9006\u8f6c\u9884\u671f\u5171\u60c5\u6a21\u5f0f\uff0c\u6a21\u578b\u603b\u4f53\u4e0a\u53cd\u6620\u73b0\u5b9e\u5171\u60c5\u8d8b\u52bf\uff0c\u4f46\u5728\u5112\u5bb6\u6587\u5316\u7b49\u7279\u5b9a\u7fa4\u4f53\u4e2d\u5b58\u5728\u660e\u663e\u504f\u5dee\u3002", "conclusion": "\u9700\u8981\u8bbe\u8ba1\u8003\u8651\u4eba\u53e3\u591a\u6837\u6027\u7684\u5171\u60c5\u611f\u77e5LLMs\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u5177\u5305\u5bb9\u6027\u548c\u516c\u5e73\u6027\u7684\u6a21\u578b\u884c\u4e3a\u3002"}}
{"id": "2510.11119", "categories": ["cs.AI", "cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11119", "abs": "https://arxiv.org/abs/2510.11119", "authors": ["Andrea Marinoni", "Sai Shivareddy", "Pietro Lio'", "Weisi Lin", "Erik Cambria", "Clare Grey"], "title": "Improving AI Efficiency in Data Centres by Power Dynamic Response", "comment": null, "summary": "The steady growth of artificial intelligence (AI) has accelerated in the\nrecent years, facilitated by the development of sophisticated models such as\nlarge language models and foundation models. Ensuring robust and reliable power\ninfrastructures is fundamental to take advantage of the full potential of AI.\nHowever, AI data centres are extremely hungry for power, putting the problem of\ntheir power management in the spotlight, especially with respect to their\nimpact on environment and sustainable development. In this work, we investigate\nthe capacity and limits of solutions based on an innovative approach for the\npower management of AI data centres, i.e., making part of the input power as\ndynamic as the power used for data-computing functions. The performance of\npassive and active devices are quantified and compared in terms of\ncomputational gain, energy efficiency, reduction of capital expenditure, and\nmanagement costs by analysing power trends from multiple data platforms\nworldwide. This strategy, which identifies a paradigm shift in the AI data\ncentre power management, has the potential to strongly improve the\nsustainability of AI hyperscalers, enhancing their footprint on environmental,\nfinancial, and societal fields.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684AI\u6570\u636e\u4e2d\u5fc3\u7535\u6e90\u7ba1\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u90e8\u5206\u8f93\u5165\u7535\u6e90\u4e0e\u6570\u636e\u8ba1\u7b97\u529f\u80fd\u7684\u529f\u8017\u4e00\u6837\u52a8\u6001\u5316\uff0c\u6765\u6539\u5584\u53ef\u6301\u7eed\u6027\u3002", "motivation": "AI\u6570\u636e\u4e2d\u5fc3\u529f\u8017\u5de8\u5927\uff0c\u5bf9\u73af\u5883\u548c\u53ef\u6301\u7eed\u53d1\u5c55\u9020\u6210\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u7535\u6e90\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u88ab\u52a8\u548c\u4e3b\u52a8\u8bbe\u5907\uff0c\u5206\u6790\u5168\u7403\u591a\u4e2a\u6570\u636e\u5e73\u53f0\u7684\u529f\u8017\u8d8b\u52bf\uff0c\u91cf\u5316\u8ba1\u7b97\u589e\u76ca\u3001\u80fd\u6548\u3001\u8d44\u672c\u652f\u51fa\u548c\u7ba1\u7406\u6210\u672c\u7684\u6539\u8fdb\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u589e\u76ca\u3001\u80fd\u6548\u3001\u8d44\u672c\u652f\u51fa\u548c\u7ba1\u7406\u6210\u672c\u65b9\u9762\u90fd\u6709\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u8fd9\u79cd\u7b56\u7565\u4ee3\u8868\u4e86AI\u6570\u636e\u4e2d\u5fc3\u7535\u6e90\u7ba1\u7406\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u6709\u6f5c\u529b\u663e\u8457\u63d0\u9ad8AI\u8d85\u5927\u89c4\u6a21\u8fd0\u8425\u5546\u7684\u53ef\u6301\u7eed\u6027\uff0c\u6539\u5584\u5176\u5728\u73af\u5883\u3001\u8d22\u52a1\u548c\u793e\u4f1a\u9886\u57df\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.10329", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10329", "abs": "https://arxiv.org/abs/2510.10329", "authors": ["Nam Luu", "Ond\u0159ej Bojar"], "title": "End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs", "comment": null, "summary": "Speech Translation (ST) is a machine translation task that involves\nconverting speech signals from one language to the corresponding text in\nanother language; this task has two different approaches, namely the\ntraditional cascade and the more recent end-to-end. This paper explores a\ncombined end-to-end architecture of pre-trained speech encoders and Large\nLanguage Models (LLMs) for performing both Automatic Speech Recognition (ASR)\nand ST simultaneously. Experiments with the English-to-German language pair\nshow that our best model not only can achieve better translation results than\nSeamlessM4T, a large foundational end-to-end, multi-modal translation model,\nbut can also match the performance of a cascaded system with Whisper and NLLB,\nwith up to a score gain of 8% in $\\text{COMET}^{\\text{DA}}_{22}$ metric.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9884\u8bad\u7ec3\u8bed\u97f3\u7f16\u7801\u5668\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7aef\u5230\u7aef\u67b6\u6784\uff0c\u7528\u4e8e\u540c\u65f6\u6267\u884c\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u548c\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u3002\u5728\u82f1\u8bed\u5230\u5fb7\u8bed\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u6a21\u578b\u4e0d\u4ec5\u4f18\u4e8eSeamlessM4T\uff0c\u8fd8\u80fd\u5ab2\u7f8eWhisper\u548cNLLB\u7ea7\u8054\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3\u8bed\u97f3\u7f16\u7801\u5668\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u7684\u7aef\u5230\u7aef\u67b6\u6784\uff0c\u4ee5\u540c\u65f6\u5904\u7406\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u548c\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\uff0c\u514b\u670d\u4f20\u7edf\u7ea7\u8054\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u8bed\u97f3\u7f16\u7801\u5668\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u7aef\u5230\u7aef\u67b6\u6784\uff0c\u540c\u65f6\u6267\u884cASR\u548cST\u4efb\u52a1\u3002", "result": "\u5728\u82f1\u8bed\u5230\u5fb7\u8bed\u7ffb\u8bd1\u4e2d\uff0c\u6700\u4f73\u6a21\u578b\u4e0d\u4ec5\u4f18\u4e8eSeamlessM4T\uff0c\u8fd8\u80fd\u5339\u914dWhisper\u548cNLLB\u7ea7\u8054\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5728COMET-DA22\u6307\u6807\u4e0a\u83b7\u5f97\u9ad8\u8fbe8%\u7684\u5206\u6570\u63d0\u5347\u3002", "conclusion": "\u7ed3\u5408\u9884\u8bad\u7ec3\u8bed\u97f3\u7f16\u7801\u5668\u548cLLM\u7684\u7aef\u5230\u7aef\u67b6\u6784\u5728\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u8d85\u8d8a\u5927\u578b\u57fa\u7840\u6a21\u578b\u5e76\u5ab2\u7f8e\u7ea7\u8054\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2510.09884", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09884", "abs": "https://arxiv.org/abs/2510.09884", "authors": ["Soheila Farokhi", "Xiaojun Qi", "Hamid Karimi"], "title": "TAWRMAC: A Novel Dynamic Graph Representation Learning Method", "comment": null, "summary": "Dynamic graph representation learning has become essential for analyzing\nevolving networks in domains such as social network analysis, recommendation\nsystems, and traffic analysis. However, existing continuous-time methods face\nthree key challenges: (1) some methods depend solely on node-specific memory\nwithout effectively incorporating information from neighboring nodes, resulting\nin embedding staleness; (2) most fail to explicitly capture correlations\nbetween node neighborhoods, limiting contextual awareness; and (3) many fail to\nfully capture the structural dynamics of evolving graphs, especially in absence\nof rich link attributes. To address these limitations, we introduce TAWRMAC-a\nnovel framework that integrates Temporal Anonymous Walks with Restart, Memory\nAugmentation, and Neighbor Co-occurrence embedding. TAWRMAC enhances embedding\nstability through a memory-augmented GNN with fixedtime encoding and improves\ncontextual representation by explicitly capturing neighbor correlations.\nAdditionally, its Temporal Anonymous Walks with Restart mechanism distinguishes\nbetween nodes exhibiting repetitive interactions and those forming new\nconnections beyond their immediate neighborhood. This approach captures\nstructural dynamics better and supports strong inductive learning. Extensive\nexperiments on multiple benchmark datasets demonstrate that TAWRMAC\nconsistently outperforms state-of-the-art methods in dynamic link prediction\nand node classification under both transductive and inductive settings across\nthree different negative sampling strategies. By providing stable,\ngeneralizable, and context-aware embeddings, TAWRMAC advances the state of the\nart in continuous-time dynamic graph learning. The code is available at\nhttps://anonymous.4open.science/r/tawrmac-A253 .", "AI": {"tldr": "TAWRMAC\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u52a8\u6001\u56fe\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u65f6\u95f4\u533f\u540d\u6e38\u8d70\u91cd\u542f\u3001\u8bb0\u5fc6\u589e\u5f3a\u548c\u90bb\u5c45\u5171\u73b0\u5d4c\u5165\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5d4c\u5165\u9648\u65e7\u6027\u3001\u90bb\u5c45\u76f8\u5173\u6027\u6355\u83b7\u548c\u7ed3\u6784\u52a8\u6001\u5efa\u6a21\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\u56fe\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u4f9d\u8d56\u8282\u70b9\u7279\u5b9a\u8bb0\u5fc6\u800c\u5ffd\u7565\u90bb\u5c45\u4fe1\u606f\u5bfc\u81f4\u5d4c\u5165\u9648\u65e7\uff1b\u672a\u80fd\u663e\u5f0f\u6355\u83b7\u8282\u70b9\u90bb\u5c45\u95f4\u7684\u76f8\u5173\u6027\uff1b\u65e0\u6cd5\u5145\u5206\u6355\u6349\u6f14\u5316\u56fe\u7684\u7ed3\u6784\u52a8\u6001\u3002", "method": "\u63d0\u51faTAWRMAC\u6846\u67b6\uff0c\u6574\u5408\u65f6\u95f4\u533f\u540d\u6e38\u8d70\u91cd\u542f\u673a\u5236\u3001\u8bb0\u5fc6\u589e\u5f3a\u7684GNN\u4e0e\u56fa\u5b9a\u65f6\u95f4\u7f16\u7801\u3001\u90bb\u5c45\u5171\u73b0\u5d4c\u5165\uff0c\u901a\u8fc7\u8bb0\u5fc6\u589e\u5f3a\u63d0\u5347\u5d4c\u5165\u7a33\u5b9a\u6027\uff0c\u663e\u5f0f\u6355\u83b7\u90bb\u5c45\u76f8\u5173\u6027\uff0c\u5e76\u533a\u5206\u91cd\u590d\u4ea4\u4e92\u548c\u65b0\u5efa\u8fde\u63a5\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTAWRMAC\u5728\u52a8\u6001\u94fe\u63a5\u9884\u6d4b\u548c\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5728\u8f6c\u5bfc\u548c\u5f52\u7eb3\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e14\u5bf9\u4e09\u79cd\u4e0d\u540c\u8d1f\u91c7\u6837\u7b56\u7565\u90fd\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "TAWRMAC\u901a\u8fc7\u63d0\u4f9b\u7a33\u5b9a\u3001\u53ef\u6cdb\u5316\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5d4c\u5165\uff0c\u63a8\u8fdb\u4e86\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\u56fe\u5b66\u4e60\u7684\u6280\u672f\u6c34\u5e73\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u548c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.11143", "categories": ["cs.AI", "cs.HC", "68U35, 62P30", "I.2.2"], "pdf": "https://arxiv.org/pdf/2510.11143", "abs": "https://arxiv.org/abs/2510.11143", "authors": ["Chuke Chen", "Biao Luo", "Nan Li", "Boxiang Wang", "Hang Yang", "Jing Guo", "Ming Xu"], "title": "Spec-Driven AI for Science: The ARIA Framework for Automated and Reproducible Data Analysis", "comment": "19 pages,5 figures", "summary": "The rapid expansion of scientific data has widened the gap between analytical\ncapability and research intent. Existing AI-based analysis tools, ranging from\nAutoML frameworks to agentic research assistants, either favor automation over\ntransparency or depend on manual scripting that hinders scalability and\nreproducibility. We present ARIA (Automated Research Intelligence Assistant), a\nspec-driven, human-in-the-loop framework for automated and interpretable data\nanalysis. ARIA integrates six interoperable layers, namely Command, Context,\nCode, Data, Orchestration, and AI Module, within a document-centric workflow\nthat unifies human reasoning and machine execution. Through natural-language\nspecifications, researchers define analytical goals while ARIA autonomously\ngenerates executable code, validates computations, and produces transparent\ndocumentation. Beyond achieving high predictive accuracy, ARIA can rapidly\nidentify optimal feature sets and select suitable models, minimizing redundant\ntuning and repetitive experimentation. In the Boston Housing case, ARIA\ndiscovered 25 key features and determined XGBoost as the best performing model\n(R square = 0.93) with minimal overfitting. Evaluations across heterogeneous\ndomains demonstrate ARIA's strong performance, interpretability, and efficiency\ncompared with state-of-the-art systems. By combining AI for research and AI for\nscience principles within a spec-driven architecture, ARIA establishes a new\nparadigm for transparent, collaborative, and reproducible scientific discovery.", "AI": {"tldr": "ARIA\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c4\u8303\u9a71\u52a8\u7684\u3001\u4eba\u673a\u534f\u4f5c\u7684\u81ea\u52a8\u5316\u53ef\u89e3\u91ca\u6570\u636e\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u5b9a\u4e49\u5206\u6790\u76ee\u6807\uff0c\u81ea\u52a8\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\u5e76\u9a8c\u8bc1\u8ba1\u7b97\uff0c\u5b9e\u73b0\u900f\u660e\u3001\u53ef\u590d\u73b0\u7684\u79d1\u5b66\u7814\u7a76\u3002", "motivation": "\u79d1\u5b66\u6570\u636e\u7684\u5feb\u901f\u589e\u957f\u5bfc\u81f4\u5206\u6790\u80fd\u529b\u4e0e\u7814\u7a76\u610f\u56fe\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u73b0\u6709AI\u5206\u6790\u5de5\u5177\u8981\u4e48\u504f\u5411\u81ea\u52a8\u5316\u800c\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u8981\u4e48\u4f9d\u8d56\u624b\u52a8\u811a\u672c\u963b\u788d\u53ef\u6269\u5c55\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002", "method": "ARIA\u91c7\u7528\u516d\u5c42\u4e92\u64cd\u4f5c\u67b6\u6784\uff08\u547d\u4ee4\u3001\u4e0a\u4e0b\u6587\u3001\u4ee3\u7801\u3001\u6570\u636e\u3001\u7f16\u6392\u548cAI\u6a21\u5757\uff09\uff0c\u5728\u6587\u6863\u4e2d\u5fc3\u5316\u5de5\u4f5c\u6d41\u4e2d\u7edf\u4e00\u4eba\u7c7b\u63a8\u7406\u548c\u673a\u5668\u6267\u884c\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u9a71\u52a8\u81ea\u52a8\u5316\u5206\u6790\u3002", "result": "\u5728\u6ce2\u58eb\u987f\u623f\u4ef7\u6848\u4f8b\u4e2d\uff0cARIA\u53d1\u73b0\u4e8625\u4e2a\u5173\u952e\u7279\u5f81\u5e76\u786e\u5b9aXGBoost\u4e3a\u6700\u4f73\u6a21\u578b\uff08R\u5e73\u65b9=0.93\uff09\uff0c\u8fc7\u62df\u5408\u6700\u5c0f\u3002\u8de8\u9886\u57df\u8bc4\u4f30\u663e\u793aARIA\u5728\u6027\u80fd\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7cfb\u7edf\u3002", "conclusion": "ARIA\u901a\u8fc7\u5c06AI\u7814\u7a76\u548cAI\u79d1\u5b66\u539f\u5219\u7ed3\u5408\u5728\u89c4\u8303\u9a71\u52a8\u67b6\u6784\u4e2d\uff0c\u4e3a\u900f\u660e\u3001\u534f\u4f5c\u548c\u53ef\u590d\u73b0\u7684\u79d1\u5b66\u53d1\u73b0\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.10384", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10384", "abs": "https://arxiv.org/abs/2510.10384", "authors": ["Hakyung Sung", "Kristopher Kyle"], "title": "ASC analyzer: A Python package for measuring argument structure construction usage in English texts", "comment": "Accepted to the 2nd Workshop on Construction Grammars and NLP\n  (CxGs+NLP)", "summary": "Argument structure constructions (ASCs) offer a theoretically grounded lens\nfor analyzing second language (L2) proficiency, yet scalable and systematic\ntools for measuring their usage remain limited. This paper introduces the ASC\nanalyzer, a publicly available Python package designed to address this gap. The\nanalyzer automatically tags ASCs and computes 50 indices that capture\ndiversity, proportion, frequency, and ASC-verb lemma association strength. To\ndemonstrate its utility, we conduct both bivariate and multivariate analyses\nthat examine the relationship between ASC-based indices and L2 writing scores.", "AI": {"tldr": "\u4ecb\u7ecdASC\u5206\u6790\u5668\uff0c\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u5206\u6790\u4e8c\u8bed\u5199\u4f5c\u4e2d\u8bba\u5143\u7ed3\u6784\u6784\u5f0f\u7684Python\u5de5\u5177\u5305\uff0c\u53ef\u8ba1\u7b9750\u4e2a\u6307\u6807\u6765\u8bc4\u4f30\u4e8c\u8bed\u719f\u7ec3\u5ea6\u3002", "motivation": "\u73b0\u6709\u7528\u4e8e\u6d4b\u91cf\u8bba\u5143\u7ed3\u6784\u6784\u5f0f\u4f7f\u7528\u7684\u53ef\u6269\u5c55\u548c\u7cfb\u7edf\u5316\u5de5\u5177\u6709\u9650\uff0c\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u5de5\u5177\u6765\u5206\u6790\u4e8c\u8bed\u5199\u4f5c\u80fd\u529b\u3002", "method": "\u5f00\u53d1ASC\u5206\u6790\u5668Python\u5305\uff0c\u81ea\u52a8\u6807\u6ce8\u8bba\u5143\u7ed3\u6784\u6784\u5f0f\u5e76\u8ba1\u7b97\u591a\u6837\u6027\u3001\u6bd4\u4f8b\u3001\u9891\u7387\u548cASC-\u52a8\u8bcd\u5173\u8054\u5f3a\u5ea6\u7b4950\u4e2a\u6307\u6807\u3002", "result": "\u901a\u8fc7\u53cc\u53d8\u91cf\u548c\u591a\u53d8\u91cf\u5206\u6790\u9a8c\u8bc1\u4e86ASC\u6307\u6807\u4e0e\u4e8c\u8bed\u5199\u4f5c\u5206\u6570\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "conclusion": "ASC\u5206\u6790\u5668\u4e3a\u4e8c\u8bed\u719f\u7ec3\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u5206\u6790\u5de5\u5177\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5de5\u5177\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.11144", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11144", "abs": "https://arxiv.org/abs/2510.11144", "authors": ["Gautier Dagan", "Frank Keller", "Alex Lascarides"], "title": "$How^{2}$: How to learn from procedural How-to questions", "comment": null, "summary": "An agent facing a planning problem can use answers to how-to questions to\nreduce uncertainty and fill knowledge gaps, helping it solve both current and\nfuture tasks. However, their open ended nature, where valid answers to \"How do\nI X?\" range from executable actions to high-level descriptions of X's\nsub-goals, makes them challenging for AI agents to ask, and for AI experts to\nanswer, in ways that support efficient planning. We introduce $How^{2}$, a\nmemory agent framework that enables agents to ask how-to questions, store the\nanswers, and reuse them for lifelong learning in interactive environments. We\nevaluate our approach in Plancraft, a Minecraft crafting environment, where\nagents must complete an assembly task by manipulating inventory items. Using\nteacher models that answer at varying levels of abstraction, from executable\naction sequences to high-level subgoal descriptions, we show that lifelong\nlearning agents benefit most from answers that are abstracted and decoupled\nfrom the current state. $How^{2}$ offers a way for LLM-based agents to improve\ntheir planning capabilities over time by asking questions in interactive\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86How2\u6846\u67b6\uff0c\u4f7fAI\u667a\u80fd\u4f53\u80fd\u591f\u8be2\u95ee\u5982\u4f55\u6267\u884c\u4efb\u52a1\u7684\u95ee\u9898\uff0c\u5b58\u50a8\u7b54\u6848\u5e76\u7528\u4e8e\u7ec8\u8eab\u5b66\u4e60\uff0c\u7279\u522b\u5728Minecraft\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u62bd\u8c61\u7b54\u6848\u5bf9\u89c4\u5212\u6548\u7387\u7684\u63d0\u5347\u3002", "motivation": "\u667a\u80fd\u4f53\u5728\u89c4\u5212\u95ee\u9898\u65f6\u9700\u8981\u901a\u8fc7\u5982\u4f55\u6267\u884c\u4efb\u52a1\u7684\u95ee\u9898\u6765\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\u548c\u586b\u8865\u77e5\u8bc6\u7a7a\u767d\uff0c\u4f46\u8fd9\u7c7b\u5f00\u653e\u6027\u95ee\u9898\u5bf9AI\u667a\u80fd\u4f53\u63d0\u95ee\u548c\u4e13\u5bb6\u56de\u7b54\u90fd\u6784\u6210\u6311\u6218\uff0c\u96be\u4ee5\u652f\u6301\u9ad8\u6548\u89c4\u5212\u3002", "method": "\u5f15\u5165How2\u8bb0\u5fc6\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u8ba9\u667a\u80fd\u4f53\u80fd\u591f\u63d0\u95ee\u3001\u5b58\u50a8\u7b54\u6848\u5e76\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u91cd\u590d\u4f7f\u7528\u8fdb\u884c\u7ec8\u8eab\u5b66\u4e60\u3002\u5728Minecraft\u5236\u4f5c\u73af\u5883\u4e2d\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e0d\u540c\u62bd\u8c61\u5c42\u6b21\u7684\u6559\u5e08\u6a21\u578b\u56de\u7b54\u95ee\u9898\u3002", "result": "\u7ec8\u8eab\u5b66\u4e60\u667a\u80fd\u4f53\u4ece\u62bd\u8c61\u4e14\u4e0e\u5f53\u524d\u72b6\u6001\u89e3\u8026\u7684\u7b54\u6848\u4e2d\u83b7\u76ca\u6700\u5927\uff0c\u8fd9\u4e9b\u7b54\u6848\u8303\u56f4\u4ece\u53ef\u6267\u884c\u52a8\u4f5c\u5e8f\u5217\u5230\u9ad8\u7ea7\u5b50\u76ee\u6807\u63cf\u8ff0\u3002", "conclusion": "How2\u4e3a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u8fc7\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u63d0\u95ee\u6765\u968f\u65f6\u95f4\u63d0\u5347\u89c4\u5212\u80fd\u529b\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.10390", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10390", "abs": "https://arxiv.org/abs/2510.10390", "authors": ["Aashiq Muhamed", "Leonardo F. R. Ribeiro", "Markus Dreyer", "Virginia Smith", "Mona T. Diab"], "title": "RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models", "comment": null, "summary": "The ability of language models in RAG systems to selectively refuse to answer\nbased on flawed context is critical for safety, yet remains a significant\nfailure point. Our large-scale study reveals that even frontier models struggle\nin this setting, with refusal accuracy dropping below 50% on multi-document\ntasks, while exhibiting either dangerous overconfidence or overcaution. Static\nbenchmarks fail to reliably evaluate this capability, as models exploit\ndataset-specific artifacts and memorize test instances. We introduce\nRefusalBench, a generative methodology that programmatically creates diagnostic\ntest cases through controlled linguistic perturbation. Our framework employs\n176 distinct perturbation strategies across six categories of informational\nuncertainty and three intensity levels. Evaluation of over 30 models uncovers\nsystematic failure patterns: refusal comprises separable detection and\ncategorization skills, and neither scale nor extended reasoning improves\nperformance. We find that selective refusal is a trainable, alignment-sensitive\ncapability, offering a clear path for improvement. We release two benchmarks --\nRefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) --\nand our complete generation framework to enable continued, dynamic evaluation\nof this critical capability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86RAG\u7cfb\u7edf\u4e2d\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u6709\u7f3a\u9677\u4e0a\u4e0b\u6587\u9009\u62e9\u6027\u62d2\u7edd\u56de\u7b54\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u524d\u6cbf\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u4e5f\u5b58\u5728\u663e\u8457\u95ee\u9898\uff0c\u62d2\u7edd\u51c6\u786e\u7387\u4f4e\u4e8e50%\u3002\u4f5c\u8005\u63d0\u51fa\u4e86RefusalBench\u8bc4\u4f30\u6846\u67b6\u6765\u52a8\u6001\u8bc4\u4f30\u8fd9\u4e00\u5173\u952e\u80fd\u529b\u3002", "motivation": "RAG\u7cfb\u7edf\u4e2d\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u6709\u7f3a\u9677\u4e0a\u4e0b\u6587\u9009\u62e9\u6027\u62d2\u7edd\u56de\u7b54\u7684\u80fd\u529b\u5bf9\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u4ecd\u662f\u663e\u8457\u7684\u5931\u8d25\u70b9\uff0c\u73b0\u6709\u9759\u6001\u57fa\u51c6\u65e0\u6cd5\u53ef\u9760\u8bc4\u4f30\u8fd9\u4e00\u80fd\u529b\u3002", "method": "\u5f15\u5165RefusalBench\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d7\u63a7\u8bed\u8a00\u6270\u52a8\u7f16\u7a0b\u521b\u5efa\u8bca\u65ad\u6d4b\u8bd5\u7528\u4f8b\uff0c\u91c7\u7528176\u79cd\u6270\u52a8\u7b56\u7565\uff0c\u6db5\u76d6\u516d\u7c7b\u4fe1\u606f\u4e0d\u786e\u5b9a\u6027\u548c\u4e09\u4e2a\u5f3a\u5ea6\u7ea7\u522b\u3002", "result": "\u8bc4\u4f3030\u591a\u4e2a\u6a21\u578b\u53d1\u73b0\u7cfb\u7edf\u6027\u5931\u8d25\u6a21\u5f0f\uff1a\u62d2\u7edd\u80fd\u529b\u5305\u542b\u53ef\u5206\u79bb\u7684\u68c0\u6d4b\u548c\u5206\u7c7b\u6280\u80fd\uff0c\u89c4\u6a21\u548c\u6269\u5c55\u63a8\u7406\u65e0\u6cd5\u63d0\u5347\u6027\u80fd\uff0c\u9009\u62e9\u6027\u62d2\u7edd\u662f\u53ef\u8bad\u7ec3\u7684\u5bf9\u9f50\u654f\u611f\u80fd\u529b\u3002", "conclusion": "\u9009\u62e9\u6027\u62d2\u7edd\u662f\u53ef\u8bad\u7ec3\u7684\u5bf9\u9f50\u654f\u611f\u80fd\u529b\uff0c\u4e3a\u6539\u8fdb\u63d0\u4f9b\u4e86\u660e\u786e\u8def\u5f84\u3002\u53d1\u5e03\u4e86RefusalBench-NQ\u548cRefusalBench-GaRAGe\u4e24\u4e2a\u57fa\u51c6\u53ca\u5b8c\u6574\u751f\u6210\u6846\u67b6\uff0c\u652f\u6301\u6301\u7eed\u52a8\u6001\u8bc4\u4f30\u8fd9\u4e00\u5173\u952e\u80fd\u529b\u3002"}}
{"id": "2510.11194", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11194", "abs": "https://arxiv.org/abs/2510.11194", "authors": ["Peiming Li", "Zhiyuan Hu", "Yang Tang", "Shiyu Li", "Xi Chen"], "title": "Aligning Deep Implicit Preferences by Learning to Reason Defensively", "comment": null, "summary": "Personalized alignment is crucial for enabling Large Language Models (LLMs)\nto engage effectively in user-centric interactions. However, current methods\nface a dual challenge: they fail to infer users' deep implicit preferences\n(including unstated goals, semantic context and risk tolerances), and they lack\nthe defensive reasoning required to navigate real-world ambiguity. This\ncognitive gap leads to responses that are superficial, brittle and\nshort-sighted. To address this, we propose Critique-Driven Reasoning Alignment\n(CDRA), which reframes alignment from a scalar reward-matching task into a\nstructured reasoning process. First, to bridge the preference inference gap, we\nintroduce the DeepPref benchmark. This dataset, comprising 3000\npreference-query pairs across 20 topics, is curated by simulating a\nmulti-faceted cognitive council that produces critique-annotated reasoning\nchains to deconstruct query semantics and reveal latent risks. Second, to\ninstill defensive reasoning, we introduce the Personalized Generative Process\nReward Model (Pers-GenPRM), which frames reward modeling as a personalized\nreasoning task. It generates a critique chain to evaluate a response's\nalignment with user preferences before outputting a final score based on this\nrationale. Ultimately, this interpretable, structured reward signal guides\npolicy model through Critique-Driven Policy Alignment, a process-level online\nreinforcement learning algorithm integrating both numerical and natural\nlanguage feedback. Experiments demonstrate that CDRA excels at discovering and\naligning with users' true preferences while executing robust reasoning. Our\ncode and dataset are available at https://github.com/Zephyrian-Hugh/Deep-pref.", "AI": {"tldr": "\u63d0\u51faCDRA\u65b9\u6cd5\uff0c\u5c06LLM\u5bf9\u9f50\u4ece\u6807\u91cf\u5956\u52b1\u5339\u914d\u91cd\u6784\u4e3a\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\uff0c\u901a\u8fc7\u6df1\u5ea6\u504f\u597d\u63a8\u65ad\u548c\u9632\u5fa1\u6027\u63a8\u7406\u89e3\u51b3\u7528\u6237\u504f\u597d\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u65e0\u6cd5\u63a8\u65ad\u7528\u6237\u7684\u6df1\u5ea6\u9690\u542b\u504f\u597d\uff08\u672a\u9648\u8ff0\u76ee\u6807\u3001\u8bed\u4e49\u4e0a\u4e0b\u6587\u548c\u98ce\u9669\u5bb9\u5fcd\u5ea6\uff09\uff0c\u4e14\u7f3a\u4e4f\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u6a21\u7cca\u6027\u7684\u9632\u5fa1\u6027\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u54cd\u5e94\u80a4\u6d45\u3001\u8106\u5f31\u548c\u77ed\u89c6\u3002", "method": "1. \u5f15\u5165DeepPref\u57fa\u51c6\u6570\u636e\u96c6\uff083000\u4e2a\u504f\u597d\u67e5\u8be2\u5bf9\uff09\uff1b2. \u63d0\u51faPers-GenPRM\u5956\u52b1\u6a21\u578b\uff0c\u5c06\u5956\u52b1\u5efa\u6a21\u6784\u5efa\u4e3a\u4e2a\u6027\u5316\u63a8\u7406\u4efb\u52a1\uff1b3. \u901a\u8fc7\u6279\u5224\u9a71\u52a8\u7684\u7b56\u7565\u5bf9\u9f50\u8fdb\u884c\u8fc7\u7a0b\u7ea7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eCDRA\u5728\u53d1\u73b0\u548c\u9002\u5e94\u7528\u6237\u771f\u5b9e\u504f\u597d\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u6267\u884c\u7a33\u5065\u7684\u63a8\u7406\u3002", "conclusion": "CDRA\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\u6709\u6548\u89e3\u51b3\u4e86LLM\u4e2a\u6027\u5316\u5bf9\u9f50\u4e2d\u7684\u6df1\u5ea6\u504f\u597d\u63a8\u65ad\u548c\u9632\u5fa1\u6027\u63a8\u7406\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002"}}
{"id": "2510.10397", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10397", "abs": "https://arxiv.org/abs/2510.10397", "authors": ["Kai Zhang", "Xinyuan Zhang", "Ejaz Ahmed", "Hongda Jiang", "Caleb Kumar", "Kai Sun", "Zhaojiang Lin", "Sanat Sharma", "Shereen Oraby", "Aaron Colak", "Ahmed Aly", "Anuj Kumar", "Xiaozhong Liu", "Xin Luna Dong"], "title": "AssoMem: Scalable Memory QA with Multi-Signal Associative Retrieval", "comment": null, "summary": "Accurate recall from large scale memories remains a core challenge for memory\naugmented AI assistants performing question answering (QA), especially in\nsimilarity dense scenarios where existing methods mainly rely on semantic\ndistance to the query for retrieval. Inspired by how humans link information\nassociatively, we propose AssoMem, a novel framework constructing an\nassociative memory graph that anchors dialogue utterances to automatically\nextracted clues. This structure provides a rich organizational view of the\nconversational context and facilitates importance aware ranking. Further,\nAssoMem integrates multi-dimensional retrieval signals-relevance, importance,\nand temporal alignment using an adaptive mutual information (MI) driven fusion\nstrategy. Extensive experiments across three benchmarks and a newly introduced\ndataset, MeetingQA, demonstrate that AssoMem consistently outperforms SOTA\nbaselines, verifying its superiority in context-aware memory recall.", "AI": {"tldr": "\u63d0\u51faAssoMem\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5173\u8054\u8bb0\u5fc6\u56fe\u6765\u6539\u8fdb\u5927\u89c4\u6a21\u8bb0\u5fc6\u68c0\u7d22\uff0c\u5728\u76f8\u4f3c\u5bc6\u96c6\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u8bb0\u5fc6\u589e\u5f3aAI\u52a9\u624b\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u53ec\u56de\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u76f8\u4f3c\u5bc6\u96c6\u573a\u666f\u4e0b\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8bed\u4e49\u8ddd\u79bb\u68c0\u7d22\u7684\u5c40\u9650\u6027", "method": "\u6784\u5efa\u5173\u8054\u8bb0\u5fc6\u56fe\uff0c\u5c06\u5bf9\u8bdd\u8bdd\u8bed\u951a\u5b9a\u5230\u81ea\u52a8\u63d0\u53d6\u7684\u7ebf\u7d22\u4e0a\uff0c\u96c6\u6210\u591a\u7ef4\u68c0\u7d22\u4fe1\u53f7\uff08\u76f8\u5173\u6027\u3001\u91cd\u8981\u6027\u3001\u65f6\u95f4\u5bf9\u9f50\uff09\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u4e92\u4fe1\u606f\u9a71\u52a8\u7684\u878d\u5408\u7b56\u7565", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u65b0\u5f15\u5165\u7684MeetingQA\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAssoMem\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "AssoMem\u5728\u4e0a\u4e0b\u6587\u611f\u77e5\u8bb0\u5fc6\u53ec\u56de\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\uff0c\u9a8c\u8bc1\u4e86\u5173\u8054\u8bb0\u5fc6\u56fe\u6846\u67b6\u7684\u6709\u6548\u6027"}}
{"id": "2510.11235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11235", "abs": "https://arxiv.org/abs/2510.11235", "authors": ["Leonard Dung", "Florian Mai"], "title": "AI Alignment Strategies from a Risk Perspective: Independent Safety Mechanisms or Shared Failures?", "comment": "under review", "summary": "AI alignment research aims to develop techniques to ensure that AI systems do\nnot cause harm. However, every alignment technique has failure modes, which are\nconditions in which there is a non-negligible chance that the technique fails\nto provide safety. As a strategy for risk mitigation, the AI safety community\nhas increasingly adopted a defense-in-depth framework: Conceding that there is\nno single technique which guarantees safety, defense-in-depth consists in\nhaving multiple redundant protections against safety failure, such that safety\ncan be maintained even if some protections fail. However, the success of\ndefense-in-depth depends on how (un)correlated failure modes are across\nalignment techniques. For example, if all techniques had the exact same failure\nmodes, the defense-in-depth approach would provide no additional protection at\nall. In this paper, we analyze 7 representative alignment techniques and 7\nfailure modes to understand the extent to which they overlap. We then discuss\nour results' implications for understanding the current level of risk and how\nto prioritize AI alignment research in the future.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e867\u79cd\u4ee3\u8868\u6027AI\u5bf9\u9f50\u6280\u672f\u548c7\u79cd\u6545\u969c\u6a21\u5f0f\uff0c\u7814\u7a76\u5b83\u4eec\u4e4b\u95f4\u7684\u91cd\u53e0\u7a0b\u5ea6\uff0c\u63a2\u8ba8\u9632\u5fa1\u6df1\u5ea6\u7b56\u7565\u7684\u6709\u6548\u6027\u53ca\u5176\u5bf9AI\u5b89\u5168\u7814\u7a76\u4f18\u5148\u7ea7\u7684\u542f\u793a\u3002", "motivation": "AI\u5bf9\u9f50\u6280\u672f\u90fd\u5b58\u5728\u6545\u969c\u6a21\u5f0f\uff0c\u9632\u5fa1\u6df1\u5ea6\u7b56\u7565\u901a\u8fc7\u591a\u91cd\u5197\u4f59\u4fdd\u62a4\u6765\u7f13\u89e3\u98ce\u9669\uff0c\u4f46\u5176\u6709\u6548\u6027\u53d6\u51b3\u4e8e\u4e0d\u540c\u6280\u672f\u6545\u969c\u6a21\u5f0f\u7684\u76f8\u5173\u6027\u7a0b\u5ea6\u3002", "method": "\u5206\u67907\u79cd\u4ee3\u8868\u6027\u5bf9\u9f50\u6280\u672f\u548c7\u79cd\u6545\u969c\u6a21\u5f0f\uff0c\u8bc4\u4f30\u5b83\u4eec\u4e4b\u95f4\u7684\u91cd\u53e0\u7a0b\u5ea6\u548c\u76f8\u5173\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u5bf9\u9f50\u6280\u672f\u7684\u6545\u969c\u6a21\u5f0f\u5b58\u5728\u4e0d\u540c\u7a0b\u5ea6\u7684\u91cd\u53e0\uff0c\u8fd9\u5f71\u54cd\u4e86\u9632\u5fa1\u6df1\u5ea6\u7b56\u7565\u7684\u5b9e\u9645\u6548\u679c\u3002", "conclusion": "\u9700\u8981\u66f4\u6df1\u5165\u5730\u7406\u89e3\u5bf9\u9f50\u6280\u672f\u6545\u969c\u6a21\u5f0f\u7684\u76f8\u5173\u6027\uff0c\u4ee5\u4f18\u5316\u9632\u5fa1\u6df1\u5ea6\u7b56\u7565\u5e76\u6307\u5bfc\u672a\u6765AI\u5b89\u5168\u7814\u7a76\u7684\u4f18\u5148\u7ea7\u8bbe\u7f6e\u3002"}}
{"id": "2510.10398", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10398", "abs": "https://arxiv.org/abs/2510.10398", "authors": ["Geunyeong Jeong", "Juoh Sun", "Seonghee Lee", "Harksoo Kim"], "title": "STEAM: A Semantic-Level Knowledge Editing Framework for Large Language Models", "comment": "Accepted to EMNLP 2025 (Findings)", "summary": "Large Language Models store extensive factual knowledge acquired during\nlarge-scale pre-training. However, this knowledge is inherently static,\nreflecting only the state of the world at the time of training. Knowledge\nediting has emerged as a promising solution for updating outdated or incorrect\nfacts without full retraining. However, most existing locate-and-edit methods\nprimarily focus on token-level likelihood optimization without addressing\nsemantic coherence. Our analysis reveals that such edited knowledge is often\nencoded as isolated residual streams in the model's latent space, distinct from\npre-existing knowledge and bypassing natural reasoning process. To address\nthis, we propose \\textsc{Steam}, a semantic-level knowledge editing framework\nthat enhances integration of updated knowledge into the model's knowledge\nstructure. \\textsc{Steam} first identifies target representations as semantic\nanchors for the updated factual association, then guides the internal\nrepresentation of the edited fact towards these anchors through an alignment\nloss during optimization. Experimental results demonstrate that \\textsc{Steam}\nimproves model's ability to reason with edited knowledge and enhances semantic\ncoherence, underscoring the importance of latent-space alignment for reliable\nand coherent knowledge editing. The code is available at\nhttps://github.com/GY-Jeong/STEAM.", "AI": {"tldr": "\u63d0\u51faSTEAM\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u7ea7\u77e5\u8bc6\u7f16\u8f91\u589e\u5f3a\u66f4\u65b0\u77e5\u8bc6\u5728\u6a21\u578b\u77e5\u8bc6\u7ed3\u6784\u4e2d\u7684\u6574\u5408\uff0c\u89e3\u51b3\u73b0\u6709\u5b9a\u4f4d-\u7f16\u8f91\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u8fde\u8d2f\u6027\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8bcd\u7ea7\u4f3c\u7136\u4f18\u5316\uff0c\u7f3a\u4e4f\u8bed\u4e49\u8fde\u8d2f\u6027\uff0c\u5bfc\u81f4\u7f16\u8f91\u540e\u7684\u77e5\u8bc6\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4f5c\u4e3a\u5b64\u7acb\u6b8b\u5dee\u6d41\u5b58\u5728\uff0c\u7ed5\u8fc7\u81ea\u7136\u63a8\u7406\u8fc7\u7a0b", "method": "STEAM\u9996\u5148\u8bc6\u522b\u76ee\u6807\u8868\u793a\u4f5c\u4e3a\u66f4\u65b0\u4e8b\u5b9e\u5173\u8054\u7684\u8bed\u4e49\u951a\u70b9\uff0c\u7136\u540e\u901a\u8fc7\u5bf9\u9f50\u635f\u5931\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5f15\u5bfc\u7f16\u8f91\u4e8b\u5b9e\u7684\u5185\u90e8\u8868\u793a\u5411\u8fd9\u4e9b\u951a\u70b9\u5bf9\u9f50", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eSTEAM\u63d0\u9ad8\u4e86\u6a21\u578b\u4f7f\u7528\u7f16\u8f91\u77e5\u8bc6\u8fdb\u884c\u63a8\u7406\u7684\u80fd\u529b\uff0c\u5e76\u589e\u5f3a\u4e86\u8bed\u4e49\u8fde\u8d2f\u6027", "conclusion": "\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u5bf9\u4e8e\u53ef\u9760\u548c\u8fde\u8d2f\u7684\u77e5\u8bc6\u7f16\u8f91\u81f3\u5173\u91cd\u8981\uff0cSTEAM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2510.09898", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09898", "abs": "https://arxiv.org/abs/2510.09898", "authors": ["Hung Phan", "Son Le Vu", "Ali Jannesari"], "title": "Learning Bug Context for PyTorch-to-JAX Translation with LLMs", "comment": null, "summary": "Despite recent progress of large language models (LLMs) on code translation\namong mainstream languages, translating PyTorch to JAX remains nontrivial. The\ntwo libraries, though both embedded in Python, differ in core design, execution\nsemantics, and ecosystem maturity; JAX is newer and comparatively\nunderrepresented in public code, and parallel PyTorch--JAX corpora are limited.\nWeaknesses in existing evaluation further complicate cross-framework\nbenchmarking. We present T2J, a prompt-augmentation framework that strengthens\nLLM-based PyTorch to JAX translation. Our pipeline (i) assembles two PyTorch\nsources -- the problem-solving set from TorchLeet (Aroori & Chien, 2025) and a\nGitHub-derived set from CodeParrot (Wolf et al., 2022) -- and uses GPT-4o-mini\nto produce initial JAX drafts; (ii) engages two professional developers to\niteratively repair those drafts until functional equivalence, yielding a\ncurated fixed-bug dataset of common errors and patches; and (iii) constructs\naugmented prompts that inject structured guidance from these fixes to steer\nlightweight LLMs (e.g., GPT-4o-mini). We also introduce three metrics tailored\nto PyTorch to JAX: T2J CodeTrans Score, T2J FixCost Score (an LLM-based\nestimate of bug-fix effort), and T2J Comparison Score (LLM-as-judge).\nEmpirically, T2J raises GPT-4o-mini performance by up to 10% on CodeBLEU, 50%\non T2J FixCost Score, 1.33 points on T2J CodeTrans Score (0--4 scale), and 100%\non T2J Comparison Score; moreover, the generated code runs up to 2.5x faster\nthan the baseline.", "AI": {"tldr": "T2J\u662f\u4e00\u4e2a\u63d0\u793a\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b\u5e38\u89c1\u9519\u8bef\u548c\u4fee\u590d\u7684\u56fa\u5b9a\u9519\u8bef\u6570\u636e\u96c6\uff0c\u4e3a\u8f7b\u91cf\u7ea7LLM\u63d0\u4f9b\u7ed3\u6784\u5316\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347PyTorch\u5230JAX\u7684\u4ee3\u7801\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "PyTorch\u548cJAX\u867d\u7136\u90fd\u5d4c\u5165\u5728Python\u4e2d\uff0c\u4f46\u5728\u6838\u5fc3\u8bbe\u8ba1\u3001\u6267\u884c\u8bed\u4e49\u548c\u751f\u6001\u7cfb\u7edf\u6210\u719f\u5ea6\u4e0a\u5b58\u5728\u5dee\u5f02\u3002JAX\u8f83\u65b0\u4e14\u5728\u516c\u5171\u4ee3\u7801\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u5e73\u884cPyTorch-JAX\u8bed\u6599\u5e93\u6709\u9650\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u8de8\u6846\u67b6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b58\u5728\u5f31\u70b9\u3002", "method": "\u6784\u5efa\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a(1)\u4f7f\u7528GPT-4o-mini\u4eceTorchLeet\u548cCodeParrot\u7684PyTorch\u6e90\u4ee3\u7801\u751f\u6210\u521d\u59cbJAX\u8349\u7a3f\uff1b(2)\u4e13\u4e1a\u5f00\u53d1\u8005\u8fed\u4ee3\u4fee\u590d\u8fd9\u4e9b\u8349\u7a3f\uff0c\u6784\u5efa\u5305\u542b\u5e38\u89c1\u9519\u8bef\u548c\u4fee\u590d\u7684\u56fa\u5b9a\u9519\u8bef\u6570\u636e\u96c6\uff1b(3)\u6784\u5efa\u589e\u5f3a\u63d0\u793a\uff0c\u4e3a\u8f7b\u91cf\u7ea7LLM\u63d0\u4f9b\u7ed3\u6784\u5316\u6307\u5bfc\u3002", "result": "T2J\u5c06GPT-4o-mini\u5728CodeBLEU\u4e0a\u7684\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe10%\uff0cT2J\u4fee\u590d\u6210\u672c\u5206\u6570\u63d0\u534750%\uff0cT2J\u4ee3\u7801\u7ffb\u8bd1\u5206\u6570\u63d0\u53471.33\u5206\uff080-4\u5206\u5236\uff09\uff0cT2J\u6bd4\u8f83\u5206\u6570\u63d0\u5347100%\u3002\u751f\u6210\u7684\u4ee3\u7801\u8fd0\u884c\u901f\u5ea6\u6bd4\u57fa\u7ebf\u5feb2.5\u500d\u3002", "conclusion": "T2J\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u6307\u5bfc\u548c\u4e13\u4e1a\u4fee\u590d\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86PyTorch\u5230JAX\u7684\u4ee3\u7801\u7ffb\u8bd1\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u8de8\u6846\u67b6\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2510.11281", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11281", "abs": "https://arxiv.org/abs/2510.11281", "authors": ["Deepeka Garg", "Sihan Zeng", "Annapoorani L. Narayanan", "Sumitra Ganesh", "Leo Ardon"], "title": "PADME: Procedure Aware DynaMic Execution", "comment": null, "summary": "Learning to autonomously execute long-horizon procedures from natural\nlanguage remains a core challenge for intelligent agents. Free-form\ninstructions such as recipes, scientific protocols, or business workflows\nencode rich procedural knowledge, but their variability and lack of structure\ncause agents driven by large language models (LLMs) to drift or fail during\nexecution. We introduce Procedure Aware DynaMic Execution (PADME), an agent\nframework that produces and exploits a graph-based representation of\nprocedures. Unlike prior work that relies on manual graph construction or\nunstructured reasoning, PADME autonomously transforms procedural text into\nexecutable graphs that capture task dependencies, decision points, and reusable\nsubroutines. Central to PADME is a two-phase methodology; Teach phase, which\nfocuses on systematic structuring, enrichment with executable logic of\nprocedures, followed by Execute phase, which enables dynamic execution in\nresponse to real-time inputs and environment feedback. This separation ensures\nquality assurance and scalability, allowing expert knowledge to be encoded once\nand reliably reused across varying contexts. The graph representation also\nprovides an inductive bias that reduces error accumulation in long-horizon\nreasoning, underscoring the importance of structured procedure modeling for\nreliable agent-driven automation. Empirically, PADME achieves state-of-the-art\nperformance on four diverse benchmarks, including ALFWorld and ScienceWorld.\nThese results demonstrate that agents equipped with graph-based procedure\nrepresentations offer a powerful intermediate abstraction for robust and\ngeneralizable execution.", "AI": {"tldr": "PADME\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u8868\u793a\u7684\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u81ea\u7136\u8bed\u8a00\u7a0b\u5e8f\u6587\u672c\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u56fe\u7ed3\u6784\uff0c\u5b9e\u73b0\u957f\u671f\u4efb\u52a1\u7684\u53ef\u9760\u81ea\u52a8\u5316\u6267\u884c\u3002", "motivation": "\u89e3\u51b3\u667a\u80fd\u4ee3\u7406\u4ece\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff08\u5982\u98df\u8c31\u3001\u79d1\u5b66\u534f\u8bae\uff09\u6267\u884c\u957f\u671f\u4efb\u52a1\u65f6\uff0c\u7531\u4e8e\u8bed\u8a00\u591a\u53d8\u6027\u548c\u7f3a\u4e4f\u7ed3\u6784\u5bfc\u81f4\u7684\u6f02\u79fb\u6216\u5931\u8d25\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1aTeach\u9636\u6bb5\u5c06\u7a0b\u5e8f\u6587\u672c\u7ed3\u6784\u5316\u5e76\u4e30\u5bcc\u53ef\u6267\u884c\u903b\u8f91\uff0cExecute\u9636\u6bb5\u6839\u636e\u5b9e\u65f6\u8f93\u5165\u548c\u73af\u5883\u53cd\u9988\u8fdb\u884c\u52a8\u6001\u6267\u884c\u3002\u4f7f\u7528\u56fe\u7ed3\u6784\u6355\u83b7\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\u3001\u51b3\u7b56\u70b9\u548c\u53ef\u91cd\u7528\u5b50\u7a0b\u5e8f\u3002", "result": "\u5728ALFWorld\u548cScienceWorld\u7b49\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8bc1\u660e\u57fa\u4e8e\u56fe\u7684\u7a0b\u5e8f\u8868\u793a\u80fd\u63d0\u4f9b\u5f3a\u5927\u7684\u4e2d\u95f4\u62bd\u8c61\u5c42\u3002", "conclusion": "\u56fe\u7ed3\u6784\u8868\u793a\u51cf\u5c11\u4e86\u957f\u671f\u63a8\u7406\u4e2d\u7684\u9519\u8bef\u7d2f\u79ef\uff0c\u7ed3\u6784\u5316\u7a0b\u5e8f\u5efa\u6a21\u5bf9\u4e8e\u53ef\u9760\u4ee3\u7406\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.10415", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10415", "abs": "https://arxiv.org/abs/2510.10415", "authors": ["Federica Bologna", "Tiffany Pan", "Matthew Wilkens", "Yue Guo", "Lucy Lu Wang"], "title": "LONGQAEVAL: Designing Reliable Evaluations of Long-Form Clinical QA under Resource Constraints", "comment": null, "summary": "Evaluating long-form clinical question answering (QA) systems is\nresource-intensive and challenging: accurate judgments require medical\nexpertise and achieving consistent human judgments over long-form text is\ndifficult. We introduce LongQAEval, an evaluation framework and set of\nevaluation recommendations for limited-resource and high-expertise settings.\nBased on physician annotations of 300 real patient questions answered by\nphysicians and LLMs, we compare coarse answer-level versus fine-grained\nsentence-level evaluation over the dimensions of correctness, relevance, and\nsafety. We find that inter-annotator agreement (IAA) varies by dimension:\nfine-grained annotation improves agreement on correctness, coarse improves\nagreement on relevance, and judgments on safety remain inconsistent.\nAdditionally, annotating only a small subset of sentences can provide\nreliability comparable to coarse annotations, reducing cost and effort.", "AI": {"tldr": "LongQAEval\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u957f\u683c\u5f0f\u4e34\u5e8a\u95ee\u7b54\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6bd4\u8f83\u7c97\u7c92\u5ea6\u7b54\u6848\u7ea7\u548c\u7ec6\u7c92\u5ea6\u53e5\u5b50\u7ea7\u8bc4\u4f30\u65b9\u6cd5\uff0c\u53d1\u73b0\u5728\u6b63\u786e\u6027\u7ef4\u5ea6\u4e0a\u7ec6\u7c92\u5ea6\u6807\u6ce8\u66f4\u597d\uff0c\u76f8\u5173\u6027\u7ef4\u5ea6\u4e0a\u7c97\u7c92\u5ea6\u66f4\u597d\uff0c\u5b89\u5168\u6027\u8bc4\u4f30\u4ecd\u4e0d\u4e00\u81f4\uff0c\u4e14\u6807\u6ce8\u5c11\u91cf\u53e5\u5b50\u5373\u53ef\u8fbe\u5230\u4e0e\u7c97\u7c92\u5ea6\u6807\u6ce8\u76f8\u5f53\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u8bc4\u4f30\u957f\u683c\u5f0f\u4e34\u5e8a\u95ee\u7b54\u7cfb\u7edf\u8d44\u6e90\u5bc6\u96c6\u4e14\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u533b\u5b66\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4e14\u5728\u957f\u6587\u672c\u4e0a\u96be\u4ee5\u83b7\u5f97\u4e00\u81f4\u7684\u4eba\u7c7b\u5224\u65ad\u3002", "method": "\u57fa\u4e8e\u533b\u751f\u5bf9300\u4e2a\u771f\u5b9e\u60a3\u8005\u95ee\u9898\u7684\u6807\u6ce8\uff0c\u6bd4\u8f83\u4e86\u7c97\u7c92\u5ea6\u7b54\u6848\u7ea7\u548c\u7ec6\u7c92\u5ea6\u53e5\u5b50\u7ea7\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u6b63\u786e\u6027\u3001\u76f8\u5173\u6027\u548c\u5b89\u5168\u6027\u4e09\u4e2a\u7ef4\u5ea6\u7684\u8bc4\u5206\u8005\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u8bc4\u5206\u8005\u95f4\u4e00\u81f4\u6027\u56e0\u7ef4\u5ea6\u800c\u5f02\uff1a\u7ec6\u7c92\u5ea6\u6807\u6ce8\u5728\u6b63\u786e\u6027\u7ef4\u5ea6\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u7c97\u7c92\u5ea6\u5728\u76f8\u5173\u6027\u7ef4\u5ea6\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u5b89\u5168\u6027\u5224\u65ad\u4ecd\u4e0d\u4e00\u81f4\u3002\u6807\u6ce8\u5c11\u91cf\u53e5\u5b50\u5373\u53ef\u8fbe\u5230\u4e0e\u7c97\u7c92\u5ea6\u6807\u6ce8\u76f8\u5f53\u7684\u53ef\u9760\u6027\u3002", "conclusion": "LongQAEval\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5728\u6709\u9650\u8d44\u6e90\u548c\u9ad8\u4e13\u4e1a\u77e5\u8bc6\u73af\u5883\u4e0b\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5efa\u8bae\u6839\u636e\u8bc4\u4f30\u7ef4\u5ea6\u9009\u62e9\u5408\u9002\u7684\u6807\u6ce8\u7c92\u5ea6\uff0c\u5e76\u53ef\u901a\u8fc7\u6807\u6ce8\u5c11\u91cf\u53e5\u5b50\u6765\u964d\u4f4e\u6210\u672c\u548c\u52aa\u529b\u3002"}}
{"id": "2510.11290", "categories": ["cs.AI", "cs.HC", "I.2.6; J.4"], "pdf": "https://arxiv.org/pdf/2510.11290", "abs": "https://arxiv.org/abs/2510.11290", "authors": ["Sheng Jin", "Haoming Wang", "Zhiqi Gao", "Yongbo Yang", "Bao Chunjia", "Chengliang Wang"], "title": "Evolution in Simulation: AI-Agent School with Dual Memory for High-Fidelity Educational Dynamics", "comment": "9 pages, 7 figures, EMNLP conference", "summary": "Large language models (LLMs) based Agents are increasingly pivotal in\nsimulating and understanding complex human systems and interactions. We propose\nthe AI-Agent School (AAS) system, built around a self-evolving mechanism that\nleverages agents for simulating complex educational dynamics. Addressing the\nfragmented issues in teaching process modeling and the limitations of agents\nperformance in simulating diverse educational participants, AAS constructs the\nZero-Exp strategy, employs a continuous \"experience-reflection-optimization\"\ncycle, grounded in a dual memory base comprising experience and knowledge bases\nand incorporating short-term and long-term memory components. Through this\nmechanism, agents autonomously evolve via situated interactions within diverse\nsimulated school scenarios. This evolution enables agents to more accurately\nmodel the nuanced, multi-faceted teacher-student engagements and underlying\nlearning processes found in physical schools. Experiment confirms that AAS can\neffectively simulate intricate educational dynamics and is effective in\nfostering advanced agent cognitive abilities, providing a foundational stepping\nstone from the \"Era of Experience\" to the \"Era of Simulation\" by generating\nhigh-fidelity behavioral and interaction data.", "AI": {"tldr": "\u63d0\u51fa\u4e86AI-Agent School\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u8fdb\u5316\u673a\u5236\u6a21\u62df\u590d\u6742\u6559\u80b2\u52a8\u6001\uff0c\u91c7\u7528\"\u7ecf\u9a8c-\u53cd\u601d-\u4f18\u5316\"\u5faa\u73af\u548c\u53cc\u8bb0\u5fc6\u5e93\uff0c\u4f7f\u667a\u80fd\u4f53\u5728\u6559\u80b2\u573a\u666f\u4e2d\u81ea\u4e3b\u6f14\u5316\u3002", "motivation": "\u89e3\u51b3\u6559\u5b66\u8fc7\u7a0b\u5efa\u6a21\u788e\u7247\u5316\u95ee\u9898\uff0c\u514b\u670d\u667a\u80fd\u4f53\u5728\u6a21\u62df\u591a\u6837\u5316\u6559\u80b2\u53c2\u4e0e\u8005\u65f6\u7684\u6027\u80fd\u9650\u5236\uff0c\u66f4\u51c6\u786e\u5730\u6a21\u62df\u771f\u5b9e\u5b66\u6821\u4e2d\u5e08\u751f\u4e92\u52a8\u548c\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u6784\u5efaZero-Exp\u7b56\u7565\uff0c\u91c7\u7528\u8fde\u7eed\"\u7ecf\u9a8c-\u53cd\u601d-\u4f18\u5316\"\u5faa\u73af\u673a\u5236\uff0c\u57fa\u4e8e\u5305\u542b\u7ecf\u9a8c\u548c\u77e5\u8bc6\u5e93\u7684\u53cc\u8bb0\u5fc6\u5e93\uff0c\u6574\u5408\u77ed\u671f\u548c\u957f\u671f\u8bb0\u5fc6\u7ec4\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9eAAS\u80fd\u6709\u6548\u6a21\u62df\u590d\u6742\u6559\u80b2\u52a8\u6001\uff0c\u4fc3\u8fdb\u667a\u80fd\u4f53\u8ba4\u77e5\u80fd\u529b\u53d1\u5c55\uff0c\u751f\u6210\u9ad8\u4fdd\u771f\u884c\u4e3a\u4ea4\u4e92\u6570\u636e\u3002", "conclusion": "AAS\u4e3a\u4ece\"\u7ecf\u9a8c\u65f6\u4ee3\"\u8fc8\u5411\"\u6a21\u62df\u65f6\u4ee3\"\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u652f\u6491\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u6a21\u62df\u6570\u636e\u63a8\u52a8\u6559\u80b2\u7cfb\u7edf\u7814\u7a76\u3002"}}
{"id": "2510.10444", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10444", "abs": "https://arxiv.org/abs/2510.10444", "authors": ["Jingyi Chen", "Zhimeng Guo", "Jiyun Chun", "Pichao Wang", "Andrew Perrault", "Micha Elsner"], "title": "Do Audio LLMs Really LISTEN, or Just Transcribe? Measuring Lexical vs. Acoustic Emotion Cues Reliance", "comment": null, "summary": "Understanding emotion from speech requires sensitivity to both lexical and\nacoustic cues. However, it remains unclear whether large audio language models\n(LALMs) genuinely process acoustic information or rely primarily on lexical\ncontent. We present LISTEN (Lexical vs. Acoustic Speech Test for Emotion in\nNarratives), a controlled benchmark designed to disentangle lexical reliance\nfrom acoustic sensitivity in emotion understanding. Across evaluations of six\nstate-of-the-art LALMs, we observe a consistent lexical dominance. Models\npredict \"neutral\" when lexical cues are neutral or absent, show limited gains\nunder cue alignment, and fail to classify distinct emotions under cue conflict.\nIn paralinguistic settings, performance approaches chance. These results\nindicate that current LALMs largely \"transcribe\" rather than \"listen,\" relying\nheavily on lexical semantics while underutilizing acoustic cues. LISTEN offers\na principled framework for assessing emotion understanding in multimodal\nmodels.", "AI": {"tldr": "LISTEN\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u5f53\u524d\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u7406\u89e3\u4e2d\u4e3b\u8981\u4f9d\u8d56\u8bcd\u6c47\u7ebf\u7d22\u800c\u975e\u58f0\u5b66\u7ebf\u7d22\uff0c\u5b58\u5728\u8bcd\u6c47\u4e3b\u5bfc\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u5904\u7406\u58f0\u5b66\u4fe1\u606f\uff0c\u8fd8\u662f\u4e3b\u8981\u4f9d\u8d56\u8bcd\u6c47\u5185\u5bb9\u8fdb\u884c\u60c5\u611f\u7406\u89e3\u3002", "method": "\u5f00\u53d1LISTEN\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u63a7\u5236\u8bcd\u6c47\u548c\u58f0\u5b66\u7ebf\u7d22\u7684\u5bf9\u9f50\u4e0e\u51b2\u7a81\u6765\u8bc4\u4f30\u6a21\u578b\u8868\u73b0\uff0c\u6d4b\u8bd5\u4e866\u4e2a\u6700\u5148\u8fdb\u7684LALMs\u3002", "result": "\u6a21\u578b\u5728\u8bcd\u6c47\u7ebf\u7d22\u4e2d\u6027\u6216\u7f3a\u5931\u65f6\u9884\u6d4b\"\u4e2d\u6027\"\uff0c\u7ebf\u7d22\u5bf9\u9f50\u65f6\u6539\u8fdb\u6709\u9650\uff0c\u7ebf\u7d22\u51b2\u7a81\u65f6\u65e0\u6cd5\u533a\u5206\u4e0d\u540c\u60c5\u611f\uff0c\u5728\u526f\u8bed\u8a00\u60c5\u5883\u4e2d\u8868\u73b0\u63a5\u8fd1\u968f\u673a\u3002", "conclusion": "\u5f53\u524dLALMs\u4e3b\u8981\u4f9d\u8d56\u8bcd\u6c47\u8bed\u4e49\u800c\u672a\u80fd\u5145\u5206\u5229\u7528\u58f0\u5b66\u7ebf\u7d22\uff0c\u66f4\u50cf\u662f\"\u8f6c\u5f55\"\u800c\u975e\"\u8046\u542c\"\uff0cLISTEN\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u60c5\u611f\u7406\u89e3\u8bc4\u4f30\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6846\u67b6\u3002"}}
{"id": "2510.09914", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.09914", "abs": "https://arxiv.org/abs/2510.09914", "authors": ["Aditya Malusare", "Vineet Punyamoorty", "Vaneet Aggarwal"], "title": "Augmenting generative models with biomedical knowledge graphs improves targeted drug discovery", "comment": "This paper has been accepted for publication in the IEEE Transactions\n  on Artificial Intelligence, October 2025", "summary": "Recent breakthroughs in generative modeling have demonstrated remarkable\ncapabilities in molecular generation, yet the integration of comprehensive\nbiomedical knowledge into these models has remained an untapped frontier. In\nthis study, we introduce K-DREAM (Knowledge-Driven Embedding-Augmented Model),\na novel framework that leverages knowledge graphs to augment diffusion-based\ngenerative models for drug discovery. By embedding structured information from\nlarge-scale knowledge graphs, K-DREAM directs molecular generation toward\ncandidates with higher biological relevance and therapeutic suitability. This\nintegration ensures that the generated molecules are aligned with specific\ntherapeutic targets, moving beyond traditional heuristic-driven approaches. In\ntargeted drug design tasks, K-DREAM generates drug candidates with improved\nbinding affinities and predicted efficacy, surpassing current state-of-the-art\ngenerative models. It also demonstrates flexibility by producing molecules\ndesigned for multiple targets, enabling applications to complex disease\nmechanisms. These results highlight the utility of knowledge-enhanced\ngenerative models in rational drug design and their relevance to practical\ntherapeutic development.", "AI": {"tldr": "K-DREAM\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u6269\u6563\u751f\u6210\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u6765\u6307\u5bfc\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u5206\u5b50\u751f\u6210\uff0c\u63d0\u9ad8\u751f\u6210\u5206\u5b50\u7684\u751f\u7269\u76f8\u5173\u6027\u548c\u6cbb\u7597\u9002\u7528\u6027\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u6a21\u578b\u5728\u5206\u5b50\u751f\u6210\u65b9\u9762\u53d6\u5f97\u7a81\u7834\uff0c\u4f46\u5c1a\u672a\u5145\u5206\u5229\u7528\u5168\u9762\u7684\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u3002\u672c\u7814\u7a76\u65e8\u5728\u5c06\u77e5\u8bc6\u56fe\u8c31\u6574\u5408\u5230\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u4f7f\u5206\u5b50\u751f\u6210\u66f4\u7b26\u5408\u7279\u5b9a\u6cbb\u7597\u9776\u70b9\u3002", "method": "\u5f00\u53d1K-DREAM\u6846\u67b6\uff0c\u5229\u7528\u5927\u89c4\u6a21\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u5316\u4fe1\u606f\u6765\u589e\u5f3a\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\uff0c\u786e\u4fdd\u751f\u6210\u7684\u5206\u5b50\u4e0e\u7279\u5b9a\u6cbb\u7597\u9776\u70b9\u5bf9\u9f50\u3002", "result": "\u5728\u9776\u5411\u836f\u7269\u8bbe\u8ba1\u4efb\u52a1\u4e2d\uff0cK-DREAM\u751f\u6210\u7684\u5019\u9009\u836f\u7269\u5177\u6709\u66f4\u597d\u7684\u7ed3\u5408\u4eb2\u548c\u529b\u548c\u9884\u6d4b\u7597\u6548\uff0c\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\uff0c\u5e76\u80fd\u9488\u5bf9\u591a\u9776\u70b9\u8bbe\u8ba1\u5206\u5b50\u3002", "conclusion": "\u77e5\u8bc6\u589e\u5f3a\u7684\u751f\u6210\u6a21\u578b\u5728\u7406\u6027\u836f\u7269\u8bbe\u8ba1\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u5bf9\u5b9e\u9645\u6cbb\u7597\u5f00\u53d1\u5177\u6709\u76f8\u5173\u6027\u3002"}}
{"id": "2510.11313", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11313", "abs": "https://arxiv.org/abs/2510.11313", "authors": ["Le Ngoc Luyen", "Marie-H\u00e9l\u00e8ne Abel"], "title": "Automated Skill Decomposition Meets Expert Ontologies: Bridging the Granularity Gap with LLMs", "comment": null, "summary": "This paper investigates automated skill decomposition using Large Language\nModels (LLMs) and proposes a rigorous, ontology-grounded evaluation framework.\nOur framework standardizes the pipeline from prompting and generation to\nnormalization and alignment with ontology nodes. To evaluate outputs, we\nintroduce two metrics: a semantic F1-score that uses optimal embedding-based\nmatching to assess content accuracy, and a hierarchy-aware F1-score that\ncredits structurally correct placements to assess granularity. We conduct\nexperiments on ROME-ESCO-DecompSkill, a curated subset of parents, comparing\ntwo prompting strategies: zero-shot and leakage-safe few-shot with exemplars.\nAcross diverse LLMs, zero-shot offers a strong baseline, while few-shot\nconsistently stabilizes phrasing and granularity and improves hierarchy-aware\nalignment. A latency analysis further shows that exemplar-guided prompts are\ncompetitive - and sometimes faster - than unguided zero-shot due to more\nschema-compliant completions. Together, the framework, benchmark, and metrics\nprovide a reproducible foundation for developing ontology-faithful skill\ndecomposition systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u6280\u80fd\u5206\u89e3\u65b9\u6cd5\uff0c\u5efa\u7acb\u5305\u542b\u8bed\u4e49F1\u548c\u5c42\u6b21\u611f\u77e5F1\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6bd4\u8f83\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\u7b56\u7565\uff0c\u53d1\u73b0\u5c11\u6837\u672c\u5728\u7a33\u5b9a\u6027\u548c\u5c42\u6b21\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u73b0\u6709\u6280\u80fd\u5206\u89e3\u65b9\u6cd5\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u9700\u8981\u5efa\u7acb\u53ef\u91cd\u73b0\u7684\u3001\u57fa\u4e8e\u672c\u4f53\u7684\u8bc4\u4f30\u4f53\u7cfb\u6765\u5f00\u53d1\u5fe0\u5b9e\u4e8e\u672c\u4f53\u7684\u6280\u80fd\u5206\u89e3\u7cfb\u7edf\u3002", "method": "\u6784\u5efa\u6807\u51c6\u5316\u6d41\u7a0b\uff1a\u63d0\u793a\u751f\u6210\u2192\u89c4\u8303\u5316\u2192\u672c\u4f53\u8282\u70b9\u5bf9\u9f50\uff0c\u5f15\u5165\u8bed\u4e49F1\uff08\u57fa\u4e8e\u5d4c\u5165\u5339\u914d\uff09\u548c\u5c42\u6b21\u611f\u77e5F1\uff08\u8bc4\u4f30\u7ed3\u6784\u6b63\u786e\u6027\uff09\u4e24\u4e2a\u6307\u6807\uff0c\u5728ROME-ESCO-DecompSkill\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u96f6\u6837\u672c\u548c\u9632\u6cc4\u9732\u5c11\u6837\u672c\u63d0\u793a\u7b56\u7565\u3002", "result": "\u96f6\u6837\u672c\u63d0\u4f9b\u5f3a\u57fa\u7ebf\uff0c\u5c11\u6837\u672c\u5728\u77ed\u8bed\u7a33\u5b9a\u6027\u548c\u7c92\u5ea6\u63a7\u5236\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u80fd\u6539\u5584\u5c42\u6b21\u611f\u77e5\u5bf9\u9f50\uff0c\u4e14\u5ef6\u8fdf\u5206\u6790\u663e\u793a\u5f15\u5bfc\u5f0f\u63d0\u793a\u6709\u65f6\u6bd4\u65e0\u5f15\u5bfc\u96f6\u6837\u672c\u66f4\u5feb\u3002", "conclusion": "\u8be5\u6846\u67b6\u3001\u57fa\u51c6\u548c\u6307\u6807\u4e3a\u5f00\u53d1\u5fe0\u5b9e\u4e8e\u672c\u4f53\u7684\u6280\u80fd\u5206\u89e3\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u91cd\u73b0\u7684\u57fa\u7840\u3002"}}
{"id": "2510.10448", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10448", "abs": "https://arxiv.org/abs/2510.10448", "authors": ["Zhichao Xu", "Minheng Wang", "Yawei Wang", "Wenqian Ye", "Yuntao Du", "Yunpu Ma", "Yijun Tian"], "title": "RECON: Reasoning with Condensation for Efficient Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-augmented generation (RAG) systems trained using reinforcement\nlearning (RL) with reasoning are hampered by inefficient context management,\nwhere long, noisy retrieved documents increase costs and degrade performance.\nWe introduce RECON (REasoning with CONdensation), a framework that integrates\nan explicit summarization module to compress evidence within the reasoning\nloop. Our summarizer is trained via a two-stage process: relevance pretraining\non QA datasets, followed by multi-aspect distillation from proprietary LLMs to\nensure factuality and clarity. Integrated into the Search-R1 pipeline, RECON\nreduces total context length by 35\\%, leading to improved training speed and\ninference latency, while simultaneously improving RAG performance on downstream\nQA benchmarks. Notably, it boosts the average EM score of the 3B model by\n14.5\\% and the 7B model by 3.0\\%, showing particular strength in multi-hop QA.\nRECON demonstrates that learned context compression is essential for building\npractical, scalable, and performant RAG systems. Our code implementation is\nmade available at https://github.com/allfornancy/RECON.", "AI": {"tldr": "RECON\u662f\u4e00\u4e2a\u96c6\u6210\u663e\u5f0f\u6458\u8981\u6a21\u5757\u7684RAG\u6846\u67b6\uff0c\u901a\u8fc7\u538b\u7f29\u68c0\u7d22\u6587\u6863\u51cf\u5c1135%\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u63d0\u5347\u8bad\u7ec3\u901f\u5ea6\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u63d0\u9ad8QA\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684RAG\u7cfb\u7edf\u5b58\u5728\u4e0a\u4e0b\u6587\u7ba1\u7406\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u957f\u800c\u5608\u6742\u7684\u68c0\u7d22\u6587\u6863\u4f1a\u589e\u52a0\u6210\u672c\u5e76\u964d\u4f4e\u6027\u80fd\u3002", "method": "\u5f15\u5165\u663e\u5f0f\u6458\u8981\u6a21\u5757\u538b\u7f29\u8bc1\u636e\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u5728QA\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76f8\u5173\u6027\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u4ece\u4e13\u6709LLM\u8fdb\u884c\u591a\u65b9\u9762\u84b8\u998f\u4ee5\u786e\u4fdd\u4e8b\u5b9e\u6027\u548c\u6e05\u6670\u5ea6\u3002", "result": "\u5728Search-R1\u7ba1\u9053\u4e2d\uff0cRECON\u5c06\u603b\u4e0a\u4e0b\u6587\u957f\u5ea6\u51cf\u5c1135%\uff0c3B\u6a21\u578b\u5e73\u5747EM\u5206\u6570\u63d0\u534714.5%\uff0c7B\u6a21\u578b\u63d0\u53473.0%\uff0c\u5728\u591a\u8df3QA\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u5b66\u4e60\u5230\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u5bf9\u4e8e\u6784\u5efa\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u548c\u9ad8\u6027\u80fd\u7684RAG\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.09916", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09916", "abs": "https://arxiv.org/abs/2510.09916", "authors": ["Manuel Segura", "Pere Verg\u00e9s", "Richard Ky", "Ramesh Arangott", "Angela Kristine Garcia", "Thang Dihn Trong", "Makoto Hyodo", "Alexandru Nicolau", "Tony Givargis", "Sergio Gago-Masague"], "title": "Advancing Intoxication Detection: A Smartwatch-Based Approach", "comment": null, "summary": "Excess alcohol consumption leads to serious health risks and severe\nconsequences for both individuals and their communities. To advocate for\nhealthier drinking habits, we introduce a groundbreaking mobile smartwatch\napplication approach to just-in-time interventions for intoxication warnings.\nIn this work, we have created a dataset gathering TAC, accelerometer,\ngyroscope, and heart rate data from the participants during a period of three\nweeks. This is the first study to combine accelerometer, gyroscope, and heart\nrate smartwatch data collected over an extended monitoring period to classify\nintoxication levels. Previous research had used limited smartphone motion data\nand conventional machine learning (ML) algorithms to classify heavy drinking\nepisodes; in this work, we use smartwatch data and perform a thorough\nevaluation of different state-of-the-art classifiers such as the Transformer,\nBidirectional Long Short-Term Memory (bi-LSTM), Gated Recurrent Unit (GRU),\nOne-Dimensional Convolutional Neural Networks (1D-CNN), and Hyperdimensional\nComputing (HDC). We have compared performance metrics for the algorithms and\nassessed their efficiency on resource-constrained environments like mobile\nhardware. The HDC model achieved the best balance between accuracy and\nefficiency, demonstrating its practicality for smartwatch-based applications.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u624b\u8868\u7684\u79fb\u52a8\u5e94\u7528\uff0c\u7528\u4e8e\u5b9e\u65f6\u5e72\u9884\u9152\u7cbe\u4e2d\u6bd2\u8b66\u544a\uff0c\u901a\u8fc7\u6536\u96c6TAC\u3001\u52a0\u901f\u5ea6\u8ba1\u3001\u9640\u87ba\u4eea\u548c\u5fc3\u7387\u6570\u636e\uff0c\u8bc4\u4f30\u591a\u79cd\u5148\u8fdb\u5206\u7c7b\u5668\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u8fc7\u91cf\u996e\u9152\u5bf9\u4e2a\u4eba\u548c\u793e\u533a\u9020\u6210\u4e25\u91cd\u5065\u5eb7\u98ce\u9669\uff0c\u9700\u8981\u5f00\u53d1\u53ca\u65f6\u5e72\u9884\u6280\u672f\u6765\u4fc3\u8fdb\u66f4\u5065\u5eb7\u7684\u996e\u9152\u4e60\u60ef\u3002", "method": "\u6536\u96c6\u4e09\u5468\u5185\u53c2\u4e0e\u8005\u7684TAC\u3001\u52a0\u901f\u5ea6\u8ba1\u3001\u9640\u87ba\u4eea\u548c\u5fc3\u7387\u6570\u636e\uff0c\u4f7f\u7528Transformer\u3001bi-LSTM\u3001GRU\u30011D-CNN\u548cHDC\u7b49\u5148\u8fdb\u5206\u7c7b\u5668\u8fdb\u884c\u4e2d\u6bd2\u6c34\u5e73\u5206\u7c7b\u3002", "result": "HDC\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u7279\u522b\u9002\u5408\u57fa\u4e8e\u667a\u80fd\u624b\u8868\u7684\u5e94\u7528\u3002", "conclusion": "\u8d85\u7ef4\u8ba1\u7b97(HDC)\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u786c\u4ef6\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u667a\u80fd\u624b\u8868\u9152\u7cbe\u4e2d\u6bd2\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.11380", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11380", "abs": "https://arxiv.org/abs/2510.11380", "authors": ["Abdullah Al Mahmud", "Prangon Chowdhury", "Mohammed Borhan Uddin", "Khaled Eabne Delowar", "Tausifur Rahman Talha", "Bijoy Dewanjee"], "title": "AI-Driven anemia diagnosis: A review of advanced models and techniques", "comment": null, "summary": "Anemia, a condition marked by insufficient levels of red blood cells or\nhemoglobin, remains a widespread health issue affecting millions of individuals\nglobally. Accurate and timely diagnosis is essential for effective management\nand treatment of anemia. In recent years, there has been a growing interest in\nthe use of artificial intelligence techniques, i.e., machine learning (ML) and\ndeep learning (DL) for the detection, classification, and diagnosis of anemia.\nThis paper provides a systematic review of the recent advancements in this\nfield, with a focus on various models applied to anemia detection. The review\nalso compares these models based on several performance metrics, including\naccuracy, sensitivity, specificity, and precision. By analyzing these metrics,\nthe paper evaluates the strengths and limitation of discussed models in\ndetecting and classifying anemia, emphasizing the importance of addressing\nthese factors to improve diagnostic accuracy.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u8d2b\u8840\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u8bca\u65ad\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u6027\u80fd\u6307\u6807\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u8d2b\u8840\u68c0\u6d4b\u4e2d\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u8d2b\u8840\u662f\u4e00\u79cd\u5f71\u54cd\u5168\u7403\u6570\u767e\u4e07\u4eba\u7684\u5e38\u89c1\u5065\u5eb7\u95ee\u9898\uff0c\u51c6\u786e\u53ca\u65f6\u7684\u8bca\u65ad\u5bf9\u4e8e\u6709\u6548\u7ba1\u7406\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u3002\u8fd1\u5e74\u6765\uff0c\u4eba\u5de5\u667a\u80fd\u6280\u672f\u5728\u8d2b\u8840\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\u3002", "method": "\u901a\u8fc7\u5bf9\u8d2b\u8840\u68c0\u6d4b\u4e2d\u5e94\u7528\u7684\u591a\u79cd\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u7efc\u8ff0\uff0c\u5e76\u57fa\u4e8e\u51c6\u786e\u7387\u3001\u654f\u611f\u6027\u3001\u7279\u5f02\u6027\u548c\u7cbe\u786e\u5ea6\u7b49\u6027\u80fd\u6307\u6807\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u8d2b\u8840\u68c0\u6d4b\u548c\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u8bc6\u522b\u4e86\u5404\u6a21\u578b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u6539\u8fdb\u8bca\u65ad\u51c6\u786e\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u6027\u80fd\u6307\u6807\uff0c\u5f3a\u8c03\u4e86\u89e3\u51b3\u8fd9\u4e9b\u56e0\u7d20\u5bf9\u4e8e\u63d0\u9ad8\u8d2b\u8840\u8bca\u65ad\u51c6\u786e\u6027\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.10452", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10452", "abs": "https://arxiv.org/abs/2510.10452", "authors": ["Utsav Maskey", "Mark Dras", "Usman Naseem"], "title": "Steering Over-refusals Towards Safety in Retrieval Augmented Generation", "comment": "Preprint", "summary": "Safety alignment in large language models (LLMs) induces over-refusals --\nwhere LLMs decline benign requests due to aggressive safety filters. We analyze\nthis phenomenon in retrieval-augmented generation (RAG), where both the query\nintent and retrieved context properties influence refusal behavior. We\nconstruct RagRefuse, a domain-stratified benchmark spanning medical, chemical,\nand open domains, pairing benign and harmful queries with controlled context\ncontamination patterns and sizes. Our analysis shows that context arrangement /\ncontamination, domain of query and context, and harmful-text density trigger\nrefusals even on benign queries, with effects depending on model-specific\nalignment choices. To mitigate over-refusals, we introduce\n\\textsc{SafeRAG-Steering}, a model-centric embedding intervention that steers\nthe embedding regions towards the confirmed safe, non-refusing output regions\nat inference time. This reduces over-refusals in contaminated RAG pipelines\nwhile preserving legitimate refusals.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u5bf9\u9f50\u4e2d\u5bfc\u81f4\u7684\u8fc7\u5ea6\u62d2\u7edd\u95ee\u9898\uff0c\u6784\u5efa\u4e86RagRefuse\u57fa\u51c6\uff0c\u5e76\u63d0\u51fa\u4e86SafeRAG-Steering\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u62d2\u7edd\uff0c\u5373\u6a21\u578b\u4f1a\u62d2\u7edd\u826f\u6027\u8bf7\u6c42\u3002\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\uff0c\u67e5\u8be2\u610f\u56fe\u548c\u68c0\u7d22\u4e0a\u4e0b\u6587\u5c5e\u6027\u90fd\u4f1a\u5f71\u54cd\u62d2\u7edd\u884c\u4e3a\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u8fd9\u79cd\u73b0\u8c61\u3002", "method": "\u6784\u5efaRagRefuse\u57fa\u51c6\uff0c\u6db5\u76d6\u533b\u7597\u3001\u5316\u5b66\u548c\u5f00\u653e\u9886\u57df\uff0c\u914d\u5bf9\u826f\u6027\u548c\u6709\u5bb3\u67e5\u8be2\uff0c\u63a7\u5236\u4e0a\u4e0b\u6587\u6c61\u67d3\u6a21\u5f0f\u548c\u5927\u5c0f\u3002\u63d0\u51faSafeRAG-Steering\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u4e2d\u5fc3\u5d4c\u5165\u5e72\u9884\u5728\u63a8\u7406\u65f6\u5c06\u5d4c\u5165\u533a\u57df\u5bfc\u5411\u786e\u8ba4\u5b89\u5168\u7684\u975e\u62d2\u7edd\u8f93\u51fa\u533a\u57df\u3002", "result": "\u5206\u6790\u663e\u793a\u4e0a\u4e0b\u6587\u5b89\u6392/\u6c61\u67d3\u3001\u67e5\u8be2\u548c\u4e0a\u4e0b\u6587\u9886\u57df\u3001\u6709\u5bb3\u6587\u672c\u5bc6\u5ea6\u90fd\u4f1a\u89e6\u53d1\u5bf9\u826f\u6027\u67e5\u8be2\u7684\u62d2\u7edd\uff0c\u6548\u679c\u53d6\u51b3\u4e8e\u6a21\u578b\u7279\u5b9a\u7684\u5bf9\u9f50\u9009\u62e9\u3002SafeRAG-Steering\u65b9\u6cd5\u80fd\u5728\u4fdd\u6301\u5408\u6cd5\u62d2\u7edd\u7684\u540c\u65f6\u51cf\u5c11\u6c61\u67d3RAG\u7ba1\u9053\u4e2d\u7684\u8fc7\u5ea6\u62d2\u7edd\u3002", "conclusion": "\u5b89\u5168\u5bf9\u9f50\u4f1a\u5bfc\u81f4LLMs\u5728RAG\u4e2d\u8fc7\u5ea6\u62d2\u7edd\u826f\u6027\u8bf7\u6c42\uff0c\u4e0a\u4e0b\u6587\u6c61\u67d3\u662f\u91cd\u8981\u56e0\u7d20\u3002\u63d0\u51fa\u7684SafeRAG-Steering\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e73\u8861\u5b89\u5168\u6027\u548c\u53ef\u7528\u6027\u3002"}}
{"id": "2510.11457", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11457", "abs": "https://arxiv.org/abs/2510.11457", "authors": ["Beining Wang", "Weihang Su", "Hongtao Tian", "Tao Yang", "Yujia Zhou", "Ting Yao", "Qingyao Ai", "Yiqun Liu"], "title": "From <Answer> to <Think>: Multidimensional Supervision of Reasoning Process for LLM Optimization", "comment": null, "summary": "Improving the multi-step reasoning ability of Large Language Models (LLMs) is\na critical yet challenging task. The dominant paradigm, outcome-supervised\nreinforcement learning (RLVR), rewards only correct final answers, often\npropagating flawed reasoning and suffering from sparse reward signals. While\nprocess-level reward models (PRMs) provide denser, step-by-step feedback, they\nlack generalizability and interpretability, requiring task-specific\nsegmentation of the reasoning process. To this end, we propose the\nDimension-level Reward Model (DRM), a new supervision framework that bridges\nthe gap between these two approaches. DRM evaluates the quality of a reasoning\nprocess along three fundamental, complementary, and interpretable dimensions:\nConfidence for uncertainty calibration, Relevance for semantic alignment, and\nCoherence for logical consistency. Together, these dimensions capture aspects\nbeyond final answer correctness and enable interpretable assessment without\nrequiring ground truth answers. Experimental results show that DRM provides\neffective supervision signals, guides the optimization of LLMs and enhances\ntheir reasoning ability. In particular, DRM-supervised training achieves\nconsistent gains on both in-distribution and out-of-distribution open-domain\ntasks, including mathematics, question answering, code execution, and puzzles.\nOur findings demonstrate that multidimensional supervision of the reasoning\nprocess can improve the generalized reasoning ability of LLMs beyond the\ntraining distribution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7ef4\u5ea6\u7ea7\u5956\u52b1\u6a21\u578b(DRM)\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u3001\u76f8\u5173\u6027\u548c\u8fde\u8d2f\u6027\u4e09\u4e2a\u7ef4\u5ea6\u6765\u8bc4\u4f30\u63a8\u7406\u8fc7\u7a0b\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7ed3\u679c\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u7684\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u63d0\u5347\u4e86LLMs\u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u7ed3\u679c\u76d1\u7763\u5f3a\u5316\u5b66\u4e60(RLVR)\u53ea\u5956\u52b1\u6700\u7ec8\u6b63\u786e\u7b54\u6848\uff0c\u5bb9\u6613\u4f20\u64ad\u9519\u8bef\u63a8\u7406\u4e14\u5956\u52b1\u4fe1\u53f7\u7a00\u758f\uff1b\u8fc7\u7a0b\u7ea7\u5956\u52b1\u6a21\u578b(PRMs)\u867d\u7136\u63d0\u4f9b\u66f4\u5bc6\u96c6\u7684\u53cd\u9988\uff0c\u4f46\u7f3a\u4e4f\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u7684\u63a8\u7406\u8fc7\u7a0b\u5206\u5272\u3002", "method": "\u63d0\u51fa\u7ef4\u5ea6\u7ea7\u5956\u52b1\u6a21\u578b(DRM)\uff0c\u4ece\u4e09\u4e2a\u57fa\u7840\u3001\u4e92\u8865\u4e14\u53ef\u89e3\u91ca\u7684\u7ef4\u5ea6\u8bc4\u4f30\u63a8\u7406\u8fc7\u7a0b\u8d28\u91cf\uff1a\u7f6e\u4fe1\u5ea6\uff08\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff09\u3001\u76f8\u5173\u6027\uff08\u8bed\u4e49\u5bf9\u9f50\uff09\u548c\u8fde\u8d2f\u6027\uff08\u903b\u8f91\u4e00\u81f4\u6027\uff09\u3002\u8fd9\u4e9b\u7ef4\u5ea6\u80fd\u591f\u6355\u6349\u8d85\u8d8a\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\u7684\u65b9\u9762\uff0c\u4e14\u65e0\u9700\u771f\u5b9e\u7b54\u6848\u5373\u53ef\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eDRM\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u6307\u5bfcLLMs\u7684\u4f18\u5316\u5e76\u589e\u5f3a\u5176\u63a8\u7406\u80fd\u529b\u3002DRM\u76d1\u7763\u8bad\u7ec3\u5728\u6570\u5b66\u3001\u95ee\u7b54\u3001\u4ee3\u7801\u6267\u884c\u548c\u8c1c\u9898\u7b49\u5f00\u653e\u57df\u4efb\u52a1\u4e0a\uff0c\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u90fd\u53d6\u5f97\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u5bf9\u63a8\u7406\u8fc7\u7a0b\u8fdb\u884c\u591a\u7ef4\u76d1\u7763\u53ef\u4ee5\u63d0\u5347LLMs\u5728\u8bad\u7ec3\u5206\u5e03\u4e4b\u5916\u7684\u6cdb\u5316\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u591a\u7ef4\u76d1\u7763\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.10453", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10453", "abs": "https://arxiv.org/abs/2510.10453", "authors": ["Peng Fan", "Wenping Wang", "Fei Deng"], "title": "End-to-end Speech Recognition with similar length speech and text", "comment": null, "summary": "The mismatch of speech length and text length poses a challenge in automatic\nspeech recognition (ASR). In previous research, various approaches have been\nemployed to align text with speech, including the utilization of Connectionist\nTemporal Classification (CTC). In earlier work, a key frame mechanism (KFDS)\nwas introduced, utilizing intermediate CTC outputs to guide downsampling and\npreserve keyframes, but traditional methods (CTC) failed to align speech and\ntext appropriately when downsampling speech to a text-similar length. In this\npaper, we focus on speech recognition in those cases where the length of speech\naligns closely with that of the corresponding text. To address this issue, we\nintroduce two methods for alignment: a) Time Independence Loss (TIL) and b)\nAligned Cross Entropy (AXE) Loss, which is based on edit distance. To enhance\nthe information on keyframes, we incorporate frame fusion by applying weights\nand summing the keyframe with its context 2 frames. Experimental results on\nAISHELL-1 and AISHELL-2 dataset subsets show that the proposed methods\noutperform the previous work and achieve a reduction of at least 86\\% in the\nnumber of frames.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u8bed\u97f3\u8bc6\u522b\u4e2d\u8bed\u97f3\u4e0e\u6587\u672c\u957f\u5ea6\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u5bf9\u9f50\u65b9\u6cd5\uff1a\u65f6\u95f4\u72ec\u7acb\u635f\u5931(TIL)\u548c\u57fa\u4e8e\u7f16\u8f91\u8ddd\u79bb\u7684\u5bf9\u9f50\u4ea4\u53c9\u71b5\u635f\u5931(AXELoss)\uff0c\u5e76\u901a\u8fc7\u5e27\u878d\u5408\u589e\u5f3a\u5173\u952e\u5e27\u4fe1\u606f\uff0c\u5728AISHELL\u6570\u636e\u96c6\u4e0a\u663e\u8457\u51cf\u5c11\u4e86\u5e27\u6570\u3002", "motivation": "\u89e3\u51b3\u8bed\u97f3\u8bc6\u522b\u4e2d\u8bed\u97f3\u957f\u5ea6\u4e0e\u6587\u672c\u957f\u5ea6\u4e0d\u5339\u914d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4e24\u8005\u957f\u5ea6\u76f8\u8fd1\u7684\u60c5\u51b5\u4e0b\uff0c\u4f20\u7edfCTC\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5bf9\u9f50\u8bed\u97f3\u548c\u6587\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u5bf9\u9f50\u65b9\u6cd5\uff1a\u65f6\u95f4\u72ec\u7acb\u635f\u5931(TIL)\u548c\u5bf9\u9f50\u4ea4\u53c9\u71b5\u635f\u5931(AXELoss)\uff0c\u5e76\u5f15\u5165\u5e27\u878d\u5408\u6280\u672f\uff0c\u901a\u8fc7\u52a0\u6743\u6c42\u548c\u5173\u952e\u5e27\u53ca\u5176\u4e0a\u4e0b\u65872\u5e27\u6765\u589e\u5f3a\u5173\u952e\u5e27\u4fe1\u606f\u3002", "result": "\u5728AISHELL-1\u548cAISHELL-2\u6570\u636e\u96c6\u5b50\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\uff0c\u5e27\u6570\u51cf\u5c11\u4e86\u81f3\u5c1186%\u3002", "conclusion": "\u63d0\u51fa\u7684TIL\u548cAXELoss\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u97f3\u4e0e\u6587\u672c\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5904\u7406\u6240\u9700\u7684\u5e27\u6570\uff0c\u63d0\u9ad8\u4e86\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2510.09926", "categories": ["cs.LG", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.09926", "abs": "https://arxiv.org/abs/2510.09926", "authors": ["Naman Agrawal"], "title": "Phase-Aware Deep Learning with Complex-Valued CNNs for Audio Signal Applications", "comment": null, "summary": "This study explores the design and application of Complex-Valued\nConvolutional Neural Networks (CVCNNs) in audio signal processing, with a focus\non preserving and utilizing phase information often neglected in real-valued\nnetworks. We begin by presenting the foundational theoretical concepts of\nCVCNNs, including complex convolutions, pooling layers, Wirtinger-based\ndifferentiation, and various complex-valued activation functions. These are\ncomplemented by critical adaptations of training techniques, including complex\nbatch normalization and weight initialization schemes, to ensure stability in\ntraining dynamics. Empirical evaluations are conducted across three stages.\nFirst, CVCNNs are benchmarked on standard image datasets, where they\ndemonstrate competitive performance with real-valued CNNs, even under synthetic\ncomplex perturbations. Although our focus is audio signal processing, we first\nevaluate CVCNNs on image datasets to establish baseline performance and\nvalidate training stability before applying them to audio tasks. In the second\nexperiment, we focus on audio classification using Mel-Frequency Cepstral\nCoefficients (MFCCs). CVCNNs trained on real-valued MFCCs slightly outperform\nreal CNNs, while preserving phase in input workflows highlights challenges in\nexploiting phase without architectural modifications. Finally, a third\nexperiment introduces GNNs to model phase information via edge weighting, where\nthe inclusion of phase yields measurable gains in both binary and multi-class\ngenre classification. These results underscore the expressive capacity of\ncomplex-valued architectures and confirm phase as a meaningful and exploitable\nfeature in audio processing applications. While current methods show promise,\nespecially with activations like cardioid, future advances in phase-aware\ndesign will be essential to leverage the potential of complex representations\nin neural networks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u590d\u503c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u97f3\u9891\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u7814\u7a76\u5982\u4f55\u4fdd\u7559\u548c\u5229\u7528\u5b9e\u503c\u7f51\u7edc\u901a\u5e38\u5ffd\u7565\u7684\u76f8\u4f4d\u4fe1\u606f\u3002", "motivation": "\u5b9e\u503c\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u97f3\u9891\u4fe1\u53f7\u65f6\u7ecf\u5e38\u5ffd\u7565\u76f8\u4f4d\u4fe1\u606f\uff0c\u800c\u590d\u503c\u7f51\u7edc\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u7559\u548c\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\uff0c\u63d0\u5347\u97f3\u9891\u5904\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u590d\u503cCNN\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5305\u62ec\u590d\u5377\u79ef\u3001\u6c60\u5316\u5c42\u3001Wirtinger\u5fae\u5206\u548c\u590d\u503c\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u91c7\u7528\u590d\u6279\u91cf\u5f52\u4e00\u5316\u548c\u6743\u91cd\u521d\u59cb\u5316\u6280\u672f\u786e\u4fdd\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\uff1a\u56fe\u50cf\u6570\u636e\u96c6\u57fa\u51c6\u6d4b\u8bd5\u3001\u57fa\u4e8eMFCC\u7684\u97f3\u9891\u5206\u7c7b\u3001\u4ee5\u53ca\u4f7f\u7528GNN\u5efa\u6a21\u76f8\u4f4d\u4fe1\u606f\u7684\u5b9e\u9a8c\u3002", "result": "\u590d\u503cCNN\u5728\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0e\u5b9e\u503cCNN\u76f8\u5f53\uff1b\u5728\u97f3\u9891\u5206\u7c7b\u4e2d\uff0c\u57fa\u4e8e\u5b9e\u503cMFCC\u7684\u590d\u503cCNN\u7565\u4f18\u4e8e\u5b9e\u503cCNN\uff1b\u901a\u8fc7GNN\u5efa\u6a21\u76f8\u4f4d\u4fe1\u606f\u5728\u6d41\u6d3e\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u53ef\u8861\u91cf\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u590d\u503c\u67b6\u6784\u5177\u6709\u5f3a\u5927\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u76f8\u4f4d\u4fe1\u606f\u662f\u97f3\u9891\u5904\u7406\u4e2d\u6709\u610f\u4e49\u4e14\u53ef\u5229\u7528\u7684\u7279\u5f81\u3002\u867d\u7136\u5f53\u524d\u65b9\u6cd5\uff08\u7279\u522b\u662fcardioid\u6fc0\u6d3b\u51fd\u6570\uff09\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u672a\u6765\u9700\u8981\u5f00\u53d1\u66f4\u591a\u76f8\u4f4d\u611f\u77e5\u8bbe\u8ba1\u6765\u5145\u5206\u53d1\u6325\u590d\u503c\u8868\u793a\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.11462", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11462", "abs": "https://arxiv.org/abs/2510.11462", "authors": ["Yisen Gao", "Jiaxin Bai", "Yi Huang", "Xingcheng Fu", "Qingyun Sun", "Yangqiu Song"], "title": "Unifying Deductive and Abductive Reasoning in Knowledge Graphs with Masked Diffusion Model", "comment": "Under Review", "summary": "Deductive and abductive reasoning are two critical paradigms for analyzing\nknowledge graphs, enabling applications from financial query answering to\nscientific discovery. Deductive reasoning on knowledge graphs usually involves\nretrieving entities that satisfy a complex logical query, while abductive\nreasoning generates plausible logical hypotheses from observations. Despite\ntheir clear synergistic potential, where deduction can validate hypotheses and\nabduction can uncover deeper logical patterns, existing methods address them in\nisolation. To bridge this gap, we propose DARK, a unified framework for\nDeductive and Abductive Reasoning in Knowledge graphs. As a masked diffusion\nmodel capable of capturing the bidirectional relationship between queries and\nconclusions, DARK has two key innovations. First, to better leverage deduction\nfor hypothesis refinement during abductive reasoning, we introduce a\nself-reflective denoising process that iteratively generates and validates\ncandidate hypotheses against the observed conclusion. Second, to discover\nricher logical associations, we propose a logic-exploration reinforcement\nlearning approach that simultaneously masks queries and conclusions, enabling\nthe model to explore novel reasoning compositions. Extensive experiments on\nmultiple benchmark knowledge graphs show that DARK achieves state-of-the-art\nperformance on both deductive and abductive reasoning tasks, demonstrating the\nsignificant benefits of our unified approach.", "AI": {"tldr": "DARK\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u6f14\u7ece\u548c\u6eaf\u56e0\u63a8\u7406\uff0c\u901a\u8fc7\u63a9\u7801\u6269\u6563\u6a21\u578b\u6355\u83b7\u67e5\u8be2\u4e0e\u7ed3\u8bba\u7684\u53cc\u5411\u5173\u7cfb\uff0c\u5728\u4e24\u79cd\u63a8\u7406\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u6f14\u7ece\u63a8\u7406\uff08\u4ece\u67e5\u8be2\u68c0\u7d22\u5b9e\u4f53\uff09\u548c\u6eaf\u56e0\u63a8\u7406\uff08\u4ece\u89c2\u5bdf\u751f\u6210\u5047\u8bbe\uff09\u5206\u5f00\u5904\u7406\uff0c\u4f46\u5b83\u4eec\u5177\u6709\u660e\u663e\u7684\u534f\u540c\u6f5c\u529b\uff0c\u9700\u8981\u7edf\u4e00\u6846\u67b6\u6765\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faDARK\u6846\u67b6\uff1a1\uff09\u81ea\u53cd\u53bb\u566a\u8fc7\u7a0b\uff0c\u5728\u6eaf\u56e0\u63a8\u7406\u4e2d\u8fed\u4ee3\u751f\u6210\u548c\u9a8c\u8bc1\u5019\u9009\u5047\u8bbe\uff1b2\uff09\u903b\u8f91\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u540c\u65f6\u63a9\u7801\u67e5\u8be2\u548c\u7ed3\u8bba\u4ee5\u63a2\u7d22\u65b0\u9896\u63a8\u7406\u7ec4\u5408\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u77e5\u8bc6\u56fe\u8c31\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDARK\u5728\u6f14\u7ece\u548c\u6eaf\u56e0\u63a8\u7406\u4efb\u52a1\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u7edf\u4e00\u6f14\u7ece\u548c\u6eaf\u56e0\u63a8\u7406\u7684\u65b9\u6cd5\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0cDARK\u6846\u67b6\u6210\u529f\u5c55\u793a\u4e86\u8fd9\u79cd\u534f\u540c\u6548\u5e94\uff0c\u4e3a\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.10457", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10457", "abs": "https://arxiv.org/abs/2510.10457", "authors": ["Shaobo Wang", "Cong Wang", "Wenjie Fu", "Yue Min", "Mingquan Feng", "Isabel Guan", "Xuming Hu", "Conghui He", "Cunxiang Wang", "Kexin Yang", "Xingzhang Ren", "Fei Huang", "Dayiheng Liu", "Linfeng Zhang"], "title": "Rethinking LLM Evaluation: Can We Evaluate LLMs with 200x Less Data?", "comment": "18 pages, 5 figures", "summary": "As the demand for comprehensive evaluations of diverse model capabilities\nsteadily increases, benchmark suites have correspondingly grown significantly\nin scale. Despite notable advances in redundancy reduction and subset-level\nperformance prediction, a systematic framework that effectively integrates\nthese methods to ensure both prediction accuracy and ranking consistency is\nstill largely elusive. In this paper, we first perform a sample-level analysis\nof benchmark redundancy and identify several highly similar samples that can be\neliminated. Besides, we frame benchmark compression as an optimization problem\nwith the aim of score reconstruction. Building on these, we then propose\nEssenceBench, a coarse-to-fine framework utilizing an iterative Genetic\nAlgorithm (GA), which takes the advantages of fitness-based subset search and\nattribution-based sample search. Compared to previous methods, our approach\nyields superior compression results with lower reconstruction error and\nmarkedly higher efficiency. In particular, on the HellaSwag benchmark (10K\nsamples), our method preserves the ranking of all models shifting within 5%\nusing 25x fewer samples, and achieves 95% ranking preservation shifting within\n5% using only 200x fewer samples.", "AI": {"tldr": "\u63d0\u51faEssenceBench\u6846\u67b6\uff0c\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u8fdb\u884c\u7c97\u5230\u7ec6\u7684\u57fa\u51c6\u538b\u7f29\uff0c\u663e\u8457\u51cf\u5c11\u6837\u672c\u6570\u91cf\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6392\u540d\u51c6\u786e\u6027", "motivation": "\u73b0\u6709\u57fa\u51c6\u5957\u4ef6\u89c4\u6a21\u5e9e\u5927\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6846\u67b6\u6765\u6574\u5408\u5197\u4f59\u51cf\u5c11\u548c\u6027\u80fd\u9884\u6d4b\u65b9\u6cd5\uff0c\u9700\u8981\u786e\u4fdd\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6392\u540d\u4e00\u81f4\u6027", "method": "\u9996\u5148\u8fdb\u884c\u6837\u672c\u7ea7\u5197\u4f59\u5206\u6790\uff0c\u5c06\u57fa\u51c6\u538b\u7f29\u6784\u5efa\u4e3a\u5206\u6570\u91cd\u6784\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u8fed\u4ee3\u9057\u4f20\u7b97\u6cd5\u7684\u7c97\u5230\u7ec6\u6846\u67b6\uff0c\u7ed3\u5408\u9002\u5e94\u5ea6\u5b50\u96c6\u641c\u7d22\u548c\u5c5e\u6027\u6837\u672c\u641c\u7d22", "result": "\u5728HellaSwag\u57fa\u51c6\u4e0a\uff0c\u4f7f\u752825\u500d\u66f4\u5c11\u6837\u672c\u65f6\u6240\u6709\u6a21\u578b\u6392\u540d\u53d8\u5316\u57285%\u4ee5\u5185\uff0c\u4f7f\u7528200\u500d\u66f4\u5c11\u6837\u672c\u65f6\u8fbe\u523095%\u6392\u540d\u4fdd\u6301\u7387", "conclusion": "EssenceBench\u65b9\u6cd5\u5728\u538b\u7f29\u6548\u7387\u548c\u6392\u540d\u4fdd\u6301\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u57fa\u51c6\u89c4\u6a21\u540c\u65f6\u4fdd\u6301\u8bc4\u4f30\u51c6\u786e\u6027"}}
{"id": "2510.09930", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09930", "abs": "https://arxiv.org/abs/2510.09930", "authors": ["Ching Chang", "Ming-Chih Lo", "Chiao-Tung Chan", "Wen-Chih Peng", "Tien-Fu Chen"], "title": "MemPromptTSS: Persistent Prompt Memory for Iterative Multi-Granularity Time Series State Segmentation", "comment": "This paper is currently under review. The code will be made available\n  upon acceptance", "summary": "Web platforms, mobile applications, and connected sensing systems generate\nmultivariate time series with states at multiple levels of granularity, from\ncoarse regimes to fine-grained events. Effective segmentation in these settings\nrequires integrating across granularities while supporting iterative refinement\nthrough sparse prompt signals, which provide a compact mechanism for injecting\ndomain knowledge. Yet existing prompting approaches for time series\nsegmentation operate only within local contexts, so the effect of a prompt\nquickly fades and cannot guide predictions across the entire sequence. To\novercome this limitation, we propose MemPromptTSS, a framework for iterative\nmulti-granularity segmentation that introduces persistent prompt memory. A\nmemory encoder transforms prompts and their surrounding subsequences into\nmemory tokens stored in a bank. This persistent memory enables each new\nprediction to condition not only on local cues but also on all prompts\naccumulated across iterations, ensuring their influence persists across the\nentire sequence. Experiments on six datasets covering wearable sensing and\nindustrial monitoring show that MemPromptTSS achieves 23% and 85% accuracy\nimprovements over the best baseline in single- and multi-granularity\nsegmentation under single iteration inference, and provides stronger refinement\nin iterative inference with average per-iteration gains of 2.66 percentage\npoints compared to 1.19 for PromptTSS. These results highlight the importance\nof persistent memory for prompt-guided segmentation, establishing MemPromptTSS\nas a practical and effective framework for real-world applications.", "AI": {"tldr": "MemPromptTSS\u662f\u4e00\u4e2a\u7528\u4e8e\u8fed\u4ee3\u591a\u7c92\u5ea6\u65f6\u95f4\u5e8f\u5217\u5206\u5272\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u6301\u4e45\u63d0\u793a\u8bb0\u5fc6\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u63d0\u793a\u5f71\u54cd\u5feb\u901f\u8870\u51cf\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u5272\u63d0\u793a\u65b9\u6cd5\u53ea\u5728\u5c40\u90e8\u4e0a\u4e0b\u6587\u4e2d\u64cd\u4f5c\uff0c\u63d0\u793a\u7684\u5f71\u54cd\u5f88\u5feb\u6d88\u5931\uff0c\u65e0\u6cd5\u5728\u6574\u4e2a\u5e8f\u5217\u4e2d\u6307\u5bfc\u9884\u6d4b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4f7f\u63d0\u793a\u5f71\u54cd\u6301\u4e45\u5316\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMemPromptTSS\u6846\u67b6\uff0c\u4f7f\u7528\u8bb0\u5fc6\u7f16\u7801\u5668\u5c06\u63d0\u793a\u53ca\u5176\u5468\u56f4\u5b50\u5e8f\u5217\u8f6c\u6362\u4e3a\u5b58\u50a8\u5728\u8bb0\u5fc6\u5e93\u4e2d\u7684\u8bb0\u5fc6\u6807\u8bb0\uff0c\u4f7f\u6bcf\u4e2a\u65b0\u9884\u6d4b\u4e0d\u4ec5\u57fa\u4e8e\u5c40\u90e8\u7ebf\u7d22\uff0c\u8fd8\u57fa\u4e8e\u6240\u6709\u7d2f\u79ef\u7684\u63d0\u793a\u3002", "result": "\u57286\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMemPromptTSS\u5728\u5355\u6b21\u8fed\u4ee3\u63a8\u7406\u4e2d\u6bd4\u6700\u4f73\u57fa\u7ebf\u5728\u5355\u7c92\u5ea6\u548c\u591a\u7c92\u5ea6\u5206\u5272\u4e0a\u5206\u522b\u63d0\u534723%\u548c85%\u7684\u51c6\u786e\u7387\uff0c\u5728\u8fed\u4ee3\u63a8\u7406\u4e2d\u6bcf\u8f6e\u5e73\u5747\u589e\u76ca\u4e3a2.66\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u6301\u4e45\u8bb0\u5fc6\u5bf9\u4e8e\u63d0\u793a\u5f15\u5bfc\u7684\u5206\u5272\u81f3\u5173\u91cd\u8981\uff0cMemPromptTSS\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u6709\u6548\u7684\u6846\u67b6\u3002"}}
{"id": "2510.11558", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11558", "abs": "https://arxiv.org/abs/2510.11558", "authors": ["Komal Gupta", "Aditya Shrivastava"], "title": "Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative Study of Market Leading Agentic AI Products", "comment": null, "summary": "Governance of data, compliance, and business privacy matters, particularly\nfor healthcare and finance businesses. Since the recent emergence of AI\nenterprise AI assistants enhancing business productivity, safeguarding private\ndata and compliance is now a priority. With the implementation of AI assistants\nacross the enterprise, the zero data retention can be achieved by implementing\nzero data retention policies by Large Language Model businesses like Open AI\nand Anthropic and Meta. In this work, we explore zero data retention policies\nfor the Enterprise apps of large language models (LLMs). Our key contribution\nis defining the architectural, compliance, and usability trade-offs of such\nsystems in parallel. In this research work, we examine the development of\ncommercial AI assistants with two industry leaders and market titans in this\narena - Salesforce and Microsoft. Both of these companies used distinct\ntechnical architecture to support zero data retention policies. Salesforce\nAgentForce and Microsoft Copilot are among the leading AI assistants providing\nmuch-needed push to business productivity in customer care. The purpose of this\npaper is to analyze the technical architecture and deployment of zero data\nretention policy by consuming applications as well as big language models\nservice providers like Open Ai, Anthropic, and Meta.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4f01\u4e1aAI\u52a9\u624b\u4e2d\u7684\u96f6\u6570\u636e\u4fdd\u7559\u653f\u7b56\uff0c\u91cd\u70b9\u5173\u6ce8Salesforce\u548cMicrosoft\u7684\u6280\u672f\u67b6\u6784\u5b9e\u73b0\uff0c\u63a2\u8ba8\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u5408\u89c4\u6027\u65b9\u9762\u7684\u6743\u8861\u3002", "motivation": "\u968f\u7740\u4f01\u4e1aAI\u52a9\u624b\u7684\u666e\u53ca\uff0c\u4fdd\u62a4\u79c1\u4eba\u6570\u636e\u548c\u786e\u4fdd\u5408\u89c4\u6027\u6210\u4e3a\u4f18\u5148\u4e8b\u9879\uff0c\u9700\u8981\u7814\u7a76\u96f6\u6570\u636e\u4fdd\u7559\u653f\u7b56\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f01\u4e1a\u5e94\u7528\u4e2d\u7684\u5b9e\u65bd\u3002", "method": "\u901a\u8fc7\u5206\u6790Salesforce AgentForce\u548cMicrosoft Copilot\u4e24\u4e2a\u884c\u4e1a\u9886\u5148\u7684AI\u52a9\u624b\uff0c\u7814\u7a76\u5b83\u4eec\u91c7\u7528\u7684\u4e0d\u540c\u6280\u672f\u67b6\u6784\u6765\u652f\u6301\u96f6\u6570\u636e\u4fdd\u7559\u653f\u7b56\u3002", "result": "\u8bc6\u522b\u4e86\u96f6\u6570\u636e\u4fdd\u7559\u7cfb\u7edf\u5728\u67b6\u6784\u3001\u5408\u89c4\u6027\u548c\u53ef\u7528\u6027\u65b9\u9762\u7684\u5173\u952e\u6743\u8861\u70b9\uff0c\u5206\u6790\u4e86\u6d88\u8d39\u5e94\u7528\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u63d0\u4f9b\u5546\u7684\u6280\u672f\u5b9e\u73b0\u65b9\u5f0f\u3002", "conclusion": "\u96f6\u6570\u636e\u4fdd\u7559\u653f\u7b56\u5bf9\u4e8e\u4f01\u4e1aAI\u52a9\u624b\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5728\u6280\u672f\u67b6\u6784\u3001\u5408\u89c4\u8981\u6c42\u548c\u7528\u6237\u4f53\u9a8c\u4e4b\u95f4\u627e\u5230\u5e73\u8861\uff0cSalesforce\u548cMicrosoft\u63d0\u4f9b\u4e86\u4e0d\u540c\u7684\u5b9e\u73b0\u65b9\u6848\u3002"}}
{"id": "2510.10459", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10459", "abs": "https://arxiv.org/abs/2510.10459", "authors": ["Prawaal Sharma", "Poonam Goyal", "Navneet Goyal", "Vidisha Sharma"], "title": "NIM: Neuro-symbolic Ideographic Metalanguage for Inclusive Communication", "comment": "9 pages, EMNLP Findings 2025", "summary": "Digital communication has become the cornerstone of modern interaction,\nenabling rapid, accessible, and interactive exchanges. However, individuals\nwith lower academic literacy often face significant barriers, exacerbating the\n\"digital divide\". In this work, we introduce a novel, universal ideographic\nmetalanguage designed as an innovative communication framework that transcends\nacademic, linguistic, and cultural boundaries. Our approach leverages\nprinciples of Neuro-symbolic AI, combining neural-based large language models\n(LLMs) enriched with world knowledge and symbolic knowledge heuristics grounded\nin the linguistic theory of Natural Semantic Metalanguage (NSM). This enables\nthe semantic decomposition of complex ideas into simpler, atomic concepts.\nAdopting a human-centric, collaborative methodology, we engaged over 200\nsemi-literate participants in defining the problem, selecting ideographs, and\nvalidating the system. With over 80\\% semantic comprehensibility, an accessible\nlearning curve, and universal adaptability, our system effectively serves\nunderprivileged populations with limited formal education.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u901a\u7528\u8868\u610f\u5143\u8bed\u8a00\uff0c\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7AI\u65b9\u6cd5\u5c06\u590d\u6742\u6982\u5ff5\u5206\u89e3\u4e3a\u539f\u5b50\u6982\u5ff5\uff0c\u5e2e\u52a9\u4f4e\u5b66\u672f\u7d20\u517b\u4eba\u7fa4\u8de8\u8d8a\u6570\u5b57\u9e3f\u6c9f\u3002", "motivation": "\u89e3\u51b3\u4f4e\u5b66\u672f\u7d20\u517b\u4eba\u7fa4\u5728\u6570\u5b57\u6c9f\u901a\u4e2d\u9762\u4e34\u7684\u969c\u788d\uff0c\u7f13\u89e3\u6570\u5b57\u9e3f\u6c9f\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u795e\u7ecf\u7b26\u53f7AI\u65b9\u6cd5\uff0c\u4f7f\u7528\u57fa\u4e8e\u4e16\u754c\u77e5\u8bc6\u7684LLMs\u548c\u57fa\u4e8e\u81ea\u7136\u8bed\u4e49\u5143\u8bed\u8a00\u7406\u8bba\u7684\u7b26\u53f7\u77e5\u8bc6\u542f\u53d1\u5f0f\uff0c\u91c7\u7528\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u534f\u4f5c\u65b9\u6cd5\uff0c\u8ba9200\u591a\u540d\u534a\u6587\u76f2\u53c2\u4e0e\u8005\u53c2\u4e0e\u95ee\u9898\u5b9a\u4e49\u3001\u8868\u610f\u7b26\u53f7\u9009\u62e9\u548c\u7cfb\u7edf\u9a8c\u8bc1\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e86\u8d85\u8fc780%\u7684\u8bed\u4e49\u53ef\u7406\u89e3\u6027\uff0c\u5177\u6709\u6613\u5b66\u7684\u5b66\u4e60\u66f2\u7ebf\u548c\u666e\u904d\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u6709\u6548\u670d\u52a1\u4e8e\u6559\u80b2\u7a0b\u5ea6\u6709\u9650\u7684\u5f31\u52bf\u7fa4\u4f53\uff0c\u5b9e\u73b0\u8de8\u5b66\u672f\u3001\u8bed\u8a00\u548c\u6587\u5316\u8fb9\u754c\u7684\u6c9f\u901a\u3002"}}
{"id": "2510.09942", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.09942", "abs": "https://arxiv.org/abs/2510.09942", "authors": ["Payel Bhattacharjee", "Fengwei Tian", "Meiyu Zhong", "Guangyi Zhang", "Osvaldo Simeone", "Ravi Tandon"], "title": "Conformal Sparsification for Bandwidth-Efficient Edge-Cloud Speculative Decoding", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: AI and ML for Next-Generation Wireless Communications and\n  Networking (AI4NextG)", "summary": "Edge-cloud speculative decoding (SD) accelerates inference by having a\ncloud-based large language model (LLM) that verifies draft tokens generated by\na resource-constrained small language model (SLM) at the edge. A central\nbottleneck is the limited bandwidth of the edge-cloud link, which necessitates\nefficient compression of draft token distributions. We first derive an\ninformation-theoretic bound that decomposes the token rejection rate into\ncontributions from SLM-LLM distribution mismatch and from quantization\ndistortion. Guided by this analysis, we propose the Sparse Quantize-and-Sample\nSD (SQS-SD) framework, which exploits distributional sparsity through\nstructured sparsification and lattice-based quantization. Within this\nframework, K-SQS applies fixed top-K truncation, while C-SQS adaptively adjusts\nthe retained token set via online conformal prediction to ensure bounded\ndeviation from the dense distribution. Empirical results confirm that both\napproaches improve end-to-end latency and rejection rates in complimentary\noperating regimes.", "AI": {"tldr": "\u63d0\u51fa\u4e86SQS-SD\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7a00\u758f\u5316\u548c\u683c\u57fa\u91cf\u5316\u6765\u538b\u7f29\u8fb9\u7f18\u8bbe\u5907\u751f\u6210\u7684\u8349\u7a3f\u4ee4\u724c\u5206\u5e03\uff0c\u4ee5\u89e3\u51b3\u8fb9\u7f18-\u4e91\u63a8\u6d4b\u89e3\u7801\u4e2d\u7684\u5e26\u5bbd\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u8fb9\u7f18-\u4e91\u63a8\u6d4b\u89e3\u7801\u4e2d\uff0c\u8fb9\u7f18\u8bbe\u5907\u4e0e\u4e91\u7aef\u4e4b\u95f4\u7684\u6709\u9650\u5e26\u5bbd\u9650\u5236\u4e86\u8349\u7a3f\u4ee4\u724c\u5206\u5e03\u7684\u4f20\u8f93\u6548\u7387\uff0c\u9700\u8981\u6709\u6548\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSQS-SD\u6846\u67b6\uff0c\u5229\u7528\u5206\u5e03\u7a00\u758f\u6027\u8fdb\u884c\u7ed3\u6784\u5316\u7a00\u758f\u5316\u548c\u683c\u57fa\u91cf\u5316\u3002\u5305\u62ec\u56fa\u5b9atop-K\u622a\u65ad\u7684K-SQS\u548c\u901a\u8fc7\u5728\u7ebf\u7b26\u5408\u9884\u6d4b\u81ea\u9002\u5e94\u8c03\u6574\u4fdd\u7559\u4ee4\u724c\u96c6\u7684C-SQS\u3002", "result": "\u4e24\u79cd\u65b9\u6cd5\u5728\u4e92\u8865\u7684\u64cd\u4f5c\u673a\u5236\u4e0b\u90fd\u6539\u5584\u4e86\u7aef\u5230\u7aef\u5ef6\u8fdf\u548c\u62d2\u7edd\u7387\u3002", "conclusion": "SQS-SD\u6846\u67b6\u901a\u8fc7\u5229\u7528\u5206\u5e03\u7a00\u758f\u6027\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18-\u4e91\u63a8\u6d4b\u89e3\u7801\u4e2d\u7684\u5e26\u5bbd\u74f6\u9888\u95ee\u9898\uff0cK-SQS\u548cC-SQS\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u90fd\u80fd\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2510.11588", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11588", "abs": "https://arxiv.org/abs/2510.11588", "authors": ["Jiateng Liu", "Zhenhailong Wang", "Xiaojiang Huang", "Yingjie Li", "Xing Fan", "Xiang Li", "Chenlei Guo", "Ruhi Sarikaya", "Heng Ji"], "title": "Analyzing and Internalizing Complex Policy Documents for LLM Agents", "comment": "42 pages", "summary": "Large Language Model (LLM)-based agentic systems rely on in-context policy\ndocuments encoding diverse business rules. As requirements grow, these\ndocuments expand rapidly, causing high computational overhead. This motivates\ndeveloping internalization methods that embed policy documents into model\npriors while preserving performance. Prior prompt compression work targets\ngeneric prompts, but agentic policy documents span multiple complexity levels\nand require deeper reasoning, making internalization harder. We introduce\nCC-Gen, an agentic benchmark generator with Controllable Complexity across four\nlevels, enabling systematic evaluation of agents' ability to handle complexity\nand offering a unified framework for assessing policy internalization. Our\nanalysis shows that complex policy specifications governing workflows pose\nmajor reasoning challenges. Supporting internalization with gold user agent\ninteraction trajectories containing chain-of-thought (CoT) annotations via\nsupervised fine-tuning (SFT) is data-intensive and degrades sharply as policy\ncomplexity increases. To mitigate data and reasoning burdens, we propose\nCategory-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline\nparses policy documents to extract key specifications, grouping them into\nfactual, behavioral, and conditional categories, and isolating complex\nconditions that drive workflow complexity. This guides targeted data synthesis\nand enables agents to internalize policy information through an autoregressive\npretraining loss. Experiments show CAP-CPT improves SFT baselines in all\nsettings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt\nlength reduction on CC-Gen and further enhancing tau-Bench with minimal SFT\ndata.", "AI": {"tldr": "\u63d0\u51fa\u4e86CC-Gen\u57fa\u51c6\u751f\u6210\u5668\u548cCAP-CPT\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3LLM\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7b56\u7565\u6587\u6863\u5185\u90e8\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u53ef\u63a7\u590d\u6742\u5ea6\u8bc4\u4f30\u548c\u9488\u5bf9\u6027\u9884\u8bad\u7ec3\u63d0\u5347\u7b56\u7565\u7406\u89e3\u80fd\u529b\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u7b56\u7565\u6587\u6863\u968f\u9700\u6c42\u589e\u957f\u800c\u5feb\u901f\u6269\u5c55\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u5f00\u9500\uff0c\u9700\u8981\u5f00\u53d1\u5c06\u7b56\u7565\u6587\u6863\u5d4c\u5165\u6a21\u578b\u5148\u9a8c\u7684\u5185\u90e8\u5316\u65b9\u6cd5\u3002", "method": "\u5f15\u5165CC-Gen\u57fa\u51c6\u751f\u6210\u5668\uff0c\u63d0\u4f9b\u56db\u4e2a\u53ef\u63a7\u590d\u6742\u5ea6\u7ea7\u522b\uff1b\u63d0\u51faCAP-CPT\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u6790\u7b56\u7565\u6587\u6863\u3001\u5206\u7c7b\u5173\u952e\u89c4\u8303\uff0c\u6307\u5bfc\u9488\u5bf9\u6027\u6570\u636e\u5408\u6210\u548c\u81ea\u56de\u5f52\u9884\u8bad\u7ec3\u3002", "result": "CAP-CPT\u5728\u6240\u6709\u8bbe\u7f6e\u4e0b\u90fd\u4f18\u4e8eSFT\u57fa\u7ebf\uff0c\u5728Qwen-3-32B\u4e0a\u63d0\u5347\u8fbe41%\u548c22%\uff0c\u5728CC-Gen\u4e0a\u5b9e\u73b097.3%\u7684\u63d0\u793a\u957f\u5ea6\u51cf\u5c11\uff0c\u5e76\u5728\u5c11\u91cfSFT\u6570\u636e\u4e0b\u589e\u5f3atau-Bench\u6027\u80fd\u3002", "conclusion": "CAP-CPT\u65b9\u6cd5\u6709\u6548\u51cf\u8f7b\u4e86\u6570\u636e\u548c\u63a8\u7406\u8d1f\u62c5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u5185\u90e8\u5316\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u7b56\u7565\u89c4\u8303\u65f6\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.10472", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10472", "abs": "https://arxiv.org/abs/2510.10472", "authors": ["Qiran Zou", "Hou Hei Lam", "Wenhao Zhao", "Yiming Tang", "Tingting Chen", "Samson Yu", "Tianyi Zhang", "Chang Liu", "Xiangyang Ji", "Dianbo Liu"], "title": "FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the Importance of Exploration Breadth", "comment": "Our benchmark is available at: https://github.com/qrzou/FML-bench", "summary": "Large language models (LLMs) have sparked growing interest in automatic\nmachine learning research agents. Among them, agents capable of autonomously\nproposing ideas and conducting machine learning experiments are particularly\npromising, as they maximize research automation and accelerate scientific\nprogress by iteratively refining ideas based on experimental results. However,\ncomprehensively evaluating such agents remains challenging. Existing benchmarks\ntend to overemphasize engineering aspects while neglecting academic rigor,\ncreating barriers that obscure a clear assessment of an agent's scientific\ncapabilities in machine learning research. They also suffer from limited task\ndiversity, an overemphasis on application-oriented tasks over fundamental\nresearch problems, and limited scalability to realistic research settings. To\naddress these limitations, we introduce FML-bench, a benchmark designed to\nevaluate automatic machine learning research agents on 8 diverse and\nfundamental machine learning research problems. It reduces coding burden,\nemphasizes fundamental problems rather than specific use cases, offers high\ntask diversity, and is extensible to real-world machine learning GitHub\nrepositories. Furthermore, we present a unified evaluation framework with five\ncomplementary metrics, designed to comprehensively assess agent performance on\nour benchmark. We evaluate state-of-the-art automatic research agents on\nFML-bench, and find that agents employing broad research exploration strategies\noutperform those focusing on narrow but deep exploration. These findings\nsuggest that emphasizing the breadth of exploration may lead to more effective\nresearch outcomes than focusing solely on incremental refinement. Our benchmark\nis available at https://github.com/qrzou/FML-bench.", "AI": {"tldr": "\u63d0\u51fa\u4e86FML-bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u673a\u5668\u5b66\u4e60\u7814\u7a76\u4ee3\u7406\u57288\u4e2a\u57fa\u7840\u673a\u5668\u5b66\u4e60\u7814\u7a76\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u5b66\u672f\u4e25\u8c28\u6027\u3001\u4efb\u52a1\u591a\u6837\u6027\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u8fc7\u5ea6\u5f3a\u8c03\u5de5\u7a0b\u65b9\u9762\u800c\u5ffd\u89c6\u5b66\u672f\u4e25\u8c28\u6027\uff0c\u4efb\u52a1\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u8fc7\u4e8e\u5173\u6ce8\u5e94\u7528\u5bfc\u5411\u4efb\u52a1\u800c\u975e\u57fa\u7840\u7814\u7a76\u95ee\u9898\uff0c\u4e14\u96be\u4ee5\u6269\u5c55\u5230\u771f\u5b9e\u7814\u7a76\u73af\u5883\u3002", "method": "\u5f00\u53d1\u4e86FML-bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b8\u4e2a\u591a\u6837\u5316\u7684\u57fa\u7840\u673a\u5668\u5b66\u4e60\u7814\u7a76\u95ee\u9898\uff0c\u51cf\u5c11\u7f16\u7801\u8d1f\u62c5\uff0c\u5f3a\u8c03\u57fa\u7840\u95ee\u9898\u800c\u975e\u7279\u5b9a\u7528\u4f8b\uff0c\u5e76\u63d0\u4f9b\u53ef\u6269\u5c55\u6027\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u5305\u542b5\u4e2a\u4e92\u8865\u6307\u6807\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u7814\u7a76\u4ee3\u7406\uff0c\u53d1\u73b0\u91c7\u7528\u5e7f\u6cdb\u7814\u7a76\u63a2\u7d22\u7b56\u7565\u7684\u4ee3\u7406\u4f18\u4e8e\u90a3\u4e9b\u4e13\u6ce8\u4e8e\u72ed\u7a84\u4f46\u6df1\u5165\u63a2\u7d22\u7684\u4ee3\u7406\u3002", "conclusion": "\u5f3a\u8c03\u63a2\u7d22\u5e7f\u5ea6\u53ef\u80fd\u6bd4\u4ec5\u5173\u6ce8\u589e\u91cf\u6539\u8fdb\u5e26\u6765\u66f4\u6709\u6548\u7684\u7814\u7a76\u6210\u679c\uff0cFML-bench\u4e3a\u8bc4\u4f30\u81ea\u52a8\u673a\u5668\u5b66\u4e60\u7814\u7a76\u4ee3\u7406\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u57fa\u51c6\u3002"}}
{"id": "2510.11595", "categories": ["cs.AI", "cs.GL"], "pdf": "https://arxiv.org/pdf/2510.11595", "abs": "https://arxiv.org/abs/2510.11595", "authors": ["Israel Mason-Williams", "Gabryel Mason-Williams"], "title": "Reproducibility: The New Frontier in AI Governance", "comment": "12 pages,6 figures,Workshop on Technical AI Governance at ICML", "summary": "AI policymakers are responsible for delivering effective governance\nmechanisms that can provide safe, aligned and trustworthy AI development.\nHowever, the information environment offered to policymakers is characterised\nby an unnecessarily low Signal-To-Noise Ratio, favouring regulatory capture and\ncreating deep uncertainty and divides on which risks should be prioritised from\na governance perspective. We posit that the current publication speeds in AI\ncombined with the lack of strong scientific standards, via weak reproducibility\nprotocols, effectively erodes the power of policymakers to enact meaningful\npolicy and governance protocols. Our paper outlines how AI research could adopt\nstricter reproducibility guidelines to assist governance endeavours and improve\nconsensus on the AI risk landscape. We evaluate the forthcoming reproducibility\ncrisis within AI research through the lens of crises in other scientific\ndomains; providing a commentary on how adopting preregistration, increased\nstatistical power and negative result publication reproducibility protocols can\nenable effective AI governance. While we maintain that AI governance must be\nreactive due to AI's significant societal implications we argue that\npolicymakers and governments must consider reproducibility protocols as a core\ntool in the governance arsenal and demand higher standards for AI research.\nCode to replicate data and figures:\nhttps://github.com/IFMW01/reproducibility-the-new-frontier-in-ai-governance", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20AI\u7814\u7a76\u5e94\u91c7\u7528\u66f4\u4e25\u683c\u7684\u53ef\u91cd\u590d\u6027\u6307\u5357\uff0c\u4ee5\u5e2e\u52a9\u6cbb\u7406\u5de5\u4f5c\u5e76\u6539\u5584\u5bf9AI\u98ce\u9669\u683c\u5c40\u7684\u5171\u8bc6\u3002", "motivation": "\u5f53\u524dAI\u653f\u7b56\u5236\u5b9a\u8005\u9762\u4e34\u4fe1\u606f\u73af\u5883\u4e2d\u4fe1\u53f7\u566a\u58f0\u6bd4\u8fc7\u4f4e\u7684\u95ee\u9898\uff0c\u8fd9\u6709\u5229\u4e8e\u76d1\u7ba1\u6355\u83b7\uff0c\u5e76\u5bfc\u81f4\u5728\u5e94\u4f18\u5148\u8003\u8651\u54ea\u4e9b\u98ce\u9669\u65b9\u9762\u5b58\u5728\u6df1\u5ea6\u4e0d\u786e\u5b9a\u6027\u548c\u5206\u6b67\u3002", "method": "\u901a\u8fc7\u501f\u9274\u5176\u4ed6\u79d1\u5b66\u9886\u57df\u7684\u53ef\u91cd\u590d\u6027\u5371\u673a\u7ecf\u9a8c\uff0c\u8bc4\u4f30AI\u7814\u7a76\u4e2d\u5373\u5c06\u51fa\u73b0\u7684\u53ef\u91cd\u590d\u6027\u5371\u673a\uff0c\u5e76\u8bc4\u8bba\u91c7\u7528\u9884\u6ce8\u518c\u3001\u589e\u52a0\u7edf\u8ba1\u80fd\u529b\u548c\u8d1f\u9762\u7ed3\u679c\u53d1\u5e03\u7b49\u53ef\u91cd\u590d\u6027\u534f\u8bae\u5982\u4f55\u5b9e\u73b0\u6709\u6548\u7684AI\u6cbb\u7406\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5f53\u524dAI\u51fa\u7248\u901f\u5ea6\u4e0e\u7f3a\u4e4f\u5f3a\u6709\u529b\u7684\u79d1\u5b66\u6807\u51c6\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u5f31\u53ef\u91cd\u590d\u6027\u534f\u8bae\u524a\u5f31\u4e86\u653f\u7b56\u5236\u5b9a\u8005\u5236\u5b9a\u6709\u610f\u4e49\u653f\u7b56\u548c\u6cbb\u7406\u534f\u8bae\u7684\u80fd\u529b\u3002", "conclusion": "\u867d\u7136AI\u6cbb\u7406\u5fc5\u987b\u5177\u6709\u53cd\u5e94\u6027\uff0c\u4f46\u653f\u7b56\u5236\u5b9a\u8005\u548c\u653f\u5e9c\u5fc5\u987b\u5c06\u53ef\u91cd\u590d\u6027\u534f\u8bae\u89c6\u4e3a\u6cbb\u7406\u5de5\u5177\u7bb1\u4e2d\u7684\u6838\u5fc3\u5de5\u5177\uff0c\u5e76\u8981\u6c42AI\u7814\u7a76\u8fbe\u5230\u66f4\u9ad8\u6807\u51c6\u3002"}}
{"id": "2510.11604", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11604", "abs": "https://arxiv.org/abs/2510.11604", "authors": ["Sanjula De Alwis", "Indrajith Ekanayake"], "title": "Explainability, risk modeling, and segmentation based customer churn analytics for personalized retention in e-commerce", "comment": null, "summary": "In online retail, customer acquisition typically incurs higher costs than\ncustomer retention, motivating firms to invest in churn analytics. However,\nmany contemporary churn models operate as opaque black boxes, limiting insight\ninto the determinants of attrition, the timing of retention opportunities, and\nthe identification of high-risk customer segments. Accordingly, the emphasis\nshould shift from prediction alone to the design of personalized retention\nstrategies grounded in interpretable evidence. This study advances a\nthree-component framework that integrates explainable AI to quantify feature\ncontributions, survival analysis to model time-to-event churn risk, and RFM\nprofiling to segment customers by transactional behaviour. In combination,\nthese methods enable the attribution of churn drivers, estimation of\nintervention windows, and prioritization of segments for targeted actions,\nthereby supporting strategies that reduce attrition and strengthen customer\nloyalty.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u7ec4\u4ef6\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u89e3\u91caAI\u3001\u751f\u5b58\u5206\u6790\u548cRFM\u5206\u6790\u6765\u6539\u8fdb\u5ba2\u6237\u6d41\u5931\u9884\u6d4b\uff0c\u4ece\u5355\u7eaf\u9884\u6d4b\u8f6c\u5411\u4e2a\u6027\u5316\u4fdd\u7559\u7b56\u7565\u8bbe\u8ba1\u3002", "motivation": "\u5f53\u524d\u6d41\u5931\u6a21\u578b\u591a\u4e3a\u9ed1\u7bb1\uff0c\u7f3a\u4e4f\u5bf9\u6d41\u5931\u539f\u56e0\u3001\u5e72\u9884\u65f6\u673a\u548c\u9ad8\u98ce\u9669\u5ba2\u6237\u7fa4\u4f53\u7684\u6d1e\u5bdf\uff0c\u9700\u8981\u4ece\u9884\u6d4b\u8f6c\u5411\u57fa\u4e8e\u53ef\u89e3\u91ca\u8bc1\u636e\u7684\u4e2a\u6027\u5316\u4fdd\u7559\u7b56\u7565\u3002", "method": "\u96c6\u6210\u53ef\u89e3\u91caAI\u91cf\u5316\u7279\u5f81\u8d21\u732e\u3001\u751f\u5b58\u5206\u6790\u5efa\u6a21\u6d41\u5931\u98ce\u9669\u65f6\u95f4\u3001RFM\u5206\u6790\u6309\u4ea4\u6613\u884c\u4e3a\u7ec6\u5206\u5ba2\u6237\u7684\u4e09\u7ec4\u4ef6\u6846\u67b6\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u8bc6\u522b\u6d41\u5931\u9a71\u52a8\u56e0\u7d20\u3001\u4f30\u8ba1\u5e72\u9884\u7a97\u53e3\u3001\u4f18\u5148\u5904\u7406\u76ee\u6807\u7ec6\u5206\u5e02\u573a\uff0c\u652f\u6301\u51cf\u5c11\u6d41\u5931\u548c\u589e\u5f3a\u5ba2\u6237\u5fe0\u8bda\u5ea6\u7684\u7b56\u7565\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u591a\u79cd\u5206\u6790\u65b9\u6cd5\uff0c\u8be5\u7814\u7a76\u4e3a\u8bbe\u8ba1\u6709\u6548\u7684\u5ba2\u6237\u4fdd\u7559\u7b56\u7565\u63d0\u4f9b\u4e86\u5168\u9762\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10475", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10475", "abs": "https://arxiv.org/abs/2510.10475", "authors": ["A H M Rezaul Karim", "Ozlem Uzuner"], "title": "Assessing Large Language Models for Structured Medical Order Extraction", "comment": null, "summary": "Medical order extraction is essential for structuring actionable clinical\ninformation, supporting decision-making, and enabling downstream applications\nsuch as documentation and workflow automation. Orders may be embedded in\ndiverse sources, including electronic health records, discharge summaries, and\nmulti-turn doctor-patient dialogues, and can span categories such as\nmedications, laboratory tests, imaging studies, and follow-up actions. The\nMEDIQA-OE 2025 shared task focuses on extracting structured medical orders from\nextended conversational transcripts, requiring the identification of order\ntype, description, reason, and provenance. We present the MasonNLP submission,\nwhich ranked 5th among 17 participating teams with 105 total submissions. Our\napproach uses a general-purpose, instruction-tuned LLaMA-4 17B model without\ndomain-specific fine-tuning, guided by a single in-context example. This\nfew-shot configuration achieved an average F1 score of 37.76, with notable\nimprovements in reason and provenance accuracy. These results demonstrate that\nlarge, non-domain-specific LLMs, when paired with effective prompt engineering,\ncan serve as strong, scalable baselines for specialized clinical NLP tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86MasonNLP\u56e2\u961f\u5728MEDIQA-OE 2025\u533b\u7597\u8ba2\u5355\u63d0\u53d6\u4efb\u52a1\u4e2d\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u901a\u7528LLaMA-4 17B\u6a21\u578b\u901a\u8fc7\u5355\u6837\u672c\u63d0\u793a\u5de5\u7a0b\u5b9e\u73b0\u533b\u7597\u8ba2\u5355\u7ed3\u6784\u5316\u63d0\u53d6\uff0c\u572817\u4e2a\u56e2\u961f\u4e2d\u6392\u540d\u7b2c5\u3002", "motivation": "\u533b\u7597\u8ba2\u5355\u63d0\u53d6\u5bf9\u4e8e\u7ed3\u6784\u5316\u4e34\u5e8a\u4fe1\u606f\u3001\u652f\u6301\u51b3\u7b56\u5236\u5b9a\u548c\u5b9e\u73b0\u6587\u6863\u53ca\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u81f3\u5173\u91cd\u8981\u3002\u8be5\u4efb\u52a1\u9700\u8981\u4ece\u591a\u6837\u5316\u533b\u7597\u6570\u636e\u6e90\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u8ba2\u5355\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u901a\u7528\u6307\u4ee4\u8c03\u4f18\u7684LLaMA-4 17B\u6a21\u578b\uff0c\u4e0d\u8fdb\u884c\u9886\u57df\u7279\u5b9a\u5fae\u8c03\uff0c\u4ec5\u901a\u8fc7\u5355\u4e2a\u4e0a\u4e0b\u6587\u793a\u4f8b\u8fdb\u884c\u5f15\u5bfc\u7684\u5c11\u6837\u672c\u914d\u7f6e\u3002", "result": "\u572817\u4e2a\u53c2\u8d5b\u56e2\u961f\u7684105\u4efd\u63d0\u4ea4\u4e2d\u6392\u540d\u7b2c5\uff0c\u5e73\u5747F1\u5206\u6570\u4e3a37.76\uff0c\u5728\u539f\u56e0\u548c\u6765\u6e90\u51c6\u786e\u6027\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u5927\u578b\u975e\u9886\u57df\u7279\u5b9aLLM\u4e0e\u6709\u6548\u63d0\u793a\u5de5\u7a0b\u7ed3\u5408\uff0c\u53ef\u4f5c\u4e3a\u4e13\u4e1a\u4e34\u5e8aNLP\u4efb\u52a1\u7684\u53ef\u6269\u5c55\u5f3a\u57fa\u7ebf\u3002"}}
{"id": "2510.09976", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09976", "abs": "https://arxiv.org/abs/2510.09976", "authors": ["Mingyang Lyu", "Yinqian Sun", "Erliang Lin", "Huangrui Li", "Ruolin Chen", "Feifei Zhao", "Yi Zeng"], "title": "Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models such as OpenVLA, Octo, and $\\pi_0$ have\nshown strong generalization by leveraging large-scale demonstrations, yet their\nperformance is still fundamentally constrained by the quality and coverage of\nsupervised data. Reinforcement learning (RL) provides a promising path for\nimproving and fine-tuning VLAs through online interaction. However,\nconventional policy gradient methods are computationally infeasible in the\ncontext of flow-matching based models due to the intractability of the\nimportance sampling process, which requires explicit computation of policy\nratios. To overcome this limitation, we propose Flow Policy Optimization (FPO)\nalgorithm, which reformulates importance sampling by leveraging per-sample\nchanges in the conditional flow-matching objective. Furthermore, FPO achieves\nstable and scalable online reinforcement fine-tuning of the $\\pi_0$ model by\nintegrating structure-aware credit assignment to enhance gradient efficiency,\nclipped surrogate objectives to stabilize optimization, multi-step latent\nexploration to encourage diverse policy updates, and a Q-ensemble mechanism to\nprovide robust value estimation. We evaluate FPO on the LIBERO benchmark and\nthe ALOHA simulation task against supervised, preference-aligned,\ndiffusion-based, autoregressive online RL, and $\\pi_0$-FAST baselines,\nobserving consistent improvements over the imitation prior and strong\nalternatives with stable learning under sparse rewards. In addition, ablation\nstudies and analyses of the latent space dynamics further highlight the\ncontributions of individual components within FPO, validating the effectiveness\nof the proposed computational modules and the stable convergence of the\nconditional flow-matching objective during online RL.", "AI": {"tldr": "\u63d0\u51fa\u4e86Flow Policy Optimization (FPO)\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5728\u6d41\u5339\u914d\u6a21\u578b\u4e2d\u7684\u8ba1\u7b97\u4e0d\u53ef\u884c\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5982OpenVLA\u3001Octo\u548c\u03c0\u2080\u867d\u7136\u901a\u8fc7\u5927\u89c4\u6a21\u6f14\u793a\u6570\u636e\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5176\u6027\u80fd\u4ecd\u53d7\u9650\u4e8e\u76d1\u7763\u6570\u636e\u7684\u8d28\u91cf\u548c\u8986\u76d6\u8303\u56f4\u3002\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u901a\u8fc7\u5728\u7ebf\u4ea4\u4e92\u6539\u8fdb\u8fd9\u4e9b\u6a21\u578b\u7684\u9014\u5f84\uff0c\u4f46\u4f20\u7edf\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5728\u6d41\u5339\u914d\u6a21\u578b\u4e2d\u7531\u4e8e\u91cd\u8981\u6027\u91c7\u6837\u8fc7\u7a0b\u96be\u4ee5\u8ba1\u7b97\u800c\u4e0d\u53ef\u884c\u3002", "method": "FPO\u7b97\u6cd5\u901a\u8fc7\u5229\u7528\u6761\u4ef6\u6d41\u5339\u914d\u76ee\u6807\u4e2d\u6bcf\u4e2a\u6837\u672c\u7684\u53d8\u5316\u6765\u91cd\u65b0\u5b9a\u4e49\u91cd\u8981\u6027\u91c7\u6837\uff0c\u5e76\u96c6\u6210\u4e86\u7ed3\u6784\u611f\u77e5\u4fe1\u7528\u5206\u914d\u3001\u88c1\u526a\u4ee3\u7406\u76ee\u6807\u3001\u591a\u6b65\u6f5c\u5728\u63a2\u7d22\u548cQ\u96c6\u6210\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u53ef\u6269\u5c55\u7684\u5728\u7ebf\u5f3a\u5316\u5fae\u8c03\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u548cALOHA\u6a21\u62df\u4efb\u52a1\u4e2d\uff0cFPO\u76f8\u6bd4\u76d1\u7763\u5b66\u4e60\u3001\u504f\u597d\u5bf9\u9f50\u3001\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u3001\u81ea\u56de\u5f52\u5728\u7ebfRL\u548c\u03c0\u2080-FAST\u57fa\u7ebf\u65b9\u6cd5\uff0c\u90fd\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb\uff0c\u5728\u7a00\u758f\u5956\u52b1\u4e0b\u5b9e\u73b0\u4e86\u7a33\u5b9a\u5b66\u4e60\u3002", "conclusion": "FPO\u7b97\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u6d41\u5339\u914d\u6a21\u578b\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u91cd\u8981\u6027\u91c7\u6837\u65b9\u6cd5\u548c\u591a\u79cd\u7a33\u5b9a\u5316\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u6709\u6548\u5fae\u8c03\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u5bf9\u7b97\u6cd5\u6027\u80fd\u7684\u8d21\u732e\u3002"}}
{"id": "2510.11608", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11608", "abs": "https://arxiv.org/abs/2510.11608", "authors": ["Shiqi Zhang", "Xinbei Ma", "Yunqing Xu", "Zouying Cao", "Pengrui Lu", "Haobo Yuan", "Tiancheng Shen", "Zhuosheng Zhang", "Hai Zhao", "Ming-Hsuan Yang"], "title": "ParaCook: On Time-Efficient Planning for Multi-Agent Systems", "comment": null, "summary": "Large Language Models (LLMs) exhibit strong reasoning abilities for planning\nlong-horizon, real-world tasks, yet existing agent benchmarks focus on task\ncompletion while neglecting time efficiency in parallel and asynchronous\noperations. To address this, we present ParaCook, a benchmark for\ntime-efficient collaborative planning. Inspired by the Overcooked game,\nParaCook provides an environment for various challenging interaction planning\nof multi-agent systems that are instantiated as cooking tasks, with a\nsimplified action space to isolate the core challenge of strategic parallel\nplanning. Through a comprehensive evaluation of state-of-the-art LLMs, we find\nthat current approaches achieve suboptimal plans, which struggle with parallel\nactions or coordination. Our analysis also reveals LLMs' potential on abstract\ntasks where they can focus on high-level parallel optimization. ParaCook\nprovides a scalable evaluation framework with adjustable complexity,\nestablishing a foundation for developing and assessing time efficiency-aware\nmulti-agent planning. The code and data are available at\nhttps://github.com/zsq259/ParaCook.", "AI": {"tldr": "ParaCook\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u65f6\u95f4\u6548\u7387\u534f\u4f5c\u89c4\u5212\u7684\u57fa\u51c6\uff0c\u57fa\u4e8e\u7b80\u5316\u7248Overcooked\u6e38\u620f\u73af\u5883\uff0c\u4e13\u6ce8\u4e8e\u5e76\u884c\u548c\u5f02\u6b65\u64cd\u4f5c\u7684\u89c4\u5212\u80fd\u529b\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u5b8c\u6210\u5ea6\uff0c\u4f46\u5ffd\u89c6\u4e86\u5e76\u884c\u548c\u5f02\u6b65\u64cd\u4f5c\u7684\u65f6\u95f4\u6548\u7387\u95ee\u9898\uff0c\u9700\u8981\u4e13\u95e8\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u65f6\u95f4\u6548\u7387\u65b9\u9762\u7684\u89c4\u5212\u80fd\u529b\u3002", "method": "\u57fa\u4e8eOvercooked\u6e38\u620f\u8bbe\u8ba1ParaCook\u73af\u5883\uff0c\u63d0\u4f9b\u7b80\u5316\u7684\u52a8\u4f5c\u7a7a\u95f4\u6765\u4e13\u6ce8\u4e8e\u6218\u7565\u5e76\u884c\u89c4\u5212\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5efa\u7acb\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5bf9\u6700\u5148\u8fdbLLMs\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5f53\u524d\u65b9\u6cd5\u751f\u6210\u6b21\u4f18\u8ba1\u5212\uff0c\u5728\u5e76\u884c\u52a8\u4f5c\u6216\u534f\u8c03\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4f46\u5728\u62bd\u8c61\u4efb\u52a1\u4e2dLLMs\u5c55\u73b0\u51fa\u5e76\u884c\u4f18\u5316\u7684\u6f5c\u529b\u3002", "conclusion": "ParaCook\u4e3a\u5f00\u53d1\u548c\u8bc4\u4f30\u65f6\u95f4\u6548\u7387\u611f\u77e5\u7684\u591a\u667a\u80fd\u4f53\u89c4\u5212\u5efa\u7acb\u4e86\u57fa\u7840\uff0c\u63ed\u793a\u4e86LLMs\u5728\u5e76\u884c\u89c4\u5212\u65b9\u9762\u7684\u5c40\u9650\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2510.10481", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10481", "abs": "https://arxiv.org/abs/2510.10481", "authors": ["Guangxin He", "Shen Nie", "Fengqi Zhu", "Yuankang Zhao", "Tianyi Bai", "Ran Yan", "Jie Fu", "Chongxuan Li", "Binhang Yuan"], "title": "UltraLLaDA: Scaling the Context Length to 128K for Diffusion Large Language Models", "comment": null, "summary": "Diffusion LLMs have attracted growing interest, with plenty of recent work\nemphasizing their great potential in various downstream tasks; yet the\nlong-context behavior of diffusion LLMs remains largely uncharted. We present a\ncase study of post-training techniques for extending the context window of\ndiffusion LLMs (i.e., LLaDA) without retraining from scratch. We show that a\nsimple modification to the standard Rotary Positional Embeddings (RoPE)\nextension effectively accommodates the probabilistic modeling inherent in the\ndiffusion process, enabling stable scaling to longer context ranges. We further\ncompare masking strategies used during post-training and analyze their impact\non optimization stability and long-range recall. Instantiating these insights,\nwe introduce UltraLLaDA, a diffusion LLM with a 128K-token context window that,\nin our empirical evaluation on long-context tasks, significantly outperforms\ntraining-free baselines. Our experimental results highlight the special\npositional extension as a key lever for scaling diffusion LLMs to extended\ncontexts and offer practical guidance for practitioners seeking 128K-scale\ncontext via efficient post-training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u540e\u8bad\u7ec3\u6280\u672f\u6269\u5c55\u6269\u6563LLMs\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u4ece\u5934\u8bad\u7ec3\uff0c\u6210\u529f\u5c06\u4e0a\u4e0b\u6587\u6269\u5c55\u5230128K token\u3002", "motivation": "\u6269\u6563LLMs\u5728\u957f\u4e0a\u4e0b\u6587\u884c\u4e3a\u65b9\u9762\u7684\u7814\u7a76\u4ecd\u7136\u4e0d\u8db3\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u6709\u6548\u6269\u5c55\u5176\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002", "method": "\u901a\u8fc7\u4fee\u6539\u6807\u51c6RoPE\u4f4d\u7f6e\u7f16\u7801\u6765\u9002\u5e94\u6269\u6563\u8fc7\u7a0b\u7684\u6982\u7387\u5efa\u6a21\uff0c\u6bd4\u8f83\u4e0d\u540c\u7684\u63a9\u7801\u7b56\u7565\u5bf9\u4f18\u5316\u7a33\u5b9a\u6027\u548c\u957f\u7a0b\u8bb0\u5fc6\u7684\u5f71\u54cd\u3002", "result": "\u63d0\u51fa\u7684UltraLLaDA\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u65e0\u9700\u8bad\u7ec3\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7279\u6b8a\u7684\u4f4d\u7f6e\u6269\u5c55\u6280\u672f\u662f\u6269\u5c55\u6269\u6563LLMs\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u5173\u952e\u6760\u6746\uff0c\u4e3a\u901a\u8fc7\u9ad8\u6548\u540e\u8bad\u7ec3\u5b9e\u73b0128K\u89c4\u6a21\u4e0a\u4e0b\u6587\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2510.09977", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09977", "abs": "https://arxiv.org/abs/2510.09977", "authors": ["Frida Cantu", "Salomon Ibarra", "Arturo Gonzales", "Jesus Barreda", "Chenang Liu", "Li Zhang"], "title": "An Unsupervised Time Series Anomaly Detection Approach for Efficient Online Process Monitoring of Additive Manufacturing", "comment": "2025 IEEE 21st International Conference on Automation Science and\n  Engineering", "summary": "Online sensing plays an important role in advancing modern manufacturing. The\nreal-time sensor signals, which can be stored as high-resolution time series\ndata, contain rich information about the operation status. One of its popular\nusages is online process monitoring, which can be achieved by effective anomaly\ndetection from the sensor signals. However, most existing approaches either\nheavily rely on labeled data for training supervised models, or are designed to\ndetect only extreme outliers, thus are ineffective at identifying subtle\nsemantic off-track anomalies to capture where new regimes or unexpected\nroutines start. To address this challenge, we propose an matrix profile-based\nunsupervised anomaly detection algorithm that captures fabrication cycle\nsimilarity and performs semantic segmentation to precisely identify the onset\nof defect anomalies in additive manufacturing. The effectiveness of the\nproposed method is demonstrated by the experiments on real-world sensor data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e9\u9635\u8f6e\u5ed3\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u76d1\u6d4b\u589e\u6750\u5236\u9020\u8fc7\u7a0b\u4e2d\u7684\u7f3a\u9677\u5f02\u5e38\u8d77\u59cb\u70b9\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\uff0c\u8981\u4e48\u53ea\u80fd\u68c0\u6d4b\u6781\u7aef\u5f02\u5e38\uff0c\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u7ec6\u5fae\u7684\u8bed\u4e49\u504f\u79bb\u5f02\u5e38\uff0c\u4ee5\u6355\u6349\u65b0\u72b6\u6001\u6216\u610f\u5916\u7a0b\u5e8f\u7684\u5f00\u59cb\u3002", "method": "\u57fa\u4e8e\u77e9\u9635\u8f6e\u5ed3\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\uff0c\u6355\u6349\u5236\u9020\u5468\u671f\u76f8\u4f3c\u6027\u5e76\u8fdb\u884c\u8bed\u4e49\u5206\u5272\uff0c\u7cbe\u786e\u8bc6\u522b\u589e\u6750\u5236\u9020\u4e2d\u7f3a\u9677\u5f02\u5e38\u7684\u8d77\u59cb\u70b9\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u4f20\u611f\u5668\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u589e\u6750\u5236\u9020\u8fc7\u7a0b\u4e2d\u7684\u8bed\u4e49\u504f\u79bb\u5f02\u5e38\uff0c\u4e3a\u5728\u7ebf\u8fc7\u7a0b\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.11661", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11661", "abs": "https://arxiv.org/abs/2510.11661", "authors": ["Shijie Xia", "Yuhan Sun", "Pengfei Liu"], "title": "SR-Scientist: Scientific Equation Discovery With Agentic AI", "comment": null, "summary": "Recently, Large Language Models (LLMs) have been applied to scientific\nequation discovery, leveraging their embedded scientific knowledge for\nhypothesis generation. However, current methods typically confine LLMs to the\nrole of an equation proposer within search algorithms like genetic programming.\nIn this paper, we present SR-Scientist, a framework that elevates the LLM from\na simple equation proposer to an autonomous AI scientist that writes code to\nanalyze data, implements the equation as code, submits it for evaluation, and\noptimizes the equation based on experimental feedback. Specifically, we wrap\nthe code interpreter into a set of tools for data analysis and equation\nevaluation. The agent is instructed to optimize the equation by utilizing these\ntools over a long horizon with minimal human-defined pipelines. Empirical\nresults show that SR-Scientist outperforms baseline methods by an absolute\nmargin of 6% to 35% on datasets covering four science disciplines.\nAdditionally, we demonstrate our method's robustness to noise, the\ngeneralization of the discovered equations to out-of-domain data, and their\nsymbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning\nframework to enhance the agent's capabilities.", "AI": {"tldr": "SR-Scientist\u6846\u67b6\u5c06LLM\u4ece\u7b80\u5355\u7684\u65b9\u7a0b\u63d0\u8bae\u8005\u63d0\u5347\u4e3a\u81ea\u4e3bAI\u79d1\u5b66\u5bb6\uff0c\u80fd\u591f\u7f16\u5199\u4ee3\u7801\u5206\u6790\u6570\u636e\u3001\u5b9e\u73b0\u65b9\u7a0b\u3001\u63d0\u4ea4\u8bc4\u4f30\uff0c\u5e76\u6839\u636e\u5b9e\u9a8c\u53cd\u9988\u4f18\u5316\u65b9\u7a0b\uff0c\u5728\u56db\u4e2a\u79d1\u5b66\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u53476%\u81f335%\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u901a\u5e38\u5c06LLM\u9650\u5236\u4e3a\u9057\u4f20\u7f16\u7a0b\u7b49\u641c\u7d22\u7b97\u6cd5\u4e2d\u7684\u65b9\u7a0b\u63d0\u8bae\u8005\u89d2\u8272\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528LLM\u7684\u6f5c\u529b\u3002", "method": "\u5c06\u4ee3\u7801\u89e3\u91ca\u5668\u5c01\u88c5\u4e3a\u6570\u636e\u5206\u6790\u548c\u65b9\u7a0b\u8bc4\u4f30\u5de5\u5177\u96c6\uff0c\u8ba9\u667a\u80fd\u4f53\u5728\u6700\u5c0f\u4eba\u5de5\u5e72\u9884\u4e0b\u957f\u671f\u4f7f\u7528\u8fd9\u4e9b\u5de5\u5177\u4f18\u5316\u65b9\u7a0b\uff0c\u5e76\u5f00\u53d1\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u589e\u5f3a\u667a\u80fd\u4f53\u80fd\u529b\u3002", "result": "\u5728\u56db\u4e2a\u79d1\u5b66\u5b66\u79d1\u7684\u6570\u636e\u96c6\u4e0a\uff0cSR-Scientist\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u7edd\u5bf9\u63d0\u53476%\u81f335%\uff0c\u8868\u73b0\u51fa\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3001\u53d1\u73b0\u65b9\u7a0b\u5bf9\u57df\u5916\u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\u4ee5\u53ca\u7b26\u53f7\u51c6\u786e\u6027\u3002", "conclusion": "SR-Scientist\u6210\u529f\u5c06LLM\u63d0\u5347\u4e3a\u81ea\u4e3bAI\u79d1\u5b66\u5bb6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79d1\u5b66\u65b9\u7a0b\u53d1\u73b0\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u826f\u597d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.10490", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10490", "abs": "https://arxiv.org/abs/2510.10490", "authors": ["Prawaal Sharma", "Poonam Goyal", "Vidisha Sharma", "Navneet Goyal"], "title": "VOLTAGE: A Versatile Contrastive Learning based OCR Methodology for ultra low-resource scripts through Auto Glyph Feature Extraction", "comment": "9 Pages, Plus Appendices, EACL 2024", "summary": "UNESCO has classified 2500 out of 7000 languages spoken worldwide as\nendangered. Attrition of a language leads to loss of traditional wisdom, folk\nliterature, and the essence of the community that uses it. It is therefore\nimperative to bring digital inclusion to these languages and avoid its\nextinction. Low resource languages are at a greater risk of extinction. Lack of\nunsupervised Optical Character Recognition(OCR) methodologies for low resource\nlanguages is one of the reasons impeding their digital inclusion. We propose\nVOLTAGE - a contrastive learning based OCR methodology, leveraging auto-glyph\nfeature recommendation for cluster-based labelling. We augment the labelled\ndata for diversity and volume using image transformations and Generative\nAdversarial Networks. Voltage has been designed using Takri - a family of\nscripts used in 16th to 20th century in the Himalayan regions of India. We\npresent results for Takri along with other Indic scripts (both low and high\nresource) to substantiate the universal behavior of the methodology. An\naccuracy of 95% for machine printed and 87% for handwritten samples on Takri\nscript has been achieved. We conduct baseline and ablation studies along with\nbuilding downstream use cases for Takri, demonstrating the usefulness of our\nwork.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684OCR\u65b9\u6cd5VOLTAGE\uff0c\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6570\u5b57\u4fdd\u62a4\uff0c\u5728Takri\u811a\u672c\u4e0a\u8fbe\u523095%\u7684\u5370\u5237\u4f53\u548c87%\u7684\u624b\u5199\u4f53\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u5168\u74037000\u79cd\u8bed\u8a00\u4e2d\u67092500\u79cd\u6fd2\u5371\uff0c\u8bed\u8a00\u6d88\u5931\u4f1a\u5bfc\u81f4\u4f20\u7edf\u667a\u6167\u3001\u6c11\u95f4\u6587\u5b66\u548c\u793e\u533a\u672c\u8d28\u7684\u4e27\u5931\u3002\u4f4e\u8d44\u6e90\u8bed\u8a00\u7531\u4e8e\u7f3a\u4e4f\u65e0\u76d1\u7763OCR\u65b9\u6cd5\u800c\u9762\u4e34\u66f4\u5927\u7684\u706d\u7edd\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u548c\u81ea\u52a8\u5b57\u5f62\u7279\u5f81\u63a8\u8350\u8fdb\u884c\u805a\u7c7b\u6807\u6ce8\uff0c\u901a\u8fc7\u56fe\u50cf\u53d8\u6362\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u589e\u5f3a\u6807\u6ce8\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u6570\u91cf\u3002", "result": "\u5728Takri\u811a\u672c\u4e0a\uff0c\u5370\u5237\u4f53\u8bc6\u522b\u51c6\u786e\u7387\u8fbe\u523095%\uff0c\u624b\u5199\u4f53\u8fbe\u523087%\u3002\u8be5\u65b9\u6cd5\u5728\u5176\u4ed6\u5370\u5ea6\u8bed\u7cfb\u811a\u672c\u4e0a\u4e5f\u8868\u73b0\u51fa\u901a\u7528\u6027\u3002", "conclusion": "VOLTAGE\u65b9\u6cd5\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6570\u5b57\u5305\u5bb9\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u57fa\u7ebf\u7814\u7a76\u548c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u6784\u5efa\u4e86\u4e0b\u6e38\u5e94\u7528\u6848\u4f8b\u3002"}}
{"id": "2510.09984", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.09984", "abs": "https://arxiv.org/abs/2510.09984", "authors": ["Kartikeya Aneja", "Nagender Aneja", "Murat Kantarcioglu"], "title": "Learning Joint Embeddings of Function and Process Call Graphs for Malware Detection", "comment": null, "summary": "Software systems can be represented as graphs, capturing dependencies among\nfunctions and processes. An interesting aspect of software systems is that they\ncan be represented as different types of graphs, depending on the extraction\ngoals and priorities. For example, function calls within the software can be\ncaptured to create function call graphs, which highlight the relationships\nbetween functions and their dependencies. Alternatively, the processes spawned\nby the software can be modeled to generate process interaction graphs, which\nfocus on runtime behavior and inter-process communication. While these graph\nrepresentations are related, each captures a distinct perspective of the\nsystem, providing complementary insights into its structure and operation.\nWhile previous studies have leveraged graph neural networks (GNNs) to analyze\nsoftware behaviors, most of this work has focused on a single type of graph\nrepresentation. The joint modeling of both function call graphs and process\ninteraction graphs remains largely underexplored, leaving opportunities for\ndeeper, multi-perspective analysis of software systems. This paper presents a\npipeline for constructing and training Function Call Graphs (FCGs) and Process\nCall Graphs (PCGs) and learning joint embeddings. We demonstrate that joint\nembeddings outperform a single-graph model. In this paper, we propose\nGeminiNet, a unified neural network approach that learns joint embeddings from\nboth FCGs and PCGs. We construct a new dataset of 635 Windows executables (318\nmalicious and 317 benign), extracting FCGs via Ghidra and PCGs via Any.Run\nsandbox. GeminiNet employs dual graph convolutional branches with an adaptive\ngating mechanism that balances contributions from static and dynamic views.", "AI": {"tldr": "GeminiNet\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u4ece\u51fd\u6570\u8c03\u7528\u56fe(FCG)\u548c\u8fdb\u7a0b\u8c03\u7528\u56fe(PCG)\u4e2d\u5b66\u4e60\u8054\u5408\u5d4c\u5165\uff0c\u901a\u8fc7\u53cc\u56fe\u5377\u79ef\u5206\u652f\u548c\u81ea\u9002\u5e94\u95e8\u63a7\u673a\u5236\u5e73\u8861\u9759\u6001\u548c\u52a8\u6001\u89c6\u56fe\u7684\u8d21\u732e\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5927\u591a\u53ea\u5173\u6ce8\u5355\u4e00\u7c7b\u578b\u7684\u56fe\u8868\u793a\uff0c\u800c\u51fd\u6570\u8c03\u7528\u56fe\u548c\u8fdb\u7a0b\u4ea4\u4e92\u56fe\u7684\u8054\u5408\u5efa\u6a21\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u8fd9\u4e3a\u8f6f\u4ef6\u7cfb\u7edf\u7684\u591a\u89c6\u89d2\u5206\u6790\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u6784\u5efaFCG\u548cPCG\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4f7f\u7528Ghidra\u63d0\u53d6FCG\uff0cAny.Run\u6c99\u7bb1\u63d0\u53d6PCG\uff0c\u91c7\u7528\u53cc\u56fe\u5377\u79ef\u5206\u652f\u548c\u81ea\u9002\u5e94\u95e8\u63a7\u673a\u5236\u5b66\u4e60\u8054\u5408\u5d4c\u5165\u3002", "result": "\u5728635\u4e2aWindows\u53ef\u6267\u884c\u6587\u4ef6\u6570\u636e\u96c6\u4e0a\uff0c\u8054\u5408\u5d4c\u5165\u6a21\u578b\u4f18\u4e8e\u5355\u56fe\u6a21\u578b\u3002", "conclusion": "GeminiNet\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u9759\u6001\u548c\u52a8\u6001\u56fe\u8868\u793a\uff0c\u4e3a\u8f6f\u4ef6\u884c\u4e3a\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u591a\u89c6\u89d2\u7406\u89e3\u3002"}}
{"id": "2510.11694", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11694", "abs": "https://arxiv.org/abs/2510.11694", "authors": ["Arjun Sahney", "Ram Gorthi", "Cezary \u0141astowski", "Javier Vega"], "title": "Operand Quant: A Single-Agent Architecture for Autonomous Machine Learning Engineering", "comment": "8 pages. No figures. Evaluated on MLE-Benchmark 2025", "summary": "We present Operand Quant, a single-agent, IDE-based architecture for\nautonomous machine learning engineering (MLE). Operand Quant departs from\nconventional multi-agent orchestration frameworks by consolidating all MLE\nlifecycle stages -- exploration, modeling, experimentation, and deployment --\nwithin a single, context-aware agent. On the MLE-Benchmark (2025), Operand\nQuant achieved a new state-of-the-art (SOTA) result, with an overall medal rate\nof 0.3956 +/- 0.0565 across 75 problems -- the highest recorded performance\namong all evaluated systems to date. The architecture demonstrates that a\nlinear, non-blocking agent, operating autonomously within a controlled IDE\nenvironment, can outperform multi-agent and orchestrated systems under\nidentical constraints.", "AI": {"tldr": "Operand Quant\u662f\u4e00\u79cd\u57fa\u4e8eIDE\u7684\u5355\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u7528\u4e8e\u81ea\u4e3b\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\uff0c\u5728MLE-Benchmark\u4e0a\u521b\u4e0b\u4e86\u65b0\u7684SOTA\u8bb0\u5f55\u3002", "motivation": "\u4f20\u7edf\u7684\u591a\u667a\u80fd\u4f53\u7f16\u6392\u6846\u67b6\u5728\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u751f\u547d\u5468\u671f\u4e2d\u5b58\u5728\u6548\u7387\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5355\u4e00\u667a\u80fd\u4f53\u67b6\u6784\u6765\u6574\u5408\u6240\u6709MLE\u9636\u6bb5\u3002", "method": "\u91c7\u7528\u5355\u667a\u80fd\u4f53\u3001IDE\u57fa\u7840\u7684\u67b6\u6784\uff0c\u5c06\u63a2\u7d22\u3001\u5efa\u6a21\u3001\u5b9e\u9a8c\u548c\u90e8\u7f72\u7b49\u6240\u6709MLE\u751f\u547d\u5468\u671f\u9636\u6bb5\u6574\u5408\u5230\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u667a\u80fd\u4f53\u4e2d\u3002", "result": "\u5728MLE-Benchmark\uff082025\uff09\u4e0a\u83b7\u5f97\u603b\u4f53\u5956\u724c\u73870.3956\u00b10.0565\uff0c\u572875\u4e2a\u95ee\u9898\u4e2d\u521b\u4e0b\u6240\u6709\u8bc4\u4f30\u7cfb\u7edf\u7684\u6700\u9ad8\u6027\u80fd\u8bb0\u5f55\u3002", "conclusion": "\u7ebf\u6027\u3001\u975e\u963b\u585e\u7684\u5355\u667a\u80fd\u4f53\u5728\u53d7\u63a7IDE\u73af\u5883\u4e2d\u53ef\u4ee5\u8d85\u8d8a\u591a\u667a\u80fd\u4f53\u548c\u7f16\u6392\u7cfb\u7edf\uff0c\u8bc1\u660e\u4e86\u5355\u667a\u80fd\u4f53\u67b6\u6784\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.10528", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10528", "abs": "https://arxiv.org/abs/2510.10528", "authors": ["Heming Xia", "Cunxiao Du", "Rui Li", "Chak Tou Leong", "Yongqi Li", "Wenjie Li"], "title": "Merlin's Whisper: Enabling Efficient Reasoning in LLMs via Black-box Adversarial Prompting", "comment": null, "summary": "Large reasoning models (LRMs) have demonstrated remarkable proficiency in\ntackling complex reasoning tasks through step-by-step thinking. However, such a\nlengthy reasoning process incurs substantial computational and latency\noverheads, hindering the practical deployment of these models. In this work, we\npresent a new perspective on mitigating overthinking in LRMs via black-box\nadversarial prompting. By treating both open-source LRMs and closed-source APIs\nas black-box communicators, we investigate how to elicit concise responses\nwithout sacrificing accuracy. We introduce AdvPrompt, an iterative refinement\nframework that generates high-quality adversarial prompts from diverse\nperspectives. Experiments across multiple benchmarks demonstrate that AdvPrompt\nconsistently reduces token usage while preserving performance. Notably,\nAdvPrompt achieves a 3x reduction in average response length on simple GSM8K\nquestions for the Qwen3 model series, and delivers an average ~40% token\nreduction across four benchmarks. For closed-source APIs, AdvPrompt reduces\ntoken usage on MATH-500 by 35% for Claude-3.7 and 47% for Gemini-2.5. Further\nanalysis reveals the generalizability of AdvPrompt across various model scales\nand families, underscoring the potential of black-box prompting as a practical\nand effective strategy for enhancing LRM efficiency.", "AI": {"tldr": "AdvPrompt\u901a\u8fc7\u9ed1\u76d2\u5bf9\u6297\u63d0\u793a\u51cf\u5c11\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u54cd\u5e94\u957f\u5ea6\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u63a8\u7406\u4efb\u52a1\u65f6\u4f1a\u4ea7\u751f\u5197\u957f\u7684\u601d\u8003\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u8ba1\u7b97\u548c\u5ef6\u8fdf\u5f00\u9500\u8fc7\u9ad8\uff0c\u963b\u788d\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u63d0\u51faAdvPrompt\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u7684\u9ed1\u76d2\u5bf9\u6297\u63d0\u793a\u6280\u672f\uff0c\u4ece\u591a\u89d2\u5ea6\u751f\u6210\u9ad8\u8d28\u91cf\u63d0\u793a\uff0c\u5f15\u5bfc\u6a21\u578b\u7ed9\u51fa\u66f4\u7b80\u6d01\u7684\u54cd\u5e94\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAdvPrompt\u663e\u8457\u51cf\u5c11token\u4f7f\u7528\u91cf\uff1aQwen3\u7cfb\u5217\u5728GSM8K\u4e0a\u54cd\u5e94\u957f\u5ea6\u51cf\u5c113\u500d\uff0c\u56db\u4e2a\u57fa\u51c6\u5e73\u5747\u51cf\u5c11\u7ea640%token\uff1bClaude-3.7\u548cGemini-2.5\u5728MATH-500\u4e0a\u5206\u522b\u51cf\u5c1135%\u548c47%token\u3002", "conclusion": "AdvPrompt\u5c55\u793a\u4e86\u9ed1\u76d2\u63d0\u793a\u4f5c\u4e3a\u63d0\u9ad8\u5927\u578b\u63a8\u7406\u6a21\u578b\u6548\u7387\u7684\u5b9e\u7528\u6709\u6548\u7b56\u7565\uff0c\u5177\u6709\u826f\u597d\u7684\u8de8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2211.13003", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2211.13003", "abs": "https://arxiv.org/abs/2211.13003", "authors": ["Md Hasibul Amin", "Harika Madanu", "Sahithi Lavu", "Hadi Mansourifar", "Dana Alsagheer", "Weidong Shi"], "title": "Detecting Conspiracy Theory Against COVID-19 Vaccines", "comment": "6 pages, 5 figures", "summary": "Since the beginning of the vaccination trial, social media has been flooded\nwith anti-vaccination comments and conspiracy beliefs. As the day passes, the\nnumber of COVID- 19 cases increases, and online platforms and a few news\nportals entertain sharing different conspiracy theories. The most popular\nconspiracy belief was the link between the 5G network spreading COVID-19 and\nthe Chinese government spreading the virus as a bioweapon, which initially\ncreated racial hatred. Although some disbelief has less impact on society,\nothers create massive destruction. For example, the 5G conspiracy led to the\nburn of the 5G Tower, and belief in the Chinese bioweapon story promoted an\nattack on the Asian-Americans. Another popular conspiracy belief was that Bill\nGates spread this Coronavirus disease (COVID-19) by launching a mass\nvaccination program to track everyone. This Conspiracy belief creates distrust\nissues among laypeople and creates vaccine hesitancy. This study aims to\ndiscover the conspiracy theory against the vaccine on social platforms. We\nperformed a sentiment analysis on the 598 unique sample comments related to\nCOVID-19 vaccines. We used two different models, BERT and Perspective API, to\nfind out the sentiment and toxicity of the sentence toward the COVID-19\nvaccine.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u793e\u4ea4\u5a92\u4f53\u4e0a\u5173\u4e8eCOVID-19\u75ab\u82d7\u7684\u9634\u8c0b\u8bba\u8bc4\u8bba\uff0c\u4f7f\u7528BERT\u548cPerspective API\u6a21\u578b\u5bf9598\u6761\u72ec\u7279\u8bc4\u8bba\u8fdb\u884c\u60c5\u611f\u5206\u6790\u548c\u6bd2\u6027\u68c0\u6d4b\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u5145\u65a5\u7740\u53cd\u75ab\u82d7\u8bc4\u8bba\u548c\u9634\u8c0b\u8bba\uff0c\u59825G\u7f51\u7edc\u4f20\u64ad\u75c5\u6bd2\u3001\u4e2d\u56fd\u751f\u7269\u6b66\u5668\u8bba\u3001\u6bd4\u5c14\u00b7\u76d6\u8328\u8ffd\u8e2a\u9634\u8c0b\u7b49\uff0c\u8fd9\u4e9b\u7406\u8bba\u5bfc\u81f4\u4e86\u5b9e\u9645\u7834\u574f\u884c\u4e3a\u548c\u75ab\u82d7\u72b9\u8c6b\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u6536\u96c6598\u6761\u4e0eCOVID-19\u75ab\u82d7\u76f8\u5173\u7684\u72ec\u7279\u8bc4\u8bba\uff0c\u4f7f\u7528BERT\u6a21\u578b\u8fdb\u884c\u60c5\u611f\u5206\u6790\uff0c\u4f7f\u7528Perspective API\u68c0\u6d4b\u8bc4\u8bba\u6bd2\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u793e\u4ea4\u5a92\u4f53\u4e0a\u9488\u5bf9COVID-19\u75ab\u82d7\u7684\u5404\u79cd\u9634\u8c0b\u8bba\u8bc4\u8bba\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u4e9b\u8bc4\u8bba\u7684\u60c5\u611f\u548c\u6bd2\u6027\u7279\u5f81\u3002", "conclusion": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u9634\u8c0b\u8bba\u8bc4\u8bba\u5bf9\u516c\u4f17\u4fe1\u4efb\u548c\u75ab\u82d7\u63a5\u79cd\u4ea7\u751f\u4e86\u8d1f\u9762\u5f71\u54cd\uff0c\u9700\u8981\u91c7\u53d6\u6709\u6548\u63aa\u65bd\u5e94\u5bf9\u8fd9\u4e9b\u6709\u5bb3\u4fe1\u606f\u3002"}}
{"id": "2510.10539", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10539", "abs": "https://arxiv.org/abs/2510.10539", "authors": ["Yujie Ren", "Niklas Gruhlke", "Anne Lauscher"], "title": "Detecting Hallucinations in Authentic LLM-Human Interactions", "comment": null, "summary": "As large language models (LLMs) are increasingly applied in sensitive domains\nsuch as medicine and law, hallucination detection has become a critical task.\nAlthough numerous benchmarks have been proposed to advance research in this\narea, most of them are artificially constructed--either through deliberate\nhallucination induction or simulated interactions--rather than derived from\ngenuine LLM-human dialogues. Consequently, these benchmarks fail to fully\ncapture the characteristics of hallucinations that occur in real-world usage.\nTo address this limitation, we introduce AuthenHallu, the first hallucination\ndetection benchmark built entirely from authentic LLM-human interactions. For\nAuthenHallu, we select and annotate samples from genuine LLM-human dialogues,\nthereby providing a faithful reflection of how LLMs hallucinate in everyday\nuser interactions. Statistical analysis shows that hallucinations occur in\n31.4% of the query-response pairs in our benchmark, and this proportion\nincreases dramatically to 60.0% in challenging domains such as Math & Number\nProblems. Furthermore, we explore the potential of using vanilla LLMs\nthemselves as hallucination detectors and find that, despite some promise,\ntheir current performance remains insufficient in real-world scenarios.", "AI": {"tldr": "AuthenHallu\u662f\u9996\u4e2a\u5b8c\u5168\u57fa\u4e8e\u771f\u5b9eLLM-\u4eba\u7c7b\u5bf9\u8bdd\u6784\u5efa\u7684\u5e7b\u89c9\u68c0\u6d4b\u57fa\u51c6\uff0c\u76f8\u6bd4\u4eba\u5de5\u6784\u9020\u7684\u57fa\u51c6\u80fd\u66f4\u771f\u5b9e\u53cd\u6620\u5b9e\u9645\u4f7f\u7528\u4e2d\u7684\u5e7b\u89c9\u7279\u5f81\u3002", "motivation": "\u73b0\u6709\u5e7b\u89c9\u68c0\u6d4b\u57fa\u51c6\u591a\u4e3a\u4eba\u5de5\u6784\u9020\uff0c\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620LLM\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d\u7684\u5e7b\u89c9\u7279\u5f81\uff0c\u9700\u8981\u57fa\u4e8e\u771f\u5b9e\u5bf9\u8bdd\u6784\u5efa\u66f4\u53ef\u9760\u7684\u57fa\u51c6\u3002", "method": "\u4ece\u771f\u5b9e\u7684LLM-\u4eba\u7c7b\u5bf9\u8bdd\u4e2d\u9009\u62e9\u6837\u672c\u8fdb\u884c\u6807\u6ce8\uff0c\u6784\u5efaAuthenHallu\u57fa\u51c6\uff0c\u5e76\u5206\u6790\u5e7b\u89c9\u5206\u5e03\u7279\u5f81\uff0c\u540c\u65f6\u63a2\u7d22\u4f7f\u7528\u666e\u901aLLM\u4f5c\u4e3a\u5e7b\u89c9\u68c0\u6d4b\u5668\u7684\u6f5c\u529b\u3002", "result": "\u7edf\u8ba1\u663e\u793a31.4%\u7684\u67e5\u8be2-\u54cd\u5e94\u5bf9\u5b58\u5728\u5e7b\u89c9\uff0c\u5728\u6570\u5b66\u548c\u6570\u5b57\u95ee\u9898\u7b49\u6311\u6218\u6027\u9886\u57df\u8fd9\u4e00\u6bd4\u4f8b\u9ad8\u8fbe60.0%\u3002\u666e\u901aLLM\u4f5c\u4e3a\u68c0\u6d4b\u5668\u8868\u73b0\u6709\u6f5c\u529b\u4f46\u5c1a\u4e0d\u5145\u5206\u3002", "conclusion": "\u57fa\u4e8e\u771f\u5b9e\u5bf9\u8bdd\u7684\u57fa\u51c6\u80fd\u66f4\u51c6\u786e\u53cd\u6620LLM\u5e7b\u89c9\u7279\u5f81\uff0c\u5f53\u524dLLM\u4f5c\u4e3a\u5e7b\u89c9\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u4ecd\u9700\u6539\u8fdb\uff0cAuthenHallu\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u57fa\u7840\u3002"}}
{"id": "2510.10004", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10004", "abs": "https://arxiv.org/abs/2510.10004", "authors": ["Jiahui Hong", "Siqing Li", "Muqing Jian", "Luming Yang"], "title": "Bidirectional Time-Frequency Pyramid Network for Enhanced Robust EEG Classification", "comment": "Accepted to IEEE BIBM 2025", "summary": "Existing EEG recognition models suffer from poor cross-paradigm\ngeneralization due to dataset-specific constraints and individual variability.\nTo overcome these limitations, we propose BITE (Bidirectional Time-Freq Pyramid\nNetwork), an end-to-end unified architecture featuring robust multistream\nsynergy, pyramid time-frequency attention (PTFA), and bidirectional adaptive\nconvolutions. The framework uniquely integrates: 1) Aligned time-frequency\nstreams maintaining temporal synchronization with STFT for bidirectional\nmodeling, 2) PTFA-based multi-scale feature enhancement amplifying critical\nneural patterns, 3) BiTCN with learnable fusion capturing forward/backward\nneural dynamics. Demonstrating enhanced robustness, BITE achieves\nstate-of-the-art performance across four divergent paradigms (BCICIV-2A/2B,\nHGD, SD-SSVEP), excelling in both within-subject accuracy and cross-subject\ngeneralization. As a unified architecture, it combines robust performance\nacross both MI and SSVEP tasks with exceptional computational efficiency. Our\nwork validates that paradigm-aligned spectral-temporal processing is essential\nfor reliable BCI systems. Just as its name suggests, BITE \"takes a bite out of\nEEG.\" The source code is available at https://github.com/cindy-hong/BiteEEG.", "AI": {"tldr": "BITE\u662f\u4e00\u4e2a\u53cc\u5411\u65f6\u9891\u91d1\u5b57\u5854\u7f51\u7edc\uff0c\u901a\u8fc7\u591a\u6d41\u534f\u540c\u3001\u91d1\u5b57\u5854\u65f6\u9891\u6ce8\u610f\u529b\u548c\u53cc\u5411\u81ea\u9002\u5e94\u5377\u79ef\uff0c\u89e3\u51b3\u4e86EEG\u8bc6\u522b\u6a21\u578b\u8de8\u8303\u5f0f\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709EEG\u8bc6\u522b\u6a21\u578b\u7531\u4e8e\u6570\u636e\u96c6\u7279\u5b9a\u7ea6\u675f\u548c\u4e2a\u4f53\u53d8\u5f02\u6027\uff0c\u5728\u8de8\u8303\u5f0f\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u67b6\u6784\u3002", "method": "\u63d0\u51faBITE\u7aef\u5230\u7aef\u7edf\u4e00\u67b6\u6784\uff0c\u5305\u542b\u5bf9\u9f50\u7684\u65f6\u9891\u6d41\u3001\u91d1\u5b57\u5854\u65f6\u9891\u6ce8\u610f\u529b\u673a\u5236\u548c\u53cc\u5411\u81ea\u9002\u5e94\u5377\u79ef\uff0c\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u589e\u5f3a\u548c\u53cc\u5411\u795e\u7ecf\u52a8\u6001\u5efa\u6a21\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u8303\u5f0f\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u4e2a\u4f53\u5185\u7cbe\u5ea6\u548c\u8de8\u4e2a\u4f53\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u517c\u5177\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8303\u5f0f\u5bf9\u9f50\u7684\u9891\u8c31-\u65f6\u95f4\u5904\u7406\u5bf9\u4e8e\u53ef\u9760BCI\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0cBITE\u9a8c\u8bc1\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.05577", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05577", "abs": "https://arxiv.org/abs/2510.05577", "authors": ["Dong Yan", "Gaochen Wu", "Bowen Zhou"], "title": "Mission Impossible: Feedback-Guided Dynamic Interactive Planning for Improving Reasoning on LLMs", "comment": null, "summary": "Recent advancements in language agents have led to significant improvements\nin multi-hop reasoning tasks. However, existing approaches often struggle with\nhandling open-domain problems, which require massive information retrieval due\nto their reliance on a fixed sequence of actions. To address this, we propose\nFeedback-Guided Dynamic Interactive Planning (FGDIP), a novel framework\ntailored to enhance reasoning in LLMs by utilizing dynamic and adaptive\nstrategies for information exploration in open-domain multi-hop reasoning\ntasks. Our approach begins by identifying key entities relevant to the problem,\nwhich serve as the initial nodes in the reasoning process. From these initial\nnodes, we then generate reasoning child nodes with the process being refined\nthrough a combination of historical error analysis and real-time feedback,\nwhich allows the framework to dynamically adjust and optimize its reasoning\nstrategies. By integrating depth-first search with an innovative node\ngeneration technique, our framework adapts based on both prior error paths and\nconcurrently generated nodes at the same hierarchical level. This dynamic\nstrategy effectively expands the search space while ensuring the reasoning\nprocess systematically converges toward accurate solutions. Experimental\nresults show that FGDIP achieved up to 54.47% F1 score on the HotpotQA dataset\nand 70.05% on the StrategyQA dataset, surpassing the best baseline by 5.03% and\n7.25% respectively, highlighting its versatility and potential to enhance\nlanguage agents in multi-hop reasoning tasks.", "AI": {"tldr": "\u63d0\u51faFGDIP\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4ea4\u4e92\u89c4\u5212\u548c\u53cd\u9988\u5f15\u5bfc\u673a\u5236\uff0c\u5728\u5f00\u653e\u57df\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5f00\u653e\u57df\u95ee\u9898\u65f6\uff0c\u7531\u4e8e\u4f9d\u8d56\u56fa\u5b9a\u7684\u52a8\u4f5c\u5e8f\u5217\uff0c\u96be\u4ee5\u5e94\u5bf9\u9700\u8981\u5927\u91cf\u4fe1\u606f\u68c0\u7d22\u7684\u591a\u8df3\u63a8\u7406\u4efb\u52a1", "method": "\u57fa\u4e8e\u5173\u952e\u5b9e\u4f53\u6784\u5efa\u521d\u59cb\u8282\u70b9\uff0c\u901a\u8fc7\u5386\u53f2\u9519\u8bef\u5206\u6790\u548c\u5b9e\u65f6\u53cd\u9988\u52a8\u6001\u751f\u6210\u63a8\u7406\u5b50\u8282\u70b9\uff0c\u7ed3\u5408\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u548c\u521b\u65b0\u7684\u8282\u70b9\u751f\u6210\u6280\u672f", "result": "\u5728HotpotQA\u6570\u636e\u96c6\u4e0a\u8fbe\u523054.47% F1\u5206\u6570\uff0cStrategyQA\u6570\u636e\u96c6\u4e0a\u8fbe\u523070.05%\uff0c\u5206\u522b\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u53475.03%\u548c7.25%", "conclusion": "FGDIP\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u7b56\u7565\u6709\u6548\u6269\u5c55\u641c\u7d22\u7a7a\u95f4\uff0c\u786e\u4fdd\u63a8\u7406\u8fc7\u7a0b\u7cfb\u7edf\u6027\u5730\u6536\u655b\u5230\u51c6\u786e\u89e3\uff0c\u5c55\u73b0\u4e86\u5728\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u548c\u901a\u7528\u6027"}}
{"id": "2510.10560", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.10560", "abs": "https://arxiv.org/abs/2510.10560", "authors": ["Euhid Aman", "Esteban Carlin", "Hsing-Kuo Pao", "Giovanni Beltrame", "Ghaluh Indah Permata Sari", "Yie-Tarng Chen"], "title": "BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices", "comment": "6 pages, BabyLM Workshop, EMNLP 2025", "summary": "Cross-attention transformers and other multimodal vision-language models\nexcel at grounding and generation; however, their extensive, full-precision\nbackbones make it challenging to deploy them on edge devices. Memory-augmented\narchitectures enhance the utilization of past context; however, most works\nrarely pair them with aggressive edge-oriented quantization. We introduce\nBitMar, a quantized multimodal transformer that proposes an external human-like\nepisodic memory for effective image-text generation on hardware with limited\nresources. BitMar utilizes 1.58-bit encoders, one for text (BitNet-style) and\none for vision (DiNOv2-based), to create compact embeddings that are combined\nand used to query a fixed-size key-value episodic memory. During vector\nretrieval, the BitNet decoder applies per-layer conditioning, which increases\nthe contextual relevance of generated content. The decoder also employs\nattention sinks with a sliding-window mechanism to process long or streaming\ninputs under tight memory budgets. The combination of per-layer conditioning\nand sliding-window attention achieves a strong quality-speed trade-off,\ndelivering competitive captioning and multimodal understanding at low latency\nwith a small model footprint. These characteristics make BitMar well-suited for\nedge deployment.", "AI": {"tldr": "BitMar\u662f\u4e00\u4e2a\u91cf\u5316\u591a\u6a21\u6001transformer\uff0c\u4f7f\u75281.58\u4f4d\u7f16\u7801\u5668\u548c\u5916\u90e8\u60c5\u666f\u8bb0\u5fc6\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u56fe\u50cf-\u6587\u672c\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u8de8\u6ce8\u610f\u529btransformer\u548c\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u64c5\u957fgrounding\u548c\u751f\u6210\uff0c\u4f46\u5176\u5168\u7cbe\u5ea6\u9aa8\u5e72\u7f51\u7edc\u96be\u4ee5\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002\u5185\u5b58\u589e\u5f3a\u67b6\u6784\u5f88\u5c11\u4e0e\u6fc0\u8fdb\u7684\u8fb9\u7f18\u5bfc\u5411\u91cf\u5316\u7ed3\u5408\u4f7f\u7528\u3002", "method": "\u4f7f\u75281.58\u4f4d\u7f16\u7801\u5668\uff08\u6587\u672c\u7528BitNet\u98ce\u683c\uff0c\u89c6\u89c9\u7528DiNOv2\u57fa\u7840\uff09\u521b\u5efa\u7d27\u51d1\u5d4c\u5165\uff0c\u7ed3\u5408\u56fa\u5b9a\u5927\u5c0f\u7684\u952e\u503c\u60c5\u666f\u8bb0\u5fc6\u3002\u89e3\u7801\u5668\u91c7\u7528\u9010\u5c42\u6761\u4ef6\u5316\u548c\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u957f\u8f93\u5165\u3002", "result": "\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684captioning\u548c\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u5728\u4f4e\u5ef6\u8fdf\u548c\u5c0f\u6a21\u578b\u5360\u7528\u4e0b\u8fbe\u5230\u826f\u597d\u7684\u8d28\u91cf-\u901f\u5ea6\u6743\u8861\u3002", "conclusion": "BitMar\u7684\u7279\u6027\u4f7f\u5176\u975e\u5e38\u9002\u5408\u8fb9\u7f18\u90e8\u7f72\u3002"}}
{"id": "2510.10023", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10023", "abs": "https://arxiv.org/abs/2510.10023", "authors": ["Yinghui He", "Abhishek Panigrahi", "Yong Lin", "Sanjeev Arora"], "title": "Skill-Targeted Adaptive Training", "comment": null, "summary": "Language models often show little to no improvement (i.e., \"saturation\") when\ntrained via vanilla supervised fine-tuning (SFT) on data similar to what they\nsaw in their training set (e.g., MATH). We introduce a new fine-tuning\nstrategy, STAT, to train such a student model by using the metacognition\nability of a stronger large language model (LLM) as the teacher. The teacher\nuses the task dataset to create a list of skills needed for the task, and then\nlabels each data point with its required skills (Didolkar et al., 2024). By\nmonitoring the student's answers, the teacher creates a Missing-Skill-Profile\nfor the student, tracking how often they failed to apply each skill in their\nresponses. We use this idea to build a modified training set in one of two\nways. In STAT-Sel, the teacher uses an existing set of training examples but\nadaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn,\nthe teacher synthesizes additional examples involving missing skills. Across\nextensive experiments on Llama and Qwen models, our methods yield improvements\nof up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore,\nSTAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25,\nAMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is\ncomplementary to RL via GRPO (Shao et al., 2024): after the model is improved\nusing STAT to address skill gaps, GRPO continues to add further gains. We\nconclude that skill-targeted adaptive training should broadly improve current\ntraining pipelines. Our code is available at:\nhttps://github.com/princeton-pli/STAT.", "AI": {"tldr": "STAT\u662f\u4e00\u79cd\u65b0\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u5229\u7528\u66f4\u5f3aLLM\u7684\u5143\u8ba4\u77e5\u80fd\u529b\u4f5c\u4e3a\u6559\u5e08\uff0c\u901a\u8fc7\u5206\u6790\u5b66\u751f\u6a21\u578b\u7684\u7f3a\u5931\u6280\u80fd\u6863\u6848\u6765\u521b\u5efa\u9488\u5bf9\u6027\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u5728\u8bad\u7ec3\u6570\u636e\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u76f8\u4f3c\u65f6\u5f80\u5f80\u6548\u679c\u6709\u9650\uff08\u9971\u548c\u73b0\u8c61\uff09\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6280\u80fd\u9488\u5bf9\u6027\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "STAT\u65b9\u6cd5\uff1a\u6559\u5e08\u6a21\u578b\u521b\u5efa\u4efb\u52a1\u6240\u9700\u6280\u80fd\u5217\u8868\u5e76\u6807\u6ce8\u6570\u636e\u70b9\u6280\u80fd\u9700\u6c42\uff0c\u901a\u8fc7\u76d1\u63a7\u5b66\u751f\u56de\u7b54\u6784\u5efa\u7f3a\u5931\u6280\u80fd\u6863\u6848\uff0c\u7136\u540e\u901a\u8fc7STAT-Sel\uff08\u91cd\u52a0\u6743\u73b0\u6709\u8bad\u7ec3\u6837\u672c\uff09\u6216STAT-Syn\uff08\u5408\u6210\u7f3a\u5931\u6280\u80fd\u76f8\u5173\u6837\u672c\uff09\u6784\u5efa\u6539\u8fdb\u7684\u8bad\u7ec3\u96c6\u3002", "result": "\u5728Llama\u548cQwen\u6a21\u578b\u4e0a\uff0cSTAT\u5728MATH\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u8fbe7.5%\uff0c\u5728\u5206\u5e03\u5916\u57fa\u51c6\uff08AIME24/25\u3001AMC23\u7b49\uff09\u4e0a\u5e73\u5747\u63d0\u53474.6%\uff0c\u4e14\u4e0eGRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e92\u8865\u3002", "conclusion": "\u6280\u80fd\u5bfc\u5411\u7684\u81ea\u9002\u5e94\u8bad\u7ec3\u5e94\u5e7f\u6cdb\u6539\u8fdb\u5f53\u524d\u8bad\u7ec3\u6d41\u7a0b\uff0cSTAT\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u6a21\u578b\u6280\u80fd\u5dee\u8ddd\u95ee\u9898\u3002"}}
{"id": "2510.10613", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10613", "abs": "https://arxiv.org/abs/2510.10613", "authors": ["Di Wu abd Shuaidong Pan"], "title": "Dynamic Topic Evolution with Temporal Decay and Attention in Large Language Models", "comment": null, "summary": "This paper proposes a modeling framework for dynamic topic evolution based on\ntemporal large language models. The method first uses a large language model to\nobtain contextual embeddings of text and then introduces a temporal decay\nfunction and an attention mechanism. These components allow the model to adjust\nthe importance of semantic units according to time intervals and capture topic\nvariations across different periods. The temporal representations are then\nmapped into a latent topic space, where a state transition matrix is applied to\ndescribe the dynamic evolution of topics. A joint optimization objective\nconstrains both semantic modeling and temporal consistency, ensuring diversity\nand smoothness in topic generation. The design emphasizes the unified modeling\nof semantic representation and temporal evolution, which improves topic\ncoherence and diversity while enhancing stability and interpretability over\ntime. Experiments on real-world corpora show that the framework effectively\ncaptures the generation, expansion, and decline of topics and outperforms\nexisting models across multiple metrics. Overall, the proposed method provides\na systematic solution for understanding dynamic semantic patterns in\nlarge-scale text, enriches the research paradigm of topic modeling, and\nsupports complex text analysis tasks in multiple domains.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u65f6\u5e8f\u5927\u8bed\u8a00\u6a21\u578b\u7684\u52a8\u6001\u4e3b\u9898\u6f14\u5316\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u8870\u51cf\u51fd\u6570\u548c\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u4e3b\u9898\u53d8\u5316\uff0c\u5728\u6f5c\u5728\u4e3b\u9898\u7a7a\u95f4\u4e2d\u4f7f\u7528\u72b6\u6001\u8f6c\u79fb\u77e9\u9635\u63cf\u8ff0\u4e3b\u9898\u52a8\u6001\u6f14\u5316\u3002", "motivation": "\u4f20\u7edf\u4e3b\u9898\u6a21\u578b\u96be\u4ee5\u6709\u6548\u6355\u6349\u4e3b\u9898\u968f\u65f6\u95f4\u7684\u52a8\u6001\u6f14\u5316\u8fc7\u7a0b\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u5efa\u6a21\u8bed\u4e49\u8868\u793a\u4e0e\u65f6\u95f4\u6f14\u5316\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u83b7\u53d6\u6587\u672c\u4e0a\u4e0b\u6587\u5d4c\u5165\uff0c\u5f15\u5165\u65f6\u95f4\u8870\u51cf\u51fd\u6570\u548c\u6ce8\u610f\u529b\u673a\u5236\u8c03\u6574\u8bed\u4e49\u5355\u5143\u91cd\u8981\u6027\uff0c\u5728\u6f5c\u5728\u4e3b\u9898\u7a7a\u95f4\u5e94\u7528\u72b6\u6001\u8f6c\u79fb\u77e9\u9635\u63cf\u8ff0\u4e3b\u9898\u52a8\u6001\u6f14\u5316\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u76ee\u6807\u7ea6\u675f\u8bed\u4e49\u5efa\u6a21\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728\u771f\u5b9e\u8bed\u6599\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u6355\u6349\u4e3b\u9898\u7684\u751f\u6210\u3001\u6269\u5c55\u548c\u8870\u9000\u8fc7\u7a0b\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3\u5927\u89c4\u6a21\u6587\u672c\u4e2d\u7684\u52a8\u6001\u8bed\u4e49\u6a21\u5f0f\u63d0\u4f9b\u4e86\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\uff0c\u4e30\u5bcc\u4e86\u4e3b\u9898\u5efa\u6a21\u7684\u7814\u7a76\u8303\u5f0f\uff0c\u652f\u6301\u591a\u9886\u57df\u7684\u590d\u6742\u6587\u672c\u5206\u6790\u4efb\u52a1\u3002"}}
{"id": "2510.10028", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10028", "abs": "https://arxiv.org/abs/2510.10028", "authors": ["Yang Li", "Ruichen Zhang", "Yinqiu Liu", "Guangyuan Liu", "Dusit Niyato", "Abbas Jamalipour", "Xianbin Wang", "Dong In Kim"], "title": "Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude Economy Networks via LLM-Enhanced Optimization", "comment": null, "summary": "The rapid advancement of Low-Altitude Economy Networks (LAENets) has enabled\na variety of applications, including aerial surveillance, environmental\nsensing, and semantic data collection. To support these scenarios, unmanned\naerial vehicles (UAVs) equipped with onboard vision-language models (VLMs)\noffer a promising solution for real-time multimodal inference. However,\nensuring both inference accuracy and communication efficiency remains a\nsignificant challenge due to limited onboard resources and dynamic network\nconditions. In this paper, we first propose a UAV-enabled LAENet system model\nthat jointly captures UAV mobility, user-UAV communication, and the onboard\nvisual question answering (VQA) pipeline. Based on this model, we formulate a\nmixed-integer non-convex optimization problem to minimize task latency and\npower consumption under user-specific accuracy constraints. To solve the\nproblem, we design a hierarchical optimization framework composed of two parts:\n(i) an Alternating Resolution and Power Optimization (ARPO) algorithm for\nresource allocation under accuracy constraints, and (ii) a Large Language\nModel-augmented Reinforcement Learning Approach (LLaRA) for adaptive UAV\ntrajectory optimization. The large language model (LLM) serves as an expert in\nrefining reward design of reinforcement learning in an offline fashion,\nintroducing no additional latency in real-time decision-making. Numerical\nresults demonstrate the efficacy of our proposed framework in improving\ninference performance and communication efficiency under dynamic LAENet\nconditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u4eba\u673a\u9a71\u52a8\u7684\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u5c42\u4f18\u5316\u6846\u67b6\u89e3\u51b3\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u5ef6\u8fdf\u548c\u529f\u8017\u4f18\u5316\u95ee\u9898\uff0c\u7ed3\u5408\u8d44\u6e90\u5206\u914d\u7b97\u6cd5\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u65e0\u4eba\u673a\u914d\u5907\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\uff0c\u4f46\u53d7\u9650\u4e8e\u673a\u8f7d\u8d44\u6e90\u548c\u52a8\u6001\u7f51\u7edc\u6761\u4ef6\uff0c\u786e\u4fdd\u63a8\u7406\u7cbe\u5ea6\u548c\u901a\u4fe1\u6548\u7387\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u4f18\u5316\u6846\u67b6\uff1aARPO\u7b97\u6cd5\u7528\u4e8e\u7cbe\u5ea6\u7ea6\u675f\u4e0b\u7684\u8d44\u6e90\u5206\u914d\uff0cLLaRA\u65b9\u6cd5\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u79bb\u7ebf\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u65e0\u4eba\u673a\u8f68\u8ff9\u4f18\u5316\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u52a8\u6001\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u6761\u4ef6\u4e0b\u80fd\u6709\u6548\u63d0\u5347\u63a8\u7406\u6027\u80fd\u548c\u901a\u4fe1\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u5c42\u4f18\u5316\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u5ef6\u8fdf\u548c\u529f\u8017\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u4e2d\u7684\u5b9e\u65f6\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10618", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10618", "abs": "https://arxiv.org/abs/2510.10618", "authors": ["Bowei He", "Lihao Yin", "Huiling Zhen", "Shuqi Liu", "Han Wu", "Xiaokun Zhang", "Mingxuan Yuan", "Chen Ma"], "title": "Preserving LLM Capabilities through Calibration Data Curation: From Analysis to Optimization", "comment": "Accepted by NeurIPS 2025", "summary": "Post-training compression has been a widely employed approach to scale down\nlarge language model (LLM) and facilitate efficient inference. In various\nproposed compression methods, including pruning and quantization, calibration\ndata plays a vital role by informing the weight importance and activation\ndynamic ranges. However, how calibration data impacts the LLM capability after\ncompression is less explored. Few of the existing works, though recognizing the\nsignificance of this study, only investigate the language modeling or\ncommonsense reasoning performance degradation from limited angles, like the\ndata sources or sample amounts. More systematic research is still needed to\nexamine the impacts on different LLM capabilities in terms of compositional\nproperties and domain correspondence of calibration data. In this work, we aim\nat bridging this gap and further analyze underlying influencing mechanisms from\nthe activation pattern perspective. Especially, we explore the calibration\ndata's impacts on high-level complex reasoning capabilities, like math problem\nsolving and code generation. Delving into the underlying mechanism, we find\nthat the representativeness and diversity in activation space more\nfundamentally determine the quality of calibration data. Finally, we propose a\ncalibration data curation framework based on such observations and analysis,\nenhancing the performance of existing post-training compression methods on\npreserving critical LLM capabilities. Our code is provided in\n\\href{https://github.com/BokwaiHo/COLA.git}{Link}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u6821\u51c6\u6570\u636e\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u538b\u7f29\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u4ee3\u8868\u6027\u548c\u591a\u6837\u6027\u662f\u51b3\u5b9a\u6821\u51c6\u6570\u636e\u8d28\u91cf\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6b64\u7684\u6821\u51c6\u6570\u636e\u7b5b\u9009\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u867d\u7136\u8ba4\u8bc6\u5230\u6821\u51c6\u6570\u636e\u7684\u91cd\u8981\u6027\uff0c\u4f46\u5bf9\u5176\u5982\u4f55\u5f71\u54cd\u538b\u7f29\u540eLLM\u80fd\u529b\u7684\u7814\u7a76\u4e0d\u591f\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u5f71\u54cd\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4ece\u6fc0\u6d3b\u6a21\u5f0f\u89d2\u5ea6\u5206\u6790\u6821\u51c6\u6570\u636e\u7684\u5f71\u54cd\u673a\u5236\uff0c\u63a2\u7d22\u5176\u5bf9\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u548c\u4ee3\u7801\u751f\u6210\u7b49\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6fc0\u6d3b\u7a7a\u95f4\u4ee3\u8868\u6027\u548c\u591a\u6837\u6027\u7684\u6821\u51c6\u6570\u636e\u7b5b\u9009\u6846\u67b6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u4ee3\u8868\u6027\u548c\u591a\u6837\u6027\u66f4\u6839\u672c\u5730\u51b3\u5b9a\u4e86\u6821\u51c6\u6570\u636e\u7684\u8d28\u91cf\uff0c\u63d0\u51fa\u7684\u6570\u636e\u7b5b\u9009\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u73b0\u6709\u540e\u8bad\u7ec3\u538b\u7f29\u65b9\u6cd5\u5728\u4fdd\u7559\u5173\u952eLLM\u80fd\u529b\u65b9\u9762\u7684\u6027\u80fd\u3002", "conclusion": "\u6821\u51c6\u6570\u636e\u7684\u8d28\u91cf\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u6570\u636e\u6765\u6e90\u548c\u6837\u672c\u6570\u91cf\uff0c\u66f4\u91cd\u8981\u7684\u662f\u5176\u5728\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u4ee3\u8868\u6027\u548c\u591a\u6837\u6027\uff0c\u8fd9\u4e3a\u6539\u8fdb\u540e\u8bad\u7ec3\u538b\u7f29\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u5de5\u5177\u3002"}}
{"id": "2510.10627", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10627", "abs": "https://arxiv.org/abs/2510.10627", "authors": ["Guy Mor-Lan", "Tamir Sheafer", "Shaul R. Shenhav"], "title": "FactAppeal: Identifying Epistemic Factual Appeals in News Media", "comment": null, "summary": "How is a factual claim made credible? We propose the novel task of Epistemic\nAppeal Identification, which identifies whether and how factual statements have\nbeen anchored by external sources or evidence. To advance research on this\ntask, we present FactAppeal, a manually annotated dataset of 3,226\nEnglish-language news sentences. Unlike prior resources that focus solely on\nclaim detection and verification, FactAppeal identifies the nuanced epistemic\nstructures and evidentiary basis underlying these claims and used to support\nthem. FactAppeal contains span-level annotations which identify factual\nstatements and mentions of sources on which they rely. Moreover, the\nannotations include fine-grained characteristics of factual appeals such as the\ntype of source (e.g. Active Participant, Witness, Expert, Direct Evidence),\nwhether it is mentioned by name, mentions of the source's role and epistemic\ncredentials, attribution to the source via direct or indirect quotation, and\nother features. We model the task with a range of encoder models and generative\ndecoder models in the 2B-9B parameter range. Our best performing model, based\non Gemma 2 9B, achieves a macro-F1 score of 0.73.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u65b0\u7684\u8ba4\u77e5\u5438\u5f15\u529b\u8bc6\u522b\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc6\u522b\u4e8b\u5b9e\u9648\u8ff0\u662f\u5426\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u5916\u90e8\u6765\u6e90\u6216\u8bc1\u636e\u8fdb\u884c\u951a\u5b9a\uff0c\u5e76\u53d1\u5e03\u4e86FactAppeal\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u4e8b\u5b9e\u4e3b\u5f20\u5982\u4f55\u53d8\u5f97\u53ef\u4fe1\uff0c\u63a2\u7d22\u4e8b\u5b9e\u9648\u8ff0\u88ab\u5916\u90e8\u6765\u6e90\u6216\u8bc1\u636e\u951a\u5b9a\u7684\u65b9\u5f0f\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u4ec5\u5173\u6ce8\u4e3b\u5f20\u68c0\u6d4b\u548c\u9a8c\u8bc1\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86FactAppeal\u6570\u636e\u96c6\uff0c\u5305\u542b3,226\u4e2a\u82f1\u6587\u65b0\u95fb\u53e5\u5b50\u7684\u624b\u52a8\u6807\u6ce8\uff0c\u8bc6\u522b\u4e8b\u5b9e\u9648\u8ff0\u548c\u4f9d\u8d56\u7684\u6765\u6e90\u63d0\u53ca\uff0c\u5e76\u91c7\u75282B-9B\u53c2\u6570\u8303\u56f4\u7684\u7f16\u7801\u5668\u548c\u751f\u6210\u89e3\u7801\u5668\u6a21\u578b\u8fdb\u884c\u5efa\u6a21\u3002", "result": "\u6700\u4f73\u6a21\u578b\u57fa\u4e8eGemma 2 9B\uff0c\u5b9e\u73b0\u4e860.73\u7684\u5b8fF1\u5206\u6570\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u4e8b\u5b9e\u9648\u8ff0\u7684\u8ba4\u77e5\u7ed3\u6784\u548c\u8bc1\u636e\u57fa\u7840\u3002", "conclusion": "\u8ba4\u77e5\u5438\u5f15\u529b\u8bc6\u522b\u662f\u4e00\u4e2a\u91cd\u8981\u4e14\u53ef\u884c\u7684\u4efb\u52a1\uff0cFactAppeal\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6a21\u578b\u4e3a\u7406\u89e3\u4e8b\u5b9e\u4e3b\u5f20\u7684\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u57fa\u7840\u3002"}}
{"id": "2510.10041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10041", "abs": "https://arxiv.org/abs/2510.10041", "authors": ["Sahng-Min Han", "Minjae Kim", "Jinho Cha", "Se-woon Choe", "Eunchan Daniel Cha", "Jungwon Choi", "Kyudong Jung"], "title": "FOSSIL: Regret-Minimizing Curriculum Learning for Metadata-Free and Low-Data Mpox Diagnosis", "comment": "35 pages, 11 figures, submitted to Computers in Biology and Medicine\n  (Elsevier, under review)", "summary": "Deep learning in small and imbalanced biomedical datasets remains\nfundamentally constrained by unstable optimization and poor generalization. We\npresent the first biomedical implementation of FOSSIL (Flexible Optimization\nvia Sample-Sensitive Importance Learning), a regret-minimizing weighting\nframework that adaptively balances training emphasis according to sample\ndifficulty. Using softmax-based uncertainty as a continuous measure of\ndifficulty, we construct a four-stage curriculum (Easy-Very Hard) and integrate\nFOSSIL into both convolutional and transformer-based architectures for Mpox\nskin lesion diagnosis. Across all settings, FOSSIL substantially improves\ndiscrimination (AUC = 0.9573), calibration (ECE = 0.053), and robustness under\nreal-world perturbations, outperforming conventional baselines without\nmetadata, manual curation, or synthetic augmentation. The results position\nFOSSIL as a generalizable, data-efficient, and interpretable framework for\ndifficulty-aware learning in medical imaging under data scarcity.", "AI": {"tldr": "\u63d0\u51fa\u4e86FOSSIL\u6846\u67b6\uff0c\u901a\u8fc7\u6837\u672c\u654f\u611f\u91cd\u8981\u6027\u5b66\u4e60\u81ea\u9002\u5e94\u5e73\u8861\u8bad\u7ec3\u91cd\u70b9\uff0c\u5728\u5c0f\u6837\u672c\u4e0d\u5e73\u8861\u751f\u7269\u533b\u5b66\u6570\u636e\u4e2d\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5c0f\u6837\u672c\u4e0d\u5e73\u8861\u751f\u7269\u533b\u5b66\u6570\u636e\u96c6\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f18\u5316\u4e0d\u7a33\u5b9a\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u57fa\u4e8esoftmax\u7684\u4e0d\u786e\u5b9a\u6027\u4f5c\u4e3a\u6837\u672c\u96be\u5ea6\u7684\u8fde\u7eed\u5ea6\u91cf\uff0c\u6784\u5efa\u56db\u9636\u6bb5\u8bfe\u7a0b\uff08\u7b80\u5355-\u975e\u5e38\u56f0\u96be\uff09\uff0c\u5c06FOSSIL\u96c6\u6210\u5230\u5377\u79ef\u548c\u57fa\u4e8etransformer\u7684\u67b6\u6784\u4e2d\u7528\u4e8eMpox\u76ae\u80a4\u75c5\u53d8\u8bca\u65ad\u3002", "result": "\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\uff0cFOSSIL\u663e\u8457\u63d0\u9ad8\u4e86\u533a\u5206\u80fd\u529b\uff08AUC = 0.9573\uff09\u3001\u6821\u51c6\u80fd\u529b\uff08ECE = 0.053\uff09\u4ee5\u53ca\u5728\u771f\u5b9e\u4e16\u754c\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FOSSIL\u662f\u4e00\u4e2a\u53ef\u63a8\u5e7f\u3001\u6570\u636e\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u533b\u5b66\u5f71\u50cf\u4e2d\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u96be\u5ea6\u611f\u77e5\u5b66\u4e60\u3002"}}
{"id": "2510.10658", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10658", "abs": "https://arxiv.org/abs/2510.10658", "authors": ["Guy Mor-Lan", "Tamir Sheafer", "Shaul R. Shenhav"], "title": "You're Not Gonna Believe This: A Computational Analysis of Factual Appeals and Sourcing in Partisan News", "comment": null, "summary": "While media bias is widely studied, the epistemic strategies behind factual\nreporting remain computationally underexplored. This paper analyzes these\nstrategies through a large-scale comparison of CNN and Fox News. To isolate\nreporting style from topic selection, we employ an article matching strategy to\ncompare reports on the same events and apply the FactAppeal framework to a\ncorpus of over 470K articles covering two highly politicized periods: the\nCOVID-19 pandemic and the Israel-Hamas war. We find that CNN's reporting\ncontains more factual statements and is more likely to ground them in external\nsources. The outlets also exhibit sharply divergent sourcing patterns: CNN\nbuilds credibility by citing Experts} and Expert Documents, constructing an\nappeal to formal authority, whereas Fox News favors News Reports and direct\nquotations. This work quantifies how partisan outlets use systematically\ndifferent epistemic strategies to construct reality, adding a new dimension to\nthe study of media bias.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6bd4\u8f83CNN\u548c\u798f\u514b\u65af\u65b0\u95fb\u5bf9\u76f8\u540c\u4e8b\u4ef6\u7684\u62a5\u9053\uff0c\u91cf\u5316\u4e86\u5a92\u4f53\u5728\u4e8b\u5b9e\u62a5\u9053\u4e2d\u4f7f\u7528\u7684\u4e0d\u540c\u8ba4\u77e5\u7b56\u7565\uff0c\u53d1\u73b0CNN\u66f4\u4f9d\u8d56\u4e13\u5bb6\u548c\u6b63\u5f0f\u6743\u5a01\uff0c\u800c\u798f\u514b\u65af\u65b0\u95fb\u66f4\u504f\u597d\u65b0\u95fb\u62a5\u9053\u548c\u76f4\u63a5\u5f15\u7528\u3002", "motivation": "\u867d\u7136\u5a92\u4f53\u504f\u89c1\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u4e8b\u5b9e\u62a5\u9053\u80cc\u540e\u7684\u8ba4\u77e5\u7b56\u7565\u5728\u8ba1\u7b97\u5c42\u9762\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5927\u89c4\u6a21\u6bd4\u8f83\u5206\u6790\u63ed\u793a\u4e0d\u540c\u5a92\u4f53\u5982\u4f55\u6784\u5efa\u73b0\u5b9e\u3002", "method": "\u91c7\u7528\u6587\u7ae0\u5339\u914d\u7b56\u7565\u6bd4\u8f83\u540c\u4e00\u4e8b\u4ef6\u7684\u62a5\u9053\uff0c\u5e94\u7528FactAppeal\u6846\u67b6\u5206\u679047\u4e07\u7bc7\u5173\u4e8eCOVID-19\u75ab\u60c5\u548c\u4ee5\u8272\u5217-\u54c8\u9a6c\u65af\u6218\u4e89\u7684\u6587\u7ae0\u3002", "result": "CNN\u7684\u62a5\u9053\u5305\u542b\u66f4\u591a\u4e8b\u5b9e\u9648\u8ff0\u4e14\u66f4\u503e\u5411\u4e8e\u5f15\u7528\u5916\u90e8\u6765\u6e90\uff0c\u4e3b\u8981\u5f15\u7528\u4e13\u5bb6\u548c\u4e13\u5bb6\u6587\u4ef6\u6784\u5efa\u6b63\u5f0f\u6743\u5a01\uff1b\u798f\u514b\u65af\u65b0\u95fb\u5219\u66f4\u504f\u597d\u65b0\u95fb\u62a5\u9053\u548c\u76f4\u63a5\u5f15\u7528\u3002", "conclusion": "\u7814\u7a76\u91cf\u5316\u4e86\u515a\u6d3e\u5a92\u4f53\u4f7f\u7528\u7cfb\u7edf\u6027\u4e0d\u540c\u7684\u8ba4\u77e5\u7b56\u7565\u6765\u6784\u5efa\u73b0\u5b9e\uff0c\u4e3a\u5a92\u4f53\u504f\u89c1\u7814\u7a76\u589e\u6dfb\u4e86\u65b0\u7ef4\u5ea6\u3002"}}
{"id": "2510.10057", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10057", "abs": "https://arxiv.org/abs/2510.10057", "authors": ["Lei Gao", "Shihong Huang", "Shengjie Wang", "Hong Ma", "Feng Zhang", "Hengda Bao", "Qichang Chen", "Weihua Zhou"], "title": "One4Many-StablePacker: An Efficient Deep Reinforcement Learning Framework for the 3D Bin Packing Problem", "comment": null, "summary": "The three-dimensional bin packing problem (3D-BPP) is widely applied in\nlogistics and warehousing. Existing learning-based approaches often neglect\npractical stability-related constraints and exhibit limitations in generalizing\nacross diverse bin dimensions. To address these limitations, we propose a novel\ndeep reinforcement learning framework, One4Many-StablePacker (O4M-SP). The\nprimary advantage of O4M-SP is its ability to handle various bin dimensions in\na single training process while incorporating support and weight constraints\ncommon in practice. Our training method introduces two innovative mechanisms.\nFirst, it employs a weighted reward function that integrates loading rate and a\nnew height difference metric for packing layouts, promoting improved bin\nutilization through flatter packing configurations. Second, it combines clipped\npolicy gradient optimization with a tailored policy drifting method to mitigate\npolicy entropy collapse, encouraging exploration at critical decision nodes\nduring packing to avoid suboptimal solutions. Extensive experiments demonstrate\nthat O4M-SP generalizes successfully across diverse bin dimensions and\nsignificantly outperforms baseline methods. Furthermore, O4M-SP exhibits strong\npractical applicability by effectively addressing packing scenarios with\nstability constraints.", "AI": {"tldr": "\u63d0\u51faOne4Many-StablePacker (O4M-SP)\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u89e3\u51b33D\u88c5\u7bb1\u95ee\u9898\uff0c\u80fd\u591f\u5904\u7406\u591a\u79cd\u7bb1\u5b50\u5c3a\u5bf8\u5e76\u8003\u8651\u5b9e\u9645\u7a33\u5b9a\u6027\u7ea6\u675f\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u5ffd\u89c6\u5b9e\u9645\u7a33\u5b9a\u6027\u7ea6\u675f\uff0c\u4e14\u5728\u8de8\u4e0d\u540c\u7bb1\u5b50\u5c3a\u5bf8\u65f6\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u4f7f\u7528\u52a0\u6743\u5956\u52b1\u51fd\u6570\u7ed3\u5408\u88c5\u8f7d\u7387\u548c\u9ad8\u5ea6\u5dee\u6307\u6807\uff0c\u91c7\u7528\u88c1\u526a\u7b56\u7565\u68af\u5ea6\u4f18\u5316\u548c\u5b9a\u5236\u7b56\u7565\u6f02\u79fb\u65b9\u6cd5\u9632\u6b62\u7b56\u7565\u71b5\u5d29\u6e83\u3002", "result": "O4M-SP\u5728\u4e0d\u540c\u7bb1\u5b50\u5c3a\u5bf8\u4e0a\u6210\u529f\u6cdb\u5316\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6709\u6548\u5904\u7406\u5e26\u7a33\u5b9a\u6027\u7ea6\u675f\u7684\u88c5\u7bb1\u573a\u666f\u3002", "conclusion": "O4M-SP\u6846\u67b6\u57283D\u88c5\u7bb1\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5b9e\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u9002\u5408\u5b9e\u9645\u7269\u6d41\u548c\u4ed3\u50a8\u5e94\u7528\u3002"}}
{"id": "2510.10661", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10661", "abs": "https://arxiv.org/abs/2510.10661", "authors": ["Omid Reza Heidari", "Siobhan Reid", "Yassine Yaakoubi"], "title": "AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL Generation", "comment": "Accepted at NeurIPS 2025, ER \"Efficient Reasoning\" workshop", "summary": "LLMs have advanced text-to-SQL generation, yet monolithic architectures\nstruggle with complex reasoning and schema diversity. We propose AGENTIQL, an\nagent-inspired multi-expert framework that combines a reasoning agent for\nquestion decomposition, a coding agent for sub-query generation, and a\nrefinement step for column selection. An adaptive router further balances\nefficiency and accuracy by selecting between our modular pipeline and a\nbaseline parser. Several steps in the pipeline can be executed in parallel,\nmaking the framework scalable to larger workloads. Evaluated on the Spider\nbenchmark, AGENTIQL improves execution accuracy and interpretability and\nachieves up to 86.07\\% EX with 14B models using the Planner&Executor merging\nstrategy. The attained performance is contingent upon the efficacy of the\nrouting mechanism, thereby narrowing the gap to GPT-4-based SOTA (89.65% EX)\nwhile using much smaller open-source LLMs. Beyond accuracy, AGENTIQL enhances\ntransparency by exposing intermediate reasoning steps, offering a robust,\nscalable, and interpretable approach to semantic parsing.", "AI": {"tldr": "AGENTIQL\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u4e13\u5bb6\u4ee3\u7406\u7684\u6587\u672c\u5230SQL\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u95ee\u9898\u5206\u89e3\u3001\u5b50\u67e5\u8be2\u751f\u6210\u548c\u5217\u9009\u62e9\u4f18\u5316\u7b49\u6a21\u5757\u5316\u6b65\u9aa4\uff0c\u5728Spider\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523086.07%\u7684\u6267\u884c\u51c6\u786e\u7387\uff0c\u63a5\u8fd1GPT-4\u6c34\u5e73\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5355\u4f53\u67b6\u6784\u5728\u590d\u6742\u63a8\u7406\u548c\u591a\u6837\u5316\u6570\u636e\u5e93\u6a21\u5f0f\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u6587\u672c\u5230SQL\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u591a\u4e13\u5bb6\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u62ec\u63a8\u7406\u4ee3\u7406\u8fdb\u884c\u95ee\u9898\u5206\u89e3\u3001\u7f16\u7801\u4ee3\u7406\u751f\u6210\u5b50\u67e5\u8be2\u3001\u4f18\u5316\u6b65\u9aa4\u8fdb\u884c\u5217\u9009\u62e9\uff0c\u4ee5\u53ca\u81ea\u9002\u5e94\u8def\u7531\u5668\u5728\u6a21\u5757\u5316\u6d41\u6c34\u7ebf\u548c\u57fa\u7ebf\u89e3\u6790\u5668\u4e4b\u95f4\u8fdb\u884c\u9009\u62e9\u3002", "result": "\u5728Spider\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u752814B\u53c2\u6570\u6a21\u578b\u8fbe\u523086.07%\u7684\u6267\u884c\u51c6\u786e\u7387\uff0c\u63a5\u8fd1GPT-4\u768489.65%\u6c34\u5e73\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "AGENTIQL\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u8bed\u4e49\u89e3\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u5e76\u884c\u5904\u7406\u80fd\u529b\u7f29\u5c0f\u4e86\u4e0eGPT-4\u7b49\u5927\u578b\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u3002"}}
{"id": "2510.10060", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10060", "abs": "https://arxiv.org/abs/2510.10060", "authors": ["Hehe Fan", "Yi Yang", "Mohan Kankanhalli", "Fei Wu"], "title": "Translution: Unifying Self-attention and Convolution for Adaptive and Relative Modeling", "comment": "technical report", "summary": "When modeling a given type of data, we consider it to involve two key\naspects: 1) identifying relevant elements (e.g., image pixels or textual words)\nto a central element, as in a convolutional receptive field, or to a query\nelement, as in self-attention, and 2) encoding these tokens effectively.\nSelf-attention can adaptively identify these elements but relies on absolute\npositional embedding for structural representation learning. In contrast,\nconvolution encodes elements in a relative manner, yet their fixed kernel size\nlimits their ability to adaptively select the relevant elements. In this paper,\nwe introduce Translution, an operation that unifies the adaptive identification\ncapability of self-attention and the relative encoding advantage of\nconvolution. However, this integration leads to a substantial increase in the\nnumber of parameters, exceeding most currently available computational\nresources. Therefore, we propose a lightweight variant of Translution, named\n{\\alpha}-Translution. Experiments on computer vision and natural language\nprocessing tasks show that Translution (including {\\alpha}-Translution)\nachieves superior accuracy compared to self-attention. The code is available at\nhttps://github.com/hehefan/Translution.", "AI": {"tldr": "Translution\u662f\u4e00\u79cd\u7edf\u4e00\u81ea\u6ce8\u610f\u529b\u81ea\u9002\u5e94\u8bc6\u522b\u80fd\u529b\u548c\u5377\u79ef\u76f8\u5bf9\u7f16\u7801\u4f18\u52bf\u7684\u64cd\u4f5c\uff0c\u5176\u8f7b\u91cf\u7ea7\u53d8\u4f53\u03b1-Translution\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u6bd4\u81ea\u6ce8\u610f\u529b\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u81ea\u6ce8\u610f\u529b\u80fd\u81ea\u9002\u5e94\u8bc6\u522b\u76f8\u5173\u5143\u7d20\u4f46\u4f9d\u8d56\u7edd\u5bf9\u4f4d\u7f6e\u5d4c\u5165\u8fdb\u884c\u7ed3\u6784\u8868\u793a\u5b66\u4e60\uff0c\u5377\u79ef\u80fd\u4ee5\u76f8\u5bf9\u65b9\u5f0f\u7f16\u7801\u5143\u7d20\u4f46\u5176\u56fa\u5b9a\u6838\u5927\u5c0f\u9650\u5236\u4e86\u81ea\u9002\u5e94\u9009\u62e9\u76f8\u5173\u5143\u7d20\u7684\u80fd\u529b\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faTranslution\u64cd\u4f5c\uff0c\u7edf\u4e00\u81ea\u6ce8\u610f\u529b\u7684\u81ea\u9002\u5e94\u8bc6\u522b\u80fd\u529b\u548c\u5377\u79ef\u7684\u76f8\u5bf9\u7f16\u7801\u4f18\u52bf\u3002\u4e3a\u89e3\u51b3\u53c2\u6570\u8fc7\u591a\u95ee\u9898\uff0c\u8fdb\u4e00\u6b65\u63d0\u51fa\u8f7b\u91cf\u7ea7\u53d8\u4f53\u03b1-Translution\u3002", "result": "\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTranslution\uff08\u5305\u62ec\u03b1-Translution\uff09\u76f8\u6bd4\u81ea\u6ce8\u610f\u529b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002", "conclusion": "Translution\u6210\u529f\u7ed3\u5408\u4e86\u81ea\u6ce8\u610f\u529b\u548c\u5377\u79ef\u7684\u4f18\u52bf\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u8ba1\u7b97\u7684\u540c\u65f6\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.10666", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10666", "abs": "https://arxiv.org/abs/2510.10666", "authors": ["Zhengbo Zhang", "Zhiheng Lyu", "Junhao Gong", "Hongzhu Yi", "Xinming Wang", "Yuxuan Zhou", "Jiabing Yang", "Ping Nie", "Yan Huang", "Wenhu Chen"], "title": "BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions", "comment": "10 pages", "summary": "Efficiently solving real-world problems with LLMs increasingly hinges on\ntheir ability to interact with dynamic web environments and autonomously\nacquire external information. While recent research like Search-R1 and\nWebDancer demonstrates strong performance in solving web tasks, they heavily\nrely on additional tools to convert the interactive web environment into static\ntext content. This is in contrast to human browsing behaviors, which involve\ndiverse interactions with the browser, such as scrolling, clicking, and typing.\nIn this paper, we propose BrowserAgent, a more interactive agent that solves\ncomplex tasks through human-inspired browser actions. BrowserAgent operates\ndirectly on raw web pages via Playwright through a set of predefined browser\nactions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and\nRejection Fine-Tuning (RFT)) to improve the model's generalization abilities.\nDespite using significantly less training data than Search-R1, BrowserAgent\nachieves more competitive results across different Open-QA tasks. Additionally,\nwe introduce an explicit memory mechanism to store key conclusions across\nsteps, further enhancing the model's reasoning capabilities for long-horizon\ntasks. Notably, BrowserAgent-7B can achieve around 20\\% improvement over\nSearch-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These\nresults indicate that BrowserAgent can serve as a more advanced framework for\nmore interactive and scalable web agents.", "AI": {"tldr": "BrowserAgent\u662f\u4e00\u4e2a\u66f4\u4ea4\u4e92\u5f0f\u7684\u7f51\u7edc\u4ee3\u7406\uff0c\u901a\u8fc7\u4eba\u7c7b\u542f\u53d1\u7684\u6d4f\u89c8\u5668\u64cd\u4f5c\uff08\u5982\u6eda\u52a8\u3001\u70b9\u51fb\u3001\u8f93\u5165\uff09\u76f4\u63a5\u5728\u539f\u59cb\u7f51\u9875\u4e0a\u89e3\u51b3\u590d\u6742\u4efb\u52a1\uff0c\u76f8\u6bd4\u4f9d\u8d56\u5de5\u5177\u8f6c\u6362\u7f51\u9875\u4e3a\u9759\u6001\u6587\u672c\u7684\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982Search-R1\u548cWebDancer\u4f9d\u8d56\u989d\u5916\u5de5\u5177\u5c06\u4ea4\u4e92\u5f0f\u7f51\u7edc\u73af\u5883\u8f6c\u6362\u4e3a\u9759\u6001\u6587\u672c\u5185\u5bb9\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u6d4f\u89c8\u884c\u4e3a\u4e0d\u7b26\u3002\u9700\u8981\u5f00\u53d1\u66f4\u63a5\u8fd1\u4eba\u7c7b\u4ea4\u4e92\u65b9\u5f0f\u7684\u7f51\u7edc\u4ee3\u7406\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u76d1\u7763\u5fae\u8c03\u548c\u62d2\u7edd\u5fae\u8c03\uff09\uff0c\u901a\u8fc7Playwright\u9884\u5b9a\u4e49\u6d4f\u89c8\u5668\u64cd\u4f5c\u96c6\uff0c\u5e76\u5f15\u5165\u663e\u5f0f\u8bb0\u5fc6\u673a\u5236\u5b58\u50a8\u5173\u952e\u7ed3\u8bba\u4ee5\u589e\u5f3a\u957f\u65f6\u4efb\u52a1\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5c3d\u7ba1\u4f7f\u7528\u6bd4Search-R1\u5c11\u5f97\u591a\u7684\u8bad\u7ec3\u6570\u636e\uff0cBrowserAgent\u5728\u4e0d\u540c\u5f00\u653e\u95ee\u7b54\u4efb\u52a1\u4e2d\u53d6\u5f97\u66f4\u5177\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u5728HotpotQA\u30012Wiki\u548cBamboogle\u7b49\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e0a\u6bd4Search-R1\u63d0\u5347\u7ea620%\u3002", "conclusion": "BrowserAgent\u53ef\u4f5c\u4e3a\u66f4\u5148\u8fdb\u3001\u66f4\u4ea4\u4e92\u5f0f\u548c\u53ef\u6269\u5c55\u7684\u7f51\u7edc\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u7c7b\u542f\u53d1\u7684\u6d4f\u89c8\u5668\u64cd\u4f5c\u76f4\u63a5\u5904\u7406\u539f\u59cb\u7f51\u9875\u3002"}}
{"id": "2510.10071", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10071", "abs": "https://arxiv.org/abs/2510.10071", "authors": ["Jinyang Zhang", "Yue Fang", "Hongxin Ding", "Weibin Liao", "Muyang Ye", "Xu Chu", "Junfeng Zhao", "Yasha Wang"], "title": "ADEPT: Continual Pretraining via Adaptive Expansion and Dynamic Decoupled Tuning", "comment": null, "summary": "Conventional continual pretraining (CPT) for large language model (LLM)\ndomain adaptation often suffers from catastrophic forgetting and limited domain\ncapacity. Existing strategies adopt layer expansion, introducing additional\ntrainable parameters to accommodate new knowledge. However, the uniform\nexpansion and updates still entangle general and domain learning, undermining\nits effectiveness. Our pilot studies reveal that LLMs exhibit functional\nspecialization, where layers and units differentially encode general-critical\ncapabilities, suggesting that parameter expansion and optimization should be\nfunction-aware. We then propose ADEPT, Adaptive Expansion and Dynamic Decoupled\nTuning for continual pretraining, a two-stage framework for domain-adaptive\nCPT. ADEPT first performs General-Competence Guided Selective Layer Expansion,\nduplicating layers least critical for the general domain to increase\nrepresentational capacity while minimizing interference with general knowledge.\nIt then applies Adaptive Unit-Wise Decoupled Tuning, disentangling parameter\nunits within expanded layers according to their general-domain importance and\nassigning asymmetric learning rates to balance knowledge injection and\nretention. Experiments on mathematical and medical benchmarks show that ADEPT\noutperforms full-parameter CPT by up to 5.76% on the general domain and 5.58%\non the target domain with only 15% of parameters tuned and less than 50%\ntraining time. Ablation studies, theoretical analysis, and extended\ninvestigations further demonstrate the necessity of targeted expansion and\ndecoupled optimization, providing new principles for efficient and robust\ndomain-adaptive CPT. Our code is open-sourced at\nhttps://github.com/PuppyKnightUniversity/ADEPT", "AI": {"tldr": "ADEPT\u662f\u4e00\u4e2a\u7528\u4e8e\u6301\u7eed\u9884\u8bad\u7ec3\u7684\u81ea\u9002\u5e94\u6269\u5c55\u548c\u89e3\u8026\u8c03\u4f18\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5c42\u6269\u5c55\u548c\u81ea\u9002\u5e94\u5355\u5143\u89e3\u8026\u8c03\u4f18\uff0c\u5728\u6570\u5b66\u548c\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5168\u53c2\u6570CPT\uff0c\u4ec5\u970015%\u53c2\u6570\u8c03\u4f18\u548c\u4e0d\u523050%\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edf\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u548c\u6709\u9650\u9886\u57df\u5bb9\u91cf\u95ee\u9898\uff0c\u73b0\u6709\u5c42\u6269\u5c55\u7b56\u7565\u4ecd\u7ea0\u7f20\u901a\u7528\u548c\u9886\u57df\u5b66\u4e60\uff0c\u9700\u8981\u529f\u80fd\u611f\u77e5\u7684\u53c2\u6570\u6269\u5c55\u548c\u4f18\u5316\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u901a\u7528\u80fd\u529b\u5f15\u5bfc\u7684\u9009\u62e9\u6027\u5c42\u6269\u5c55\uff0c\u590d\u5236\u5bf9\u901a\u7528\u9886\u57df\u6700\u4e0d\u5173\u952e\u7684\u5c42\uff1b2) \u81ea\u9002\u5e94\u5355\u5143\u89e3\u8026\u8c03\u4f18\uff0c\u6839\u636e\u901a\u7528\u9886\u57df\u91cd\u8981\u6027\u89e3\u8026\u53c2\u6570\u5355\u5143\u5e76\u5206\u914d\u975e\u5bf9\u79f0\u5b66\u4e60\u7387\u3002", "result": "\u5728\u6570\u5b66\u548c\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cADEPT\u76f8\u6bd4\u5168\u53c2\u6570CPT\u5728\u901a\u7528\u9886\u57df\u63d0\u53475.76%\uff0c\u76ee\u6807\u9886\u57df\u63d0\u53475.58%\uff0c\u4ec5\u8c03\u4f1815%\u53c2\u6570\u4e14\u8bad\u7ec3\u65f6\u95f4\u4e0d\u523050%\u3002", "conclusion": "\u5b9a\u5411\u6269\u5c55\u548c\u89e3\u8026\u4f18\u5316\u662f\u9ad8\u6548\u7a33\u5065\u9886\u57df\u81ea\u9002\u5e94\u6301\u7eed\u9884\u8bad\u7ec3\u7684\u65b0\u539f\u5219\uff0c\u6d88\u878d\u7814\u7a76\u548c\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u5fc5\u8981\u6027\u3002"}}
{"id": "2510.10677", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10677", "abs": "https://arxiv.org/abs/2510.10677", "authors": ["Zhuowei Chen", "Bowei Zhang", "Nankai Lin", "Tian Hou", "Lianxi Wang"], "title": "Unlocking LLM Safeguards for Low-Resource Languages via Reasoning and Alignment with Minimal Training Data", "comment": "Accepted to MRL Workshop at EMNLP 2025", "summary": "Recent advances in LLMs have enhanced AI capabilities, but also increased the\nrisk posed by malicious requests, highlighting the need for effective LLM\nsafeguards to detect such queries. Existing approaches largely rely on\nclassifier-based methods that lack interpretability and perform poorly on\nlow-resource languages. To address these limitations, we propose\nConsistentGuard, a novel reasoning-based multilingual safeguard, which enhances\nexplainability via reasoning and boosts knowledge transfer between languages\nthrough alignment. With only 1,000 training samples, our method demonstrates\nsuperior performance on three datasets across six languages, outperforming\nlarger models trained with significantly more data, and exhibits strong\ninterpretability and generalization ability. We also contribute a multilingual\nbenchmark extension and release our codes to support future research.", "AI": {"tldr": "\u63d0\u51faConsistentGuard\uff0c\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406\u7684\u591a\u8bed\u8a00\u4fdd\u62a4\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a8\u7406\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u5bf9\u9f50\u4fc3\u8fdb\u8bed\u8a00\u95f4\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4ec5\u97001000\u4e2a\u8bad\u7ec3\u6837\u672c\u5373\u53ef\u57286\u79cd\u8bed\u8a00\u76843\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709LLM\u4fdd\u62a4\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u57fa\u4e8e\u5206\u7c7b\u5668\u7684\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a8\u7406\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u5bf9\u9f50\u4fc3\u8fdb\u8bed\u8a00\u95f4\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6837\u672c\u3002", "result": "\u4ec5\u75281000\u4e2a\u8bad\u7ec3\u6837\u672c\uff0c\u57286\u79cd\u8bed\u8a00\u76843\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u4f7f\u7528\u66f4\u591a\u6570\u636e\u8bad\u7ec3\u7684\u5927\u578b\u6a21\u578b\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ConsistentGuard\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709LLM\u4fdd\u62a4\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u591a\u8bed\u8a00\u57fa\u51c6\u6269\u5c55\u5e76\u5f00\u6e90\u4ee3\u7801\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2510.10075", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10075", "abs": "https://arxiv.org/abs/2510.10075", "authors": ["Salomon Ibarra", "Frida Cantu", "Kaixiong Zhou", "Li Zhang"], "title": "Gradient-based Model Shortcut Detection for Time Series Classification", "comment": "Code available at: https://github.com/IvorySnake02/SAG.git", "summary": "Deep learning models have attracted lots of research attention in time series\nclassification (TSC) task in the past two decades. Recently, deep neural\nnetworks (DNN) have surpassed classical distance-based methods and achieved\nstate-of-the-art performance. Despite their promising performance, deep neural\nnetworks (DNNs) have been shown to rely on spurious correlations present in the\ntraining data, which can hinder generalization. For instance, a model might\nincorrectly associate the presence of grass with the label ``cat\" if the\ntraining set have majority of cats lying in grassy backgrounds. However, the\nshortcut behavior of DNNs in time series remain under-explored. Most existing\nshortcut work are relying on external attributes such as gender, patients\ngroup, instead of focus on the internal bias behavior in time series models.\n  In this paper, we take the first step to investigate and establish\npoint-based shortcut learning behavior in deep learning time series\nclassification. We further propose a simple detection method based on other\nclass to detect shortcut occurs without relying on test data or clean training\nclasses. We test our proposed method in UCR time series datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7814\u7a76\u6df1\u5ea6\u5b66\u4e60\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u7684\u57fa\u4e8e\u70b9\u7684\u6377\u5f84\u5b66\u4e60\u884c\u4e3a\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56\u6d4b\u8bd5\u6570\u636e\u6216\u5e72\u51c0\u8bad\u7ec3\u7c7b\u522b\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u53ef\u80fd\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u865a\u5047\u76f8\u5173\u6027\uff0c\u8fd9\u79cd\u6377\u5f84\u884c\u4e3a\u5728\u65f6\u95f4\u5e8f\u5217\u9886\u57df\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5916\u90e8\u5c5e\u6027\u800c\u975e\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u5185\u90e8\u504f\u5dee\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5176\u4ed6\u7c7b\u522b\u7684\u7b80\u5355\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u6377\u5f84\u5b66\u4e60\u884c\u4e3a\uff0c\u8be5\u65b9\u6cd5\u4e0d\u9700\u8981\u6d4b\u8bd5\u6570\u636e\u6216\u5e72\u51c0\u7684\u8bad\u7ec3\u7c7b\u522b\u3002", "result": "\u5728UCR\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u7684\u6377\u5f84\u5b66\u4e60\u884c\u4e3a\u63d0\u4f9b\u4e86\u521d\u6b65\u63a2\u7d22\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2510.10681", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10681", "abs": "https://arxiv.org/abs/2510.10681", "authors": ["Zichun Yu", "Chenyan Xiong"], "title": "RePro: Training Language Models to Faithfully Recycle the Web for Pretraining", "comment": null, "summary": "High-quality pretraining data is the fossil fuel of large language models\n(LLMs), yet its reserves are running low for frontier models. In this paper, we\nintroduce RePro, a novel web recycling method that trains a relatively small LM\nwith reinforcement learning to generate effective and faithful rephrasings of\npretraining data. Specifically, we design one quality reward and three\nfaithfulness rewards, optimizing the LM rephraser to convert organic data into\nhigh-quality rephrasings while maintaining its core semantics and structure. In\nour experiment, we train a 4B rephraser to recycle 72B tokens sampled from\nDCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that\nRePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on\n22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web\nrecycling method that prompts a 70B rephraser, as well as the organic baseline\nwith a 4x larger data pool. Experiments with different amounts of recycled data\nhighlight that RePro improves organic data efficiency by 2-3x. Individual and\ndistributional analyses validate that RePro preserves more critical information\nand faithfully reflects the characteristics of organic data compared to\nprompting-based methods. Together, these results show that RePro provides an\nefficient and controllable path to effectively harness the fossil fuel of LLM\npretraining. We open-source our code, rephraser, and recycled data at\nhttps://github.com/cxcscmu/RePro.", "AI": {"tldr": "RePro\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7f51\u9875\u6570\u636e\u56de\u6536\u65b9\u6cd5\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u5fe0\u5b9e\u4e8e\u539f\u610f\u7684\u91cd\u8ff0\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u9884\u8bad\u7ec3\u6570\u636e\u6548\u7387\u3002", "motivation": "\u9ad8\u8d28\u91cf\u9884\u8bad\u7ec3\u6570\u636e\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\"\u5316\u77f3\u71c3\u6599\"\uff0c\u4f46\u524d\u6cbf\u6a21\u578b\u7684\u53ef\u7528\u6570\u636e\u50a8\u5907\u6b63\u5728\u67af\u7aed\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u6570\u636e\u56de\u6536\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec34B\u53c2\u6570\u7684\u91cd\u8ff0\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e00\u4e2a\u8d28\u91cf\u5956\u52b1\u548c\u4e09\u4e2a\u5fe0\u5b9e\u5ea6\u5956\u52b1\uff0c\u5c06\u539f\u59cb\u6570\u636e\u8f6c\u6362\u4e3a\u9ad8\u8d28\u91cf\u91cd\u8ff0\u7248\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6838\u5fc3\u8bed\u4e49\u548c\u7ed3\u6784\u3002", "result": "\u5728400M\u548c1.4B\u6a21\u578b\u4e0a\uff0cRePro\u76f8\u6bd4\u4ec5\u4f7f\u7528\u539f\u59cb\u6570\u636e\u7684\u57fa\u7ebf\uff0c\u572822\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u5e26\u67654.7%-14.0%\u7684\u76f8\u5bf9\u51c6\u786e\u7387\u63d0\u5347\uff0c\u6570\u636e\u6548\u7387\u63d0\u9ad82-3\u500d\u3002", "conclusion": "RePro\u63d0\u4f9b\u4e86\u4e00\u6761\u9ad8\u6548\u53ef\u63a7\u7684\u8def\u5f84\uff0c\u6709\u6548\u5229\u7528LLM\u9884\u8bad\u7ec3\u7684\"\u5316\u77f3\u71c3\u6599\"\uff0c\u5728\u4fdd\u6301\u6570\u636e\u5fe0\u5b9e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.10729", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10729", "abs": "https://arxiv.org/abs/2510.10729", "authors": ["Manas Zambre", "Sarika Bobade"], "title": "Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning Framework", "comment": "4 pages, 5 figures", "summary": "Sarcasm is a nuanced and often misinterpreted form of communication,\nespecially in text, where tone and body language are absent. This paper\nproposes a modular deep learning framework for sarcasm detection, leveraging\nDeep Convolutional Neural Networks (DCNNs) and contextual models such as BERT\nto analyze linguistic, emotional, and contextual cues. The system integrates\nsentiment analysis, contextual embeddings, linguistic feature extraction, and\nemotion detection through a multi-layer architecture. While the model is in the\nconceptual stage, it demonstrates feasibility for real-world applications such\nas chatbots and social media analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548cBERT\u7684\u6a21\u5757\u5316\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6587\u672c\u53cd\u8bbd\u68c0\u6d4b\uff0c\u6574\u5408\u4e86\u60c5\u611f\u5206\u6790\u3001\u4e0a\u4e0b\u6587\u5d4c\u5165\u3001\u8bed\u8a00\u7279\u5f81\u63d0\u53d6\u548c\u60c5\u611f\u68c0\u6d4b\u3002", "motivation": "\u53cd\u8bbd\u662f\u4e00\u79cd\u5fae\u5999\u4e14\u5e38\u88ab\u8bef\u89e3\u7684\u4ea4\u6d41\u5f62\u5f0f\uff0c\u7279\u522b\u662f\u5728\u7f3a\u4e4f\u8bed\u8c03\u548c\u80a2\u4f53\u8bed\u8a00\u7684\u6587\u672c\u4e2d\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548cBERT\u7b49\u4e0a\u4e0b\u6587\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u5c42\u67b6\u6784\u6574\u5408\u60c5\u611f\u5206\u6790\u3001\u4e0a\u4e0b\u6587\u5d4c\u5165\u3001\u8bed\u8a00\u7279\u5f81\u63d0\u53d6\u548c\u60c5\u611f\u68c0\u6d4b\u3002", "result": "\u867d\u7136\u6a21\u578b\u4ecd\u5904\u4e8e\u6982\u5ff5\u9636\u6bb5\uff0c\u4f46\u8bc1\u660e\u4e86\u5728\u804a\u5929\u673a\u5668\u4eba\u548c\u793e\u4ea4\u5a92\u4f53\u5206\u6790\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u6a21\u5757\u5316\u6846\u67b6\u5c55\u793a\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u68c0\u6d4b\u6587\u672c\u53cd\u8bbd\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.10101", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10101", "abs": "https://arxiv.org/abs/2510.10101", "authors": ["Martin Carrasco", "Caio Deberaldini Netto", "Vahan A. Martirosyan", "Aneeqa Mehrab", "Ehimare Okoyomon", "Caterina Graziani"], "title": "Rademacher Meets Colors: More Expressivity, but at What Cost ?", "comment": null, "summary": "The expressive power of graph neural networks (GNNs) is typically understood\nthrough their correspondence with graph isomorphism tests such as the\nWeisfeiler-Leman (WL) hierarchy. While more expressive GNNs can distinguish a\nricher set of graphs, they are also observed to suffer from higher\ngeneralization error. This work provides a theoretical explanation for this\ntrade-off by linking expressivity and generalization through the lens of\ncoloring algorithms. Specifically, we show that the number of equivalence\nclasses induced by WL colorings directly bounds the GNNs Rademacher complexity\n-- a key data-dependent measure of generalization. Our analysis reveals that\ngreater expressivity leads to higher complexity and thus weaker generalization\nguarantees. Furthermore, we prove that the Rademacher complexity is stable\nunder perturbations in the color counts across different samples, ensuring\nrobustness to sampling variability across datasets. Importantly, our framework\nis not restricted to message-passing GNNs or 1-WL, but extends to arbitrary GNN\narchitectures and expressivity measures that partition graphs into equivalence\nclasses. These results unify the study of expressivity and generalization in\nGNNs, providing a principled understanding of why increasing expressive power\noften comes at the cost of generalization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7740\u8272\u7b97\u6cd5\u5c06\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\u8054\u7cfb\u8d77\u6765\uff0c\u8bc1\u660e\u66f4\u5f3a\u7684\u8868\u8fbe\u80fd\u529b\u4f1a\u5bfc\u81f4\u66f4\u9ad8\u7684Rademacher\u590d\u6742\u5ea6\uff0c\u4ece\u800c\u524a\u5f31\u6cdb\u5316\u4fdd\u8bc1\u3002", "motivation": "\u89e3\u91ca\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u8868\u8fbe\u80fd\u529b\u4e0e\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u73b0\u8c61\uff1a\u867d\u7136\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\u7684GNN\u80fd\u533a\u5206\u66f4\u591a\u56fe\u7ed3\u6784\uff0c\u4f46\u901a\u5e38\u6cdb\u5316\u8bef\u5dee\u66f4\u9ad8\u3002", "method": "\u901a\u8fc7WL\u7740\u8272\u7b97\u6cd5\uff0c\u5206\u6790\u7740\u8272\u8bf1\u5bfc\u7684\u7b49\u4ef7\u7c7b\u6570\u91cf\u5982\u4f55\u9650\u5236GNN\u7684Rademacher\u590d\u6742\u5ea6\uff0c\u5efa\u7acb\u8868\u8fbe\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\u3002", "result": "\u8bc1\u660e\u8868\u8fbe\u80fd\u529b\u8d8a\u5f3a\uff0cRademacher\u590d\u6742\u5ea6\u8d8a\u9ad8\uff0c\u6cdb\u5316\u4fdd\u8bc1\u8d8a\u5f31\uff1b\u540c\u65f6\u8bc1\u660eRademacher\u590d\u6742\u5ea6\u5728\u4e0d\u540c\u6837\u672c\u7684\u7740\u8272\u8ba1\u6570\u6270\u52a8\u4e0b\u662f\u7a33\u5b9a\u7684\u3002", "conclusion": "\u8be5\u6846\u67b6\u7edf\u4e00\u4e86GNN\u8868\u8fbe\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\u7684\u7814\u7a76\uff0c\u4e3a\u7406\u89e3\u8868\u8fbe\u80fd\u529b\u589e\u5f3a\u5f80\u5f80\u4ee5\u6cdb\u5316\u80fd\u529b\u4e3a\u4ee3\u4ef7\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2510.10762", "categories": ["cs.CL", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.10762", "abs": "https://arxiv.org/abs/2510.10762", "authors": ["Wenqing Zhang", "Trang Nguyen", "Elizabeth A. Stuart", "Yiqun T. Chen"], "title": "Large Language Models for Full-Text Methods Assessment: A Case Study on Mediation Analysis", "comment": null, "summary": "Systematic reviews are crucial for synthesizing scientific evidence but\nremain labor-intensive, especially when extracting detailed methodological\ninformation. Large language models (LLMs) offer potential for automating\nmethodological assessments, promising to transform evidence synthesis. Here,\nusing causal mediation analysis as a representative methodological domain, we\nbenchmarked state-of-the-art LLMs against expert human reviewers across 180\nfull-text scientific articles. Model performance closely correlated with human\njudgments (accuracy correlation 0.71; F1 correlation 0.97), achieving\nnear-human accuracy on straightforward, explicitly stated methodological\ncriteria. However, accuracy sharply declined on complex, inference-intensive\nassessments, lagging expert reviewers by up to 15%. Errors commonly resulted\nfrom superficial linguistic cues -- for instance, models frequently\nmisinterpreted keywords like \"longitudinal\" or \"sensitivity\" as automatic\nevidence of rigorous methodological approache, leading to systematic\nmisclassifications. Longer documents yielded lower model accuracy, whereas\npublication year showed no significant effect. Our findings highlight an\nimportant pattern for practitioners using LLMs for methods review and synthesis\nfrom full texts: current LLMs excel at identifying explicit methodological\nfeatures but require human oversight for nuanced interpretations. Integrating\nautomated information extraction with targeted expert review thus provides a\npromising approach to enhance efficiency and methodological rigor in evidence\nsynthesis across diverse scientific fields.", "AI": {"tldr": "LLMs\u5728\u65b9\u6cd5\u5b66\u8bc4\u4f30\u4e2d\u8868\u73b0\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\uff0c\u4f46\u590d\u6742\u63a8\u7406\u4efb\u52a1\u51c6\u786e\u7387\u4e0b\u964d15%\uff0c\u9700\u8981\u4eba\u7c7b\u76d1\u7763\u7ed3\u5408\u81ea\u52a8\u5316\u63d0\u53d6\u6765\u63d0\u9ad8\u8bc1\u636e\u5408\u6210\u6548\u7387\u3002", "motivation": "\u7cfb\u7edf\u8bc4\u4ef7\u5de5\u4f5c\u91cf\u5927\uff0c\u7279\u522b\u662f\u63d0\u53d6\u65b9\u6cd5\u5b66\u4fe1\u606f\u3002LLMs\u6709\u6f5c\u529b\u81ea\u52a8\u5316\u65b9\u6cd5\u5b66\u8bc4\u4f30\uff0c\u6539\u53d8\u8bc1\u636e\u5408\u6210\u65b9\u5f0f\u3002", "method": "\u4ee5\u56e0\u679c\u4e2d\u4ecb\u5206\u6790\u4e3a\u4ee3\u8868\u9886\u57df\uff0c\u5728180\u7bc7\u5168\u6587\u79d1\u5b66\u6587\u7ae0\u4e2d\uff0c\u5c06\u6700\u5148\u8fdbLLMs\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u5ba1\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6a21\u578b\u6027\u80fd\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u76f8\u5173\uff08\u51c6\u786e\u7387\u76f8\u5173\u60270.71\uff1bF1\u76f8\u5173\u60270.97\uff09\uff0c\u5728\u660e\u786e\u9648\u8ff0\u7684\u65b9\u6cd5\u5b66\u6807\u51c6\u4e0a\u8fbe\u5230\u63a5\u8fd1\u4eba\u7c7b\u51c6\u786e\u7387\uff0c\u4f46\u5728\u590d\u6742\u63a8\u7406\u8bc4\u4f30\u4e2d\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d15%\u3002", "conclusion": "\u5f53\u524dLLMs\u64c5\u957f\u8bc6\u522b\u660e\u786e\u65b9\u6cd5\u5b66\u7279\u5f81\uff0c\u4f46\u9700\u8981\u4eba\u7c7b\u76d1\u7763\u8fdb\u884c\u7ec6\u81f4\u89e3\u91ca\u3002\u81ea\u52a8\u5316\u4fe1\u606f\u63d0\u53d6\u4e0e\u4e13\u5bb6\u8bc4\u5ba1\u7ed3\u5408\u662f\u63d0\u9ad8\u8bc1\u636e\u5408\u6210\u6548\u7387\u548c\u4e25\u8c28\u6027\u7684\u6709\u524d\u666f\u65b9\u6cd5\u3002"}}
{"id": "2510.10102", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10102", "abs": "https://arxiv.org/abs/2510.10102", "authors": ["Guilin Li", "Yun Zhang", "Xiuyuan Chen", "Chengqi Li", "Bo Wang", "Linghe Kong", "Wenjia Wang", "Weiran Huang", "Matthias Hwai Yong Tan"], "title": "PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling", "comment": null, "summary": "Large language models (LLMs) have shown that generative pretraining can\ndistill vast world knowledge into compact token representations. While LLMs\nencapsulate extensive world knowledge, they remain limited in modeling the\nbehavioral knowledge contained within user interaction histories. User behavior\nforms a distinct modality, where each action, defined by multi-dimensional\nattributes such as time, context, and transaction type, constitutes a\nbehavioral token. Modeling these high-cardinality sequences is challenging, and\ndiscriminative models often falter under limited supervision. To bridge this\ngap, we extend generative pretraining to user behavior, learning transferable\nrepresentations from unlabeled behavioral data analogous to how LLMs learn from\ntext. We present PANTHER, a hybrid generative-discriminative framework that\nunifies user behavior pretraining and downstream adaptation, enabling\nlarge-scale sequential user representation learning and real-time inference.\nPANTHER introduces: (1) Structured Tokenization to compress multi-dimensional\ntransaction attributes into an interpretable vocabulary; (2) Sequence Pattern\nRecognition Module (SPRM) for modeling periodic transaction motifs; (3) a\nUnified User-Profile Embedding that fuses static demographics with dynamic\ntransaction histories; and (4) Real-time scalability enabled by offline caching\nof pretrained embeddings for millisecond-level inference. Fully deployed and\noperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in\nnext-transaction prediction HitRate@1 and a 38.6 percent relative improvement\nin fraud detection recall over baselines. Cross-domain evaluations on public\nbenchmarks show strong generalization, achieving up to 21 percent HitRate@1\ngains over transformer baselines, establishing PANTHER as a scalable,\nhigh-performance framework for industrial sequential user behavior modeling.", "AI": {"tldr": "PANTHER\u662f\u4e00\u4e2a\u6df7\u5408\u751f\u6210-\u5224\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u5b66\u4e60\u7528\u6237\u884c\u4e3a\u8868\u793a\uff0c\u5728\u5fae\u4fe1\u652f\u4ed8\u4e2d\u5b9e\u73b0\u4e8625.6%\u7684\u4e0b\u4e00\u6b21\u4ea4\u6613\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u5347\u548c38.6%\u7684\u6b3a\u8bc8\u68c0\u6d4b\u53ec\u56de\u7387\u76f8\u5bf9\u6539\u8fdb\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u5c01\u88c5\u5e7f\u6cdb\u7684\u4e16\u754c\u77e5\u8bc6\uff0c\u4f46\u5728\u5efa\u6a21\u7528\u6237\u4ea4\u4e92\u5386\u53f2\u4e2d\u7684\u884c\u4e3a\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002\u7528\u6237\u884c\u4e3a\u6784\u6210\u72ec\u7279\u6a21\u6001\uff0c\u6bcf\u4e2a\u52a8\u4f5c\u5305\u542b\u65f6\u95f4\u3001\u4e0a\u4e0b\u6587\u7b49\u591a\u7ef4\u5c5e\u6027\uff0c\u5efa\u6a21\u8fd9\u4e9b\u9ad8\u57fa\u6570\u5e8f\u5217\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faPANTHER\u6846\u67b6\uff0c\u5305\u542b\uff1a\u7ed3\u6784\u5316\u6807\u8bb0\u5316\u538b\u7f29\u591a\u7ef4\u4ea4\u6613\u5c5e\u6027\uff1b\u5e8f\u5217\u6a21\u5f0f\u8bc6\u522b\u6a21\u5757\u5efa\u6a21\u5468\u671f\u6027\u4ea4\u6613\u6a21\u5f0f\uff1b\u7edf\u4e00\u7528\u6237\u753b\u50cf\u5d4c\u5165\u878d\u5408\u9759\u6001\u4eba\u53e3\u7edf\u8ba1\u548c\u52a8\u6001\u4ea4\u6613\u5386\u53f2\uff1b\u901a\u8fc7\u79bb\u7ebf\u7f13\u5b58\u9884\u8bad\u7ec3\u5d4c\u5165\u5b9e\u73b0\u5b9e\u65f6\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5728\u5fae\u4fe1\u652f\u4ed8\u4e2d\u90e8\u7f72\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e0b\u4e00\u6b21\u4ea4\u6613\u9884\u6d4bHitRate@1\u63d0\u534725.6%\uff0c\u6b3a\u8bc8\u68c0\u6d4b\u53ec\u56de\u7387\u76f8\u5bf9\u6539\u8fdb38.6%\u3002\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4transformer\u57fa\u7ebf\u5b9e\u73b0\u9ad8\u8fbe21%\u7684HitRate@1\u589e\u76ca\u3002", "conclusion": "PANTHER\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u9ad8\u6027\u80fd\u7684\u5de5\u4e1a\u7ea7\u5e8f\u5217\u7528\u6237\u884c\u4e3a\u5efa\u6a21\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u5728\u7528\u6237\u884c\u4e3a\u5efa\u6a21\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.10776", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10776", "abs": "https://arxiv.org/abs/2510.10776", "authors": ["James Ald Teves", "Ray Daniel Cal", "Josh Magdiel Villaluz", "Jean Malolos", "Mico Magtira", "Ramon Rodriguez", "Mideth Abisado", "Joseph Marvin Imperial"], "title": "HiligayNER: A Baseline Named Entity Recognition Model for Hiligaynon", "comment": "Camera-ready for PACLIC 2025 (ACL Proceedings)", "summary": "The language of Hiligaynon, spoken predominantly by the people of Panay\nIsland, Negros Occidental, and Soccsksargen in the Philippines, remains\nunderrepresented in language processing research due to the absence of\nannotated corpora and baseline models. This study introduces HiligayNER, the\nfirst publicly available baseline model for the task of Named Entity\nRecognition (NER) in Hiligaynon. The dataset used to build HiligayNER contains\nover 8,000 annotated sentences collected from publicly available news articles,\nsocial media posts, and literary texts. Two Transformer-based models, mBERT and\nXLM-RoBERTa, were fine-tuned on this collected corpus to build versions of\nHiligayNER. Evaluation results show strong performance, with both models\nachieving over 80% in precision, recall, and F1-score across entity types.\nFurthermore, cross-lingual evaluation with Cebuano and Tagalog demonstrates\npromising transferability, suggesting the broader applicability of HiligayNER\nfor multilingual NLP in low-resource settings. This work aims to contribute to\nlanguage technology development for underrepresented Philippine languages,\nspecifically for Hiligaynon, and support future research in regional language\nprocessing.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86HiligayNER\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u5e0c\u5229\u76d6\u519c\u8bed\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u516c\u5f00\u57fa\u7ebf\u6a21\u578b\uff0c\u57fa\u4e8e\u8d85\u8fc78000\u4e2a\u6807\u6ce8\u53e5\u5b50\u6784\u5efa\uff0c\u4f7f\u7528mBERT\u548cXLM-RoBERTa\u6a21\u578b\u5fae\u8c03\uff0c\u5728\u5404\u9879\u6307\u6807\u4e0a\u8fbe\u523080%\u4ee5\u4e0a\u6027\u80fd\u3002", "motivation": "\u5e0c\u5229\u76d6\u519c\u8bed\u4f5c\u4e3a\u83f2\u5f8b\u5bbe\u7684\u4e00\u79cd\u4ee3\u8868\u6027\u8bed\u8a00\uff0c\u5728\u8bed\u8a00\u5904\u7406\u7814\u7a76\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u6807\u6ce8\u8bed\u6599\u5e93\u548c\u57fa\u7ebf\u6a21\u578b\u3002", "method": "\u4ece\u516c\u5f00\u65b0\u95fb\u6587\u7ae0\u3001\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u548c\u6587\u5b66\u6587\u672c\u6536\u96c6\u8d85\u8fc78000\u4e2a\u6807\u6ce8\u53e5\u5b50\uff0c\u4f7f\u7528mBERT\u548cXLM-RoBERTa\u4e24\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u4e24\u4e2a\u6a21\u578b\u5728\u6240\u6709\u5b9e\u4f53\u7c7b\u578b\u4e0a\u7684\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u5747\u8d85\u8fc780%\uff0c\u4e0e\u5bbf\u52a1\u8bed\u548c\u4ed6\u52a0\u7984\u8bed\u7684\u8de8\u8bed\u8a00\u8bc4\u4f30\u663e\u793a\u51fa\u826f\u597d\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u83f2\u5f8b\u5bbe\u8bed\u8a00\uff08\u7279\u522b\u662f\u5e0c\u5229\u76d6\u519c\u8bed\uff09\u7684\u8bed\u8a00\u6280\u672f\u53d1\u5c55\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u652f\u6301\u4e86\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u591a\u8bed\u8a00NLP\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.10105", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10105", "abs": "https://arxiv.org/abs/2510.10105", "authors": ["Yanping Zheng", "Zhewei Wei", "Frank de Hoog", "Xu Chen", "Hongteng Xu", "Yuhang Ye", "Jiadeng Huang"], "title": "Lighter-X: An Efficient and Plug-and-play Strategy for Graph-based Recommendation through Decoupled Propagation", "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness in\nrecommendation systems. However, conventional graph-based recommenders, such as\nLightGCN, require maintaining embeddings of size $d$ for each node, resulting\nin a parameter complexity of $\\mathcal{O}(n \\times d)$, where $n$ represents\nthe total number of users and items. This scaling pattern poses significant\nchallenges for deployment on large-scale graphs encountered in real-world\napplications. To address this scalability limitation, we propose\n\\textbf{Lighter-X}, an efficient and modular framework that can be seamlessly\nintegrated with existing GNN-based recommender architectures. Our approach\nsubstantially reduces both parameter size and computational complexity while\npreserving the theoretical guarantees and empirical performance of the base\nmodels, thereby enabling practical deployment at scale. Specifically, we\nanalyze the original structure and inherent redundancy in their parameters,\nidentifying opportunities for optimization. Based on this insight, we propose\nan efficient compression scheme for the sparse adjacency structure and\nhigh-dimensional embedding matrices, achieving a parameter complexity of\n$\\mathcal{O}(h \\times d)$, where $h \\ll n$. Furthermore, the model is optimized\nthrough a decoupled framework, reducing computational complexity during the\ntraining process and enhancing scalability. Extensive experiments demonstrate\nthat Lighter-X achieves comparable performance to baseline models with\nsignificantly fewer parameters. In particular, on large-scale interaction\ngraphs with millions of edges, we are able to attain even better results with\nonly 1\\% of the parameter over LightGCN.", "AI": {"tldr": "Lighter-X\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u538b\u7f29\u7a00\u758f\u90bb\u63a5\u7ed3\u6784\u548c\u9ad8\u7ef4\u5d4c\u5165\u77e9\u9635\uff0c\u5c06\u53c2\u6570\u590d\u6742\u5ea6\u4eceO(n\u00d7d)\u964d\u4f4e\u5230O(h\u00d7d)\uff0c\u663e\u8457\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u56fe\u7684\u63a8\u8350\u7cfb\u7edf\uff08\u5982LightGCN\uff09\u9700\u8981\u4e3a\u6bcf\u4e2a\u8282\u70b9\u7ef4\u62a4\u5927\u5c0f\u4e3ad\u7684\u5d4c\u5165\uff0c\u5bfc\u81f4\u53c2\u6570\u590d\u6742\u5ea6\u4e3aO(n\u00d7d)\uff0c\u5728\u5927\u89c4\u6a21\u56fe\u5e94\u7528\u4e2d\u9762\u4e34\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002", "method": "\u5206\u6790\u539f\u59cb\u7ed3\u6784\u548c\u53c2\u6570\u5197\u4f59\uff0c\u63d0\u51fa\u7a00\u758f\u90bb\u63a5\u7ed3\u6784\u548c\u9ad8\u7ef4\u5d4c\u5165\u77e9\u9635\u7684\u9ad8\u6548\u538b\u7f29\u65b9\u6848\uff0c\u91c7\u7528\u89e3\u8026\u6846\u67b6\u4f18\u5316\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728\u5927\u89c4\u6a21\u4ea4\u4e92\u56fe\u4e0a\uff0c\u4ec5\u4f7f\u7528LightGCN 1%\u7684\u53c2\u6570\u5c31\u80fd\u8fbe\u5230\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "Lighter-X\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u6a21\u5757\u5316\u7684\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u53c2\u6570\u5927\u5c0f\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8bc1\u6027\u80fd\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2510.10787", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10787", "abs": "https://arxiv.org/abs/2510.10787", "authors": ["Zhichao Wang", "Cheng Wan", "Dong Nie"], "title": "Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG", "comment": null, "summary": "The performance gains of LLMs have historically been driven by scaling up\nmodel size and training data. However, the rapidly diminishing availability of\nhigh-quality training data is introducing a fundamental bottleneck, shifting\nthe focus of research toward inference-time scaling. This paradigm uses\nadditional computation at the time of deployment to substantially improve LLM\nperformance on downstream tasks without costly model re-training. This review\nsystematically surveys the diverse techniques contributing to this new era of\ninference-time scaling, organizing the rapidly evolving field into two\ncomprehensive perspectives: Output-focused and Input-focused methods.\nOutput-focused techniques encompass complex, multi-step generation strategies,\nincluding reasoning (e.g., CoT, ToT, ReAct), various search and decoding\nmethods (e.g., MCTS, beam search), training for long CoT (e.g., RLVR, GRPO),\nand model ensemble methods. Input-focused techniques are primarily categorized\nby few-shot and RAG, with RAG as the central focus. The RAG section is further\ndetailed through a structured examination of query expansion, data, retrieval\nand reranker, LLM generation methods, and multi-modal RAG.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u7cfb\u7edf\u6027\u5730\u8c03\u67e5\u4e86\u63a8\u7406\u65f6\u6269\u5c55\u6280\u672f\uff0c\u5c06\u8fd9\u4e00\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\u7ec4\u7ec7\u4e3a\u8f93\u51fa\u5bfc\u5411\u548c\u8f93\u5165\u5bfc\u5411\u4e24\u5927\u89c6\u89d2\uff0c\u8be6\u7ec6\u5206\u6790\u4e86\u5404\u79cd\u65b9\u6cd5\u53ca\u5176\u5e94\u7528\u3002", "motivation": "\u968f\u7740\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u5feb\u901f\u51cf\u5c11\uff0c\u7814\u7a76\u91cd\u70b9\u4ece\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u8f6c\u5411\u63a8\u7406\u65f6\u6269\u5c55\uff0c\u901a\u8fc7\u5728\u90e8\u7f72\u65f6\u4f7f\u7528\u989d\u5916\u8ba1\u7b97\u6765\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\uff0c\u800c\u65e0\u9700\u6602\u8d35\u7684\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u5c06\u63a8\u7406\u65f6\u6269\u5c55\u6280\u672f\u5206\u4e3a\u8f93\u51fa\u5bfc\u5411\u65b9\u6cd5\uff08\u5305\u62ec\u590d\u6742\u591a\u6b65\u751f\u6210\u7b56\u7565\u3001\u63a8\u7406\u65b9\u6cd5\u3001\u641c\u7d22\u89e3\u7801\u65b9\u6cd5\u3001\u957fCoT\u8bad\u7ec3\u548c\u6a21\u578b\u96c6\u6210\uff09\u548c\u8f93\u5165\u5bfc\u5411\u65b9\u6cd5\uff08\u4e3b\u8981\u5173\u6ce8\u5c11\u6837\u672c\u5b66\u4e60\u548cRAG\uff0c\u5176\u4e2dRAG\u90e8\u5206\u8be6\u7ec6\u5206\u6790\u4e86\u67e5\u8be2\u6269\u5c55\u3001\u6570\u636e\u3001\u68c0\u7d22\u4e0e\u91cd\u6392\u5e8f\u3001LLM\u751f\u6210\u65b9\u6cd5\u548c\u591a\u6a21\u6001RAG\uff09\u3002", "result": "\u7cfb\u7edf\u6027\u5730\u7ec4\u7ec7\u4e86\u63a8\u7406\u65f6\u6269\u5c55\u7684\u591a\u6837\u5316\u6280\u672f\uff0c\u4e3a\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5206\u7c7b\u6846\u67b6\u548c\u5206\u6790\u89c6\u89d2\u3002", "conclusion": "\u63a8\u7406\u65f6\u6269\u5c55\u4ee3\u8868\u4e86LLM\u53d1\u5c55\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u90e8\u7f72\u65f6\u7684\u989d\u5916\u8ba1\u7b97\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4e3aLLM\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.10116", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.10116", "abs": "https://arxiv.org/abs/2510.10116", "authors": ["Xing Wei", "Chunchun Chen", "Rui Fan", "Xiaofeng Cao", "Sourav Medya", "Wei Ye"], "title": "Preference-driven Knowledge Distillation for Few-shot Node Classification", "comment": "Accepted at NeurIPS 2025", "summary": "Graph neural networks (GNNs) can efficiently process text-attributed graphs\n(TAGs) due to their message-passing mechanisms, but their training heavily\nrelies on the human-annotated labels. Moreover, the complex and diverse local\ntopologies of nodes of real-world TAGs make it challenging for a single\nmechanism to handle. Large language models (LLMs) perform well in\nzero-/few-shot learning on TAGs but suffer from a scalability challenge.\nTherefore, we propose a preference-driven knowledge distillation (PKD)\nframework to synergize the complementary strengths of LLMs and various GNNs for\nfew-shot node classification. Specifically, we develop a GNN-preference-driven\nnode selector that effectively promotes prediction distillation from LLMs to\nteacher GNNs. To further tackle nodes' intricate local topologies, we develop a\nnode-preference-driven GNN selector that identifies the most suitable teacher\nGNN for each node, thereby facilitating tailored knowledge distillation from\nteacher GNNs to the student GNN. Extensive experiments validate the efficacy of\nour proposed framework in few-shot node classification on real-world TAGs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u504f\u597d\u9a71\u52a8\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u7ed3\u5408LLMs\u548cGNNs\u7684\u4f18\u52bf\u8fdb\u884c\u5c11\u6837\u672c\u8282\u70b9\u5206\u7c7b\uff0c\u901a\u8fc7GNN\u504f\u597d\u9a71\u52a8\u7684\u8282\u70b9\u9009\u62e9\u5668\u548c\u8282\u70b9\u504f\u597d\u9a71\u52a8\u7684GNN\u9009\u62e9\u5668\u5b9e\u73b0\u9ad8\u6548\u77e5\u8bc6\u84b8\u998f\u3002", "motivation": "GNNs\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6807\u7b7e\u4e14\u96be\u4ee5\u5904\u7406\u590d\u6742\u591a\u6837\u7684\u5c40\u90e8\u62d3\u6251\u7ed3\u6784\uff0cLLMs\u5728\u5c11\u6837\u672c\u5b66\u4e60\u4e0a\u8868\u73b0\u826f\u597d\u4f46\u5b58\u5728\u6269\u5c55\u6027\u95ee\u9898\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u8005\u7684\u4e92\u8865\u4f18\u52bf\u3002", "method": "\u5f00\u53d1\u4e86GNN\u504f\u597d\u9a71\u52a8\u7684\u8282\u70b9\u9009\u62e9\u5668\u4fc3\u8fdbLLMs\u5230\u6559\u5e08GNNs\u7684\u9884\u6d4b\u84b8\u998f\uff0c\u4ee5\u53ca\u8282\u70b9\u504f\u597d\u9a71\u52a8\u7684GNN\u9009\u62e9\u5668\u4e3a\u6bcf\u4e2a\u8282\u70b9\u9009\u62e9\u6700\u5408\u9002\u7684\u6559\u5e08GNN\u8fdb\u884c\u5b9a\u5236\u5316\u77e5\u8bc6\u84b8\u998f\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6587\u672c\u5c5e\u6027\u56fe\u7684\u5c11\u6837\u672c\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u504f\u597d\u9a71\u52a8\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86LLMs\u548cGNNs\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u5728\u5c11\u6837\u672c\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.10801", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10801", "abs": "https://arxiv.org/abs/2510.10801", "authors": ["Bahar \u0130lgen", "Georges Hattab"], "title": "Toward Human-Centered Readability Evaluation", "comment": "Accepted to the 4th Workshop on Bridging Human-Computer Interaction\n  and NLP (HCI+NLP) at EMNLP 2025, Suzhou, China", "summary": "Text simplification is essential for making public health information\naccessible to diverse populations, including those with limited health\nliteracy. However, commonly used evaluation metrics in Natural Language\nProcessing (NLP), such as BLEU, FKGL, and SARI, mainly capture surface-level\nfeatures and fail to account for human-centered qualities like clarity,\ntrustworthiness, tone, cultural relevance, and actionability. This limitation\nis particularly critical in high-stakes health contexts, where communication\nmust be not only simple but also usable, respectful, and trustworthy. To\naddress this gap, we propose the Human-Centered Readability Score (HCRS), a\nfive-dimensional evaluation framework grounded in Human-Computer Interaction\n(HCI) and health communication research. HCRS integrates automatic measures\nwith structured human feedback to capture the relational and contextual aspects\nof readability. We outline the framework, discuss its integration into\nparticipatory evaluation workflows, and present a protocol for empirical\nvalidation. This work aims to advance the evaluation of health text\nsimplification beyond surface metrics, enabling NLP systems that align more\nclosely with diverse users' needs, expectations, and lived experiences.", "AI": {"tldr": "\u63d0\u51faHCRS\u6846\u67b6\uff0c\u8d85\u8d8a\u4f20\u7edf\u8868\u9762\u6307\u6807\uff0c\u6574\u5408\u81ea\u52a8\u6d4b\u91cf\u548c\u7ed3\u6784\u5316\u4eba\u7c7b\u53cd\u9988\uff0c\u8bc4\u4f30\u5065\u5eb7\u6587\u672c\u7b80\u5316\u5728\u6e05\u6670\u5ea6\u3001\u53ef\u4fe1\u5ea6\u3001\u6587\u5316\u76f8\u5173\u6027\u7b49\u4e94\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u4f20\u7edfNLP\u8bc4\u4f30\u6307\u6807\uff08\u5982BLEU\u3001FKGL\u3001SARI\uff09\u4e3b\u8981\u6355\u6349\u8868\u9762\u7279\u5f81\uff0c\u65e0\u6cd5\u8861\u91cf\u4eba\u7c7b\u4e2d\u5fc3\u7684\u8d28\u91cf\uff0c\u5982\u6e05\u6670\u5ea6\u3001\u53ef\u4fe1\u5ea6\u3001\u6587\u5316\u76f8\u5173\u6027\u7b49\uff0c\u8fd9\u5728\u5065\u5eb7\u6c9f\u901a\u7b49\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u5c24\u4e3a\u5173\u952e\u3002", "method": "\u63d0\u51fa\u4eba\u7c7b\u4e2d\u5fc3\u53ef\u8bfb\u6027\u8bc4\u5206\uff08HCRS\uff09\uff0c\u4e00\u4e2a\u57fa\u4e8e\u4eba\u673a\u4ea4\u4e92\u548c\u5065\u5eb7\u6c9f\u901a\u7814\u7a76\u7684\u4e94\u7ef4\u8bc4\u4f30\u6846\u67b6\uff0c\u6574\u5408\u81ea\u52a8\u6d4b\u91cf\u4e0e\u7ed3\u6784\u5316\u4eba\u7c7b\u53cd\u9988\uff0c\u6355\u6349\u53ef\u8bfb\u6027\u7684\u5173\u7cfb\u548c\u60c5\u5883\u65b9\u9762\u3002", "result": "\u6982\u8ff0\u4e86HCRS\u6846\u67b6\uff0c\u8ba8\u8bba\u4e86\u5176\u5728\u53c2\u4e0e\u5f0f\u8bc4\u4f30\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u6574\u5408\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\u534f\u8bae\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u65e8\u5728\u63a8\u52a8\u5065\u5eb7\u6587\u672c\u7b80\u5316\u8bc4\u4f30\u8d85\u8d8a\u8868\u9762\u6307\u6807\uff0c\u4f7fNLP\u7cfb\u7edf\u66f4\u8d34\u8fd1\u4e0d\u540c\u7528\u6237\u7684\u9700\u6c42\u3001\u671f\u671b\u548c\u751f\u6d3b\u7ecf\u9a8c\u3002"}}
{"id": "2510.10129", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10129", "abs": "https://arxiv.org/abs/2510.10129", "authors": ["Bin Yang", "Qiuyu Leng", "Jun Zeng", "Zhenhua Wu"], "title": "CacheClip: Accelerating RAG with Effective KV Cache Reuse", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems suffer from severe\ntime-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV\ncache reuse methods face a fundamental trade-off: prefix caching requires\nidentical prefixes that rarely occur in RAG scenarios, while direct\nprecomputation sacrifices quality due to missing inter-chunk attention and\nrepeated attention sinks. Recent methods like APE and CacheBlend partially\naddress these issues but remain inadequate for robust RAG applications. This\npaper presents CacheClip, a novel framework that achieves both fast TTFT and\nhigh generation quality. Our key insight is that small auxiliary LLMs exhibit\nsimilar last-layer attention distributions to primary LLMs (the target model\nfor generation), enabling efficient identification of tokens critical for\nrestoring inter-chunk attention, thereby significantly improving response\nquality on cross-chunk reasoning tasks. CacheClip integrates three techniques:\n(1) auxiliary-model-guided token selection for selective KV cache\nrecomputation, where the auxiliary model is finetuned to improve selection\naccuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)\ngrouping strategy to maintain local coherence during partial KV cache updates.\nExperiments show CacheClip retains up to 94.8% and 85.0% of full-attention\nperformance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%\nand 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM\ninference by up to 1.92x in prefill time, providing a practical solution to the\nefficiency-quality trade-off in RAG systems.", "AI": {"tldr": "CacheClip\u662f\u4e00\u4e2a\u89e3\u51b3RAG\u7cfb\u7edf\u4e2dTTFT\u74f6\u9888\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8f85\u52a9\u5c0f\u6a21\u578b\u8bc6\u522b\u5173\u952etoken\u6765\u9009\u62e9\u6027\u91cd\u8ba1\u7b97KV\u7f13\u5b58\uff0c\u5728\u4fdd\u630194.8%\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b01.92\u500d\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u73b0\u6709KV\u7f13\u5b58\u590d\u7528\u65b9\u6cd5\u5728RAG\u573a\u666f\u4e2d\u5b58\u5728\u6839\u672c\u6027\u6743\u8861\uff1a\u524d\u7f00\u7f13\u5b58\u9700\u8981\u76f8\u540c\u524d\u7f00\u4f46RAG\u4e2d\u5f88\u5c11\u51fa\u73b0\uff0c\u76f4\u63a5\u9884\u8ba1\u7b97\u4f1a\u56e0\u7f3a\u5c11\u8de8\u5757\u6ce8\u610f\u529b\u548c\u91cd\u590d\u6ce8\u610f\u529bsink\u800c\u727a\u7272\u8d28\u91cf\u3002", "method": "\u96c6\u6210\u4e09\u79cd\u6280\u672f\uff1a\u8f85\u52a9\u6a21\u578b\u5f15\u5bfc\u7684token\u9009\u62e9\u3001\u5171\u4eab\u524d\u7f00\u6d88\u9664\u5197\u4f59\u6ce8\u610f\u529bsink\u3001\u5206\u7ec4\u7b56\u7565\u4fdd\u6301\u5c40\u90e8\u4e00\u81f4\u6027\u3002\u8f85\u52a9\u6a21\u578b\u7ecf\u8fc7\u5fae\u8c03\u4ee5\u63d0\u9ad8\u9009\u62e9\u51c6\u786e\u6027\u3002", "result": "\u5728NIAH\u548cLongBench\u4e0a\u5206\u522b\u4fdd\u630194.8%\u548c85.0%\u7684\u5168\u6ce8\u610f\u529b\u6027\u80fd\uff0c\u6bd4APE\u548cCacheBlend\u5728NIAH\u4e0a\u63d0\u534725.2%\u548c35.1%\u3002\u9884\u586b\u5145\u65f6\u95f4\u52a0\u901f\u8fbe1.92\u500d\u3002", "conclusion": "CacheClip\u4e3aRAG\u7cfb\u7edf\u4e2d\u7684\u6548\u7387-\u8d28\u91cf\u6743\u8861\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u5feb\u901fTTFT\u548c\u9ad8\u751f\u6210\u8d28\u91cf\u7684\u5e73\u8861\u3002"}}
{"id": "2510.10806", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10806", "abs": "https://arxiv.org/abs/2510.10806", "authors": ["Mihir Gupte", "Paolo Giusto", "Ramesh S"], "title": "Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures", "comment": "Waiting for Conference Response", "summary": "Large Language Models (LLMs) are adept at generating responses based on\ninformation within their context. While this ability is useful for interacting\nwith structured data like code files, another popular method,\nRetrieval-Augmented Generation (RAG), retrieves relevant documents to augment\nthe model's in-context learning. However, it is not well-explored how to best\nrepresent this retrieved knowledge for generating responses on structured data,\nparticularly hierarchical structures like trees. In this work, we propose a\nnovel bottom-up method to linearize knowledge from tree-like structures (like a\nGitHub repository) by generating implicit, aggregated summaries at each\nhierarchical level. This approach enables the knowledge to be stored in a\nknowledge base and used directly with RAG. We then compare our method to using\nRAG on raw, unstructured code, evaluating the accuracy and quality of the\ngenerated responses. Our results show that while response quality is comparable\nacross both methods, our approach generates over 68% fewer documents in the\nretriever, a significant gain in efficiency. This finding suggests that\nleveraging implicit, linearized knowledge may be a highly effective and\nscalable strategy for handling complex, hierarchical data structures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u5e95\u5411\u4e0a\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u6811\u72b6\u7ed3\u6784\uff08\u5982GitHub\u4ed3\u5e93\uff09\u7684\u77e5\u8bc6\u7ebf\u6027\u5316\uff0c\u901a\u8fc7\u5728\u6bcf\u4e2a\u5c42\u7ea7\u751f\u6210\u9690\u5f0f\u805a\u5408\u6458\u8981\uff0c\u4f7f\u77e5\u8bc6\u80fd\u591f\u5b58\u50a8\u5728\u77e5\u8bc6\u5e93\u4e2d\u5e76\u4e0eRAG\u76f4\u63a5\u4f7f\u7528\u3002", "motivation": "\u867d\u7136LLMs\u64c5\u957f\u57fa\u4e8e\u4e0a\u4e0b\u6587\u4fe1\u606f\u751f\u6210\u54cd\u5e94\uff0c\u4f46\u5bf9\u4e8e\u7ed3\u6784\u5316\u6570\u636e\uff08\u7279\u522b\u662f\u6811\u72b6\u5c42\u6b21\u7ed3\u6784\uff09\uff0c\u5982\u4f55\u6700\u4f73\u5730\u8868\u793a\u68c0\u7d22\u5230\u7684\u77e5\u8bc6\u4ee5\u751f\u6210\u54cd\u5e94\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u81ea\u5e95\u5411\u4e0a\u7684\u65b9\u6cd5\uff0c\u5728\u6811\u72b6\u7ed3\u6784\u7684\u6bcf\u4e2a\u5c42\u7ea7\u751f\u6210\u9690\u5f0f\u3001\u805a\u5408\u7684\u6458\u8981\uff0c\u5c06\u5c42\u6b21\u77e5\u8bc6\u7ebf\u6027\u5316\uff0c\u7136\u540e\u4e0eRAG\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u4e0e\u5728\u539f\u59cb\u975e\u7ed3\u6784\u5316\u4ee3\u7801\u4e0a\u4f7f\u7528RAG\u76f8\u6bd4\uff0c\u867d\u7136\u54cd\u5e94\u8d28\u91cf\u76f8\u5f53\uff0c\u4f46\u8be5\u65b9\u6cd5\u5728\u68c0\u7d22\u5668\u4e2d\u751f\u6210\u7684\u6587\u6863\u6570\u91cf\u51cf\u5c11\u4e8668%\u4ee5\u4e0a\uff0c\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u5229\u7528\u9690\u5f0f\u7ebf\u6027\u5316\u77e5\u8bc6\u53ef\u80fd\u662f\u5904\u7406\u590d\u6742\u5c42\u6b21\u6570\u636e\u7ed3\u6784\u7684\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7b56\u7565\u3002"}}
{"id": "2510.10136", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10136", "abs": "https://arxiv.org/abs/2510.10136", "authors": ["Lancheng Zou", "Shuo Yin", "Zehua Pei", "Tsung-Yi Ho", "Farzan Farnia", "Bei Yu"], "title": "PermLLM: Learnable Channel Permutation for N:M Sparse Large Language Models", "comment": "Accepted by NeurIPS 2025", "summary": "Channel permutation is a powerful technique for enhancing the accuracy of N:M\nsparse models by reordering the channels of weight matrices to prioritize the\nretention of important weights. However, traditional channel permutation\nmethods rely on handcrafted quality metrics, which often fail to accurately\ncapture the true impact of pruning on model performance. To address this\nlimitation, we propose PermLLM, a novel post-training pruning framework that\nintroduces learnable channel permutation (LCP) for N:M sparsity. LCP leverages\nSinkhorn normalization to transform discrete permutation matrices into\ndifferentiable soft permutation matrices, enabling end-to-end optimization.\nAdditionally, PermLLM incorporates an efficient block-wise channel permutation\nstrategy, which significantly reduces the number of learnable parameters and\ncomputational complexity. PermLLM seamlessly integrates with existing one-shot\npruning methods to adaptively optimize channel permutations, effectively\nmitigating pruning-induced errors. Extensive experiments on the LLaMA series,\nQwen, and OPT models demonstrate that PermLLM achieves superior performance in\noptimizing N:M sparse models. The code is available at\nhttps://github.com/lanchengzou/PermLLM.", "AI": {"tldr": "PermLLM\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u901a\u9053\u7f6e\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7Sinkhorn\u5f52\u4e00\u5316\u5c06\u79bb\u6563\u7f6e\u6362\u77e9\u9635\u8f6c\u6362\u4e3a\u53ef\u5fae\u7684\u8f6f\u7f6e\u6362\u77e9\u9635\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86N:M\u7a00\u758f\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u901a\u9053\u7f6e\u6362\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u8d28\u91cf\u6307\u6807\uff0c\u5f80\u5f80\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u526a\u679d\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u771f\u5b9e\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u53ef\u5b66\u4e60\u901a\u9053\u7f6e\u6362(LCP)\uff0c\u4f7f\u7528Sinkhorn\u5f52\u4e00\u5316\u5904\u7406\u79bb\u6563\u7f6e\u6362\u77e9\u9635\uff1b\u91c7\u7528\u9ad8\u6548\u7684\u5757\u7ea7\u901a\u9053\u7f6e\u6362\u7b56\u7565\u51cf\u5c11\u53ef\u5b66\u4e60\u53c2\u6570\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b\u4e0e\u73b0\u6709\u4e00\u6b21\u6027\u526a\u679d\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728LLaMA\u7cfb\u5217\u3001Qwen\u548cOPT\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPermLLM\u5728\u4f18\u5316N:M\u7a00\u758f\u6a21\u578b\u65b9\u9762\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "PermLLM\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u901a\u9053\u7f6e\u6362\u6709\u6548\u7f13\u89e3\u4e86\u526a\u679d\u5f15\u8d77\u7684\u8bef\u5dee\uff0c\u4e3aN:M\u7a00\u758f\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2510.10827", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10827", "abs": "https://arxiv.org/abs/2510.10827", "authors": ["Haeji Jung", "Jinju Kim", "Kyungjin Kim", "Youjeong Roh", "David R. Mortensen"], "title": "Happiness is Sharing a Vocabulary: A Study of Transliteration Methods", "comment": null, "summary": "Transliteration has emerged as a promising means to bridge the gap between\nvarious languages in multilingual NLP, showing promising results especially for\nlanguages using non-Latin scripts. We investigate the degree to which shared\nscript, overlapping token vocabularies, and shared phonology contribute to\nperformance of multilingual models. To this end, we conduct controlled\nexperiments using three kinds of transliteration (romanization, phonemic\ntranscription, and substitution ciphers) as well as orthography. We evaluate\neach model on two downstream tasks -- named entity recognition (NER) and\nnatural language inference (NLI) -- and find that romanization significantly\noutperforms other input types in 7 out of 8 evaluation settings, largely\nconsistent with our hypothesis that it is the most effective approach. We\nfurther analyze how each factor contributed to the success, and suggest that\nhaving longer (subword) tokens shared with pre-trained languages leads to\nbetter utilization of the model.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e0d\u540c\u97f3\u8bd1\u65b9\u6cd5\u5bf9\u591a\u8bed\u8a00NLP\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7f57\u9a6c\u5316\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u80fd\u4ea7\u751f\u4e0e\u9884\u8bad\u7ec3\u8bed\u8a00\u5171\u4eab\u7684\u66f4\u957f\u5b50\u8bcd\u6807\u8bb0\u3002", "motivation": "\u7814\u7a76\u4e0d\u540c\u97f3\u8bd1\u65b9\u6cd5\uff08\u5171\u4eab\u811a\u672c\u3001\u91cd\u53e0\u8bcd\u6c47\u3001\u5171\u4eab\u97f3\u7cfb\uff09\u5bf9\u591a\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u8d21\u732e\u7a0b\u5ea6\uff0c\u4ee5\u5f25\u5408\u4f7f\u7528\u975e\u62c9\u4e01\u6587\u5b57\u8bed\u8a00\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u97f3\u8bd1\u65b9\u6cd5\uff08\u7f57\u9a6c\u5316\u3001\u97f3\u4f4d\u8f6c\u5f55\u3001\u66ff\u6362\u5bc6\u7801\uff09\u4ee5\u53ca\u6b63\u5b57\u6cd5\u8fdb\u884c\u53d7\u63a7\u5b9e\u9a8c\uff0c\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u7f57\u9a6c\u5316\u57288\u4e2a\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u76847\u4e2a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u8f93\u5165\u7c7b\u578b\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u6700\u6709\u6548\u65b9\u6cd5\u7684\u5047\u8bbe\u3002", "conclusion": "\u7f57\u9a6c\u5316\u662f\u6700\u6709\u6548\u7684\u97f3\u8bd1\u65b9\u6cd5\uff0c\u5176\u6210\u529f\u4e3b\u8981\u5f52\u56e0\u4e8e\u80fd\u4ea7\u751f\u4e0e\u9884\u8bad\u7ec3\u8bed\u8a00\u5171\u4eab\u7684\u66f4\u957f\u5b50\u8bcd\u6807\u8bb0\uff0c\u4ece\u800c\u66f4\u597d\u5730\u5229\u7528\u6a21\u578b\u80fd\u529b\u3002"}}
{"id": "2510.10846", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10846", "abs": "https://arxiv.org/abs/2510.10846", "authors": ["Kaixuan Ren", "Preslav Nakov", "Usman Naseem"], "title": "DUAL-Bench: Measuring Over-Refusal and Robustness in Vision-Language Models", "comment": "25 pages, 91 figures, submitted to Oct ARR under reviewing", "summary": "As vision-language models become increasingly capable, maintaining a balance\nbetween safety and usefulness remains a central challenge. Safety mechanisms,\nwhile essential, can backfire, causing over-refusal, where models decline\nbenign requests out of excessive caution. Yet, no existing benchmark has\nsystematically addressed over-refusal in the visual modality. This setting\nintroduces unique challenges, such as dual-use cases where an instruction is\nharmless, but the accompanying image contains harmful content. Models\nfrequently fail in such scenarios, either refusing too conservatively or\ncompleting tasks unsafely, which highlights the need for more fine-grained\nalignment. The ideal behavior is safe completion, i.e., fulfilling the benign\nparts of a request while explicitly warning about any potentially harmful\nelements. To address this, we present DUAL-Bench, the first multimodal\nbenchmark focused on over-refusal and safe completion in VLMs. We evaluated 18\nVLMs across 12 hazard categories, with focus on their robustness under\nsemantics-preserving visual perturbations. The results reveal substantial room\nfor improvement: GPT-5-Nano achieves 12.9% safe completion, GPT-5 models\naverage 7.9%, and Qwen models only 3.9%. We hope that DUAL-Bench will foster\nthe development of more nuanced alignment strategies that ensure models remain\nboth safe and useful in complex multimodal settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86DUAL-Bench\uff0c\u9996\u4e2a\u4e13\u6ce8\u4e8e\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u8fc7\u5ea6\u62d2\u7edd\u548c\u5b89\u5168\u5b8c\u6210\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e8618\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u572812\u79cd\u5371\u9669\u7c7b\u522b\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u7cfb\u7edf\u89e3\u51b3\u89c6\u89c9\u6a21\u6001\u4e2d\u7684\u8fc7\u5ea6\u62d2\u7edd\u95ee\u9898\uff0c\u6a21\u578b\u5728\u53cc\u91cd\u7528\u9014\u573a\u666f\u4e2d\u7ecf\u5e38\u5931\u8d25\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u5bf9\u9f50\u7b56\u7565\u6765\u5e73\u8861\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u6784\u5efaDUAL-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30VLMs\u5728\u8bed\u4e49\u4fdd\u6301\u7684\u89c6\u89c9\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u5b89\u5168\u5b8c\u6210\u4efb\u52a1\u7684\u80fd\u529b\u3002", "result": "\u6a21\u578b\u8868\u73b0\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\uff1aGPT-5-Nano\u8fbe\u523012.9%\u7684\u5b89\u5168\u5b8c\u6210\u7387\uff0cGPT-5\u6a21\u578b\u5e73\u57477.9%\uff0cQwen\u6a21\u578b\u4ec53.9%\u3002", "conclusion": "DUAL-Bench\u5c06\u4fc3\u8fdb\u66f4\u7ec6\u81f4\u5bf9\u9f50\u7b56\u7565\u7684\u53d1\u5c55\uff0c\u786e\u4fdd\u6a21\u578b\u5728\u590d\u6742\u591a\u6a21\u6001\u73af\u5883\u4e2d\u65e2\u5b89\u5168\u53c8\u6709\u7528\u3002"}}
{"id": "2510.10145", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10145", "abs": "https://arxiv.org/abs/2510.10145", "authors": ["Cheng He", "Xijie Liang", "Zengrong Zheng", "Patrick P. C. Lee", "Xu Huang", "Zhaoyi Li", "Hong Xie", "Defu Lian", "Enhong Chen"], "title": "A Unified Frequency Domain Decomposition Framework for Interpretable and Robust Time Series Forecasting", "comment": null, "summary": "Current approaches for time series forecasting, whether in the time or\nfrequency domain, predominantly use deep learning models based on linear layers\nor transformers. They often encode time series data in a black-box manner and\nrely on trial-and-error optimization solely based on forecasting performance,\nleading to limited interpretability and theoretical understanding. Furthermore,\nthe dynamics in data distribution over time and frequency domains pose a\ncritical challenge to accurate forecasting. We propose FIRE, a unified\nfrequency domain decomposition framework that provides a mathematical\nabstraction for diverse types of time series, so as to achieve interpretable\nand robust time series forecasting. FIRE introduces several key innovations:\n(i) independent modeling of amplitude and phase components, (ii) adaptive\nlearning of weights of frequency basis components, (iii) a targeted loss\nfunction, and (iv) a novel training paradigm for sparse data. Extensive\nexperiments demonstrate that FIRE consistently outperforms state-of-the-art\nmodels on long-term forecasting benchmarks, achieving superior predictive\nperformance and significantly enhancing interpretability of time series", "AI": {"tldr": "FIRE\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u9891\u57df\u5206\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u72ec\u7acb\u5efa\u6a21\u632f\u5e45\u548c\u76f8\u4f4d\u5206\u91cf\u3001\u81ea\u9002\u5e94\u5b66\u4e60\u9891\u7387\u57fa\u5206\u91cf\u6743\u91cd\u3001\u9488\u5bf9\u6027\u635f\u5931\u51fd\u6570\u548c\u7a00\u758f\u6570\u636e\u8bad\u7ec3\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u4e14\u9c81\u68d2\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002", "motivation": "\u5f53\u524d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u591a\u4e3a\u57fa\u4e8e\u7ebf\u6027\u5c42\u6216Transformer\u7684\u9ed1\u76d2\u6a21\u578b\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u7406\u8bba\u7406\u89e3\uff0c\u4e14\u6570\u636e\u5206\u5e03\u5728\u65f6\u57df\u548c\u9891\u57df\u7684\u52a8\u6001\u53d8\u5316\u7ed9\u51c6\u786e\u9884\u6d4b\u5e26\u6765\u6311\u6218\u3002", "method": "\u63d0\u51faFIRE\u6846\u67b6\uff0c\u5728\u9891\u57df\u8fdb\u884c\u5206\u89e3\uff0c\u72ec\u7acb\u5efa\u6a21\u632f\u5e45\u548c\u76f8\u4f4d\uff0c\u81ea\u9002\u5e94\u5b66\u4e60\u9891\u7387\u57fa\u5206\u91cf\u6743\u91cd\uff0c\u4f7f\u7528\u9488\u5bf9\u6027\u635f\u5931\u51fd\u6570\u548c\u7a00\u758f\u6570\u636e\u8bad\u7ec3\u8303\u5f0f\u3002", "result": "\u5728\u957f\u671f\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFIRE\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\u5e76\u663e\u8457\u589e\u5f3a\u4e86\u65f6\u95f4\u5e8f\u5217\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "FIRE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u9891\u57df\u5206\u89e3\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u89e3\u91ca\u4e14\u9c81\u68d2\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u5728\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.10885", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10885", "abs": "https://arxiv.org/abs/2510.10885", "authors": ["Jiajing Guo", "Kenil Patel", "Jorge Piazentin Ono", "Wenbin He", "Liu Ren"], "title": "Rethinking Agentic Workflows: Evaluating Inference-Based Test-Time Scaling Strategies in Text2SQL Tasks", "comment": "Accepted at COLM 2025 SCALR Workshop", "summary": "Large language models (LLMs) are increasingly powering Text-to-SQL (Text2SQL)\nsystems, enabling non-expert users to query industrial databases using natural\nlanguage. While test-time scaling strategies have shown promise in LLM-based\nsolutions, their effectiveness in real-world applications, especially with the\nlatest reasoning models, remains uncertain. In this work, we benchmark six\nlightweight, industry-oriented test-time scaling strategies and four LLMs,\nincluding two reasoning models, evaluating their performance on the BIRD\nMini-Dev benchmark. Beyond standard accuracy metrics, we also report inference\nlatency and token consumption, providing insights relevant for practical system\ndeployment. Our findings reveal that Divide-and-Conquer prompting and few-shot\ndemonstrations consistently enhance performance for both general-purpose and\nreasoning-focused LLMs. However, introducing additional workflow steps yields\nmixed results, and base model selection plays a critical role. This work sheds\nlight on the practical trade-offs between accuracy, efficiency, and complexity\nwhen deploying Text2SQL systems.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e866\u79cd\u8f7b\u91cf\u7ea7\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u548c4\u4e2aLLM\uff08\u5305\u62ec2\u4e2a\u63a8\u7406\u6a21\u578b\uff09\u5728BIRD Mini-Dev\u57fa\u51c6\u4e0a\u7684Text-to-SQL\u6027\u80fd\uff0c\u91cd\u70b9\u5173\u6ce8\u51c6\u786e\u7387\u3001\u63a8\u7406\u5ef6\u8fdf\u548ctoken\u6d88\u8017\u7684\u5b9e\u9645\u6743\u8861\u3002", "motivation": "\u867d\u7136\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u5728LLM-based\u89e3\u51b3\u65b9\u6848\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u4e0e\u6700\u65b0\u63a8\u7406\u6a21\u578b\u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u4ecd\u4e0d\u786e\u5b9a\u3002", "method": "\u5728BIRD Mini-Dev\u57fa\u51c6\u4e0a\u5bf96\u79cd\u8f7b\u91cf\u7ea7\u3001\u9762\u5411\u5de5\u4e1a\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u548c4\u4e2aLLM\uff08\u5305\u62ec2\u4e2a\u63a8\u7406\u6a21\u578b\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9664\u4e86\u6807\u51c6\u51c6\u786e\u7387\u6307\u6807\u5916\uff0c\u8fd8\u62a5\u544a\u63a8\u7406\u5ef6\u8fdf\u548ctoken\u6d88\u8017\u3002", "result": "\u5206\u6cbb\u63d0\u793a\u548c\u5c11\u6837\u672c\u6f14\u793a\u6301\u7eed\u63d0\u5347\u901a\u7528\u548c\u63a8\u7406\u578bLLM\u7684\u6027\u80fd\uff0c\u4f46\u5f15\u5165\u989d\u5916\u5de5\u4f5c\u6d41\u6b65\u9aa4\u6548\u679c\u4e0d\u4e00\uff0c\u57fa\u7840\u6a21\u578b\u9009\u62e9\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63ed\u793a\u4e86\u5728\u90e8\u7f72Text2SQL\u7cfb\u7edf\u65f6\u51c6\u786e\u7387\u3001\u6548\u7387\u548c\u590d\u6742\u6027\u4e4b\u95f4\u7684\u5b9e\u9645\u6743\u8861\u5173\u7cfb\u3002"}}
{"id": "2510.10149", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10149", "abs": "https://arxiv.org/abs/2510.10149", "authors": ["Xin Chen", "Gillian Dobbie", "Xinyu Wang", "Feng Liu", "Di Wang", "Jingfeng Zhang"], "title": "Robust Learning of Diffusion Models with Extremely Noisy Conditions", "comment": null, "summary": "Conditional diffusion models have the generative controllability by\nincorporating external conditions. However, their performance significantly\ndegrades with noisy conditions, such as corrupted labels in the image\ngeneration or unreliable observations or states in the control policy\ngeneration. This paper introduces a robust learning framework to address\nextremely noisy conditions in conditional diffusion models. We empirically\ndemonstrate that existing noise-robust methods fail when the noise level is\nhigh. To overcome this, we propose learning pseudo conditions as surrogates for\nclean conditions and refining pseudo ones progressively via the technique of\ntemporal ensembling. Additionally, we develop a Reverse-time Diffusion\nCondition (RDC) technique, which diffuses pseudo conditions to reinforce the\nmemorization effect and further facilitate the refinement of the pseudo\nconditions. Experimentally, our approach achieves state-of-the-art performance\nacross a range of noise levels on both class-conditional image generation and\nvisuomotor policy generation tasks.The code can be accessible via the project\npage https://robustdiffusionpolicy.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4e2d\u6781\u7aef\u566a\u58f0\u6761\u4ef6\u7684\u9c81\u68d2\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f2a\u6761\u4ef6\u5b66\u4e60\u548c\u65f6\u95f4\u96c6\u6210\u6280\u672f\u63d0\u5347\u6a21\u578b\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800c\u73b0\u6709\u7684\u566a\u58f0\u9c81\u68d2\u65b9\u6cd5\u5728\u9ad8\u566a\u58f0\u6c34\u5e73\u4e0b\u5931\u6548", "method": "\u63d0\u51fa\u5b66\u4e60\u4f2a\u6761\u4ef6\u4f5c\u4e3a\u5e72\u51c0\u6761\u4ef6\u7684\u66ff\u4ee3\uff0c\u901a\u8fc7\u65f6\u95f4\u96c6\u6210\u6280\u672f\u9010\u6b65\u4f18\u5316\u4f2a\u6761\u4ef6\uff0c\u5e76\u5f00\u53d1\u4e86\u53cd\u65f6\u6269\u6563\u6761\u4ef6(RDC)\u6280\u672f\u6765\u589e\u5f3a\u8bb0\u5fc6\u6548\u5e94", "result": "\u5728\u7c7b\u522b\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u548c\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u751f\u6210\u4efb\u52a1\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u566a\u58f0\u6c34\u5e73\u4e0b\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6781\u7aef\u566a\u58f0\u6761\u4ef6\uff0c\u4e3a\u566a\u58f0\u73af\u5883\u4e0b\u7684\u53ef\u63a7\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.10890", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10890", "abs": "https://arxiv.org/abs/2510.10890", "authors": ["Yu Chao", "Siyu Lin", "xiaorong wang", "Zhu Zhang", "Zihan Zhou", "Haoyu Wang", "Shuo Wang", "Jie Zhou", "Zhiyuan Liu", "Maosong Sun"], "title": "LLM$\\times$MapReduce-V3: Enabling Interactive In-Depth Survey Generation through a MCP-Driven Hierarchically Modular Agent System", "comment": "Accepted by EMNLP2025 System Demonstration", "summary": "We introduce LLM x MapReduce-V3, a hierarchically modular agent system\ndesigned for long-form survey generation. Building on the prior work, LLM x\nMapReduce-V2, this version incorporates a multi-agent architecture where\nindividual functional components, such as skeleton initialization, digest\nconstruction, and skeleton refinement, are implemented as independent\nmodel-context-protocol (MCP) servers. These atomic servers can be aggregated\ninto higher-level servers, creating a hierarchically structured system. A\nhigh-level planner agent dynamically orchestrates the workflow by selecting\nappropriate modules based on their MCP tool descriptions and the execution\nhistory. This modular decomposition facilitates human-in-the-loop intervention,\naffording users greater control and customization over the research process.\nThrough a multi-turn interaction, the system precisely captures the intended\nresearch perspectives to generate a comprehensive skeleton, which is then\ndeveloped into an in-depth survey. Human evaluations demonstrate that our\nsystem surpasses representative baselines in both content depth and length,\nhighlighting the strength of MCP-based modular planning.", "AI": {"tldr": "LLM x MapReduce-V3\u662f\u4e00\u4e2a\u5206\u5c42\u6a21\u5757\u5316\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u957f\u7bc7\u7efc\u8ff0\u3002\u5b83\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5c06\u529f\u80fd\u7ec4\u4ef6\u5b9e\u73b0\u4e3a\u72ec\u7acb\u7684MCP\u670d\u52a1\u5668\uff0c\u5e76\u901a\u8fc7\u9ad8\u5c42\u89c4\u5212\u5668\u52a8\u6001\u7f16\u6392\u5de5\u4f5c\u6d41\u7a0b\uff0c\u652f\u6301\u4eba\u673a\u4ea4\u4e92\u5e72\u9884\u3002", "motivation": "\u57fa\u4e8e\u5148\u524d\u5de5\u4f5cLLM x MapReduce-V2\uff0c\u672c\u7248\u672c\u65e8\u5728\u901a\u8fc7\u6a21\u5757\u5316\u5206\u89e3\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u7814\u7a76\u8fc7\u7a0b\u63a7\u5236\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u66f4\u597d\u5730\u5b9a\u5236\u548c\u5e72\u9884\u7efc\u8ff0\u751f\u6210\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5c06\u9aa8\u67b6\u521d\u59cb\u5316\u3001\u6458\u8981\u6784\u5efa\u548c\u9aa8\u67b6\u4f18\u5316\u7b49\u529f\u80fd\u7ec4\u4ef6\u5b9e\u73b0\u4e3a\u72ec\u7acb\u7684MCP\u670d\u52a1\u5668\u3002\u9ad8\u5c42\u89c4\u5212\u5668\u6839\u636eMCP\u5de5\u5177\u63cf\u8ff0\u548c\u6267\u884c\u5386\u53f2\u52a8\u6001\u9009\u62e9\u6a21\u5757\uff0c\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u7cbe\u786e\u6355\u6349\u7814\u7a76\u89c6\u89d2\u3002", "result": "\u4eba\u7c7b\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u5185\u5bb9\u6df1\u5ea6\u548c\u957f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u4ee3\u8868\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7a81\u663e\u4e86\u57fa\u4e8eMCP\u7684\u6a21\u5757\u5316\u89c4\u5212\u7684\u4f18\u52bf\u3002", "conclusion": "LLM x MapReduce-V3\u901a\u8fc7\u5206\u5c42\u6a21\u5757\u5316\u8bbe\u8ba1\u548cMCP\u670d\u52a1\u5668\u67b6\u6784\uff0c\u6709\u6548\u63d0\u5347\u4e86\u957f\u7bc7\u7efc\u8ff0\u751f\u6210\u7684\u8d28\u91cf\u548c\u53ef\u63a7\u6027\uff0c\u4e3a\u4eba\u673a\u534f\u4f5c\u7684\u7814\u7a76\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2510.10150", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10150", "abs": "https://arxiv.org/abs/2510.10150", "authors": ["Zhezheng Hao", "Hong Wang", "Haoyang Liu", "Jian Luo", "Jiarui Yu", "Hande Dong", "Qiang Lin", "Can Wang", "Jiawei Chen"], "title": "Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective", "comment": null, "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) can enhance LLM\nreasoning, its training process poses a critical risk: entropy collapse. This\nphenomenon is a rapid loss of policy diversity, stemming from the\nexploration-exploitation imbalance and leading to a lack of generalization.\nRecent entropy-intervention methods aim to prevent \\coloredtext{entropy\ncollapse}, yet their underlying mechanisms remain unclear. In this paper, we\nconduct a quantitative analysis to reveal token-level entropy changes and how\nexisting entropy intervention methods help avoid entropy collapse. Our findings\npoint out a fundamental limitation of existing methods: they attempt to control\nentropy dynamics indirectly. By only affecting related factors, such as the\nadvantage signal and generation probability, their effectiveness is inherently\nlimited and could potentially fail. To address this limitation, we introduce an\nentropy-change-aware reweighting scheme, namely Stabilizing Token-level\nEntropy-changE via Reweighting (STEER), that adaptively stabilizes entropy\ndynamics through fine-grained token-level adjustments. Our approach mitigates\nover-exploitation while fostering robust exploration. Extensive experiments\ndemonstrate that STEER significantly mitigates entropy collapse, stabilizes\nentropy dynamics, and achieves stronger downstream performance across various\nmathematical reasoning benchmarks \\footnote{Our code is available at\nhttps://github.com/zz-haooo/STEER.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u51fa\u4e86STEER\u65b9\u6cd5\u901a\u8fc7\u7ec6\u7c92\u5ea6token\u7ea7\u8c03\u6574\u6765\u7a33\u5b9a\u71b5\u52a8\u6001\uff0c\u6709\u6548\u7f13\u89e3\u71b5\u5d29\u6e83\u5e76\u63d0\u5347\u6570\u5b66\u63a8\u7406\u6027\u80fd\u3002", "motivation": "RLVR\u8bad\u7ec3\u5b58\u5728\u71b5\u5d29\u6e83\u98ce\u9669\uff0c\u5373\u7b56\u7565\u591a\u6837\u6027\u5feb\u901f\u4e27\u5931\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u73b0\u6709\u71b5\u5e72\u9884\u65b9\u6cd5\u673a\u5236\u4e0d\u660e\u786e\u4e14\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u76f4\u63a5\u7684\u71b5\u52a8\u6001\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSTEER\u65b9\u6cd5\uff0c\u901a\u8fc7\u71b5\u53d8\u5316\u611f\u77e5\u7684\u91cd\u65b0\u52a0\u6743\u65b9\u6848\uff0c\u5728token\u7ea7\u522b\u81ea\u9002\u5e94\u7a33\u5b9a\u71b5\u52a8\u6001\uff0c\u7f13\u89e3\u8fc7\u5ea6\u5229\u7528\u5e76\u4fc3\u8fdb\u7a33\u5065\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSTEER\u663e\u8457\u7f13\u89e3\u71b5\u5d29\u6e83\uff0c\u7a33\u5b9a\u71b5\u52a8\u6001\uff0c\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u66f4\u5f3a\u7684\u4e0b\u6e38\u6027\u80fd\u3002", "conclusion": "STEER\u901a\u8fc7\u76f4\u63a5\u63a7\u5236token\u7ea7\u71b5\u53d8\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3aRLVR\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u7684\u71b5\u52a8\u6001\u7ba1\u7406\u65b9\u6848\u3002"}}
{"id": "2510.10913", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10913", "abs": "https://arxiv.org/abs/2510.10913", "authors": ["Ki Jung Seo", "Sehun Lim", "Taeuk Kim"], "title": "ADVICE: Answer-Dependent Verbalized Confidence Estimation", "comment": null, "summary": "Recent progress in large language models (LLMs) has enabled them to express\ntheir confidence in natural language, enhancing transparency and reliability.\nHowever, their confidence often exhibits overconfidence, the cause of which\nremains poorly understood. In this work, we conduct a detailed analysis of the\ndynamics underlying verbalized confidence and identify answer-independence as a\nkey factor, defined as the model's failure to condition confidence on its own\nanswer. To address this, we propose ADVICE (Answer-Dependent Verbalized\nConfidence Estimation), a fine-tuning framework that facilitates\nanswer-grounded confidence estimation. Extensive experiments show that ADVICE\nsubstantially improves confidence calibration while preserving task\nperformance. Further analyses confirm that ADVICE strengthens\nanswer-groundedness, leading to more balanced and well-calibrated confidence\ndistributions. Our findings shed light on the origin of overconfidence and\nestablish a framework for more trustworthy confidence verbalization.", "AI": {"tldr": "\u63d0\u51faADVICE\u6846\u67b6\uff0c\u901a\u8fc7\u7b54\u6848\u76f8\u5173\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u6765\u89e3\u51b3LLM\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u6539\u5584\u7f6e\u4fe1\u5ea6\u6821\u51c6", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u591f\u7528\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u7f6e\u4fe1\u5ea6\uff0c\u4f46\u7ecf\u5e38\u8868\u73b0\u51fa\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u5176\u539f\u56e0\u5c1a\u4e0d\u6e05\u695a", "method": "\u63d0\u51faADVICE\uff08\u7b54\u6848\u76f8\u5173\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff09\u5fae\u8c03\u6846\u67b6\uff0c\u4fc3\u8fdb\u57fa\u4e8e\u7b54\u6848\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1", "result": "ADVICE\u663e\u8457\u6539\u5584\u4e86\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4efb\u52a1\u6027\u80fd\uff0c\u589e\u5f3a\u4e86\u7b54\u6848\u76f8\u5173\u6027", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u8fc7\u5ea6\u81ea\u4fe1\u7684\u6839\u6e90\uff0c\u5e76\u5efa\u7acb\u4e86\u66f4\u53ef\u4fe1\u7684\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u6846\u67b6"}}
{"id": "2510.10188", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10188", "abs": "https://arxiv.org/abs/2510.10188", "authors": ["Linfei Li", "Fengyi Zhang", "Zhong Wang", "Lin Zhang", "Ying Shen"], "title": "INR-Bench: A Unified Benchmark for Implicit Neural Representations in Multi-Domain Regression and Reconstruction", "comment": null, "summary": "Implicit Neural Representations (INRs) have gained success in various signal\nprocessing tasks due to their advantages of continuity and infinite resolution.\nHowever, the factors influencing their effectiveness and limitations remain\nunderexplored. To better understand these factors, we leverage insights from\nNeural Tangent Kernel (NTK) theory to analyze how model architectures (classic\nMLP and emerging KAN), positional encoding, and nonlinear primitives affect the\nresponse to signals of varying frequencies. Building on this analysis, we\nintroduce INR-Bench, the first comprehensive benchmark specifically designed\nfor multimodal INR tasks. It includes 56 variants of Coordinate-MLP models\n(featuring 4 types of positional encoding and 14 activation functions) and 22\nCoordinate-KAN models with distinct basis functions, evaluated across 9\nimplicit multimodal tasks. These tasks cover both forward and inverse problems,\noffering a robust platform to highlight the strengths and limitations of\ndifferent neural models, thereby establishing a solid foundation for future\nresearch. The code and dataset are available at\nhttps://github.com/lif314/INR-Bench.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86INR-Bench\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4e13\u95e8\u4e3a\u591a\u6a21\u6001\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u4efb\u52a1\u8bbe\u8ba1\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u5305\u542b56\u79cdCoordinate-MLP\u53d8\u4f53\u548c22\u79cdCoordinate-KAN\u6a21\u578b\uff0c\u57289\u4e2a\u9690\u5f0f\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u5728\u4fe1\u53f7\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6709\u6548\u6027\u548c\u9650\u5236\u56e0\u7d20\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e9b\u56e0\u7d20\uff0c\u4f5c\u8005\u5229\u7528\u795e\u7ecf\u6b63\u5207\u6838\u7406\u8bba\u5206\u6790\u6a21\u578b\u67b6\u6784\u3001\u4f4d\u7f6e\u7f16\u7801\u548c\u975e\u7ebf\u6027\u539f\u8bed\u5bf9\u4e0d\u540c\u9891\u7387\u4fe1\u53f7\u54cd\u5e94\u7684\u5f71\u54cd\u3002", "method": "\u57fa\u4e8eNTK\u7406\u8bba\u5206\u6790\uff0c\u5f15\u5165INR-Bench\u57fa\u51c6\uff0c\u5305\u542b56\u79cdCoordinate-MLP\u6a21\u578b\uff084\u79cd\u4f4d\u7f6e\u7f16\u7801\u7c7b\u578b\u548c14\u79cd\u6fc0\u6d3b\u51fd\u6570\uff09\u548c22\u79cd\u5177\u6709\u4e0d\u540c\u57fa\u51fd\u6570\u7684Coordinate-KAN\u6a21\u578b\uff0c\u57289\u4e2a\u9690\u5f0f\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5efa\u7acb\u4e86\u5168\u9762\u7684\u57fa\u51c6\u5e73\u53f0\uff0c\u80fd\u591f\u7a81\u51fa\u4e0d\u540c\u795e\u7ecf\u6a21\u578b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u575a\u5b9e\u57fa\u7840\u3002", "conclusion": "INR-Bench\u4e3a\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7814\u7a76\u63d0\u4f9b\u4e86\u9996\u4e2a\u4e13\u95e8\u7684\u591a\u6a21\u6001\u4efb\u52a1\u57fa\u51c6\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2510.10927", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10927", "abs": "https://arxiv.org/abs/2510.10927", "authors": ["Yawen Yang", "Fukun Ma", "Shiao Meng", "Aiwei Liu", "Lijie Wen"], "title": "GapDNER: A Gap-Aware Grid Tagging Model for Discontinuous Named Entity Recognition", "comment": "Accepted by IJCNN 2025", "summary": "In biomedical fields, one named entity may consist of a series of\nnon-adjacent tokens and overlap with other entities. Previous methods recognize\ndiscontinuous entities by connecting entity fragments or internal tokens, which\nface challenges of error propagation and decoding ambiguity due to the wide\nvariety of span or word combinations. To address these issues, we deeply\nexplore discontinuous entity structures and propose an effective Gap-aware grid\ntagging model for Discontinuous Named Entity Recognition, named GapDNER. Our\nGapDNER innovatively applies representation learning on the context gaps\nbetween entity fragments to resolve decoding ambiguity and enhance\ndiscontinuous NER performance. Specifically, we treat the context gap as an\nadditional type of span and convert span classification into a token-pair grid\ntagging task. Subsequently, we design two interactive components to\ncomprehensively model token-pair grid features from both intra- and inter-span\nperspectives. The intra-span regularity extraction module employs the biaffine\nmechanism along with linear attention to capture the internal regularity of\neach span, while the inter-span relation enhancement module utilizes\ncriss-cross attention to obtain semantic relations among different spans. At\nthe inference stage of entity decoding, we assign a directed edge to each\nentity fragment and context gap, then use the BFS algorithm to search for all\nvalid paths from the head to tail of grids with entity tags. Experimental\nresults on three datasets demonstrate that our GapDNER achieves new\nstate-of-the-art performance on discontinuous NER and exhibits remarkable\nadvantages in recognizing complex entity structures.", "AI": {"tldr": "\u63d0\u51faGapDNER\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u4e0a\u4e0b\u6587\u95f4\u9699\u4f5c\u4e3a\u989d\u5916\u8de8\u5ea6\u7c7b\u578b\uff0c\u4f7f\u7528\u7f51\u683c\u6807\u6ce8\u65b9\u6cd5\u89e3\u51b3\u975e\u8fde\u7eed\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4e2d\u7684\u89e3\u7801\u6a21\u7cca\u95ee\u9898\uff0c\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u9886\u57df\u4e2d\uff0c\u547d\u540d\u5b9e\u4f53\u53ef\u80fd\u7531\u4e00\u7cfb\u5217\u975e\u76f8\u90bb\u6807\u8bb0\u7ec4\u6210\u5e76\u4e0e\u5176\u4ed6\u5b9e\u4f53\u91cd\u53e0\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u8fde\u63a5\u5b9e\u4f53\u7247\u6bb5\u6216\u5185\u90e8\u6807\u8bb0\u6765\u8bc6\u522b\u975e\u8fde\u7eed\u5b9e\u4f53\uff0c\u4f46\u7531\u4e8e\u8de8\u5ea6\u6216\u5355\u8bcd\u7ec4\u5408\u7684\u591a\u6837\u6027\uff0c\u9762\u4e34\u9519\u8bef\u4f20\u64ad\u548c\u89e3\u7801\u6a21\u7cca\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faGapDNER\u6a21\u578b\uff0c\u5c06\u4e0a\u4e0b\u6587\u95f4\u9699\u89c6\u4e3a\u989d\u5916\u8de8\u5ea6\u7c7b\u578b\uff0c\u5c06\u8de8\u5ea6\u5206\u7c7b\u8f6c\u6362\u4e3a\u6807\u8bb0\u5bf9\u7f51\u683c\u6807\u6ce8\u4efb\u52a1\u3002\u8bbe\u8ba1\u4e24\u4e2a\u4ea4\u4e92\u7ec4\u4ef6\uff1a\u8de8\u5ea6\u5185\u89c4\u5f8b\u63d0\u53d6\u6a21\u5757\u4f7f\u7528\u53cc\u4eff\u5c04\u673a\u5236\u548c\u7ebf\u6027\u6ce8\u610f\u529b\u6355\u83b7\u6bcf\u4e2a\u8de8\u5ea6\u7684\u5185\u90e8\u89c4\u5f8b\uff1b\u8de8\u95f4\u5173\u7cfb\u589e\u5f3a\u6a21\u5757\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u83b7\u53d6\u4e0d\u540c\u8de8\u5ea6\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\u3002\u5728\u63a8\u7406\u9636\u6bb5\u4f7f\u7528BFS\u7b97\u6cd5\u641c\u7d22\u6709\u6548\u8def\u5f84\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGapDNER\u5728\u975e\u8fde\u7eedNER\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5728\u8bc6\u522b\u590d\u6742\u5b9e\u4f53\u7ed3\u6784\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "GapDNER\u901a\u8fc7\u521b\u65b0\u6027\u5730\u5bf9\u4e0a\u4e0b\u6587\u95f4\u9699\u8fdb\u884c\u8868\u793a\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u975e\u8fde\u7eedNER\u4e2d\u7684\u89e3\u7801\u6a21\u7cca\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u975e\u8fde\u7eed\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.10195", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10195", "abs": "https://arxiv.org/abs/2510.10195", "authors": ["Hong-Kun Zhang", "Xin Li", "Sikun Yang", "Zhihong Xia"], "title": "CauchyNet: Compact and Data-Efficient Learning using Holomorphic Activation Functions", "comment": null, "summary": "A novel neural network inspired by Cauchy's integral formula, is proposed for\nfunction approximation tasks that include time series forecasting, missing data\nimputation, etc. Hence, the novel neural network is named CauchyNet. By\nembedding real-valued data into the complex plane, CauchyNet efficiently\ncaptures complex temporal dependencies, surpassing traditional real-valued\nmodels in both predictive performance and computational efficiency. Grounded in\nCauchy's integral formula and supported by the universal approximation theorem,\nCauchyNet offers strong theoretical guarantees for function approximation. The\narchitecture incorporates complex-valued activation functions, enabling robust\nlearning from incomplete data while maintaining a compact parameter footprint\nand reducing computational overhead. Through extensive experiments in diverse\ndomains, including transportation, energy consumption, and epidemiological\ndata, CauchyNet consistently outperforms state-of-the-art models in predictive\naccuracy, often achieving a 50% lower mean absolute error with fewer\nparameters. These findings highlight CauchyNet's potential as an effective and\nefficient tool for data-driven predictive modeling, particularly in\nresource-constrained and data-scarce environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u67ef\u897f\u79ef\u5206\u516c\u5f0f\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edcCauchyNet\uff0c\u7528\u4e8e\u51fd\u6570\u903c\u8fd1\u4efb\u52a1\uff0c\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u548c\u7f3a\u5931\u6570\u636e\u586b\u8865\u7b49\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u5b9e\u503c\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u4e14\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u51fd\u6570\u903c\u8fd1\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5c06\u5b9e\u503c\u6570\u636e\u5d4c\u5165\u590d\u5e73\u9762\uff0c\u5229\u7528\u67ef\u897f\u79ef\u5206\u516c\u5f0f\u548c\u590d\u503c\u6fc0\u6d3b\u51fd\u6570\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\u6355\u83b7\u3002", "result": "\u5728\u4ea4\u901a\u3001\u80fd\u6e90\u6d88\u8017\u548c\u6d41\u884c\u75c5\u5b66\u6570\u636e\u7b49\u591a\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cCauchyNet\u5728\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u964d\u4f4e50%\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\u3002", "conclusion": "CauchyNet\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u9a71\u52a8\u9884\u6d4b\u5efa\u6a21\u5de5\u5177\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u548c\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.10930", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10930", "abs": "https://arxiv.org/abs/2510.10930", "authors": ["Katherine M. Collins", "Cedegao E. Zhang", "Graham Todd", "Lance Ying", "Mauricio Barba da Costa", "Ryan Liu", "Prafull Sharma", "Adrian Weller", "Ionatan Kuperwajs", "Lionel Wong", "Joshua B. Tenenbaum", "Thomas L. Griffiths"], "title": "Evaluating Language Models' Evaluations of Games", "comment": "Pre-print", "summary": "Reasoning is not just about solving problems -- it is also about evaluating\nwhich problems are worth solving at all. Evaluations of artificial intelligence\n(AI) systems primarily focused on problem solving, historically by studying how\nmodels play games such as chess and Go. In this paper, we advocate for a new\nparadigm that assesses AI systems' evaluation of games. First, we introduce a\nformalism for evaluating such evaluations. We then leverage a large-scale\ndataset of over $100$ novel board games and over 450 human judgments to compare\nevaluations produced by modern language and reasoning models against those of\npeople and symbolic computational agents. We consider two kinds of evaluative\nqueries: assessing the payoff (or fairness) and the funness of games. These\nqueries span two dimensions relevant to the design of evaluations of AI\nevaluations: how complex a query is to compute and how difficult a query is to\nquantify. Our results show that reasoning models are generally more aligned to\npeople in their evaluations of games than non-reasoning language models.\nHowever, we observe a non-monotonic relationship: as models get closer to\ngame-theoretic optimal, their fit to human data weakens. We also observe more\n\"jaggedness\" across models for assessing funness, in line with the greater\ndifficulty of quantifying this query. Across queries and games, reasoning\nmodels show highly variable and unpredictable resource usage when assessing\nqueries, pointing to the importance of imbuing more resource-rational\nmeta-reasoning in language and reasoning models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8bc4\u4f30AI\u7cfb\u7edf\u5bf9\u6e38\u620f\u8bc4\u4f30\u80fd\u529b\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7100\u591a\u4e2a\u65b0\u9896\u68cb\u76d8\u6e38\u620f\u548c450\u591a\u4eba\u5224\u65ad\uff0c\u6bd4\u8f83\u73b0\u4ee3\u8bed\u8a00\u63a8\u7406\u6a21\u578b\u4e0e\u4eba\u7c7b\u5728\u8bc4\u4f30\u6e38\u620f\u516c\u5e73\u6027\u548c\u8da3\u5473\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u4f20\u7edfAI\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u800c\u672c\u6587\u8ba4\u4e3a\u8bc4\u4f30AI\u7cfb\u7edf\u5bf9\u54ea\u4e9b\u95ee\u9898\u503c\u5f97\u89e3\u51b3\u7684\u5224\u65ad\u80fd\u529b\u540c\u6837\u91cd\u8981\uff0c\u9700\u8981\u5efa\u7acb\u8bc4\u4f30AI\u8bc4\u4f30\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u8bc4\u4f30AI\u8bc4\u4f30\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6bd4\u8f83\u8bed\u8a00\u6a21\u578b\u3001\u63a8\u7406\u6a21\u578b\u4e0e\u4eba\u7c7b\u5728\u6e38\u620f\u516c\u5e73\u6027\u548c\u8da3\u5473\u6027\u8bc4\u4f30\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u91cf\u5316\u96be\u5ea6\u4e24\u4e2a\u7ef4\u5ea6\u3002", "result": "\u63a8\u7406\u6a21\u578b\u5728\u6e38\u620f\u8bc4\u4f30\u4e0a\u6bd4\u975e\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u66f4\u63a5\u8fd1\u4eba\u7c7b\u5224\u65ad\uff0c\u4f46\u968f\u7740\u6a21\u578b\u63a5\u8fd1\u535a\u5f08\u8bba\u6700\u4f18\uff0c\u4e0e\u4eba\u7c7b\u6570\u636e\u7684\u62df\u5408\u5ea6\u53cd\u800c\u4e0b\u964d\u3002\u8bc4\u4f30\u8da3\u5473\u6027\u65f6\u6a21\u578b\u8868\u73b0\u66f4\u4e0d\u7a33\u5b9a\uff0c\u4e14\u63a8\u7406\u6a21\u578b\u5728\u8d44\u6e90\u4f7f\u7528\u4e0a\u9ad8\u5ea6\u53ef\u53d8\u3002", "conclusion": "\u9700\u8981\u5728\u8bed\u8a00\u548c\u63a8\u7406\u6a21\u578b\u4e2d\u6ce8\u5165\u66f4\u591a\u8d44\u6e90\u7406\u6027\u7684\u5143\u63a8\u7406\u80fd\u529b\uff0c\u4ee5\u63d0\u9ad8\u8bc4\u4f30\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u9884\u6d4b\u6027\u3002"}}
{"id": "2510.10201", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10201", "abs": "https://arxiv.org/abs/2510.10201", "authors": ["Jinghao Zhang", "Naishan Zheng", "Ruilin Li", "Dongzhou Cheng", "Zheming Liang", "Feng Zhao", "Jiaqi Wang"], "title": "RLFR: Extending Reinforcement Learning for LLMs with Flow Environment", "comment": "Project Website: https://jinghaoleven.github.io/RLFR/", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na promising framework for improving reasoning abilities in Large Language\nModels (LLMs). However, policy optimized with binary verification prone to\noverlook potential valuable exploration in reasoning trajectory. In view of\nheavy annotation cost of golden Process Reward Models (PRMs), recent works\nattempt using auxiliary signals for reward shaping of process tokens, involving\nentropy and likelihood collected from logit space. In this work, we offer a\nnovel perspective on shaping RLVR with flow rewards derived from latent space,\nand propose RLFR, where the flow fields of model latents are constructed from\neither off-policy high-quality data and on-policy rejection sampling data, and\nthe velocity deviations of policy latents within it are quantified to serve as\na reward signal. RLFR first demonstrates that a well-established flow field can\nbe a sound environment for reward signal collection, highlighting the\nexpressive latent space is much underexplored. Moreover, RLFR is able to\ncompress any off-policy expert data as reference for constituting reward\nsignals, and we show that the efficient context dependence compressed within\nthe hidden states are utilized, rather than individual token-level denotation\nfor context comprehending. Experiments on both language and multimodal\nreasoning benchmarks demonstrate the reliability of flow rewards, and\nsuggesting a promising paradigm for reward shaping with auxiliary signals.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRLFR\u65b9\u6cd5\uff0c\u5229\u7528\u6f5c\u5728\u7a7a\u95f4\u7684\u6d41\u573a\u6784\u5efa\u5956\u52b1\u4fe1\u53f7\uff0c\u901a\u8fc7\u91cf\u5316\u7b56\u7565\u6f5c\u5728\u5728\u6d41\u573a\u4e2d\u7684\u901f\u5ea6\u504f\u5dee\u6765\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4e8c\u5143\u9a8c\u8bc1\u7684\u5f3a\u5316\u5b66\u4e60\u5bb9\u6613\u5ffd\u7565\u63a8\u7406\u8f68\u8ff9\u4e2d\u6709\u4ef7\u503c\u7684\u63a2\u7d22\uff0c\u800c\u6784\u5efa\u9ec4\u91d1\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u71b5\u548c\u4f3c\u7136\u5ea6\u7b49\u8f85\u52a9\u4fe1\u53f7\u8fdb\u884c\u5956\u52b1\u5851\u9020\uff0c\u4f46\u6f5c\u5728\u7a7a\u95f4\u7684\u8868\u8fbe\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "RLFR\u4ece\u79bb\u7b56\u7565\u9ad8\u8d28\u91cf\u6570\u636e\u6216\u5728\u7ebf\u62d2\u7edd\u91c7\u6837\u6570\u636e\u6784\u5efa\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u7684\u6d41\u573a\uff0c\u91cf\u5316\u7b56\u7565\u6f5c\u5728\u5728\u6d41\u573a\u4e2d\u7684\u901f\u5ea6\u504f\u5dee\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u538b\u7f29\u4efb\u4f55\u79bb\u7b56\u7565\u4e13\u5bb6\u6570\u636e\u4f5c\u4e3a\u5956\u52b1\u53c2\u8003\uff0c\u5e76\u5229\u7528\u9690\u85cf\u72b6\u6001\u4e2d\u7684\u9ad8\u6548\u4e0a\u4e0b\u6587\u4f9d\u8d56\u3002", "result": "\u5728\u8bed\u8a00\u548c\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u6d41\u5956\u52b1\u7684\u53ef\u9760\u6027\uff0c\u8868\u660e\u57fa\u4e8e\u6d41\u573a\u7684\u5956\u52b1\u5851\u9020\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u8303\u5f0f\u3002", "conclusion": "RLFR\u8bc1\u660e\u4e86\u826f\u597d\u5efa\u7acb\u7684\u6d41\u573a\u53ef\u4ee5\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u6536\u96c6\u7684\u6709\u6548\u73af\u5883\uff0c\u5c55\u793a\u4e86\u6f5c\u5728\u7a7a\u95f4\u8868\u8fbe\u80fd\u529b\u5728\u5956\u52b1\u5851\u9020\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.10936", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10936", "abs": "https://arxiv.org/abs/2510.10936", "authors": ["Anirudh Ganesh", "Jayavardhan Reddy"], "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF: A Reproducibility Study", "comment": null, "summary": "We present a reproducibility study of the state-of-the-art neural\narchitecture for sequence labeling proposed by Ma and Hovy\n(2016)\\cite{ma2016end}. The original BiLSTM-CNN-CRF model combines\ncharacter-level representations via Convolutional Neural Networks (CNNs),\nword-level context modeling through Bi-directional Long Short-Term Memory\nnetworks (BiLSTMs), and structured prediction using Conditional Random Fields\n(CRFs). This end-to-end approach eliminates the need for hand-crafted features\nwhile achieving excellent performance on named entity recognition (NER) and\npart-of-speech (POS) tagging tasks. Our implementation successfully reproduces\nthe key results, achieving 91.18\\% F1-score on CoNLL-2003 NER and demonstrating\nthe model's effectiveness across sequence labeling tasks. We provide a detailed\nanalysis of the architecture components and release an open-source PyTorch\nimplementation to facilitate further research.", "AI": {"tldr": "\u6210\u529f\u590d\u73b0\u4e86Ma\u548cHovy(2016)\u63d0\u51fa\u7684BiLSTM-CNN-CRF\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u5b57\u7b26\u7ea7CNN\u8868\u793a\u3001\u8bcd\u7ea7BiLSTM\u4e0a\u4e0b\u6587\u5efa\u6a21\u548cCRF\u7ed3\u6784\u5316\u9884\u6d4b\uff0c\u5728NER\u548cPOS\u6807\u6ce8\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9a8c\u8bc1\u548c\u590d\u73b0\u6700\u5148\u8fdb\u7684\u5e8f\u5217\u6807\u6ce8\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5206\u6790\u5176\u7ec4\u4ef6\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90\u5b9e\u73b0\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "method": "\u4f7f\u7528BiLSTM-CNN-CRF\u67b6\u6784\uff1a\u5b57\u7b26\u7ea7CNN\u63d0\u53d6\u5b57\u7b26\u8868\u793a\uff0cBiLSTM\u5efa\u6a21\u8bcd\u7ea7\u4e0a\u4e0b\u6587\uff0cCRF\u8fdb\u884c\u7ed3\u6784\u5316\u9884\u6d4b\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\u65e0\u9700\u624b\u5de5\u7279\u5f81\u3002", "result": "\u5728CoNLL-2003 NER\u4efb\u52a1\u4e0a\u8fbe\u523091.18%\u7684F1\u5206\u6570\uff0c\u6210\u529f\u590d\u73b0\u4e86\u539f\u8bba\u6587\u7684\u5173\u952e\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u6a21\u578b\u5728\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u590d\u73b0\u4e86BiLSTM-CNN-CRF\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\u5f3a\u5927\u57fa\u51c6\u7684\u6709\u6548\u6027\uff0c\u5f00\u6e90\u5b9e\u73b0\u5c06\u6709\u52a9\u4e8e\u793e\u533a\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.10211", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10211", "abs": "https://arxiv.org/abs/2510.10211", "authors": ["Yida Xiong", "Jiameng Chen", "Kun Li", "Hongzhi Zhang", "Xiantao Cai", "Wenbin Hu"], "title": "Hierarchical Bayesian Flow Networks for Molecular Graph Generation", "comment": null, "summary": "Molecular graph generation is essentially a classification generation\nproblem, aimed at predicting categories of atoms and bonds. Currently,\nprevailing paradigms such as continuous diffusion models are trained to predict\ncontinuous numerical values, treating the training process as a regression\ntask. However, the final generation necessitates a rounding step to convert\nthese predictions back into discrete classification categories, which is\nintrinsically a classification operation. Given that the rounding operation is\nnot incorporated during training, there exists a significant discrepancy\nbetween the model's training objective and its inference procedure. As a\nconsequence, an excessive emphasis on point-wise precision can lead to\noverfitting and inefficient learning. This occurs because considerable efforts\nare devoted to capturing intra-bin variations that are ultimately irrelevant to\nthe discrete nature of the task at hand. Such a flaw results in diminished\nmolecular diversity and constrains the model's generalization capabilities. To\naddress this fundamental limitation, we propose GraphBFN, a novel hierarchical\ncoarse-to-fine framework based on Bayesian Flow Networks that operates on the\nparameters of distributions. By innovatively introducing Cumulative\nDistribution Function, GraphBFN is capable of calculating the probability of\nselecting the correct category, thereby unifying the training objective with\nthe sampling rounding operation. We demonstrate that our method achieves\nsuperior performance and faster generation, setting new state-of-the-art\nresults on the QM9 and ZINC250k molecular graph generation benchmarks.", "AI": {"tldr": "GraphBFN\u662f\u4e00\u4e2a\u57fa\u4e8e\u8d1d\u53f6\u65af\u6d41\u7f51\u7edc\u7684\u5206\u5b50\u56fe\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\u7edf\u4e00\u8bad\u7ec3\u76ee\u6807\u4e0e\u91c7\u6837\u820d\u5165\u64cd\u4f5c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8fde\u7eed\u6269\u6563\u6a21\u578b\u5728\u8bad\u7ec3\u4e0e\u63a8\u7406\u9636\u6bb5\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u7684\u8fde\u7eed\u6269\u6563\u6a21\u578b\u5728\u8bad\u7ec3\u65f6\u9884\u6d4b\u8fde\u7eed\u6570\u503c\uff08\u56de\u5f52\u4efb\u52a1\uff09\uff0c\u4f46\u5728\u63a8\u7406\u65f6\u9700\u8981\u820d\u5165\u64cd\u4f5c\u8f6c\u6362\u4e3a\u79bb\u6563\u7c7b\u522b\uff08\u5206\u7c7b\u64cd\u4f5c\uff09\uff0c\u8fd9\u79cd\u8bad\u7ec3\u4e0e\u63a8\u7406\u7684\u4e0d\u4e00\u81f4\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u5173\u6ce8\u70b9\u5bf9\u70b9\u7cbe\u5ea6\uff0c\u9650\u5236\u4e86\u5206\u5b50\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86GraphBFN\uff0c\u4e00\u4e2a\u57fa\u4e8e\u8d1d\u53f6\u65af\u6d41\u7f51\u7edc\u7684\u5206\u5c42\u4ece\u7c97\u5230\u7ec6\u6846\u67b6\uff0c\u5728\u5206\u5e03\u53c2\u6570\u4e0a\u64cd\u4f5c\u3002\u901a\u8fc7\u521b\u65b0\u6027\u5730\u5f15\u5165\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\uff0c\u80fd\u591f\u8ba1\u7b97\u9009\u62e9\u6b63\u786e\u7c7b\u522b\u7684\u6982\u7387\uff0c\u4ece\u800c\u7edf\u4e00\u8bad\u7ec3\u76ee\u6807\u4e0e\u91c7\u6837\u820d\u5165\u64cd\u4f5c\u3002", "result": "\u5728QM9\u548cZINC250k\u5206\u5b50\u56fe\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u66f4\u5feb\u7684\u751f\u6210\u901f\u5ea6\uff0c\u521b\u9020\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "GraphBFN\u901a\u8fc7\u7edf\u4e00\u8bad\u7ec3\u4e0e\u63a8\u7406\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fde\u7eed\u6269\u6563\u6a21\u578b\u5728\u5206\u5b50\u56fe\u751f\u6210\u4e2d\u7684\u6839\u672c\u9650\u5236\uff0c\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2510.10951", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10951", "abs": "https://arxiv.org/abs/2510.10951", "authors": ["Eitan Klinger", "Vivaan Wadhwa", "Jungyeul Park"], "title": "Punctuation-aware treebank tree binarization", "comment": null, "summary": "This article presents a curated resource and evaluation suite for\npunctuation-aware treebank binarization. Standard binarization pipelines drop\npunctuation before head selection, which alters constituent shape and harms\nhead-child identification. We release (1) a reproducible pipeline that\npreserves punctuation as sibling nodes prior to binarization, (2) derived\nartifacts and metadata (intermediate @X markers, reversibility signatures,\nalignment indices), and (3) an accompanying evaluation suite covering\nhead-child prediction, round-trip reversibility, and structural compatibility\nwith derivational resources (CCGbank). On the Penn Treebank, punctuation-aware\npreprocessing improves head prediction accuracy from 73.66\\% (Collins rules)\nand 86.66\\% (MLP) to 91.85\\% with the same classifier, and achieves competitive\nalignment against CCGbank derivations. All code, configuration files, and\ndocumentation are released to enable replication and extension to other\ncorpora.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6807\u70b9\u7b26\u53f7\u611f\u77e5\u7684\u6811\u5e93\u4e8c\u503c\u5316\u8d44\u6e90\u4e0e\u8bc4\u4f30\u5957\u4ef6\uff0c\u901a\u8fc7\u4fdd\u7559\u6807\u70b9\u7b26\u53f7\u4f5c\u4e3a\u5144\u5f1f\u8282\u70b9\u6765\u6539\u8fdb\u6807\u51c6\u4e8c\u503c\u5316\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5934\u90e8\u9884\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u6807\u51c6\u7684\u4e8c\u503c\u5316\u6d41\u7a0b\u5728\u5934\u90e8\u9009\u62e9\u524d\u4f1a\u4e22\u5f03\u6807\u70b9\u7b26\u53f7\uff0c\u8fd9\u4f1a\u6539\u53d8\u6210\u5206\u7ed3\u6784\u5e76\u635f\u5bb3\u5934\u90e8-\u5b50\u8282\u70b9\u8bc6\u522b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u6d41\u7a0b\uff0c\u5728\u4e8c\u503c\u5316\u524d\u5c06\u6807\u70b9\u7b26\u53f7\u4fdd\u7559\u4e3a\u5144\u5f1f\u8282\u70b9\uff0c\u5e76\u63d0\u4f9b\u4e86\u6d3e\u751f\u5de5\u4ef6\u3001\u5143\u6570\u636e\u548c\u8bc4\u4f30\u5957\u4ef6\u3002", "result": "\u5728Penn Treebank\u4e0a\uff0c\u6807\u70b9\u7b26\u53f7\u611f\u77e5\u9884\u5904\u7406\u5c06\u5934\u90e8\u9884\u6d4b\u51c6\u786e\u7387\u4ece73.66%\uff08Collins\u89c4\u5219\uff09\u548c86.66%\uff08MLP\uff09\u63d0\u5347\u523091.85%\uff0c\u5e76\u5728CCGbank\u63a8\u5bfc\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6240\u6709\u4ee3\u7801\u3001\u914d\u7f6e\u6587\u4ef6\u548c\u6587\u6863\u5747\u5df2\u53d1\u5e03\uff0c\u652f\u6301\u590d\u73b0\u5e76\u6269\u5c55\u5230\u5176\u4ed6\u8bed\u6599\u5e93\u3002"}}
{"id": "2510.10232", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10232", "abs": "https://arxiv.org/abs/2510.10232", "authors": ["Xuening Wu", "Shenqin Yin", "Yanlan Kang", "Xinhang Zhang", "Qianya Xu", "Zeping Chen", "Wenqiang Zhang"], "title": "SGM: A Statistical Godel Machine for Risk-Controlled Recursive Self-Modification", "comment": null, "summary": "Recursive self-modification is increasingly central in AutoML, neural\narchitecture search, and adaptive optimization, yet no existing framework\nensures that such changes are made safely. Godel machines offer a principled\nsafeguard by requiring formal proofs of improvement before rewriting code;\nhowever, such proofs are unattainable in stochastic, high-dimensional settings.\nWe introduce the Statistical Godel Machine (SGM), the first statistical safety\nlayer for recursive edits. SGM replaces proof-based requirements with\nstatistical confidence tests (e-values, Hoeffding bounds), admitting a\nmodification only when superiority is certified at a chosen confidence level,\nwhile allocating a global error budget to bound cumulative risk across\nrounds.We also propose Confirm-Triggered Harmonic Spending (CTHS), which\nindexes spending by confirmation events rather than rounds, concentrating the\nerror budget on promising edits while preserving familywise\nvalidity.Experiments across supervised learning, reinforcement learning, and\nblack-box optimization validate this role: SGM certifies genuine gains on\nCIFAR-100, rejects spurious improvement on ImageNet-100, and demonstrates\nrobustness on RL and optimization benchmarks.Together, these results position\nSGM as foundational infrastructure for continual, risk-aware self-modification\nin learning systems.Code is available at:\nhttps://github.com/gravitywavelet/sgm-anon.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7edf\u8ba1\u54e5\u5fb7\u5c14\u673a\uff08SGM\uff09\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u9012\u5f52\u7f16\u8f91\u7684\u7edf\u8ba1\u5b89\u5168\u5c42\uff0c\u7528\u7edf\u8ba1\u7f6e\u4fe1\u5ea6\u6d4b\u8bd5\u53d6\u4ee3\u57fa\u4e8e\u8bc1\u660e\u7684\u8981\u6c42\uff0c\u5728\u9009\u5b9a\u7684\u7f6e\u4fe1\u6c34\u5e73\u4e0b\u4ec5\u5f53\u6539\u8fdb\u88ab\u8bc1\u660e\u65f6\u624d\u5141\u8bb8\u4fee\u6539\uff0c\u540c\u65f6\u5206\u914d\u5168\u5c40\u9519\u8bef\u9884\u7b97\u6765\u9650\u5236\u591a\u8f6e\u7d2f\u79ef\u98ce\u9669\u3002", "motivation": "\u9012\u5f52\u81ea\u4fee\u6539\u5728AutoML\u3001\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u548c\u81ea\u9002\u5e94\u4f18\u5316\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6846\u67b6\u65e0\u6cd5\u786e\u4fdd\u6b64\u7c7b\u66f4\u6539\u7684\u5b89\u5168\u6027\u3002\u54e5\u5fb7\u5c14\u673a\u901a\u8fc7\u8981\u6c42\u5728\u91cd\u5199\u4ee3\u7801\u524d\u63d0\u4f9b\u5f62\u5f0f\u5316\u6539\u8fdb\u8bc1\u660e\u6765\u63d0\u4f9b\u539f\u5219\u6027\u4fdd\u969c\uff0c\u4f46\u5728\u968f\u673a\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u6b64\u7c7b\u8bc1\u660e\u65e0\u6cd5\u83b7\u5f97\u3002", "method": "\u5f15\u5165\u7edf\u8ba1\u54e5\u5fb7\u5c14\u673a\uff08SGM\uff09\uff0c\u7528\u7edf\u8ba1\u7f6e\u4fe1\u5ea6\u6d4b\u8bd5\uff08e\u503c\u3001Hoeffding\u754c\uff09\u66ff\u4ee3\u57fa\u4e8e\u8bc1\u660e\u7684\u8981\u6c42\uff1b\u63d0\u51fa\u786e\u8ba4\u89e6\u53d1\u8c10\u6ce2\u652f\u51fa\uff08CTHS\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u786e\u8ba4\u4e8b\u4ef6\u800c\u975e\u8f6e\u6b21\u6765\u7d22\u5f15\u652f\u51fa\uff0c\u5c06\u9519\u8bef\u9884\u7b97\u96c6\u4e2d\u5728\u6709\u5e0c\u671b\u7684\u7f16\u8f91\u4e0a\u540c\u65f6\u4fdd\u6301\u65cf\u6709\u6548\u6027\u3002", "result": "\u5728\u76d1\u7763\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u9ed1\u76d2\u4f18\u5316\u7684\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86SGM\u7684\u4f5c\u7528\uff1a\u5728CIFAR-100\u4e0a\u8ba4\u8bc1\u4e86\u771f\u5b9e\u589e\u76ca\uff0c\u5728ImageNet-100\u4e0a\u62d2\u7edd\u4e86\u865a\u5047\u6539\u8fdb\uff0c\u5728RL\u548c\u4f18\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5c06SGM\u5b9a\u4f4d\u4e3a\u5b66\u4e60\u7cfb\u7edf\u4e2d\u6301\u7eed\u3001\u98ce\u9669\u611f\u77e5\u81ea\u4fee\u6539\u7684\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2510.10961", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.10961", "abs": "https://arxiv.org/abs/2510.10961", "authors": ["Yejin Lee", "Su-Hyeon Kim", "Hyundong Jin", "Dayoung Kim", "Yeonsoo Kim", "Yo-Sub Han"], "title": "KOTOX: A Korean Toxic Dataset for Deobfuscation and Detoxification", "comment": "25 pages, 5 figures, 25 tables", "summary": "Toxic content has become an increasingly critical social issue with the rapid\nexpansion of online communication. While numerous studies explored methods for\ndetecting and detoxifying such content, most have focused primarily on English,\nleaving low-resource language underrepresented. Consequently, Large Language\nModels~(LLMs) often struggle to identify and neutralize toxic expressions in\nthese languages. This challenge becomes even more pronounced when user employ\nobfuscation techniques to evade detection systems. Therefore, we propose a\n\\textbf{KOTOX: Korean Toxic Dataset} for deobfuscation and detoxicification to\naddress this issue. We categorize various obfuscation approaches based on\nlinguistic characteristics of Korean and define a set of transformation rules\ngrounded in real-word examples. Using these rules, we construct three dataset\nversions (easy, normal, and hard) representing different levels of obfuscation\ndifficulty. This is the first dataset that simultaneously supports\ndeobfuscation and detoxification for the Korean language. We expect it to\nfacilitate better understanding and mitigating of obfuscated toxic content in\nLLM for low-resource languages. Our code and data are available at\nhttps://github.com/leeyejin1231/KOTOX.", "AI": {"tldr": "\u63d0\u51fa\u4e86KOTOX\uff1a\u9996\u4e2a\u540c\u65f6\u652f\u6301\u97e9\u8bed\u53bb\u6df7\u6dc6\u548c\u53bb\u6bd2\u5316\u7684\u6570\u636e\u96c6\uff0c\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u9690\u6666\u6bd2\u6027\u5185\u5bb9\u95ee\u9898\u3002", "motivation": "\u5728\u7ebf\u4ea4\u6d41\u4e2d\u6bd2\u6027\u5185\u5bb9\u65e5\u76ca\u4e25\u91cd\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u82f1\u8bed\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u4ee3\u8868\u6027\u4e0d\u8db3\uff0cLLMs\u96be\u4ee5\u8bc6\u522b\u548c\u4e2d\u548c\u8fd9\u4e9b\u8bed\u8a00\u4e2d\u7684\u6bd2\u6027\u8868\u8fbe\uff0c\u7279\u522b\u662f\u5f53\u7528\u6237\u4f7f\u7528\u6df7\u6dc6\u6280\u672f\u9003\u907f\u68c0\u6d4b\u65f6\u3002", "method": "\u57fa\u4e8e\u97e9\u8bed\u8bed\u8a00\u7279\u5f81\u5206\u7c7b\u5404\u79cd\u6df7\u6dc6\u65b9\u6cd5\uff0c\u5b9a\u4e49\u57fa\u4e8e\u771f\u5b9e\u793a\u4f8b\u7684\u8f6c\u6362\u89c4\u5219\uff0c\u6784\u5efa\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\uff08\u7b80\u5355\u3001\u666e\u901a\u3001\u56f0\u96be\uff09\u7684\u6570\u636e\u96c6\u7248\u672c\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u540c\u65f6\u652f\u6301\u97e9\u8bed\u53bb\u6df7\u6dc6\u548c\u53bb\u6bd2\u5316\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e0d\u540c\u6df7\u6dc6\u96be\u5ea6\u7684\u4e09\u4e2a\u7248\u672c\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u548c\u51cf\u8f7b\u4f4e\u8d44\u6e90\u8bed\u8a00LLM\u4e2d\u6df7\u6dc6\u6bd2\u6027\u5185\u5bb9\u7684\u95ee\u9898\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.10244", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10244", "abs": "https://arxiv.org/abs/2510.10244", "authors": ["Ziyu Zhou", "Keyan Hu", "Ling Zhang", "Zhaohui Xue", "Yutian Fang", "Yusha Zheng"], "title": "Progressive Scale Convolutional Network for Spatio-Temporal Downscaling of Soil Moisture: A Case Study Over the Tibetan Plateau", "comment": null, "summary": "Soil moisture (SM) plays a critical role in hydrological and meteorological\nprocesses. High-resolution SM can be obtained by combining coarse passive\nmicrowave data with fine-scale auxiliary variables. However, the inversion of\nSM at the temporal scale is hindered by the incompleteness of surface auxiliary\nfactors. To address this issue, first, we introduce validated high temporal\nresolution ERA5-Land variables into the downscaling process of the\nlow-resolution SMAP SM product. Subsequently, we design a progressive scale\nconvolutional network (PSCNet), at the core of which are two innovative\ncomponents: a multi-frequency temporal fusion module (MFTF) for capturing\ntemporal dynamics, and a bespoke squeeze-and-excitation (SE) block designed to\npreserve fine-grained spatial details. Using this approach, we obtained\nseamless SM products for the Tibetan Plateau (TP) from 2016 to 2018 at 10-km\nspatial and 3-hour temporal resolution. The experimental results on the TP\ndemonstrated the following: 1) In the satellite product validation, the PSCNet\nexhibited comparable accuracy and lower error, with a mean R value of 0.881,\noutperforming other methods. 2) In the in-situ site validation, PSCNet\nconsistently ranked among the top three models for the R metric across all\nsites, while also showing superior performance in overall error reduction. 3)\nIn the temporal generalization validation, the feasibility of using\nhigh-temporal resolution ERA5-Land variables for downscaling was confirmed, as\nall methods maintained an average relative error within 6\\% for the R metric\nand 2\\% for the ubRMSE metric. 4) In the temporal dynamics and visualization\nvalidation, PSCNet demonstrated excellent temporal sensitivity and vivid\nspatial details. Overall, PSCNet provides a promising solution for\nspatio-temporal downscaling by effectively modeling the intricate\nspatio-temporal relationships in SM data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u5c3a\u5ea6\u5377\u79ef\u7f51\u7edc\uff08PSCNet\uff09\uff0c\u7ed3\u5408ERA5-Land\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u6570\u636e\uff0c\u5c06\u4f4e\u5206\u8fa8\u7387SMAP\u571f\u58e4\u6c34\u5206\u4ea7\u54c1\u964d\u5c3a\u5ea6\u523010\u516c\u91cc\u7a7a\u95f4\u5206\u8fa8\u7387\u548c3\u5c0f\u65f6\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u5728\u9752\u85cf\u9ad8\u539f\u5730\u533a\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u571f\u58e4\u6c34\u5206\u65f6\u95f4\u5c3a\u5ea6\u53cd\u6f14\u4e2d\u56e0\u5730\u8868\u8f85\u52a9\u56e0\u5b50\u4e0d\u5b8c\u6574\u800c\u53d7\u963b\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387ERA5-Land\u53d8\u91cf\u6765\u6539\u8fdb\u4f4e\u5206\u8fa8\u7387SMAP\u4ea7\u54c1\u7684\u964d\u5c3a\u5ea6\u8fc7\u7a0b\u3002", "method": "\u8bbe\u8ba1\u4e86PSCNet\u7f51\u7edc\uff0c\u5305\u542b\u591a\u9891\u65f6\u95f4\u878d\u5408\u6a21\u5757\uff08MFTF\uff09\u6355\u6349\u65f6\u95f4\u52a8\u6001\uff0c\u4ee5\u53ca\u5b9a\u5236\u7684\u6324\u538b\u6fc0\u52b1\uff08SE\uff09\u5757\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u7ec6\u8282\uff0c\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u5c3a\u5ea6\u5377\u79ef\u3002", "result": "\u5728\u9752\u85cf\u9ad8\u539f2016-2018\u5e74\u7684\u9a8c\u8bc1\u4e2d\uff1a1\uff09\u536b\u661f\u4ea7\u54c1\u9a8c\u8bc1R\u503c0.881\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff1b2\uff09\u7ad9\u70b9\u9a8c\u8bc1\u5728\u6240\u6709\u7ad9\u70b9R\u6307\u6807\u5747\u6392\u540d\u524d\u4e09\uff1b3\uff09\u65f6\u95f4\u6cdb\u5316\u9a8c\u8bc1\u76f8\u5bf9\u8bef\u5dee\u63a7\u5236\u57286%\u4ee5\u5185\uff1b4\uff09\u65f6\u95f4\u52a8\u6001\u548c\u53ef\u89c6\u5316\u9a8c\u8bc1\u663e\u793a\u4f18\u5f02\u7684\u65f6\u95f4\u654f\u611f\u6027\u548c\u7a7a\u95f4\u7ec6\u8282\u3002", "conclusion": "PSCNet\u901a\u8fc7\u6709\u6548\u5efa\u6a21\u571f\u58e4\u6c34\u5206\u6570\u636e\u4e2d\u590d\u6742\u7684\u65f6\u7a7a\u5173\u7cfb\uff0c\u4e3a\u65f6\u7a7a\u964d\u5c3a\u5ea6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10965", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10965", "abs": "https://arxiv.org/abs/2510.10965", "authors": ["Jidong Li", "Lingyong Fang", "Haodong Zhao", "Sufeng Duan", "Gongshen Liu"], "title": "Judge Before Answer: Can MLLM Discern the False Premise in Question?", "comment": null, "summary": "Multimodal large language models (MLLMs) have witnessed astonishing\nadvancements in recent years. Despite these successes, MLLMs remain vulnerable\nto flase premise problems. However, existing benchmarks targeting this issue\nare limited in scope: they often lack fine-grained categorization, exhibit\ninsufficient coverage, and thus fail to provide a rigorous evaluation of the\nability of models to recognize false premises. To bridge this gap, we introduce\na fully automated pipeline for constructing a comprehensive benchmark of false\npremise questions. Our method systematically categorizes the premises into\nthree main types and thirteen subtypes according to the abilities required to\nidentify the premises, resulting in the JBA dataset.Results show current MLLMs\nstill struggle with false premise recognition. Building upon this benchmark, we\nfurther propose a recognition enhancement framework tailored to strengthen the\nrobustness of MLLMs to detect false premises. Extensive experiments demonstrate\nthat models trained with our framework achieve significant improvements in\nfalse premise recognition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6784\u5efa\u865a\u5047\u524d\u63d0\u95ee\u9898\u57fa\u51c6\u7684\u65b9\u6cd5\uff0c\u521b\u5efa\u4e86JBA\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u589e\u5f3aMLLMs\u8bc6\u522b\u865a\u5047\u524d\u63d0\u80fd\u529b\u7684\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u865a\u5047\u524d\u63d0\u95ee\u9898\u4e0a\u5b58\u5728\u8106\u5f31\u6027\uff0c\u800c\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u5206\u7c7b\u548c\u8db3\u591f\u8986\u76d6\u8303\u56f4\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u6a21\u578b\u8bc6\u522b\u865a\u5047\u524d\u63d0\u7684\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u6d41\u6c34\u7ebf\u6765\u6784\u5efa\u865a\u5047\u524d\u63d0\u95ee\u9898\u57fa\u51c6\uff0c\u5c06\u524d\u63d0\u7cfb\u7edf\u5206\u7c7b\u4e3a3\u4e2a\u4e3b\u8981\u7c7b\u578b\u548c13\u4e2a\u5b50\u7c7b\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e13\u95e8\u7684\u8bc6\u522b\u589e\u5f3a\u6846\u67b6\u3002", "result": "\u5f53\u524dMLLMs\u5728\u865a\u5047\u524d\u63d0\u8bc6\u522b\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u56f0\u96be\uff0c\u4f46\u4f7f\u7528\u6240\u63d0\u6846\u67b6\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u865a\u5047\u524d\u63d0\u8bc6\u522b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u865a\u5047\u524d\u63d0\u8bc4\u4f30\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u589e\u5f3a\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86MLLMs\u8bc6\u522b\u865a\u5047\u524d\u63d0\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.10248", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10248", "abs": "https://arxiv.org/abs/2510.10248", "authors": ["Jiaxi Zhuang", "Yaorui Shi", "Jue Hou", "Yunong He", "Mingwei Ye", "Mingjun Xu", "Yuming Su", "Linfeng Zhang", "Linfeng Zhang", "Guolin Ke", "Hengxing Cai"], "title": "Reasoning-Enhanced Large Language Models for Molecular Property Prediction", "comment": null, "summary": "Molecular property prediction is crucial for drug discovery and materials\nscience, yet existing approaches suffer from limited interpretability, poor\ncross-task generalization, and lack of chemical reasoning capabilities.\nTraditional machine learning models struggle with task transferability, while\nspecialized molecular language models provide little insight into their\ndecision-making processes. To address these limitations, we propose\n\\textbf{MPPReasoner}, a multimodal large language model that incorporates\nchemical reasoning for molecular property prediction. Our approach, built upon\nQwen2.5-VL-7B-Instruct, integrates molecular images with SMILES strings to\nenable comprehensive molecular understanding. We develop a two-stage training\nstrategy: supervised fine-tuning (SFT) using 16,000 high-quality reasoning\ntrajectories generated through expert knowledge and multiple teacher models,\nfollowed by Reinforcement Learning from Principle-Guided Rewards (RLPGR). RLPGR\nemploys verifiable, rule-based rewards that systematically evaluate chemical\nprinciple application, molecular structure analysis, and logical consistency\nthrough computational verification. Extensive experiments across 8 datasets\ndemonstrate significant performance improvements, with MPPReasoner\noutperforming the best baselines by 7.91\\% and 4.53\\% on in-distribution and\nout-of-distribution tasks respectively. MPPReasoner exhibits exceptional\ncross-task generalization and generates chemically sound reasoning paths that\nprovide valuable insights into molecular property analysis, substantially\nenhancing both interpretability and practical utility for chemists. Code is\navailable at https://anonymous.4open.science/r/MPPReasoner-12687.", "AI": {"tldr": "\u63d0\u51fa\u4e86MPPReasoner\uff0c\u4e00\u79cd\u7528\u4e8e\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u5206\u5b50\u56fe\u50cf\u548cSMILES\u5b57\u7b26\u4e32\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u5f31\u3001\u7f3a\u4e4f\u5316\u5b66\u63a8\u7406\u80fd\u529b\u7b49\u95ee\u9898\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4efb\u52a1\u8fc1\u79fb\u6027\u5dee\uff0c\u4e13\u7528\u5206\u5b50\u8bed\u8a00\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\u3002", "method": "\u57fa\u4e8eQwen2.5-VL-7B-Instruct\u6784\u5efa\uff0c\u6574\u5408\u5206\u5b50\u56fe\u50cf\u548cSMILES\u5b57\u7b26\u4e32\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u4f7f\u752816,000\u4e2a\u9ad8\u8d28\u91cf\u63a8\u7406\u8f68\u8ff9\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u7136\u540e\u8fdb\u884c\u57fa\u4e8e\u539f\u5219\u6307\u5bfc\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u89c4\u5219\u5956\u52b1\u8bc4\u4f30\u5316\u5b66\u539f\u7406\u5e94\u7528\u3001\u5206\u5b50\u7ed3\u6784\u5206\u6790\u548c\u903b\u8f91\u4e00\u81f4\u6027\u3002", "result": "\u57288\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cMPPReasoner\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u5206\u522b\u6bd4\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e867.91%\u548c4.53%\uff0c\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u80fd\u751f\u6210\u5316\u5b66\u4e0a\u5408\u7406\u7684\u63a8\u7406\u8def\u5f84\u3002", "conclusion": "MPPReasoner\u663e\u8457\u63d0\u5347\u4e86\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u7684\u6027\u80fd\u3001\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u5316\u5b66\u5bb6\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5206\u5b50\u6027\u8d28\u5206\u6790\u89c1\u89e3\u3002"}}
{"id": "2510.10971", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.10971", "abs": "https://arxiv.org/abs/2510.10971", "authors": ["Yejin Lee", "Hyeseon Ahn", "Yo-Sub Han"], "title": "RV-HATE: Reinforced Multi-Module Voting for Implicit Hate Speech Detection", "comment": "10 pages, 9 figures, 12 tables", "summary": "Hate speech remains prevalent in human society and continues to evolve in its\nforms and expressions. Modern advancements in internet and online anonymity\naccelerate its rapid spread and complicate its detection. However, hate speech\ndatasets exhibit diverse characteristics primarily because they are constructed\nfrom different sources and platforms, each reflecting different linguistic\nstyles and social contexts. Despite this diversity, prior studies on hate\nspeech detection often rely on fixed methodologies without adapting to\ndata-specific features. We introduce RV-HATE, a detection framework designed to\naccount for the dataset-specific characteristics of each hate speech dataset.\nRV-HATE consists of multiple specialized modules, where each module focuses on\ndistinct linguistic or contextual features of hate speech. The framework\nemploys reinforcement learning to optimize weights that determine the\ncontribution of each module for a given dataset. A voting mechanism then\naggregates the module outputs to produce the final decision. RV-HATE offers two\nprimary advantages: (1)~it improves detection accuracy by tailoring the\ndetection process to dataset-specific attributes, and (2)~it also provides\ninterpretable insights into the distinctive features of each dataset.\nConsequently, our approach effectively addresses implicit hate speech and\nachieves superior performance compared to conventional static methods. Our code\nis available at https://github.com/leeyejin1231/RV-HATE.", "AI": {"tldr": "RV-HATE\u662f\u4e00\u4e2a\u9488\u5bf9\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u591a\u4e2a\u4e13\u95e8\u6a21\u5757\u7684\u6743\u91cd\uff0c\u9002\u5e94\u4e0d\u540c\u6570\u636e\u96c6\u7684\u7279\u5b9a\u7279\u5f81\uff0c\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4ec7\u6068\u8a00\u8bba\u5728\u5f62\u5f0f\u548c\u8868\u8fbe\u4e0a\u4e0d\u65ad\u6f14\u53d8\uff0c\u4e0d\u540c\u6765\u6e90\u548c\u5e73\u53f0\u7684\u6570\u636e\u96c6\u5177\u6709\u591a\u6837\u6027\u7279\u5f81\uff0c\u4f46\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5f80\u5f80\u91c7\u7528\u56fa\u5b9a\u65b9\u6cd5\u800c\u4e0d\u9002\u5e94\u6570\u636e\u7279\u5b9a\u7279\u5f81\u3002", "method": "RV-HATE\u5305\u542b\u591a\u4e2a\u4e13\u95e8\u6a21\u5757\uff0c\u6bcf\u4e2a\u6a21\u5757\u5173\u6ce8\u4ec7\u6068\u8a00\u8bba\u7684\u4e0d\u540c\u8bed\u8a00\u6216\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6a21\u5757\u6743\u91cd\uff0c\u5e76\u901a\u8fc7\u6295\u7968\u673a\u5236\u805a\u5408\u6a21\u5757\u8f93\u51fa\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u6548\u5904\u7406\u9690\u542b\u4ec7\u6068\u8a00\u8bba\uff0c\u76f8\u6bd4\u4f20\u7edf\u9759\u6001\u65b9\u6cd5\u83b7\u5f97\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "RV-HATE\u901a\u8fc7\u9002\u5e94\u6570\u636e\u96c6\u7279\u5b9a\u7279\u5f81\u63d0\u9ad8\u4e86\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.10262", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10262", "abs": "https://arxiv.org/abs/2510.10262", "authors": ["Jingwen Li", "Zhiguang Cao", "Yaoxin Wu", "Tang Liu"], "title": "Enhancing the Cross-Size Generalization for Solving Vehicle Routing Problems via Continual Learning", "comment": null, "summary": "Exploring machine learning techniques for addressing vehicle routing problems\nhas attracted considerable research attention. To achieve decent and efficient\nsolutions, existing deep models for vehicle routing problems are typically\ntrained and evaluated using instances of a single size. This substantially\nlimits their ability to generalize across different problem sizes and thus\nhampers their practical applicability. To address the issue, we propose a\ncontinual learning based framework that sequentially trains a deep model with\ninstances of ascending problem sizes. Specifically, on the one hand, we design\nan inter-task regularization scheme to retain the knowledge acquired from\nsmaller problem sizes in the model training on a larger size. On the other\nhand, we introduce an intra-task regularization scheme to consolidate the model\nby imitating the latest desirable behaviors during training on each size.\nAdditionally, we exploit the experience replay to revisit instances of formerly\ntrained sizes for mitigating the catastrophic forgetting. Experimental results\nshow that our approach achieves predominantly superior performance across\nvarious problem sizes (either seen or unseen in the training), as compared to\nstate-of-the-art deep models including the ones specialized for\ngeneralizability enhancement. Meanwhile, the ablation studies on the key\ndesigns manifest their synergistic effect in the proposed framework.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6301\u7eed\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6309\u95ee\u9898\u89c4\u6a21\u9012\u589e\u7684\u987a\u5e8f\u8bad\u7ec3\u6df1\u5ea6\u6a21\u578b\u6765\u89e3\u51b3\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u7684\u6cdb\u5316\u80fd\u529b\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u6a21\u578b\u901a\u5e38\u9488\u5bf9\u5355\u4e00\u89c4\u6a21\u95ee\u9898\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u95ee\u9898\u89c4\u6a21\u95f4\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u6309\u95ee\u9898\u89c4\u6a21\u9012\u589e\u987a\u5e8f\u8bad\u7ec3\u6a21\u578b\uff1b\u8bbe\u8ba1\u4efb\u52a1\u95f4\u6b63\u5219\u5316\u4fdd\u7559\u5c0f\u89c4\u6a21\u77e5\u8bc6\uff0c\u4efb\u52a1\u5185\u6b63\u5219\u5316\u6a21\u4eff\u6700\u65b0\u884c\u4e3a\uff0c\u5e76\u5229\u7528\u7ecf\u9a8c\u56de\u653e\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u95ee\u9898\u89c4\u6a21\uff08\u5305\u62ec\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u89c4\u6a21\uff09\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u6a21\u578b\uff0c\u5305\u62ec\u4e13\u95e8\u7528\u4e8e\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u7684\u6a21\u578b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u6df1\u5ea6\u6a21\u578b\u5728\u4e0d\u540c\u89c4\u6a21\u95f4\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5173\u952e\u8bbe\u8ba1\u7684\u534f\u540c\u6548\u5e94\u3002"}}
{"id": "2510.10974", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10974", "abs": "https://arxiv.org/abs/2510.10974", "authors": ["Zhiwen Ruan", "Yixia Li", "He Zhu", "Yun Chen", "Peng Li", "Yang Liu", "Guanhua Chen"], "title": "Enhancing Large Language Model Reasoning via Selective Critical Token Fine-Tuning", "comment": null, "summary": "Large language models (LLMs) primarily rely on supervised fine-tuning (SFT)\nas a key method to adapt pre-trained models to domain-specific tasks such as\nmathematical reasoning. However, standard SFT uniformly penalizes all tokens,\nneglecting that only a small subset of critical tokens determines reasoning\ncorrectness. This uniform supervision often causes reduced output diversity and\nlimited generalization. We propose Critical Token Fine-tuning (CFT), a simple\nyet effective approach that updates only tokens identified as functionally\nindispensable via counterfactual perturbations. By focusing gradient signals on\nthese decisive reasoning steps while preserving the diversity of non-critical\ntokens, CFT can enhance both generation and diversity. Extensive experiments on\nfive models across three families (Qwen, OLMo, LLaMA) and eleven mathematical\nreasoning benchmarks show that CFT, despite fine-tuning on less than 12% of\ntokens, consistently outperforms standard SFT. Moreover, CFT enables test-time\nscaling through improved sampling diversity and provides a stronger\ninitialization for reinforcement learning, sustaining performance gains in\nlater training stages while maintaining higher entropy for better exploration.\nThese results highlight CFT as a practical and general framework for efficient\nand robust LLM fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5173\u952e\u4ee4\u724c\u5fae\u8c03(CFT)\u65b9\u6cd5\uff0c\u53ea\u66f4\u65b0\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6270\u52a8\u8bc6\u522b\u7684\u529f\u80fd\u4e0d\u53ef\u6216\u7f3a\u7684\u4ee4\u724c\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6807\u51c6\u76d1\u7763\u5fae\u8c03(SFT)\u3002", "motivation": "\u6807\u51c6SFT\u5bf9\u6240\u6709\u4ee4\u724c\u8fdb\u884c\u7edf\u4e00\u60e9\u7f5a\uff0c\u5ffd\u89c6\u4e86\u53ea\u6709\u5c11\u91cf\u5173\u952e\u4ee4\u724c\u51b3\u5b9a\u63a8\u7406\u6b63\u786e\u6027\uff0c\u5bfc\u81f4\u8f93\u51fa\u591a\u6837\u6027\u51cf\u5c11\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "CFT\u65b9\u6cd5\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6270\u52a8\u8bc6\u522b\u5173\u952e\u4ee4\u724c\uff0c\u53ea\u5bf9\u8fd9\u4e9b\u51b3\u5b9a\u6027\u63a8\u7406\u6b65\u9aa4\u8fdb\u884c\u68af\u5ea6\u66f4\u65b0\uff0c\u540c\u65f6\u4fdd\u7559\u975e\u5173\u952e\u4ee4\u724c\u7684\u591a\u6837\u6027\u3002", "result": "\u57285\u4e2a\u6a21\u578b\u300111\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCFT\u4ec5\u5fae\u8c03\u4e0d\u523012%\u7684\u4ee4\u724c\uff0c\u4f46\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6SFT\uff0c\u5e76\u6539\u5584\u4e86\u91c7\u6837\u591a\u6837\u6027\u548c\u5f3a\u5316\u5b66\u4e60\u521d\u59cb\u5316\u3002", "conclusion": "CFT\u662f\u4e00\u4e2a\u5b9e\u7528\u4e14\u901a\u7528\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u548c\u7a33\u5065\u7684LLM\u5fae\u8c03\uff0c\u5728\u4fdd\u6301\u66f4\u9ad8\u71b5\u7684\u540c\u65f6\u7ef4\u6301\u6027\u80fd\u589e\u76ca\u3002"}}
{"id": "2510.10276", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.10276", "abs": "https://arxiv.org/abs/2510.10276", "authors": ["Nikolaus Salvatore", "Hao Wang", "Qiong Zhang"], "title": "Lost in the Middle: An Emergent Property from Information Retrieval Demands in LLMs", "comment": null, "summary": "The performance of Large Language Models (LLMs) often degrades when crucial\ninformation is in the middle of a long context, a \"lost-in-the-middle\"\nphenomenon that mirrors the primacy and recency effects in human memory. We\npropose that this behavior is not simply a flaw indicative of information loss\nbut an adaptation to different information retrieval demands during\npre-training: some tasks require uniform recall across the entire input (a\nlong-term memory demand), while others prioritize the most recent information\n(a short-term memory demand). Consistent with this view, we show that this\nU-shaped performance curve emerges when LLMs (GPT-2 and Llama variants) are\ntrained from scratch on two simple human memory paradigms simulating long-term\nand short-term memory demands. Our analysis reveals that while the recency\neffect directly aligns with short-term memory demand in the training data, the\nprimacy effect is induced by the uniform long-term memory demand and is\nadditionally influenced by the model's autoregressive properties and the\nformation of attention sinks. Our main findings from simple human memory\nparadigms also generalize to a sequence completion task, which more closely\nresembles the next-token prediction process in LLM pre-training. Together, our\nfindings reveal how information retrieval demands, model architecture, and\nstructural attention dynamics during model training can jointly produce\npositional bias observed in LLMs.", "AI": {"tldr": "LLMs\u5728\u957f\u6587\u672c\u4e2d\u8868\u73b0\u51fa'\u4e2d\u95f4\u4fe1\u606f\u4e22\u5931'\u73b0\u8c61\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u8bb0\u5fc6\u7684\u9996\u56e0\u6548\u5e94\u548c\u8fd1\u56e0\u6548\u5e94\u7c7b\u4f3c\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u4e0d\u662f\u7b80\u5355\u7684\u4fe1\u606f\u4e22\u5931\u7f3a\u9677\uff0c\u800c\u662f\u9884\u8bad\u7ec3\u4e2d\u4e0d\u540c\u4fe1\u606f\u68c0\u7d22\u9700\u6c42\u9002\u5e94\u7684\u7ed3\u679c\u3002", "motivation": "\u63a2\u7a76LLMs\u5728\u957f\u6587\u672c\u4e2d\u4fe1\u606f\u68c0\u7d22\u6027\u80fd\u4e0b\u964d\u7684\u539f\u56e0\uff0c\u7279\u522b\u662f\u4e2d\u95f4\u4fe1\u606f\u4e22\u5931\u73b0\u8c61\uff0c\u5e76\u7406\u89e3\u5176\u4e0e\u4eba\u7c7b\u8bb0\u5fc6\u6548\u5e94\u7684\u76f8\u4f3c\u6027\u3002", "method": "\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3GPT-2\u548cLlama\u53d8\u4f53\u6a21\u578b\uff0c\u4f7f\u7528\u6a21\u62df\u4eba\u7c7b\u957f\u671f\u548c\u77ed\u671f\u8bb0\u5fc6\u9700\u6c42\u7684\u7b80\u5355\u8303\u5f0f\uff0c\u5206\u6790\u5e8f\u5217\u5b8c\u6210\u4efb\u52a1\u3002", "result": "\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u786e\u5b9e\u51fa\u73b0\u4e86U\u5f62\u6027\u80fd\u66f2\u7ebf\uff0c\u8fd1\u56e0\u6548\u5e94\u4e0e\u77ed\u671f\u8bb0\u5fc6\u9700\u6c42\u76f4\u63a5\u5bf9\u5e94\uff0c\u9996\u56e0\u6548\u5e94\u7531\u957f\u671f\u8bb0\u5fc6\u9700\u6c42\u8bf1\u5bfc\uff0c\u5e76\u53d7\u81ea\u56de\u5f52\u7279\u6027\u548c\u6ce8\u610f\u529b\u6c47\u805a\u5f71\u54cd\u3002", "conclusion": "\u4fe1\u606f\u68c0\u7d22\u9700\u6c42\u3001\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u6784\u6ce8\u610f\u529b\u52a8\u6001\u5171\u540c\u5bfc\u81f4\u4e86LLMs\u4e2d\u89c2\u5bdf\u5230\u7684\u4f4d\u7f6e\u504f\u5dee\u3002"}}
{"id": "2510.10994", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10994", "abs": "https://arxiv.org/abs/2510.10994", "authors": ["Wei-Chieh Huang", "Henry Peng Zou", "Yaozu Wu", "Dongyuan Li", "Yankai Chen", "Weizhi Zhang", "Yangning Li", "Angelo Zangari", "Jizhou Guo", "Chunyu Miao", "Liancheng Fang", "Langzhou He", "Renhe Jiang", "Philip S. Yu"], "title": "DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety", "comment": null, "summary": "Deep research frameworks have shown promising capabilities in synthesizing\ncomprehensive reports from web sources. While deep research possesses\nsignificant potential to address complex issues through planning and research\ncycles, existing frameworks are deficient in sufficient evaluation procedures\nand stage-specific protections. They typically treat evaluation as exact match\naccuracy of question-answering, but overlook crucial aspects of report quality\nsuch as credibility, coherence, breadth, depth, and safety. This oversight may\nresult in hazardous or malicious sources being integrated into the final\nreport. To address these issues, we introduce DEEPRESEARCHGUARD, a\ncomprehensive framework featuring four-stage safeguards with open-domain\nevaluation of references and reports. We assess performance across multiple\nmetrics, e.g., defense success rate and over-refusal rate, and five key report\ndimensions. In the absence of a suitable safety benchmark, we introduce\nDRSAFEBENCH, a stage-wise benchmark for deep research safety. Our evaluation\nspans diverse state-of-the-art LLMs, including GPT-4o, Gemini-2.5-flash,\nDeepSeek-v3, and o4-mini. DEEPRESEARCHGUARD achieves an average defense success\nrate improvement of 18.16% while reducing over-refusal rate by 6%. The input\nguard provides the most substantial early-stage protection by filtering out\nobvious risks, while the plan and research guards enhance citation discipline\nand source credibility. Through extensive experiments, we show that\nDEEPRESEARCHGUARD enables comprehensive open-domain evaluation and stage-aware\ndefenses that effectively block harmful content propagation, while\nsystematically improving report quality without excessive over-refusal rates.\nThe code can be found via https://github.com/Jasonya/DeepResearchGuard.", "AI": {"tldr": "\u63d0\u51fa\u4e86DEEPRESEARCHGUARD\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u9636\u6bb5\u4fdd\u62a4\u673a\u5236\u548c\u5f00\u653e\u57df\u8bc4\u4f30\u6765\u89e3\u51b3\u6df1\u5ea6\u7814\u7a76\u6846\u67b6\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5728\u63d0\u9ad8\u9632\u5fa1\u6210\u529f\u738718.16%\u7684\u540c\u65f6\u964d\u4f4e\u8fc7\u5ea6\u62d2\u7edd\u73876%\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u7814\u7a76\u6846\u67b6\u7f3a\u4e4f\u5145\u5206\u7684\u8bc4\u4f30\u7a0b\u5e8f\u548c\u9636\u6bb5\u7279\u5b9a\u4fdd\u62a4\uff0c\u901a\u5e38\u53ea\u5173\u6ce8\u95ee\u7b54\u51c6\u786e\u7387\u800c\u5ffd\u89c6\u62a5\u544a\u8d28\u91cf\u7684\u5173\u952e\u65b9\u9762\uff08\u53ef\u4fe1\u5ea6\u3001\u8fde\u8d2f\u6027\u3001\u5e7f\u5ea6\u3001\u6df1\u5ea6\u548c\u5b89\u5168\u6027\uff09\uff0c\u53ef\u80fd\u5bfc\u81f4\u5371\u9669\u6216\u6076\u610f\u6e90\u88ab\u6574\u5408\u5230\u6700\u7ec8\u62a5\u544a\u4e2d\u3002", "method": "\u5f15\u5165DEEPRESEARCHGUARD\u6846\u67b6\uff0c\u5305\u542b\u56db\u9636\u6bb5\u4fdd\u62a4\u673a\u5236\u548c\u5f00\u653e\u57df\u8bc4\u4f30\uff0c\u4f7f\u7528DRSAFEBENCH\u4f5c\u4e3a\u9636\u6bb5\u5316\u5b89\u5168\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u6700\u5148\u8fdbLLM\u6a21\u578b\u3002", "result": "DEEPRESEARCHGUARD\u5e73\u5747\u9632\u5fa1\u6210\u529f\u7387\u63d0\u9ad818.16%\uff0c\u8fc7\u5ea6\u62d2\u7edd\u7387\u964d\u4f4e6%\u3002\u8f93\u5165\u9632\u62a4\u63d0\u4f9b\u6700\u663e\u8457\u7684\u524d\u671f\u4fdd\u62a4\uff0c\u8ba1\u5212\u548c\u7814\u7a76\u9632\u62a4\u589e\u5f3a\u4e86\u5f15\u7528\u7eaa\u5f8b\u548c\u6e90\u53ef\u4fe1\u5ea6\u3002", "conclusion": "DEEPRESEARCHGUARD\u80fd\u591f\u5b9e\u73b0\u5168\u9762\u7684\u5f00\u653e\u57df\u8bc4\u4f30\u548c\u9636\u6bb5\u611f\u77e5\u9632\u5fa1\uff0c\u6709\u6548\u963b\u6b62\u6709\u5bb3\u5185\u5bb9\u4f20\u64ad\uff0c\u540c\u65f6\u7cfb\u7edf\u6027\u5730\u63d0\u9ad8\u62a5\u544a\u8d28\u91cf\u800c\u4e0d\u8fc7\u5ea6\u62d2\u7edd\u3002"}}
{"id": "2510.10278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10278", "abs": "https://arxiv.org/abs/2510.10278", "authors": ["Christopher Chiu", "Silviu Pitis", "Mihaela van der Schaar"], "title": "Simulating Viva Voce Examinations to Evaluate Clinical Reasoning in Large Language Models", "comment": null, "summary": "Clinical reasoning in medicine is a hypothesis-driven process where\nphysicians refine diagnoses from limited information through targeted history,\nphysical examination, and diagnostic investigations. In contrast, current\nmedical benchmarks for large language models (LLMs) primarily assess knowledge\nrecall through single-turn questions, where complete clinical information is\nprovided upfront. To address this gap, we introduce VivaBench, a multi-turn\nbenchmark that evaluates sequential clinical reasoning in LLM agents. Our\ndataset consists of 1762 physician-curated clinical vignettes structured as\ninteractive scenarios that simulate a (oral) examination in medical training,\nrequiring agents to actively probe for relevant findings, select appropriate\ninvestigations, and synthesize information across multiple steps to reach a\ndiagnosis. While current LLMs demonstrate competence in diagnosing conditions\nfrom well-described clinical presentations, their performance degrades\nsignificantly when required to navigate iterative diagnostic reasoning under\nuncertainty in our evaluation. Our analysis identified several failure modes\nthat mirror common cognitive errors in clinical practice, including: (1)\nfixation on initial hypotheses, (2) inappropriate investigation ordering, (3)\npremature diagnostic closure, and (4) failing to screen for critical\nconditions. These patterns reveal fundamental limitations in how current LLMs\nreason and make decisions under uncertainty. Through VivaBench, we provide a\nstandardized benchmark for evaluating conversational medical AI systems for\nreal-world clinical decision support. Beyond medical applications, we\ncontribute to the larger corpus of research on agentic AI by demonstrating how\nsequential reasoning trajectories can diverge in complex decision-making\nenvironments.", "AI": {"tldr": "VivaBench\u662f\u4e00\u4e2a\u591a\u8f6e\u4e34\u5e8a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30LLM\u5728\u6a21\u62df\u533b\u5b66\u8003\u8bd5\u4e2d\u7684\u8fed\u4ee3\u8bca\u65ad\u80fd\u529b\uff0c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u8fdb\u884c\u987a\u5e8f\u63a8\u7406\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u8bc4\u4f30\u77e5\u8bc6\u56de\u5fc6\uff0c\u800c\u771f\u5b9e\u7684\u4e34\u5e8a\u63a8\u7406\u662f\u5047\u8bbe\u9a71\u52a8\u7684\u8fed\u4ee3\u8fc7\u7a0b\uff0c\u9700\u8981\u4ece\u6709\u9650\u4fe1\u606f\u4e2d\u9010\u6b65\u5b8c\u5584\u8bca\u65ad\u3002", "method": "\u6784\u5efa1762\u4e2a\u533b\u751f\u7b56\u5212\u7684\u4e34\u5e8a\u6848\u4f8b\uff0c\u6a21\u62df\u533b\u5b66\u53e3\u8bd5\u573a\u666f\uff0c\u8981\u6c42LLM\u4ee3\u7406\u4e3b\u52a8\u63a2\u67e5\u76f8\u5173\u4fe1\u606f\u3001\u9009\u62e9\u9002\u5f53\u68c0\u67e5\uff0c\u5e76\u901a\u8fc7\u591a\u6b65\u9aa4\u5408\u6210\u4fe1\u606f\u8fbe\u5230\u8bca\u65ad\u3002", "result": "\u5f53\u524dLLM\u5728\u5b8c\u6574\u4e34\u5e8a\u63cf\u8ff0\u4e0b\u8bca\u65ad\u80fd\u529b\u826f\u597d\uff0c\u4f46\u5728\u8fed\u4ee3\u8bca\u65ad\u63a8\u7406\u4e2d\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u51fa\u73b0\u56db\u79cd\u5e38\u89c1\u8ba4\u77e5\u9519\u8bef\u6a21\u5f0f\u3002", "conclusion": "VivaBench\u4e3a\u8bc4\u4f30\u5bf9\u8bdd\u5f0f\u533b\u5b66AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u63a8\u7406\u51b3\u7b56\u7684\u57fa\u672c\u5c40\u9650\u6027\u3002"}}
{"id": "2510.10304", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10304", "abs": "https://arxiv.org/abs/2510.10304", "authors": ["Michael Y. Hu", "Benjamin Van Durme", "Jacob Andreas", "Harsh Jhamtani"], "title": "Sample-Efficient Online Learning in LM Agents via Hindsight Trajectory Rewriting", "comment": null, "summary": "Language model (LM) agents deployed in novel environments often exhibit poor\nsample efficiency when learning from sequential interactions. This\nsignificantly hinders the usefulness of such agents in environments where\ninteraction is costly (for example, when they interact with humans or reset\nphysical systems). While a number of existing LM agent architectures\nincorporate various mechanisms for experience storage and reflection, they make\nlimited use of LMs' abilities to directly generate or reason about full\ncounterfactual trajectories. We introduce ECHO (Experience Consolidation via\nHindsight Optimization), a prompting framework that adapts hindsight experience\nreplay from reinforcement learning for language model agents. ECHO generates\noptimized trajectories for alternative goals that could have been achieved\nduring failed attempts, effectively creating synthetic positive examples from\nunsuccessful interactions. Our approach consists of two components: a hindsight\nrule that uses the language model itself to identify relevant subgoals and\ngenerate optimized trajectories, and an update rule that maintains compressed\ntrajectory representations in memory. We evaluate ECHO on stateful versions of\nXMiniGrid, a text-based navigation and planning benchmark, and PeopleJoinQA, a\ncollaborative information-gathering enterprise simulation. Across both domains,\nECHO outperforms vanilla language agent baselines by up to 80%; in XMiniGrid,\nit also outperforms a number of sophisticated agent architectures including\nReflexion and AWM, demonstrating faster adaptation to novel environments\nthrough more effective utilization of past experiences.", "AI": {"tldr": "ECHO\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u540e\u7ecf\u9a8c\u56de\u653e\u6280\u672f\u4ece\u5931\u8d25\u5c1d\u8bd5\u4e2d\u751f\u6210\u4f18\u5316\u7684\u66ff\u4ee3\u8f68\u8ff9\uff0c\u63d0\u9ad8\u5728\u964c\u751f\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u5728\u65b0\u73af\u5883\u4e2d\u5b66\u4e60\u6548\u7387\u4f4e\uff0c\u4ea4\u4e92\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u67b6\u6784\u672a\u80fd\u5145\u5206\u5229\u7528\u8bed\u8a00\u6a21\u578b\u751f\u6210\u548c\u63a8\u7406\u5b8c\u6574\u53cd\u4e8b\u5b9e\u8f68\u8ff9\u7684\u80fd\u529b\u3002", "method": "ECHO\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a\u4e8b\u540e\u89c4\u5219\uff08\u8bc6\u522b\u76f8\u5173\u5b50\u76ee\u6807\u5e76\u751f\u6210\u4f18\u5316\u8f68\u8ff9\uff09\u548c\u66f4\u65b0\u89c4\u5219\uff08\u7ef4\u62a4\u538b\u7f29\u7684\u8f68\u8ff9\u8868\u793a\uff09\u3002", "result": "\u5728XMiniGrid\u548cPeopleJoinQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cECHO\u6bd4\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe80%\uff0c\u5728XMiniGrid\u4e2d\u4f18\u4e8eReflexion\u548cAWM\u7b49\u590d\u6742\u67b6\u6784\u3002", "conclusion": "ECHO\u901a\u8fc7\u66f4\u6709\u6548\u5730\u5229\u7528\u8fc7\u53bb\u7ecf\u9a8c\uff0c\u5b9e\u73b0\u4e86\u5728\u964c\u751f\u73af\u5883\u4e2d\u66f4\u5feb\u7684\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2510.11001", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11001", "abs": "https://arxiv.org/abs/2510.11001", "authors": ["Tieyuan Chen", "Xiaodong Chen", "Haoxing Chen", "Zhenzhong Lan", "Weiyao Lin", "Jianguo Li"], "title": "DND: Boosting Large Language Models with Dynamic Nested Depth", "comment": "TL;DR: We introduce Dynamic Nested Depth (DND), an efficient paradigm\n  that adaptively identifies critical tokens and selectively deepens their\n  computation via nested re-processing", "summary": "We introduce Dynamic Nested Depth (DND), a novel method that improves\nperformance for off-the-shelf LLMs by selecting critical tokens to reprocess in\na nested depth manner. Specifically, at the end of the given transformer layer,\nDND identifies more critical tokens with a router and feeds them back for an\nextra round of processing, effectively ``reviewing\" difficult tokens while\navoiding redundant computation for easier ones. The dynamic selection mechanism\nis tailored for precise control via two novel strategies: a router controlling\nloss to enhance token selection distinguishability, and a threshold control\nscheme to ensure selection stability. We demonstrate the effectiveness of DND\nby directly integrating it into pre-trained dense and MoE models during a\npost-training phase. On diverse benchmarks, this approach boosts the\nperformances of the dense Qwen3-1.7B by 1.88% and the MoE Qwen3-30B-A3B by\n0.87%, all with a minimal parameter and computing increase.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u52a8\u6001\u5d4c\u5957\u6df1\u5ea6\uff08DND\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728Transformer\u5c42\u672b\u7aef\u9009\u62e9\u5173\u952etoken\u8fdb\u884c\u5d4c\u5957\u6df1\u5ea6\u5904\u7406\uff0c\u63d0\u5347LLM\u6027\u80fd\uff0c\u540c\u65f6\u907f\u514d\u5bf9\u7b80\u5355token\u7684\u5197\u4f59\u8ba1\u7b97\u3002", "motivation": "\u73b0\u6709LLM\u5728\u5904\u7406\u590d\u6742token\u65f6\u6548\u7387\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u52a8\u6001\u8bc6\u522b\u5e76\u91cd\u65b0\u5904\u7406\u5173\u952etoken\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u800c\u4e0d\u663e\u8457\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u3002", "method": "DND\u5728Transformer\u5c42\u672b\u7aef\u4f7f\u7528\u8def\u7531\u5668\u8bc6\u522b\u5173\u952etoken\uff0c\u5c06\u5176\u53cd\u9988\u8fdb\u884c\u989d\u5916\u5904\u7406\u3002\u91c7\u7528\u8def\u7531\u5668\u63a7\u5236\u635f\u5931\u548c\u9608\u503c\u63a7\u5236\u65b9\u6848\u6765\u786e\u4fdd\u9009\u62e9\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDND\u4f7f\u7a20\u5bc6\u6a21\u578bQwen3-1.7B\u6027\u80fd\u63d0\u53471.88%\uff0c\u6df7\u5408\u4e13\u5bb6\u6a21\u578bQwen3-30B-A3B\u6027\u80fd\u63d0\u53470.87%\uff0c\u53c2\u6570\u548c\u8ba1\u7b97\u5f00\u9500\u4ec5\u8f7b\u5fae\u589e\u52a0\u3002", "conclusion": "DND\u662f\u4e00\u79cd\u6709\u6548\u7684\u540e\u8bad\u7ec3\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u9884\u8bad\u7ec3LLM\u7684\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5904\u7406\u590d\u6742token\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u53ef\u63a7\u3002"}}
{"id": "2510.10341", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10341", "abs": "https://arxiv.org/abs/2510.10341", "authors": ["Shiyu Chen", "Ningyuan", "Huang", "Soledad Villar"], "title": "Multi-View Graph Learning with Graph-Tuple", "comment": "Submitted to TAG workshop", "summary": "Graph Neural Networks (GNNs) typically scale with the number of graph edges,\nmaking them well suited for sparse graphs but less efficient on dense graphs,\nsuch as point clouds or molecular interactions. A common remedy is to sparsify\nthe graph via similarity thresholding or distance pruning, but this forces an\narbitrary choice of a single interaction scale and discards crucial information\nfrom other scales. To overcome this limitation, we introduce a multi-view\ngraph-tuple framework. Instead of a single graph, our graph-tuple framework\npartitions the graph into disjoint subgraphs, capturing primary local\ninteractions and weaker, long-range connections. We then learn multi-view\nrepresentations from the graph-tuple via a heterogeneous message-passing\narchitecture inspired by the theory of non-commuting operators, which we\nformally prove is strictly more expressive and guarantees a lower oracle risk\ncompared to single-graph message-passing models. We instantiate our framework\non two scientific domains: molecular property prediction from feature-scarce\nCoulomb matrices and cosmological parameter inference from geometric point\nclouds. On both applications, our multi-view graph-tuple models demonstrate\nbetter performance than single-graph baselines, highlighting the power and\nversatility of our multi-view approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u591a\u89c6\u56fe\u56fe\u5143\u7ec4\u6846\u67b6\u6765\u89e3\u51b3GNN\u5728\u7a20\u5bc6\u56fe\u4e0a\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u56fe\u5212\u5206\u4e3a\u4e0d\u76f8\u4ea4\u5b50\u56fe\u6765\u6355\u6349\u4e0d\u540c\u5c3a\u5ea6\u7684\u4ea4\u4e92\uff0c\u5e76\u8bc1\u660e\u8be5\u65b9\u6cd5\u6bd4\u5355\u56fe\u6d88\u606f\u4f20\u9012\u6a21\u578b\u66f4\u6709\u6548\u3002", "motivation": "\u4f20\u7edfGNN\u5728\u7a20\u5bc6\u56fe\uff08\u5982\u70b9\u4e91\u3001\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\uff09\u4e0a\u6548\u7387\u4f4e\u4e0b\uff0c\u5e38\u7528\u7684\u7a00\u758f\u5316\u65b9\u6cd5\u4f1a\u4e22\u5931\u591a\u5c3a\u5ea6\u4fe1\u606f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u6355\u6349\u5c40\u90e8\u548c\u957f\u7a0b\u4ea4\u4e92\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u591a\u89c6\u56fe\u56fe\u5143\u7ec4\u6846\u67b6\uff0c\u5c06\u56fe\u5212\u5206\u4e3a\u4e0d\u76f8\u4ea4\u5b50\u56fe\uff0c\u4f7f\u7528\u53d7\u975e\u4ea4\u6362\u7b97\u5b50\u7406\u8bba\u542f\u53d1\u7684\u5f02\u6784\u6d88\u606f\u4f20\u9012\u67b6\u6784\u5b66\u4e60\u591a\u89c6\u56fe\u8868\u793a\u3002", "result": "\u5728\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u548c\u5b87\u5b99\u5b66\u53c2\u6570\u63a8\u65ad\u4e24\u4e2a\u5e94\u7528\u4e0a\uff0c\u591a\u89c6\u56fe\u56fe\u5143\u7ec4\u6a21\u578b\u5747\u4f18\u4e8e\u5355\u56fe\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u591a\u89c6\u56fe\u56fe\u5143\u7ec4\u65b9\u6cd5\u5728\u4fdd\u6301\u8868\u8fbe\u529b\u7684\u540c\u65f6\u63d0\u9ad8\u4e86GNN\u5728\u7a20\u5bc6\u56fe\u4e0a\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u591a\u89c6\u56fe\u65b9\u6cd5\u7684\u5f3a\u5927\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2510.11031", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11031", "abs": "https://arxiv.org/abs/2510.11031", "authors": ["Yiwei Liu", "Yucheng Li", "Xiao Li", "Gong Cheng"], "title": "LogiNumSynth: Synthesizing Joint Logical-Numerical Reasoning Problems for Language Models", "comment": "30 pages, 3 figures", "summary": "Joint logical-numerical reasoning remains a major challenge for language\nmodels, yet existing datasets rely on fixed rule sets and offer limited control\nover task complexity, constraining their generalizability for evaluation and\ntraining. We present LogiNumSynth, a flexible natural language problem\nsynthesizer that synthesizes tasks requiring proficiency in joint logical\nreasoning (e.g., rule-based reasoning) and numerical reasoning (e.g.,\narithmetic computation). LogiNumSynth supports fine-grained control over\nreasoning world richness, logical reasoning depth, and the complexity of\nnumerical computations, enabling flexible data synthesis across difficulty\nlevels. We demonstrate three key contributions: (1) Synthesizer -- synthesizing\nfully controllable joint reasoning tasks over natural language; (2) Evaluation\n& Process Analysis -- evaluating both process accuracy and answer accuracy; (3)\nTargeted Training -- using synthesized data to enhance LLMs' reasoning\nperformance. Experiments with multiple LLMs highlight persistent weaknesses in\nlogical-numerical reasoning, showing that LogiNumSynth can serve as both a\ndiagnostic tool and a source of targeted supervision for advancing integrated\nreasoning skills.", "AI": {"tldr": "LogiNumSynth\u662f\u4e00\u4e2a\u7075\u6d3b\u7684\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u5408\u6210\u5668\uff0c\u80fd\u591f\u751f\u6210\u9700\u8981\u903b\u8f91\u63a8\u7406\u548c\u6570\u503c\u63a8\u7406\u80fd\u529b\u7684\u8054\u5408\u63a8\u7406\u4efb\u52a1\uff0c\u652f\u6301\u5bf9\u63a8\u7406\u4e16\u754c\u4e30\u5bcc\u5ea6\u3001\u903b\u8f91\u63a8\u7406\u6df1\u5ea6\u548c\u6570\u503c\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u7cbe\u7ec6\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u5408\u903b\u8f91-\u6570\u503c\u63a8\u7406\u6570\u636e\u96c6\u4f9d\u8d56\u56fa\u5b9a\u89c4\u5219\u96c6\uff0c\u4efb\u52a1\u590d\u6742\u5ea6\u63a7\u5236\u6709\u9650\uff0c\u9650\u5236\u4e86\u5176\u5728\u8bc4\u4f30\u548c\u8bad\u7ec3\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f00\u53d1LogiNumSynth\u5408\u6210\u5668\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u63a7\u5236\u63a8\u7406\u4e16\u754c\u4e30\u5bcc\u5ea6\u3001\u903b\u8f91\u63a8\u7406\u6df1\u5ea6\u548c\u6570\u503c\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0\u8de8\u96be\u5ea6\u7ea7\u522b\u7684\u7075\u6d3b\u6570\u636e\u5408\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u591a\u4e2aLLM\u5728\u903b\u8f91-\u6570\u503c\u63a8\u7406\u65b9\u9762\u5b58\u5728\u6301\u7eed\u5f31\u70b9\uff0cLogiNumSynth\u65e2\u80fd\u4f5c\u4e3a\u8bca\u65ad\u5de5\u5177\uff0c\u4e5f\u80fd\u4e3a\u63d0\u5347\u96c6\u6210\u63a8\u7406\u6280\u80fd\u63d0\u4f9b\u9488\u5bf9\u6027\u76d1\u7763\u3002", "conclusion": "LogiNumSynth\u53ef\u4f5c\u4e3a\u8bca\u65ad\u5de5\u5177\u548c\u9488\u5bf9\u6027\u76d1\u7763\u6e90\uff0c\u7528\u4e8e\u63a8\u8fdb\u8bed\u8a00\u6a21\u578b\u7684\u96c6\u6210\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.10364", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10364", "abs": "https://arxiv.org/abs/2510.10364", "authors": ["Ali Mirzazadeh", "Simon Cadavid", "Kaiwen Zha", "Chao Li", "Sultan Alzahrani", "Manar Alawajy", "Joshua Korzenik", "Kreshnik Hoti", "Charles Reynolds", "David Mischoulon", "John Winkelman", "Maurizio Fava", "Dina Katabi"], "title": "Transformer Model Detects Antidepressant Use From a Single Night of Sleep, Unlocking an Adherence Biomarker", "comment": null, "summary": "Antidepressant nonadherence is pervasive, driving relapse, hospitalization,\nsuicide risk, and billions in avoidable costs. Clinicians need tools that\ndetect adherence lapses promptly, yet current methods are either invasive\n(serum assays, neuroimaging) or proxy-based and inaccurate (pill counts,\npharmacy refills). We present the first noninvasive biomarker that detects\nantidepressant intake from a single night of sleep. A transformer-based model\nanalyzes sleep data from a consumer wearable or contactless wireless sensor to\ninfer antidepressant intake, enabling remote, effortless, daily adherence\nassessment at home. Across six datasets comprising 62,000 nights from >20,000\nparticipants (1,800 antidepressant users), the biomarker achieved AUROC = 0.84,\ngeneralized across drug classes, scaled with dose, and remained robust to\nconcomitant psychotropics. Longitudinal monitoring captured real-world\ninitiation, tapering, and lapses. This approach offers objective, scalable\nadherence surveillance with potential to improve depression care and outcomes.", "AI": {"tldr": "\u5f00\u53d1\u9996\u4e2a\u975e\u4fb5\u5165\u6027\u751f\u7269\u6807\u5fd7\u7269\uff0c\u901a\u8fc7\u5355\u665a\u7761\u7720\u6570\u636e\u68c0\u6d4b\u6297\u6291\u90c1\u836f\u7269\u6444\u5165\uff0c\u5b9e\u73b0\u8fdc\u7a0b\u3001\u65e0\u611f\u7684\u65e5\u5e38\u4f9d\u4ece\u6027\u8bc4\u4f30", "motivation": "\u6297\u6291\u90c1\u836f\u7269\u4e0d\u4f9d\u4ece\u73b0\u8c61\u666e\u904d\uff0c\u5bfc\u81f4\u590d\u53d1\u3001\u4f4f\u9662\u3001\u81ea\u6740\u98ce\u9669\u589e\u52a0\u548c\u5de8\u989d\u6210\u672c\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4fb5\u5165\u6027\u5f3a\uff0c\u8981\u4e48\u57fa\u4e8e\u4ee3\u7406\u6307\u6807\u4e0d\u51c6\u786e\uff0c\u9700\u8981\u80fd\u53ca\u65f6\u68c0\u6d4b\u4f9d\u4ece\u6027\u95ee\u9898\u7684\u5de5\u5177", "method": "\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5206\u6790\u6765\u81ea\u6d88\u8d39\u7ea7\u53ef\u7a7f\u6234\u8bbe\u5907\u6216\u975e\u63a5\u89e6\u5f0f\u65e0\u7ebf\u4f20\u611f\u5668\u7684\u7761\u7720\u6570\u636e\uff0c\u63a8\u65ad\u6297\u6291\u90c1\u836f\u7269\u6444\u5165\u60c5\u51b5", "result": "\u57286\u4e2a\u6570\u636e\u96c6\uff0862,000\u4e2a\u591c\u665a\uff0c>20,000\u540d\u53c2\u4e0e\u8005\uff0c1,800\u540d\u6297\u6291\u90c1\u836f\u7269\u4f7f\u7528\u8005\uff09\u4e2d\uff0c\u751f\u7269\u6807\u5fd7\u7269AUROC\u8fbe\u52300.84\uff0c\u8de8\u836f\u7269\u7c7b\u522b\u6cdb\u5316\u826f\u597d\uff0c\u4e0e\u5242\u91cf\u5448\u6bd4\u4f8b\u5173\u7cfb\uff0c\u5bf9\u4f34\u968f\u7cbe\u795e\u836f\u7269\u4fdd\u6301\u7a33\u5065", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u5ba2\u89c2\u3001\u53ef\u6269\u5c55\u7684\u4f9d\u4ece\u6027\u76d1\u6d4b\uff0c\u6709\u6f5c\u529b\u6539\u5584\u6291\u90c1\u75c7\u62a4\u7406\u548c\u9884\u540e"}}
{"id": "2510.11040", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11040", "abs": "https://arxiv.org/abs/2510.11040", "authors": ["Wenya Xie", "Qingying Xiao", "Yu Zheng", "Xidong Wang", "Junying Chen", "Ke Ji", "Anningzhe Gao", "Prayag Tiwari", "Xiang Wan", "Feng Jiang", "Benyou Wang"], "title": "Enabling Doctor-Centric Medical AI with LLMs through Workflow-Aligned Tasks and Benchmarks", "comment": null, "summary": "The rise of large language models (LLMs) has transformed healthcare by\noffering clinical guidance, yet their direct deployment to patients poses\nsafety risks due to limited domain expertise. To mitigate this, we propose\nrepositioning LLMs as clinical assistants that collaborate with experienced\nphysicians rather than interacting with patients directly. We conduct a\ntwo-stage inspiration-feedback survey to identify real-world needs in clinical\nworkflows. Guided by this, we construct DoctorFLAN, a large-scale Chinese\nmedical dataset comprising 92,000 Q&A instances across 22 clinical tasks and 27\nspecialties. To evaluate model performance in doctor-facing applications, we\nintroduce DoctorFLAN-test (550 single-turn Q&A items) and DotaBench (74\nmulti-turn conversations). Experimental results with over ten popular LLMs\ndemonstrate that DoctorFLAN notably improves the performance of open-source\nLLMs in medical contexts, facilitating their alignment with physician workflows\nand complementing existing patient-oriented models. This work contributes a\nvaluable resource and framework for advancing doctor-centered medical LLM\ndevelopment", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u91cd\u65b0\u5b9a\u4f4d\u4e3a\u4e0e\u7ecf\u9a8c\u4e30\u5bcc\u7684\u533b\u751f\u534f\u4f5c\u7684\u4e34\u5e8a\u52a9\u624b\uff0c\u800c\u975e\u76f4\u63a5\u4e0e\u60a3\u8005\u4e92\u52a8\uff0c\u4ee5\u964d\u4f4e\u5b89\u5168\u98ce\u9669\u3002\u4f5c\u8005\u6784\u5efa\u4e86DoctorFLAN\u4e2d\u6587\u533b\u7597\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6570\u636e\u96c6\u80fd\u663e\u8457\u63d0\u5347\u5f00\u6e90LLM\u5728\u533b\u7597\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u63d0\u4f9b\u4e34\u5e8a\u6307\u5bfc\uff0c\u4f46\u76f4\u63a5\u90e8\u7f72\u7ed9\u60a3\u8005\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u56e0\u4e3a\u6a21\u578b\u7f3a\u4e4f\u8db3\u591f\u7684\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5b89\u5168\u7684\u65b9\u5f0f\u6765\u5229\u7528LLM\u7684\u6f5c\u529b\u3002", "method": "\u8fdb\u884c\u4e24\u9636\u6bb5\u7075\u611f-\u53cd\u9988\u8c03\u67e5\u8bc6\u522b\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u771f\u5b9e\u9700\u6c42\uff1b\u6784\u5efa\u5305\u542b92,000\u4e2a\u95ee\u7b54\u5b9e\u4f8b\u7684DoctorFLAN\u4e2d\u6587\u533b\u7597\u6570\u636e\u96c6\uff0c\u6db5\u76d622\u4e2a\u4e34\u5e8a\u4efb\u52a1\u548c27\u4e2a\u4e13\u79d1\uff1b\u521b\u5efaDoctorFLAN-test\u548cDotaBench\u8bc4\u4f30\u96c6\uff1b\u5bf910\u591a\u4e2a\u6d41\u884cLLM\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "DoctorFLAN\u663e\u8457\u63d0\u5347\u4e86\u5f00\u6e90LLM\u5728\u533b\u7597\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u66f4\u597d\u5730\u4e0e\u533b\u751f\u5de5\u4f5c\u6d41\u7a0b\u5bf9\u9f50\uff0c\u5e76\u8865\u5145\u73b0\u6709\u7684\u9762\u5411\u60a3\u8005\u7684\u6a21\u578b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u63a8\u8fdb\u4ee5\u533b\u751f\u4e3a\u4e2d\u5fc3\u7684\u533b\u7597LLM\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u548c\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u5b9a\u4f4dLLM\u89d2\u8272\u6765\u5e73\u8861\u6280\u672f\u521b\u65b0\u4e0e\u60a3\u8005\u5b89\u5168\u3002"}}
{"id": "2510.11052", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11052", "abs": "https://arxiv.org/abs/2510.11052", "authors": ["Qinglin Zhu", "Yizhen Yao", "Runcong Zhao", "Yanzheng Xiang", "Amrutha Saseendran", "Chen Jin", "Philip Alexander Teare", "Bin Liang", "Yulan He", "Lin Gui"], "title": "Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States", "comment": null, "summary": "Autoregressive (AR) models remain the standard for natural language\ngeneration but still suffer from high latency due to strictly sequential\ndecoding. Recent diffusion-inspired approaches, such as LlaDA and Dream,\nmitigate this by generating in parallel, yet they suffer from two core\nlimitations: information loss, as predictive distributions for non-finalized\ntokens are discarded at each step, and premature commitment, where local\ndecisions are made without sufficient global coordination. We introduce Latent\nRefinement Decoding (LRD), a two-stage framework with Latent Refinement and a\nPredictive Feedback Loop. The first stage maintains masked positions as\ndistributional mixtures of predicted tokens and the mask embedding, allowing\nthe model to establish more globally consistent beliefs. The second stage\nprogressively finalizes confident tokens while retaining uncertain ones for\niterative feedback. KL-divergence dynamics provide a principled and reliable\ncriterion for convergence and early stopping. Experiments across coding\n(HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that\nLRD improves accuracy while delivering speedups of up to 10.6x, making it a\nstrong and versatile alternative for parallel sequence generation.", "AI": {"tldr": "LRD\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u5e76\u884c\u5e8f\u5217\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u7ec6\u5316\u548c\u9884\u6d4b\u53cd\u9988\u5faa\u73af\u89e3\u51b3\u73b0\u6709\u6269\u6563\u65b9\u6cd5\u7684\u4fe1\u606f\u4e22\u5931\u548c\u8fc7\u65e9\u51b3\u7b56\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8fbe10.6\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u5b58\u5728\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u800c\u73b0\u6709\u6269\u6563\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u548c\u8fc7\u65e9\u51b3\u7b56\u4e24\u4e2a\u6838\u5fc3\u9650\u5236\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5c06\u63a9\u7801\u4f4d\u7f6e\u4fdd\u6301\u4e3a\u9884\u6d4b\u6807\u8bb0\u548c\u63a9\u7801\u5d4c\u5165\u7684\u5206\u5e03\u6df7\u5408\uff1b\u7b2c\u4e8c\u9636\u6bb5\u9010\u6b65\u786e\u5b9a\u7f6e\u4fe1\u6807\u8bb0\uff0c\u4fdd\u7559\u4e0d\u786e\u5b9a\u6807\u8bb0\u8fdb\u884c\u8fed\u4ee3\u53cd\u9988\uff0c\u4f7f\u7528KL\u6563\u5ea6\u52a8\u6001\u4f5c\u4e3a\u6536\u655b\u6807\u51c6\u3002", "result": "\u5728\u7f16\u7a0b\u4efb\u52a1\uff08HumanEval +6.3\uff0cMBPP +2.6\uff09\u548c\u63a8\u7406\u4efb\u52a1\uff08GSM8K +2.9\uff0cMATH500 +3.8\uff09\u4e0a\u51c6\u786e\u7387\u63d0\u5347\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u8fbe10.6\u500d\u7684\u52a0\u901f\u3002", "conclusion": "LRD\u662f\u5e76\u884c\u5e8f\u5217\u751f\u6210\u7684\u4e00\u4e2a\u5f3a\u5927\u4e14\u591a\u529f\u80fd\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u901f\u5ea6\u3002"}}
{"id": "2510.10375", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.10375", "abs": "https://arxiv.org/abs/2510.10375", "authors": ["Kenichi Satoh"], "title": "Applying non-negative matrix factorization with covariates to label matrix for classification", "comment": "2 figures, R package: nmfkc published in GitHub,\n  https://github.com/ksatohds/nmfkc", "summary": "Non-negative matrix factorization (NMF) is widely used for dimensionality\nreduction and interpretable analysis, but standard formulations are\nunsupervised and cannot directly exploit class labels. Existing supervised or\nsemi-supervised extensions usually incorporate labels only via penalties or\ngraph constraints, still requiring an external classifier. We propose\n\\textit{NMF-LAB} (Non-negative Matrix Factorization for Label Matrix), which\nredefines classification as the inverse problem of non-negative matrix\ntri-factorization (tri-NMF). Unlike joint NMF methods, which reconstruct both\nfeatures and labels, NMF-LAB directly factorizes the label matrix $Y$ as the\nobservation, while covariates $A$ are treated as given explanatory variables.\nThis yields a direct probabilistic mapping from covariates to labels,\ndistinguishing our method from label-matrix factorization approaches that\nmainly model label correlations or impute missing labels. Our inversion offers\ntwo key advantages: (i) class-membership probabilities are obtained directly\nfrom the factorization without a separate classifier, and (ii) covariates,\nincluding kernel-based similarities, can be seamlessly integrated to generalize\npredictions to unseen samples. In addition, unlabeled data can be encoded as\nuniform distributions, supporting semi-supervised learning. Experiments on\ndiverse datasets, from small-scale benchmarks to the large-scale MNIST dataset,\ndemonstrate that NMF-LAB achieves competitive predictive accuracy, robustness\nto noisy or incomplete labels, and scalability to high-dimensional problems,\nwhile preserving interpretability. By unifying regression and classification\nwithin the tri-NMF framework, NMF-LAB provides a novel, probabilistic, and\nscalable approach to modern classification tasks.", "AI": {"tldr": "NMF-LAB\u662f\u4e00\u79cd\u65b0\u7684\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u5206\u7c7b\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u975e\u8d1f\u77e9\u9635\u4e09\u56e0\u5b50\u5206\u89e3\u7684\u9006\u95ee\u9898\uff0c\u76f4\u63a5\u5206\u89e3\u6807\u7b7e\u77e9\u9635\uff0c\u65e0\u9700\u5916\u90e8\u5206\u7c7b\u5668\u5373\u53ef\u83b7\u5f97\u7c7b\u522b\u6982\u7387\u3002", "motivation": "\u4f20\u7edf\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u662f\u65e0\u76d1\u7763\u7684\uff0c\u65e0\u6cd5\u76f4\u63a5\u5229\u7528\u7c7b\u522b\u6807\u7b7e\u3002\u73b0\u6709\u7684\u76d1\u7763\u6269\u5c55\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u60e9\u7f5a\u9879\u6216\u56fe\u7ea6\u675f\u7ed3\u5408\u6807\u7b7e\uff0c\u4f46\u4ecd\u9700\u8981\u5916\u90e8\u5206\u7c7b\u5668\u3002", "method": "\u5c06\u5206\u7c7b\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u975e\u8d1f\u77e9\u9635\u4e09\u56e0\u5b50\u5206\u89e3\u7684\u9006\u95ee\u9898\uff0c\u76f4\u63a5\u5206\u89e3\u6807\u7b7e\u77e9\u9635Y\uff0c\u5c06\u534f\u53d8\u91cfA\u4f5c\u4e3a\u5df2\u77e5\u89e3\u91ca\u53d8\u91cf\uff0c\u83b7\u5f97\u4ece\u534f\u53d8\u91cf\u5230\u6807\u7b7e\u7684\u76f4\u63a5\u6982\u7387\u6620\u5c04\u3002", "result": "\u5728\u4ece\u5c0f\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\u5230\u5927\u89c4\u6a21MNIST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNMF-LAB\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5bf9\u566a\u58f0\u6216\u4e0d\u5b8c\u6574\u6807\u7b7e\u5177\u6709\u9c81\u68d2\u6027\uff0c\u53ef\u6269\u5c55\u5230\u9ad8\u7ef4\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u901a\u8fc7\u5728tri-NMF\u6846\u67b6\u5185\u7edf\u4e00\u56de\u5f52\u548c\u5206\u7c7b\uff0cNMF-LAB\u4e3a\u73b0\u4ee3\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u3001\u6982\u7387\u5316\u548c\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.11104", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11104", "abs": "https://arxiv.org/abs/2510.11104", "authors": ["Junjie Lu", "Yuliang Liu", "Chaofeng Qu", "Wei Shen", "Zhouhan Lin", "Min Xu"], "title": "Enhancing LLM Reasoning via Non-Human-Like Reasoning Path Preference Optimization", "comment": "13 pages", "summary": "Current approaches for strengthening LLM reasoning tend to introduce a\ntraining bias toward human-like reasoning trajectories. In step-wise preference\noptimization, in particular, dependence on human or higher-capacity model\nannotations for intermediate steps limits exploration of alternative,\nnon-human-like reasoning paths and thus constrains achievable performance.\nFurthermore, through a small-scale pilot study, we observed that in\napproximately 75% of cases, the model's first erroneous step occurs after the\nlowest-confidence point. This suggests that guiding the model at its\nlowest-confidence point before an error provides more accurate supervision than\nlocating the first explicit error. In this paper, we propose Confidence-Guided\nReasoning Path Preference Optimization (CGPO), a method that leverages a\nconfidence signal to identify points of maximal uncertainty in the model's\nreasoning process and applies self-generated, non-human-like reasoning-path\nguidance to mitigate trajectory drift. Our experiments span diverse models\napplied to both code and mathematical reasoning tasks. The results show that,\nwith the same amount of training data, our method using data generated by a\nsmall model can achieve better performance in most cases compared with\napproaches using data generated by a strong model or human-annotated.", "AI": {"tldr": "CGPO\u662f\u4e00\u79cd\u5229\u7528\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u8bc6\u522b\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6700\u5927\u4e0d\u786e\u5b9a\u6027\u70b9\uff0c\u5e76\u5e94\u7528\u81ea\u751f\u6210\u7684\u975e\u4eba\u7c7b\u63a8\u7406\u8def\u5f84\u6307\u5bfc\u6765\u51cf\u8f7b\u8f68\u8ff9\u6f02\u79fb\u7684\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316LLM\u63a8\u7406\u7684\u65b9\u6cd5\u503e\u5411\u4e8e\u5f15\u5165\u5bf9\u4eba\u7c7b\u63a8\u7406\u8f68\u8ff9\u7684\u8bad\u7ec3\u504f\u5dee\uff0c\u4f9d\u8d56\u4eba\u7c7b\u6216\u66f4\u9ad8\u80fd\u529b\u6a21\u578b\u7684\u4e2d\u95f4\u6b65\u9aa4\u6ce8\u91ca\u9650\u5236\u4e86\u63a2\u7d22\u66ff\u4ee3\u6027\u975e\u4eba\u7c7b\u63a8\u7406\u8def\u5f84\uff0c\u4ece\u800c\u7ea6\u675f\u4e86\u53ef\u5b9e\u73b0\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u63a8\u7406\u8def\u5f84\u504f\u597d\u4f18\u5316(CGPO)\uff0c\u5229\u7528\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u8bc6\u522b\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6700\u5927\u4e0d\u786e\u5b9a\u6027\u70b9\uff0c\u5e76\u5728\u9519\u8bef\u53d1\u751f\u524d\u7684\u6700\u4f4e\u7f6e\u4fe1\u70b9\u5e94\u7528\u81ea\u751f\u6210\u7684\u975e\u4eba\u7c7b\u63a8\u7406\u8def\u5f84\u6307\u5bfc\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u76f8\u540c\u8bad\u7ec3\u6570\u636e\u91cf\u4e0b\uff0c\u4f7f\u7528\u5c0f\u6a21\u578b\u751f\u6210\u6570\u636e\u7684CGPO\u65b9\u6cd5\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u6bd4\u4f7f\u7528\u5f3a\u6a21\u578b\u751f\u6210\u6570\u636e\u6216\u4eba\u5de5\u6807\u6ce8\u7684\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "CGPO\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u5728\u5173\u952e\u4e0d\u786e\u5b9a\u6027\u70b9\u63d0\u4f9b\u6307\u5bfc\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u63a8\u7406\u6027\u80fd\uff0c\u4e14\u4e0d\u4f9d\u8d56\u4eba\u7c7b\u6216\u5f3a\u6a21\u578b\u7684\u6ce8\u91ca\u3002"}}
{"id": "2510.10402", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.10402", "abs": "https://arxiv.org/abs/2510.10402", "authors": ["Jiachi Zhao", "Zehong Wang", "Yamei Liao", "Chuxu Zhang", "Yanfang Ye"], "title": "Controllable Graph Generation with Diffusion Models via Inference-Time Tree Search Guidance", "comment": null, "summary": "Graph generation is a fundamental problem in graph learning with broad\napplications across Web-scale systems, knowledge graphs, and scientific domains\nsuch as drug and material discovery. Recent approaches leverage diffusion\nmodels for step-by-step generation, yet unconditional diffusion offers little\ncontrol over desired properties, often leading to unstable quality and\ndifficulty in incorporating new objectives. Inference-time guidance methods\nmitigate these issues by adjusting the sampling process without retraining, but\nthey remain inherently local, heuristic, and limited in controllability. To\novercome these limitations, we propose TreeDiff, a Monte Carlo Tree Search\n(MCTS) guided dual-space diffusion framework for controllable graph generation.\nTreeDiff is a plug-and-play inference-time method that expands the search space\nwhile keeping computation tractable. Specifically, TreeDiff introduces three\nkey designs to make it practical and scalable: (1) a macro-step expansion\nstrategy that groups multiple denoising updates into a single transition,\nreducing tree depth and enabling long-horizon exploration; (2) a dual-space\ndenoising mechanism that couples efficient latent-space denoising with\nlightweight discrete correction in graph space, ensuring both scalability and\nstructural fidelity; and (3) a dual-space verifier that predicts long-term\nrewards from partially denoised graphs, enabling early value estimation and\nremoving the need for full rollouts. Extensive experiments on 2D and 3D\nmolecular generation benchmarks, under both unconditional and conditional\nsettings, demonstrate that TreeDiff achieves state-of-the-art performance.\nNotably, TreeDiff exhibits favorable inference-time scaling: it continues to\nimprove with additional computation, while existing inference-time methods\nplateau early under limited resources.", "AI": {"tldr": "TreeDiff\uff1a\u4e00\u79cd\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u53cc\u7a7a\u95f4\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u63a7\u56fe\u751f\u6210\uff0c\u901a\u8fc7\u5b8f\u6b65\u6269\u5c55\u3001\u53cc\u7a7a\u95f4\u53bb\u566a\u548c\u53cc\u7a7a\u95f4\u9a8c\u8bc1\u5668\u5b9e\u73b0\u9ad8\u6548\u53ef\u63a7\u7684\u56fe\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u56fe\u751f\u6210\u4e2d\u7f3a\u4e4f\u5bf9\u671f\u671b\u5c5e\u6027\u7684\u63a7\u5236\uff0c\u5bfc\u81f4\u8d28\u91cf\u4e0d\u7a33\u5b9a\u4e14\u96be\u4ee5\u878d\u5165\u65b0\u76ee\u6807\u3002\u63a8\u7406\u65f6\u5f15\u5bfc\u65b9\u6cd5\u867d\u7136\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u4f46\u672c\u8d28\u4e0a\u4ecd\u662f\u5c40\u90e8\u3001\u542f\u53d1\u5f0f\u4e14\u53ef\u63a7\u6027\u6709\u9650\u3002", "method": "\u63d0\u51faTreeDiff\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a(1)\u5b8f\u6b65\u6269\u5c55\u7b56\u7565\uff0c\u5c06\u591a\u4e2a\u53bb\u566a\u66f4\u65b0\u5206\u7ec4\u4e3a\u5355\u6b21\u8f6c\u79fb\uff0c\u51cf\u5c11\u6811\u6df1\u5ea6\uff1b(2)\u53cc\u7a7a\u95f4\u53bb\u566a\u673a\u5236\uff0c\u7ed3\u5408\u6f5c\u5728\u7a7a\u95f4\u9ad8\u6548\u53bb\u566a\u548c\u56fe\u7a7a\u95f4\u8f7b\u91cf\u79bb\u6563\u6821\u6b63\uff1b(3)\u53cc\u7a7a\u95f4\u9a8c\u8bc1\u5668\uff0c\u4ece\u90e8\u5206\u53bb\u566a\u56fe\u4e2d\u9884\u6d4b\u957f\u671f\u5956\u52b1\uff0c\u65e0\u9700\u5b8c\u6574\u5c55\u5f00\u3002", "result": "\u57282D\u548c3D\u5206\u5b50\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65e0\u8bba\u662f\u65e0\u6761\u4ef6\u8fd8\u662f\u6709\u6761\u4ef6\u8bbe\u7f6e\uff0cTreeDiff\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002\u63a8\u7406\u65f6\u95f4\u6269\u5c55\u6027\u4f18\u5f02\uff0c\u968f\u7740\u8ba1\u7b97\u8d44\u6e90\u589e\u52a0\u6301\u7eed\u6539\u8fdb\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u65e9\u671f\u5373\u8fbe\u5230\u5e73\u53f0\u671f\u3002", "conclusion": "TreeDiff\u901a\u8fc7MCTS\u5f15\u5bfc\u7684\u53cc\u7a7a\u95f4\u6269\u6563\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u53ef\u63a7\u7684\u56fe\u751f\u6210\uff0c\u5728\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5177\u6709\u826f\u597d\u7684\u8ba1\u7b97\u6269\u5c55\u6027\u3002"}}
{"id": "2510.11151", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.11151", "abs": "https://arxiv.org/abs/2510.11151", "authors": ["Alexander Sternfeld", "Andrei Kucharavy", "Ljiljana Dolamic"], "title": "TypePilot: Leveraging the Scala Type System for Secure LLM-generated Code", "comment": null, "summary": "Large language Models (LLMs) have shown remarkable proficiency in code\ngeneration tasks across various programming languages. However, their outputs\noften contain subtle but critical vulnerabilities, posing significant risks\nwhen deployed in security-sensitive or mission-critical systems. This paper\nintroduces TypePilot, an agentic AI framework designed to enhance the security\nand robustness of LLM-generated code by leveraging strongly typed and\nverifiable languages, using Scala as a representative example. We evaluate the\neffectiveness of our approach in two settings: formal verification with the\nStainless framework and general-purpose secure code generation. Our experiments\nwith leading open-source LLMs reveal that while direct code generation often\nfails to enforce safety constraints, just as naive prompting for more secure\ncode, our type-focused agentic pipeline substantially mitigates input\nvalidation and injection vulnerabilities. The results demonstrate the potential\nof structured, type-guided LLM workflows to improve the SotA of the\ntrustworthiness of automated code generation in high-assurance domains.", "AI": {"tldr": "TypePilot\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u7c7b\u578b\u548c\u53ef\u9a8c\u8bc1\u8bed\u8a00\u7684AI\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7Scala\u8bed\u8a00\u589e\u5f3aLLM\u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8f93\u5165\u9a8c\u8bc1\u548c\u6ce8\u5165\u6f0f\u6d1e\u3002", "motivation": "LLM\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8f93\u51fa\u5e38\u5305\u542b\u5fae\u5999\u4f46\u5173\u952e\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5728\u5b89\u5168\u654f\u611f\u6216\u5173\u952e\u4efb\u52a1\u7cfb\u7edf\u4e2d\u90e8\u7f72\u65f6\u5b58\u5728\u91cd\u5927\u98ce\u9669\u3002", "method": "\u4f7f\u7528Scala\u4f5c\u4e3a\u4ee3\u8868\u6027\u793a\u4f8b\uff0c\u6784\u5efa\u57fa\u4e8e\u7c7b\u578b\u7684AI\u4ee3\u7406\u6846\u67b6\uff0c\u7ed3\u5408Stainless\u6846\u67b6\u8fdb\u884c\u5f62\u5f0f\u9a8c\u8bc1\u548c\u901a\u7528\u5b89\u5168\u4ee3\u7801\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f4\u63a5\u4ee3\u7801\u751f\u6210\u548c\u7b80\u5355\u63d0\u793a\u65b9\u6cd5\u96be\u4ee5\u5f3a\u5236\u6267\u884c\u5b89\u5168\u7ea6\u675f\uff0c\u800c\u57fa\u4e8e\u7c7b\u578b\u7684\u4ee3\u7406\u7ba1\u9053\u663e\u8457\u51cf\u8f7b\u4e86\u8f93\u5165\u9a8c\u8bc1\u548c\u6ce8\u5165\u6f0f\u6d1e\u3002", "conclusion": "\u7ed3\u6784\u5316\u3001\u7c7b\u578b\u5f15\u5bfc\u7684LLM\u5de5\u4f5c\u6d41\u7a0b\u6709\u6f5c\u529b\u63d0\u9ad8\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u5728\u9ad8\u4fdd\u8bc1\u9886\u57df\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2510.10425", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10425", "abs": "https://arxiv.org/abs/2510.10425", "authors": ["Sara Dragutinovi\u0107", "Andrew M. Saxe", "Aaditya K. Singh"], "title": "Softmax $\\geq$ Linear: Transformers may learn to classify in-context by kernel gradient descent", "comment": null, "summary": "The remarkable ability of transformers to learn new concepts solely by\nreading examples within the input prompt, termed in-context learning (ICL), is\na crucial aspect of intelligent behavior. Here, we focus on understanding the\nlearning algorithm transformers use to learn from context. Existing theoretical\nwork, often based on simplifying assumptions, has primarily focused on linear\nself-attention and continuous regression tasks, finding transformers can learn\nin-context by gradient descent. Given that transformers are typically trained\non discrete and complex tasks, we bridge the gap from this existing work to the\nsetting of classification, with non-linear (importantly, softmax) activation.\nWe find that transformers still learn to do gradient descent in-context, though\non functionals in the kernel feature space and with a context-adaptive learning\nrate in the case of softmax transformer. These theoretical findings suggest a\ngreater adaptability to context for softmax attention, which we empirically\nverify and study through ablations. Overall, we hope this enhances theoretical\nunderstanding of in-context learning algorithms in more realistic settings,\npushes forward our intuitions and enables further theory bridging to larger\nmodels.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86transformer\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\uff0c\u53d1\u73b0\u5373\u4f7f\u4f7f\u7528softmax\u975e\u7ebf\u6027\u6fc0\u6d3b\uff0ctransformer\u4ecd\u80fd\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u4f46\u5b66\u4e60\u8fc7\u7a0b\u53d1\u751f\u5728\u6838\u7279\u5f81\u7a7a\u95f4\u4e2d\u4e14\u5177\u6709\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u3002", "motivation": "\u73b0\u6709\u7406\u8bba\u7814\u7a76\u4e3b\u8981\u57fa\u4e8e\u7b80\u5316\u7684\u7ebf\u6027\u81ea\u6ce8\u610f\u529b\u548c\u8fde\u7eed\u56de\u5f52\u4efb\u52a1\uff0c\u800ctransformer\u901a\u5e38\u5728\u79bb\u6563\u590d\u6742\u4efb\u52a1\u4e0a\u8bad\u7ec3\u3002\u672c\u6587\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u7814\u7a76\u5728\u5206\u7c7b\u4efb\u52a1\u548c\u975e\u7ebf\u6027softmax\u6fc0\u6d3b\u4e0b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u7ecf\u9a8c\u9a8c\u8bc1\uff0c\u7814\u7a76transformer\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f7f\u7528softmax\u6fc0\u6d3b\u65f6\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u4e0e\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u53d1\u73b0softmax transformer\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u6267\u884c\u68af\u5ea6\u4e0b\u964d\uff0c\u4f46\u5b66\u4e60\u8fc7\u7a0b\u53d1\u751f\u5728\u6838\u7279\u5f81\u7a7a\u95f4\u4e2d\uff0c\u4e14\u5177\u6709\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u5b66\u4e60\u7387\uff0c\u8868\u73b0\u51fa\u6bd4\u7ebf\u6027\u6ce8\u610f\u529b\u66f4\u5f3a\u7684\u4e0a\u4e0b\u6587\u9002\u5e94\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u589e\u5f3a\u4e86\u5728\u66f4\u73b0\u5b9e\u8bbe\u7f6e\u4e0b\u5bf9\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b97\u6cd5\u7684\u7406\u8bba\u7406\u89e3\uff0c\u4e3a\u5411\u66f4\u5927\u6a21\u578b\u7684\u7406\u8bba\u62d3\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.11160", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11160", "abs": "https://arxiv.org/abs/2510.11160", "authors": ["Jens Van Nooten", "Andriy Kosar", "Guy De Pauw", "Walter Daelemans"], "title": "One Size Does Not Fit All: Exploring Variable Thresholds for Distance-Based Multi-Label Text Classification", "comment": null, "summary": "Distance-based unsupervised text classification is a method within text\nclassification that leverages the semantic similarity between a label and a\ntext to determine label relevance. This method provides numerous benefits,\nincluding fast inference and adaptability to expanding label sets, as opposed\nto zero-shot, few-shot, and fine-tuned neural networks that require re-training\nin such cases. In multi-label distance-based classification and information\nretrieval algorithms, thresholds are required to determine whether a text\ninstance is \"similar\" to a label or query. Similarity between a text and label\nis determined in a dense embedding space, usually generated by state-of-the-art\nsentence encoders. Multi-label classification complicates matters, as a text\ninstance can have multiple true labels, unlike in multi-class or binary\nclassification, where each instance is assigned only one label. We expand upon\nprevious literature on this underexplored topic by thoroughly examining and\nevaluating the ability of sentence encoders to perform distance-based\nclassification. First, we perform an exploratory study to verify whether the\nsemantic relationships between texts and labels vary across models, datasets,\nand label sets by conducting experiments on a diverse collection of realistic\nmulti-label text classification (MLTC) datasets. We find that similarity\ndistributions show statistically significant differences across models,\ndatasets and even label sets. We propose a novel method for optimizing\nlabel-specific thresholds using a validation set. Our label-specific\nthresholding method achieves an average improvement of 46% over normalized 0.5\nthresholding and outperforms uniform thresholding approaches from previous work\nby an average of 14%. Additionally, the method demonstrates strong performance\neven with limited labeled examples.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u8ddd\u79bb\u7684\u65e0\u76d1\u7763\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a2\u7d22\u53e5\u5b50\u7f16\u7801\u5668\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u8ba1\u7b97\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6807\u7b7e\u7279\u5b9a\u9608\u503c\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6807\u7b7e\u6587\u672c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u8ddd\u79bb\u7684\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u5177\u6709\u5feb\u901f\u63a8\u7406\u548c\u9002\u5e94\u6269\u5c55\u6807\u7b7e\u96c6\u7684\u4f18\u52bf\uff0c\u4f46\u5728\u591a\u6807\u7b7e\u5206\u7c7b\u4e2d\u9700\u8981\u786e\u5b9a\u76f8\u4f3c\u6027\u9608\u503c\u3002\u73b0\u6709\u65b9\u6cd5\u5bf9\u9608\u503c\u9009\u62e9\u7814\u7a76\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u6807\u7b7e\u96c6\u4e0b\u8bed\u4e49\u5173\u7cfb\u7684\u53d8\u5316\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u9996\u5148\u901a\u8fc7\u63a2\u7d22\u6027\u7814\u7a76\u9a8c\u8bc1\u6587\u672c\u548c\u6807\u7b7e\u95f4\u8bed\u4e49\u5173\u7cfb\u5728\u4e0d\u540c\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u6807\u7b7e\u96c6\u95f4\u7684\u53d8\u5316\uff0c\u7136\u540e\u63d0\u51fa\u4f7f\u7528\u9a8c\u8bc1\u96c6\u4f18\u5316\u6807\u7b7e\u7279\u5b9a\u9608\u503c\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u76f8\u4f3c\u6027\u5206\u5e03\u5728\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u6807\u7b7e\u96c6\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u63d0\u51fa\u7684\u6807\u7b7e\u7279\u5b9a\u9608\u503c\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6\u53160.5\u9608\u503c\u5e73\u5747\u63d0\u534746%\uff0c\u6bd4\u5148\u524d\u5de5\u4f5c\u7684\u7edf\u4e00\u9608\u503c\u65b9\u6cd5\u5e73\u5747\u63d0\u534714%\uff0c\u4e14\u5728\u6709\u9650\u6807\u6ce8\u6837\u672c\u4e0b\u4ecd\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u57fa\u4e8e\u8ddd\u79bb\u7684\u65e0\u76d1\u7763\u6587\u672c\u5206\u7c7b\u4e2d\uff0c\u6807\u7b7e\u7279\u5b9a\u9608\u503c\u4f18\u5316\u662f\u6709\u6548\u7684\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u591a\u6807\u7b7e\u5206\u7c7b\u573a\u666f\u4e0b\u3002"}}
{"id": "2510.10432", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10432", "abs": "https://arxiv.org/abs/2510.10432", "authors": ["Zhichen Zeng", "Mengyue Hang", "Xiaolong Liu", "Xiaoyi Liu", "Xiao Lin", "Ruizhong Qiu", "Tianxin Wei", "Zhining Liu", "Siyang Yuan", "Chaofei Yang", "Yiqun Liu", "Hang Yin", "Jiyan Yang", "Hanghang Tong"], "title": "Hierarchical LoRA MoE for Efficient CTR Model Scaling", "comment": "13 pages, 9 figures", "summary": "Deep models have driven significant advances in click-through rate (CTR)\nprediction. While vertical scaling via layer stacking improves model\nexpressiveness, the layer-by-layer sequential computation poses challenges to\nefficient scaling. Conversely, horizontal scaling through Mixture of Experts\n(MoE) achieves efficient scaling by activating a small subset of experts in\nparallel, but flat MoE layers may struggle to capture the hierarchical\nstructure inherent in recommendation tasks. To push the Return-On-Investment\n(ROI) boundary, we explore the complementary strengths of both directions and\npropose HiLoMoE, a hierarchical LoRA MoE framework that enables holistic\nscaling in a parameter-efficient manner. Specifically, HiLoMoE employs\nlightweight rank-1 experts for parameter-efficient horizontal scaling, and\nstacks multiple MoE layers with hierarchical routing to enable combinatorially\ndiverse expert compositions. Unlike conventional stacking, HiLoMoE routes based\non prior layer scores rather than outputs, allowing all layers to execute in\nparallel. A principled three-stage training framework ensures stable\noptimization and expert diversity. Experiments on four public datasets show\nthat HiLoMoE achieving better performance-efficiency tradeoff, achieving an\naverage AUC improvement of 0.20\\% in AUC and 18.5\\% reduction in FLOPs compared\nto the non-MoE baseline.", "AI": {"tldr": "\u63d0\u51faHiLoMoE\u6846\u67b6\uff0c\u7ed3\u5408\u5782\u76f4\u548c\u6c34\u5e73\u6269\u5c55\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u5206\u5c42LoRA MoE\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u7684\u6574\u4f53\u6269\u5c55\uff0c\u5728CTR\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd-\u6548\u7387\u6743\u8861\u3002", "motivation": "\u89e3\u51b3CTR\u9884\u6d4b\u4e2d\u5782\u76f4\u6269\u5c55\u7684\u5e8f\u5217\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u548c\u6c34\u5e73\u6269\u5c55\u7684MoE\u5c42\u96be\u4ee5\u6355\u6349\u63a8\u8350\u4efb\u52a1\u5c42\u6b21\u7ed3\u6784\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u4e24\u79cd\u6269\u5c55\u65b9\u5411\u7684\u4e92\u8865\u4f18\u52bf\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7rank-1\u4e13\u5bb6\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u7684\u6c34\u5e73\u6269\u5c55\uff0c\u5806\u53e0\u591a\u4e2aMoE\u5c42\u5e76\u901a\u8fc7\u5206\u5c42\u8def\u7531\u5b9e\u73b0\u7ec4\u5408\u591a\u6837\u7684\u4e13\u5bb6\u7ec4\u5408\uff0c\u57fa\u4e8e\u524d\u5c42\u5206\u6570\u800c\u975e\u8f93\u51fa\u8fdb\u884c\u5e76\u884c\u8def\u7531\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u975eMoE\u57fa\u7ebf\uff0c\u5e73\u5747AUC\u63d0\u53470.20%\uff0cFLOPs\u51cf\u5c1118.5%\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd-\u6548\u7387\u6743\u8861\u3002", "conclusion": "HiLoMoE\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86\u5782\u76f4\u548c\u6c34\u5e73\u6269\u5c55\u7684\u4f18\u52bf\uff0c\u5728\u53c2\u6570\u9ad8\u6548\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e86CTR\u9884\u6d4b\u4efb\u52a1\u7684\u6574\u4f53\u6269\u5c55\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.11167", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11167", "abs": "https://arxiv.org/abs/2510.11167", "authors": ["Paloma Piot", "Jos\u00e9 Ramom Pichel Campos", "Javier Parapar"], "title": "Bridging Gaps in Hate Speech Detection: Meta-Collections and Benchmarks for Low-Resource Iberian Languages", "comment": null, "summary": "Hate speech poses a serious threat to social cohesion and individual\nwell-being, particularly on social media, where it spreads rapidly. While\nresearch on hate speech detection has progressed, it remains largely focused on\nEnglish, resulting in limited resources and benchmarks for low-resource\nlanguages. Moreover, many of these languages have multiple linguistic\nvarieties, a factor often overlooked in current approaches. At the same time,\nlarge language models require substantial amounts of data to perform reliably,\na requirement that low-resource languages often cannot meet. In this work, we\naddress these gaps by compiling a meta-collection of hate speech datasets for\nEuropean Spanish, standardised with unified labels and metadata. This\ncollection is based on a systematic analysis and integration of existing\nresources, aiming to bridge the data gap and support more consistent and\nscalable hate speech detection. We extended this collection by translating it\ninto European Portuguese and into a Galician standard that is more convergent\nwith Spanish and another Galician variant that is more convergent with\nPortuguese, creating aligned multilingual corpora. Using these resources, we\nestablish new benchmarks for hate speech detection in Iberian languages. We\nevaluate state-of-the-art large language models in zero-shot, few-shot, and\nfine-tuning settings, providing baseline results for future research. Moreover,\nwe perform a cross-lingual analysis with our target languages. Our findings\nunderscore the importance of multilingual and variety-aware approaches in hate\nspeech detection and offer a foundation for improved benchmarking in\nunderrepresented European languages.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u9488\u5bf9\u6b27\u6d32\u897f\u73ed\u7259\u8bed\u7684\u4ec7\u6068\u8a00\u8bba\u6570\u636e\u96c6\u5143\u96c6\u5408\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u5230\u6b27\u6d32\u8461\u8404\u7259\u8bed\u548c\u4e24\u79cd\u52a0\u5229\u897f\u4e9a\u8bed\u53d8\u4f53\uff0c\u5efa\u7acb\u4e86\u4f0a\u6bd4\u5229\u4e9a\u8bed\u8a00\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7684\u65b0\u57fa\u51c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u4ec7\u6068\u8a00\u8bba\u5bf9\u793e\u4f1a\u548c\u4e2a\u4eba\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u82f1\u8bed\uff0c\u7f3a\u4e4f\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u53ca\u5176\u8bed\u8a00\u53d8\u4f53\u7684\u5173\u6ce8\u3002\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u800c\u4f4e\u8d44\u6e90\u8bed\u8a00\u5f80\u5f80\u65e0\u6cd5\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u548c\u6574\u5408\u73b0\u6709\u8d44\u6e90\uff0c\u6784\u5efa\u4e86\u6807\u51c6\u5316\u7684\u6b27\u6d32\u897f\u73ed\u7259\u8bed\u4ec7\u6068\u8a00\u8bba\u6570\u636e\u96c6\u5143\u96c6\u5408\uff0c\u5e76\u5c06\u5176\u7ffb\u8bd1\u6269\u5c55\u5230\u6b27\u6d32\u8461\u8404\u7259\u8bed\u548c\u4e24\u79cd\u52a0\u5229\u897f\u4e9a\u8bed\u53d8\u4f53\uff0c\u521b\u5efa\u5bf9\u9f50\u7684\u591a\u8bed\u8a00\u8bed\u6599\u5e93\u3002", "result": "\u5efa\u7acb\u4e86\u4f0a\u6bd4\u5229\u4e9a\u8bed\u8a00\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7684\u65b0\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u8de8\u8bed\u8a00\u5206\u6790\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u8bed\u8a00\u548c\u53d8\u4f53\u611f\u77e5\u65b9\u6cd5\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u6b27\u6d32\u8bed\u8a00\u63d0\u4f9b\u4e86\u6539\u8fdb\u57fa\u51c6\u7684\u57fa\u7840\u3002"}}
{"id": "2510.10433", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10433", "abs": "https://arxiv.org/abs/2510.10433", "authors": ["Zixiang Xu", "Menghui Zhou", "Jun Qi", "Xuanhan Fan", "Yun Yang", "Po Yang"], "title": "Multi-Task Learning with Feature-Similarity Laplacian Graphs for Predicting Alzheimer's Disease Progression", "comment": null, "summary": "Alzheimer's Disease (AD) is the most prevalent neurodegenerative disorder in\naging populations, posing a significant and escalating burden on global\nhealthcare systems. While Multi-Tusk Learning (MTL) has emerged as a powerful\ncomputational paradigm for modeling longitudinal AD data, existing frameworks\ndo not account for the time-varying nature of feature correlations. To address\nthis limitation, we propose a novel MTL framework, named Feature Similarity\nLaplacian graph Multi-Task Learning (MTL-FSL). Our framework introduces a novel\nFeature Similarity Laplacian (FSL) penalty that explicitly models the\ntime-varying relationships between features. By simultaneously considering\ntemporal smoothness among tasks and the dynamic correlations among features,\nour model enhances both predictive accuracy and biological interpretability. To\nsolve the non-smooth optimization problem arising from our proposed penalty\nterms, we adopt the Alternating Direction Method of Multipliers (ADMM)\nalgorithm. Experiments conducted on the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) dataset demonstrate that our proposed MTL-FSL framework\nachieves state-of-the-art performance, outperforming various baseline methods.\nThe implementation source can be found at https://github.com/huatxxx/MTL-FSL.", "AI": {"tldr": "\u63d0\u51fa\u4e86MTL-FSL\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u76f8\u4f3c\u6027\u62c9\u666e\u62c9\u65af\u56fe\u5efa\u6a21\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u6570\u636e\u4e2d\u7279\u5f81\u95f4\u7684\u65f6\u53d8\u76f8\u5173\u6027\uff0c\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u548c\u751f\u7269\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u672a\u80fd\u8003\u8651\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u6570\u636e\u4e2d\u7279\u5f81\u76f8\u5173\u6027\u7684\u65f6\u53d8\u7279\u6027\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5f15\u5165\u7279\u5f81\u76f8\u4f3c\u6027\u62c9\u666e\u62c9\u65af\u60e9\u7f5a\u9879\uff0c\u540c\u65f6\u8003\u8651\u4efb\u52a1\u95f4\u7684\u65f6\u95f4\u5e73\u6ed1\u6027\u548c\u7279\u5f81\u95f4\u7684\u52a8\u6001\u76f8\u5173\u6027\uff0c\u4f7f\u7528ADMM\u7b97\u6cd5\u6c42\u89e3\u975e\u5149\u6ed1\u4f18\u5316\u95ee\u9898\u3002", "result": "\u5728ADNI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMTL-FSL\u6846\u67b6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MTL-FSL\u6846\u67b6\u901a\u8fc7\u5efa\u6a21\u7279\u5f81\u95f4\u65f6\u53d8\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.11196", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11196", "abs": "https://arxiv.org/abs/2510.11196", "authors": ["Johannes Moll", "Markus Graf", "Tristan Lemke", "Nicolas Lenhart", "Daniel Truhn", "Jean-Benoit Delbrouck", "Jiazhen Pan", "Daniel Rueckert", "Lisa C. Adams", "Keno K. Bressem"], "title": "Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations", "comment": null, "summary": "Vision-language models (VLMs) often produce chain-of-thought (CoT)\nexplanations that sound plausible yet fail to reflect the underlying decision\nprocess, undermining trust in high-stakes clinical use. Existing evaluations\nrarely catch this misalignment, prioritizing answer accuracy or adherence to\nformats. We present a clinically grounded framework for chest X-ray visual\nquestion answering (VQA) that probes CoT faithfulness via controlled text and\nimage modifications across three axes: clinical fidelity, causal attribution,\nand confidence calibration. In a reader study (n=4), evaluator-radiologist\ncorrelations fall within the observed inter-radiologist range for all axes,\nwith strong alignment for attribution (Kendall's $\\tau_b=0.670$), moderate\nalignment for fidelity ($\\tau_b=0.387$), and weak alignment for confidence tone\n($\\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows\nthat answer accuracy and explanation quality are decoupled, acknowledging\ninjected cues does not ensure grounding, and text cues shift explanations more\nthan visual cues. While some open-source models match final answer accuracy,\nproprietary models score higher on attribution (25.0% vs. 1.4%) and often on\nfidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to\nevaluate beyond final answer accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e34\u5e8a\u57fa\u7840\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u80f8\u90e8X\u5149\u89c6\u89c9\u95ee\u7b54\u4e2d\u601d\u7ef4\u94fe\u89e3\u91ca\u7684\u5fe0\u5b9e\u5ea6\uff0c\u53d1\u73b0\u7b54\u6848\u51c6\u786e\u6027\u4e0e\u89e3\u91ca\u8d28\u91cf\u8131\u8282\uff0c\u4e13\u6709\u6a21\u578b\u5728\u5f52\u56e0\u548c\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u7684\u601d\u7ef4\u94fe\u89e3\u91ca\u5f80\u5f80\u542c\u8d77\u6765\u5408\u7406\u4f46\u672a\u80fd\u53cd\u6620\u771f\u5b9e\u51b3\u7b56\u8fc7\u7a0b\uff0c\u8fd9\u5728\u4e34\u5e8a\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u4f1a\u524a\u5f31\u4fe1\u4efb\uff0c\u800c\u73b0\u6709\u8bc4\u4f30\u5f88\u5c11\u53d1\u73b0\u8fd9\u79cd\u4e0d\u4e00\u81f4\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u6587\u672c\u548c\u56fe\u50cf\u4fee\u6539\u5728\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u63a2\u6d4b\u601d\u7ef4\u94fe\u5fe0\u5b9e\u5ea6\uff1a\u4e34\u5e8a\u4fdd\u771f\u5ea6\u3001\u56e0\u679c\u5f52\u56e0\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u5e76\u8fdb\u884c\u4e86\u8bfb\u8005\u7814\u7a76\uff08n=4\uff09\u548c\u516d\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u8bc4\u4f30\u8005\u4e0e\u653e\u5c04\u79d1\u533b\u751f\u7684\u76f8\u5173\u6027\u5728\u6240\u6709\u7ef4\u5ea6\u4e0a\u90fd\u843d\u5728\u89c2\u5bdf\u5230\u7684\u653e\u5c04\u79d1\u533b\u751f\u95f4\u5dee\u5f02\u8303\u56f4\u5185\uff0c\u4e13\u6709\u6a21\u578b\u5728\u5f52\u56e0\uff0825.0% vs 1.4%\uff09\u548c\u4fdd\u771f\u5ea6\uff0836.1% vs 31.7%\uff09\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u6587\u672c\u7ebf\u7d22\u6bd4\u89c6\u89c9\u7ebf\u7d22\u66f4\u80fd\u6539\u53d8\u89e3\u91ca\u3002", "conclusion": "\u7b54\u6848\u51c6\u786e\u6027\u548c\u89e3\u91ca\u8d28\u91cf\u662f\u8131\u8282\u7684\uff0c\u627f\u8ba4\u6ce8\u5165\u7684\u7ebf\u7d22\u4e0d\u80fd\u786e\u4fdd\u57fa\u7840\u6027\uff0c\u9700\u8981\u8d85\u8d8a\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\u6765\u8bc4\u4f30\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u4e34\u5e8a\u90e8\u7f72\u4e2d\u3002"}}
{"id": "2510.10446", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10446", "abs": "https://arxiv.org/abs/2510.10446", "authors": ["Masoud Makrehchi"], "title": "Reverse Supervision at Scale: Exponential Search Meets the Economics of Annotation", "comment": "10 pages", "summary": "We analyze a reversed-supervision strategy that searches over labelings of a\nlarge unlabeled set \\(B\\) to minimize error on a small labeled set \\(A\\). The\nsearch space is \\(2^n\\), and the resulting complexity remains exponential even\nunder large constant-factor speedups (e.g., quantum or massively parallel\nhardware). Consequently, arbitrarily fast -- but not exponentially faster --\ncomputation does not obviate the need for informative labels or priors. In\npractice, the machine learning pipeline still requires an initial human\ncontribution: specifying the objective, defining classes, and providing a seed\nset of representative annotations that inject inductive bias and align models\nwith task semantics. Synthetic labels from generative AI can partially\nsubstitute provided their quality is human-grade and anchored by a\nhuman-specified objective, seed supervision, and validation. In this view,\ngenerative models function as \\emph{label amplifiers}, leveraging small\nhuman-curated cores via active, semi-supervised, and self-training loops, while\nhumans retain oversight for calibration, drift detection, and failure auditing.\nThus, extreme computational speed reduces wall-clock time but not the\nfundamental supervision needs of learning; initial human (or human-grade) input\nremains necessary to ground the system in the intended task.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u53cd\u5411\u76d1\u7763\u7b56\u7565\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u6307\u51fa\u5373\u4f7f\u6709\u91cf\u5b50\u8ba1\u7b97\u7b49\u52a0\u901f\u6280\u672f\uff0c\u76d1\u7763\u5b66\u4e60\u4ecd\u7136\u9700\u8981\u4eba\u7c7b\u63d0\u4f9b\u521d\u59cb\u6807\u7b7e\u548c\u5148\u9a8c\u77e5\u8bc6\u3002\u751f\u6210\u5f0fAI\u53ef\u4ee5\u4f5c\u4e3a\u6807\u7b7e\u653e\u5927\u5668\uff0c\u4f46\u9700\u8981\u4eba\u7c7b\u76d1\u7763\u6765\u786e\u4fdd\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u5728\u8ba1\u7b97\u80fd\u529b\u6781\u5927\u63d0\u5347\u7684\u80cc\u666f\u4e0b\uff0c\u76d1\u7763\u5b66\u4e60\u662f\u5426\u4ecd\u7136\u9700\u8981\u4eba\u7c7b\u53c2\u4e0e\u3002\u63a2\u8ba8\u751f\u6210\u5f0fAI\u80fd\u5426\u5b8c\u5168\u66ff\u4ee3\u4eba\u7c7b\u6807\u6ce8\u5de5\u4f5c\u3002", "method": "\u5206\u6790\u53cd\u5411\u76d1\u7763\u7b56\u7565\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u8be5\u7b56\u7565\u5728\u5927\u578b\u672a\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u641c\u7d22\u6807\u7b7e\u5206\u914d\u4ee5\u6700\u5c0f\u5316\u5c0f\u578b\u6807\u6ce8\u6570\u636e\u96c6\u7684\u8bef\u5dee\u3002", "result": "\u53d1\u73b0\u641c\u7d22\u7a7a\u95f4\u4e3a2^n\uff0c\u5373\u4f7f\u6709\u5e38\u6570\u500d\u52a0\u901f\uff0c\u590d\u6742\u5ea6\u4ecd\u4e3a\u6307\u6570\u7ea7\u3002\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u53ea\u80fd\u51cf\u5c11\u8fd0\u884c\u65f6\u95f4\uff0c\u4e0d\u80fd\u6d88\u9664\u5bf9\u521d\u59cb\u76d1\u7763\u7684\u9700\u6c42\u3002", "conclusion": "\u6781\u7aef\u8ba1\u7b97\u901f\u5ea6\u65e0\u6cd5\u66ff\u4ee3\u76d1\u7763\u5b66\u4e60\u7684\u57fa\u672c\u9700\u6c42\uff0c\u4eba\u7c7b\uff08\u6216\u4eba\u7c7b\u7ea7\uff09\u7684\u521d\u59cb\u8f93\u5165\u4ecd\u7136\u662f\u5fc5\u8981\u7684\uff0c\u751f\u6210\u5f0fAI\u53ea\u80fd\u4f5c\u4e3a\u6807\u7b7e\u653e\u5927\u5668\u5728\u4eba\u7c7b\u76d1\u7763\u4e0b\u5de5\u4f5c\u3002"}}
{"id": "2510.11210", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11210", "abs": "https://arxiv.org/abs/2510.11210", "authors": ["Yisong Miao", "Min-Yen Kan"], "title": "Discursive Circuits: How Do Language Models Understand Discourse Relations?", "comment": "Accepted to EMNLP 2025 (Main Conference); 9 pages, 8 figures, 5\n  tables (20 pages, 12 figures, 14 tables including references and appendices)", "summary": "Which components in transformer language models are responsible for discourse\nunderstanding? We hypothesize that sparse computational graphs, termed as\ndiscursive circuits, control how models process discourse relations. Unlike\nsimpler tasks, discourse relations involve longer spans and complex reasoning.\nTo make circuit discovery feasible, we introduce a task called Completion under\nDiscourse Relation (CuDR), where a model completes a discourse given a\nspecified relation. To support this task, we construct a corpus of minimal\ncontrastive pairs tailored for activation patching in circuit discovery.\nExperiments show that sparse circuits ($\\approx 0.2\\%$ of a full GPT-2 model)\nrecover discourse understanding in the English PDTB-based CuDR task. These\ncircuits generalize well to unseen discourse frameworks such as RST and SDRT.\nFurther analysis shows lower layers capture linguistic features such as lexical\nsemantics and coreference, while upper layers encode discourse-level\nabstractions. Feature utility is consistent across frameworks (e.g.,\ncoreference supports Expansion-like relations).", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.10451", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10451", "abs": "https://arxiv.org/abs/2510.10451", "authors": ["Keisuke Fujii", "Kazushi Tsutsui", "Yu Teshima", "Makoto Itoh", "Naoya Takeishi", "Nozomi Nishiumi", "Ryoya Tanaka", "Shunsuke Shigaki", "Yoshinobu Kawahara"], "title": "Data-driven simulator of multi-animal behavior with unknown dynamics via offline and online reinforcement learning", "comment": "21 pages, 7 figures", "summary": "Simulators of animal movements play a valuable role in studying behavior.\nAdvances in imitation learning for robotics have expanded possibilities for\nreproducing human and animal movements. A key challenge for realistic\nmulti-animal simulation in biology is bridging the gap between unknown\nreal-world transition models and their simulated counterparts. Because\nlocomotion dynamics are seldom known, relying solely on mathematical models is\ninsufficient; constructing a simulator that both reproduces real trajectories\nand supports reward-driven optimization remains an open problem. We introduce a\ndata-driven simulator for multi-animal behavior based on deep reinforcement\nlearning and counterfactual simulation. We address the ill-posed nature of the\nproblem caused by high degrees of freedom in locomotion by estimating movement\nvariables of an incomplete transition model as actions within an RL framework.\nWe also employ a distance-based pseudo-reward to align and compare states\nbetween cyber and physical spaces. Validated on artificial agents, flies,\nnewts, and silkmoth, our approach achieves higher reproducibility of\nspecies-specific behaviors and improved reward acquisition compared with\nstandard imitation and RL methods. Moreover, it enables counterfactual behavior\nprediction in novel experimental settings and supports multi-individual\nmodeling for flexible what-if trajectory generation, suggesting its potential\nto simulate and elucidate complex multi-animal behaviors.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u53cd\u4e8b\u5b9e\u6a21\u62df\u7684\u591a\u52a8\u7269\u884c\u4e3a\u6570\u636e\u9a71\u52a8\u6a21\u62df\u5668\uff0c\u89e3\u51b3\u672a\u77e5\u771f\u5b9e\u4e16\u754c\u8f6c\u79fb\u6a21\u578b\u4e0e\u6a21\u62df\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u52a8\u7269\u8fd0\u52a8\u6a21\u62df\u5668\u5728\u884c\u4e3a\u7814\u7a76\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u590d\u73b0\u771f\u5b9e\u8f68\u8ff9\u548c\u652f\u6301\u5956\u52b1\u9a71\u52a8\u4f18\u5316\uff0c\u7279\u522b\u662f\u5728\u591a\u52a8\u7269\u573a\u666f\u4e0b\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4f30\u8ba1\u4e0d\u5b8c\u6574\u8f6c\u79fb\u6a21\u578b\u4e2d\u7684\u8fd0\u52a8\u53d8\u91cf\u4f5c\u4e3a\u52a8\u4f5c\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u8ddd\u79bb\u7684\u4f2a\u5956\u52b1\u6765\u5bf9\u9f50\u7f51\u7edc\u7a7a\u95f4\u548c\u7269\u7406\u7a7a\u95f4\u7684\u72b6\u6001\u3002", "result": "\u5728\u4eba\u5de5\u4ee3\u7406\u3001\u82cd\u8747\u3001\u877e\u8788\u548c\u8695\u86fe\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6\u6a21\u4eff\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u66f4\u597d\u5730\u590d\u73b0\u7269\u79cd\u7279\u5f02\u6027\u884c\u4e3a\u5e76\u83b7\u5f97\u66f4\u9ad8\u7684\u5956\u52b1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9884\u6d4b\u65b0\u5b9e\u9a8c\u73af\u5883\u4e2d\u7684\u53cd\u4e8b\u5b9e\u884c\u4e3a\uff0c\u652f\u6301\u591a\u4e2a\u4f53\u5efa\u6a21\u8fdb\u884c\u7075\u6d3b\u7684\u5047\u8bbe\u8f68\u8ff9\u751f\u6210\uff0c\u6709\u671b\u7528\u4e8e\u6a21\u62df\u548c\u9610\u660e\u590d\u6742\u7684\u591a\u52a8\u7269\u884c\u4e3a\u3002"}}
{"id": "2510.11217", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11217", "abs": "https://arxiv.org/abs/2510.11217", "authors": ["Chris Xing Tian", "Weihao Xie", "Zhen Chen", "Zhengyuan Yi", "Hui Liu", "Haoliang Li", "Shiqi Wang", "Siwei Ma"], "title": "Domain-Specific Data Generation Framework for RAG Adaptation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) combines the language understanding and\nreasoning power of large language models (LLMs) with external retrieval to\nenable domain-grounded responses. Effectively adapting RAG systems to\ndomain-specific settings requires specialized, context-rich training data\nbeyond general-purpose question-answering. Here, we propose RAGen, a scalable\nand modular framework for generating domain-grounded question-answer-context\n(QAC) triples tailored to diverse RAG adaptation approaches. RAGen produces\nthese QAC triples by identifying key concepts in documents, generating diverse\nquestions guided by Bloom's Taxonomy-inspired principles, and pairing them with\nprecise answers extracted from relevant contexts. RAGen supports multiple RAG\nadaptation strategies, including the optimization of key components such as the\nLLM, retriever, and embedding model, etc. Its modular pipeline features\nsemantic chunking, hierarchical concept extraction, and multi-chunk retrieval,\nalong with the introduction of curated distractor contexts to promote robust\nreasoning. Designed for scalability, RAGen efficiently handles large and\nevolving document corpora without redundant processing, making it especially\nsuitable for dynamic evolving domains such as scientific research and\nenterprise knowledge bases.", "AI": {"tldr": "RAGen\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9886\u57df\u7279\u5b9a\u7684\u95ee\u7b54\u4e0a\u4e0b\u6587\u4e09\u5143\u7ec4\uff0c\u4ee5\u652f\u6301\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7cfb\u7edf\u7684\u9886\u57df\u9002\u5e94\u3002", "motivation": "\u6709\u6548\u9002\u5e94RAG\u7cfb\u7edf\u5230\u7279\u5b9a\u9886\u57df\u9700\u8981\u4e13\u95e8\u7684\u3001\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u901a\u7528\u95ee\u7b54\u6570\u636e\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u6587\u6863\u4e2d\u7684\u5173\u952e\u6982\u5ff5\uff0c\u57fa\u4e8e\u5e03\u9c81\u59c6\u5206\u7c7b\u5b66\u539f\u5219\u751f\u6210\u591a\u6837\u5316\u95ee\u9898\uff0c\u5e76\u4ece\u76f8\u5173\u4e0a\u4e0b\u6587\u4e2d\u63d0\u53d6\u7cbe\u786e\u7b54\u6848\u6765\u751f\u6210\u95ee\u7b54\u4e0a\u4e0b\u6587\u4e09\u5143\u7ec4\u3002\u652f\u6301\u591a\u79cdRAG\u9002\u5e94\u7b56\u7565\uff0c\u5305\u62ec\u8bed\u4e49\u5206\u5757\u3001\u5c42\u6b21\u6982\u5ff5\u63d0\u53d6\u548c\u591a\u5757\u68c0\u7d22\u3002", "result": "RAGen\u80fd\u591f\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u548c\u4e0d\u65ad\u6f14\u5316\u7684\u6587\u6863\u8bed\u6599\u5e93\uff0c\u65e0\u9700\u5197\u4f59\u5904\u7406\uff0c\u7279\u522b\u9002\u5408\u52a8\u6001\u6f14\u5316\u7684\u9886\u57df\u3002", "conclusion": "RAGen\u4e3aRAG\u7cfb\u7edf\u7684\u9886\u57df\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6a21\u5757\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u79d1\u5b66\u7814\u7a76\u548c\u4f01\u4e1a\u77e5\u8bc6\u5e93\u7b49\u52a8\u6001\u9886\u57df\u3002"}}
{"id": "2510.10465", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10465", "abs": "https://arxiv.org/abs/2510.10465", "authors": ["Yi Ren", "Xinjie Yu"], "title": "LightSAE: Parameter-Efficient and Heterogeneity-Aware Embedding for IoT Multivariate Time Series Forecasting", "comment": "Submitted to IEEE IoT-J", "summary": "Modern Internet of Things (IoT) systems generate massive, heterogeneous\nmultivariate time series data. Accurate Multivariate Time Series Forecasting\n(MTSF) of such data is critical for numerous applications. However, existing\nmethods almost universally employ a shared embedding layer that processes all\nchannels identically, creating a representational bottleneck that obscures\nvaluable channel-specific information. To address this challenge, we introduce\na Shared-Auxiliary Embedding (SAE) framework that decomposes the embedding into\na shared base component capturing common patterns and channel-specific\nauxiliary components modeling unique deviations. Within this decomposition, we\n\\rev{empirically observe} that the auxiliary components tend to exhibit\nlow-rank and clustering characteristics, a structural pattern that is\nsignificantly less apparent when using purely independent embeddings.\nConsequently, we design LightSAE, a parameter-efficient embedding module that\noperationalizes these observed characteristics through low-rank factorization\nand a shared, gated component pool. Extensive experiments across 9 IoT-related\ndatasets and 4 backbone architectures demonstrate LightSAE's effectiveness,\nachieving MSE improvements of up to 22.8\\% with only 4.0\\% parameter increase.", "AI": {"tldr": "\u63d0\u51fa\u4e86LightSAE\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab-\u8f85\u52a9\u5d4c\u5165\u5206\u89e3\u89e3\u51b3\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u8868\u793a\u74f6\u9888\u95ee\u9898\uff0c\u5728\u53c2\u6570\u6548\u7387\u663e\u8457\u63d0\u5347\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9884\u6d4b\u6027\u80fd\u7684\u5927\u5e45\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u666e\u904d\u4f7f\u7528\u5171\u4eab\u5d4c\u5165\u5c42\u5904\u7406\u6240\u6709\u901a\u9053\uff0c\u9020\u6210\u8868\u793a\u74f6\u9888\uff0c\u6a21\u7cca\u4e86\u6709\u4ef7\u503c\u7684\u901a\u9053\u7279\u5b9a\u4fe1\u606f\u3002", "method": "\u5f15\u5165\u5171\u4eab-\u8f85\u52a9\u5d4c\u5165(SAE)\u6846\u67b6\uff0c\u5c06\u5d4c\u5165\u5206\u89e3\u4e3a\u6355\u83b7\u5171\u540c\u6a21\u5f0f\u7684\u5171\u4eab\u57fa\u7840\u7ec4\u4ef6\u548c\u5efa\u6a21\u72ec\u7279\u504f\u5dee\u7684\u901a\u9053\u7279\u5b9a\u8f85\u52a9\u7ec4\u4ef6\uff0c\u5e76\u57fa\u4e8e\u89c2\u5bdf\u5230\u7684\u4f4e\u79e9\u548c\u805a\u7c7b\u7279\u6027\u8bbe\u8ba1\u53c2\u6570\u9ad8\u6548\u7684LightSAE\u6a21\u5757\u3002", "result": "\u57289\u4e2a\u7269\u8054\u7f51\u6570\u636e\u96c6\u548c4\u4e2a\u9aa8\u5e72\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLightSAE\u4ec5\u589e\u52a04.0%\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u9ad8\u8fbe22.8%\u7684MSE\u6539\u8fdb\u3002", "conclusion": "LightSAE\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u8868\u793a\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2510.11218", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11218", "abs": "https://arxiv.org/abs/2510.11218", "authors": ["Saad Obaid ul Islam", "Anne Lauscher", "Goran Glava\u0161"], "title": "The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form Answers", "comment": null, "summary": "Large language models (LLMs) can correctly answer \"When was Einstein born?\"\nyet fail to provide the same date when writing about Einstein's life revealing\na fundamental inconsistency in how models access factual knowledge across task\ncomplexities. While models display impressive accuracy on factual\nquestion-answering benchmarks, the reliability gap between simple and complex\nqueries remains poorly understood, eroding their trustworthiness. In this work,\nwe introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a\ncontrolled evaluation framework that compares LLMs' answers to the same factual\nquestions asked (a) in isolation (short) vs. (b) integrated into complex\nqueries (long). Looking at 16 LLMs across 600 queries, we find a systematic\nmisalignment of answers to the corresponding short and long queries. We further\nuncover position-dependent accuracy loss and momentum effects where consecutive\ncorrect or incorrect answers create self-reinforcing patterns. Through\nmechanistic analysis, we find that aligned facts activate overlapping model\ninternals, and that metrics based on mechanistic similarity can predict\nshort-long answer alignment with up to 78% accuracy. Our work establishes\nfactual consistency over query complexity as an important aspect of LLMs'\ntrustworthiness and challenges current evaluation practices, which implicitly\nassume that good performance for simple factual queries implies reliability in\nmore complex knowledge-seeking tasks too.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0LLMs\u5728\u7b80\u5355\u95ee\u7b54\u548c\u590d\u6742\u67e5\u8be2\u4e2d\u5bf9\u540c\u4e00\u4e8b\u5b9e\u7684\u56de\u7b54\u5b58\u5728\u7cfb\u7edf\u6027\u4e0d\u4e00\u81f4\uff0c\u63d0\u51fa\u4e86SLAQ\u8bc4\u4f30\u6846\u67b6\u6765\u91cf\u5316\u8fd9\u79cd\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u4f4d\u7f6e\u4f9d\u8d56\u7684\u51c6\u786e\u6027\u635f\u5931\u548c\u52a8\u91cf\u6548\u5e94\u3002", "motivation": "LLMs\u5728\u7b80\u5355\u4e8b\u5b9e\u95ee\u7b54\u4e2d\u8868\u73b0\u51c6\u786e\uff0c\u4f46\u5728\u590d\u6742\u67e5\u8be2\u4e2d\u56de\u7b54\u540c\u4e00\u4e8b\u5b9e\u65f6\u51fa\u73b0\u4e0d\u4e00\u81f4\uff0c\u8fd9\u79cd\u53ef\u9760\u6027\u5dee\u8ddd\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u63d0\u51faSLAQ\u8bc4\u4f30\u6846\u67b6\uff0c\u6bd4\u8f83LLMs\u5bf9\u540c\u4e00\u4e8b\u5b9e\u5728\u7b80\u5355\u67e5\u8be2\u548c\u590d\u6742\u67e5\u8be2\u4e2d\u7684\u56de\u7b54\u4e00\u81f4\u6027\uff0c\u5206\u6790\u4e8616\u4e2aLLM\u5728600\u4e2a\u67e5\u8be2\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u77ed\u957f\u67e5\u8be2\u56de\u7b54\u5b58\u5728\u7cfb\u7edf\u6027\u4e0d\u4e00\u81f4\uff0c\u5b58\u5728\u4f4d\u7f6e\u4f9d\u8d56\u7684\u51c6\u786e\u6027\u635f\u5931\u548c\u52a8\u91cf\u6548\u5e94\uff0c\u673a\u5236\u5206\u6790\u663e\u793a\u5bf9\u9f50\u4e8b\u5b9e\u6fc0\u6d3b\u91cd\u53e0\u7684\u6a21\u578b\u5185\u90e8\u7ed3\u6784\u3002", "conclusion": "\u4e8b\u5b9e\u4e00\u81f4\u6027\u662fLLMs\u53ef\u4fe1\u5ea6\u7684\u91cd\u8981\u65b9\u9762\uff0c\u5f53\u524d\u8bc4\u4f30\u5b9e\u8df5\u5047\u8bbe\u7b80\u5355\u67e5\u8be2\u6027\u80fd\u597d\u610f\u5473\u7740\u590d\u6742\u4efb\u52a1\u53ef\u9760\u6027\u7684\u505a\u6cd5\u5b58\u5728\u95ee\u9898\u3002"}}
{"id": "2510.10467", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10467", "abs": "https://arxiv.org/abs/2510.10467", "authors": ["Gunho Park", "Jeongin Bae", "Beomseok Kwon", "Byeongwook Kim", "Se Jung Kwon", "Dongsoo Lee"], "title": "AnyBCQ: Hardware Efficient Flexible Binary-Coded Quantization for Multi-Precision LLMs", "comment": null, "summary": "The deployment of large language models (LLMs) is increasingly constrained by\nmemory and latency bottlenecks, motivating the need for quantization techniques\nthat flexibly balance accuracy and efficiency. Recent work has introduced\nmulti-precision models, which enable inference at multiple precisions within a\nsingle model depending on runtime constraints. To support such flexibility,\nquantized weights are often stored as bit-planes, where hardware efficiency\nimproves when the compute operates directly at the bit-plane level and\nactivates only the precision required by each request. In this work, we present\nAnyBCQ, a hardware-friendly multi-precision extension of Binary-Coded\nQuantization (BCQ) that supports direct bit-plane operations. By representing\nweights as binary bit-planes with corresponding scale factors, AnyBCQ enables\nbit-plane-level computation and maps naturally to accelerator-friendly,\nbit-parallel arithmetic. Our progressive precision expansion mechanism\nincrementally refines scaling factors while reusing previously assigned binary\ncodes, yielding monotonic improvements in accuracy as additional bits are\nenabled. We further co-design a specialized kernel that exploits the BCQ\nstructure to support dynamic per-request precision selection with negligible\noverhead. Experiments on recent LLMs demonstrate that AnyBCQ significantly\nnarrows the accuracy drop in the low-bit regime (e.g. 2-bit), remains\ncompetitive at higher precision, and achieves throughput gains of up to 3.0x\nover half precision and 1.2x over state-of-the-art multi-precision methods. By\naligning algorithmic flexibility with hardware efficiency, AnyBCQ provides a\npractical foundation for multi-precision LLM deployment across diverse\nservice-level objectives.", "AI": {"tldr": "AnyBCQ\u662f\u4e00\u79cd\u786c\u4ef6\u53cb\u597d\u7684\u591a\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e8c\u8fdb\u5236\u4f4d\u5e73\u9762\u8868\u793a\u6743\u91cd\uff0c\u652f\u6301\u76f4\u63a5\u4f4d\u5e73\u9762\u64cd\u4f5c\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u9762\u4e34\u5185\u5b58\u548c\u5ef6\u8fdf\u74f6\u9888\uff0c\u9700\u8981\u7075\u6d3b\u5e73\u8861\u7cbe\u5ea6\u548c\u6548\u7387\u7684\u91cf\u5316\u6280\u672f\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u652f\u6301\u8fd0\u884c\u65f6\u6839\u636e\u7ea6\u675f\u52a8\u6001\u9009\u62e9\u7cbe\u5ea6\u3002", "method": "\u6269\u5c55Binary-Coded Quantization(BCQ)\uff0c\u5c06\u6743\u91cd\u8868\u793a\u4e3a\u4e8c\u8fdb\u5236\u4f4d\u5e73\u9762\u548c\u7f29\u653e\u56e0\u5b50\uff0c\u652f\u6301\u4f4d\u5e73\u9762\u7ea7\u8ba1\u7b97\u548c\u6e10\u8fdb\u7cbe\u5ea6\u6269\u5c55\u673a\u5236\u3002", "result": "\u5728\u4f4e\u6bd4\u7279\u4f4d(\u59822\u4f4d)\u663e\u8457\u51cf\u5c11\u7cbe\u5ea6\u635f\u5931\uff0c\u5728\u9ad8\u7cbe\u5ea6\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u76f8\u6bd4\u534a\u7cbe\u5ea6\u5b9e\u73b03.0\u500d\u541e\u5410\u91cf\u63d0\u5347\uff0c\u6bd4\u73b0\u6709\u591a\u7cbe\u5ea6\u65b9\u6cd5\u63d0\u53471.2\u500d\u3002", "conclusion": "AnyBCQ\u901a\u8fc7\u7b97\u6cd5\u7075\u6d3b\u6027\u4e0e\u786c\u4ef6\u6548\u7387\u7684\u5bf9\u9f50\uff0c\u4e3a\u591a\u7cbe\u5ea6LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u670d\u52a1\u7ea7\u522b\u76ee\u6807\u3002"}}
{"id": "2510.11221", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11221", "abs": "https://arxiv.org/abs/2510.11221", "authors": ["Tao Li", "Jinlong Hu", "Yang Wang", "Junfeng Liu", "Xuejun Liu"], "title": "WebRouter: Query-specific Router via Variational Information Bottleneck for Cost-sensitive Web Agent", "comment": null, "summary": "LLM-brained web agents offer powerful capabilities for web automation but\nface a critical cost-performance trade-off. The challenge is amplified by web\nagents' inherently complex prompts that include goals, action histories, and\nenvironmental states, leading to degraded LLM ensemble performance. To address\nthis, we introduce WebRouter, a novel query-specific router trained from an\ninformation-theoretic perspective. Our core contribution is a cost-aware\nVariational Information Bottleneck (ca-VIB) objective, which learns a\ncompressed representation of the input prompt while explicitly penalizing the\nexpected operational cost. Experiments on five real-world websites from the\nWebVoyager benchmark show that WebRouter reduces operational costs by a\nstriking 87.8\\% compared to a GPT-4o baseline, while incurring only a 3.8\\%\naccuracy drop.", "AI": {"tldr": "WebRouter\u662f\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u67e5\u8be2\u7279\u5b9a\u8def\u7531\u5668\uff0c\u901a\u8fc7\u6210\u672c\u611f\u77e5\u7684\u53d8\u5206\u4fe1\u606f\u74f6\u9888\u76ee\u6807\uff0c\u5728\u538b\u7f29\u8f93\u5165\u63d0\u793a\u7684\u540c\u65f6\u663e\u5f0f\u60e9\u7f5a\u9884\u671f\u64cd\u4f5c\u6210\u672c\uff0c\u663e\u8457\u964d\u4f4eLLM\u9a71\u52a8\u7684Web\u4ee3\u7406\u7684\u8fd0\u8425\u6210\u672c\u3002", "motivation": "LLM\u9a71\u52a8\u7684Web\u4ee3\u7406\u9762\u4e34\u6210\u672c\u4e0e\u6027\u80fd\u7684\u5173\u952e\u6743\u8861\uff0c\u590d\u6742\u7684\u63d0\u793a\uff08\u5305\u542b\u76ee\u6807\u3001\u884c\u52a8\u5386\u53f2\u548c\u72b6\u6001\uff09\u5bfc\u81f4LLM\u96c6\u6210\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u6210\u672c\u6548\u76ca\u6311\u6218\u3002", "method": "\u63d0\u51faWebRouter\uff0c\u91c7\u7528\u6210\u672c\u611f\u77e5\u7684\u53d8\u5206\u4fe1\u606f\u74f6\u9888\uff08ca-VIB\uff09\u76ee\u6807\uff0c\u5b66\u4e60\u8f93\u5165\u63d0\u793a\u7684\u538b\u7f29\u8868\u793a\uff0c\u540c\u65f6\u663e\u5f0f\u60e9\u7f5a\u9884\u671f\u64cd\u4f5c\u6210\u672c\u3002", "result": "\u5728WebVoyager\u57fa\u51c6\u7684\u4e94\u4e2a\u771f\u5b9e\u7f51\u7ad9\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0eGPT-4o\u57fa\u7ebf\u76f8\u6bd4\uff0cWebRouter\u5c06\u8fd0\u8425\u6210\u672c\u964d\u4f4e\u4e8687.8%\uff0c\u800c\u51c6\u786e\u7387\u4ec5\u4e0b\u964d3.8%\u3002", "conclusion": "WebRouter\u901a\u8fc7\u4fe1\u606f\u8bba\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u9a71\u52a8\u7684Web\u4ee3\u7406\u7684\u6210\u672c\u6027\u80fd\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6210\u672c\u8282\u7ea6\u548c\u53ef\u63a5\u53d7\u7684\u6027\u80fd\u635f\u5931\u3002"}}
{"id": "2510.10477", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10477", "abs": "https://arxiv.org/abs/2510.10477", "authors": ["Zhijian Zhou", "Liuhua Peng", "Xunye Tian", "Feng Liu"], "title": "Anchor-based Maximum Discrepancy for Relative Similarity Testing", "comment": null, "summary": "The relative similarity testing aims to determine which of the distributions,\nP or Q, is closer to an anchor distribution U. Existing kernel-based approaches\noften test the relative similarity with a fixed kernel in a manually specified\nalternative hypothesis, e.g., Q is closer to U than P. Although kernel\nselection is known to be important to kernel-based testing methods, the\nmanually specified hypothesis poses a significant challenge for kernel\nselection in relative similarity testing: Once the hypothesis is specified\nfirst, we can always find a kernel such that the hypothesis is rejected. This\nchallenge makes relative similarity testing ill-defined when we want to select\na good kernel after the hypothesis is specified. In this paper, we cope with\nthis challenge via learning a proper hypothesis and a kernel simultaneously,\ninstead of learning a kernel after manually specifying the hypothesis. We\npropose an anchor-based maximum discrepancy (AMD), which defines the relative\nsimilarity as the maximum discrepancy between the distances of (U, P) and (U,\nQ) in a space of deep kernels. Based on AMD, our testing incorporates two\nphases. In Phase I, we estimate the AMD over the deep kernel space and infer\nthe potential hypothesis. In Phase II, we assess the statistical significance\nof the potential hypothesis, where we propose a unified testing framework to\nderive thresholds for tests over different possible hypotheses from Phase I.\nLastly, we validate our method theoretically and demonstrate its effectiveness\nvia extensive experiments on benchmark datasets. Codes are publicly available\nat: https://github.com/zhijianzhouml/AMD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u951a\u70b9\u7684\u6700\u5927\u5dee\u5f02(AMD)\u65b9\u6cd5\uff0c\u7528\u4e8e\u76f8\u5bf9\u76f8\u4f3c\u6027\u6d4b\u8bd5\uff0c\u901a\u8fc7\u540c\u65f6\u5b66\u4e60\u5047\u8bbe\u548c\u6838\u51fd\u6570\u6765\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u6838\u9009\u62e9\u4e0e\u5047\u8bbe\u6307\u5b9a\u987a\u5e8f\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6838\u7684\u76f8\u5bf9\u76f8\u4f3c\u6027\u6d4b\u8bd5\u65b9\u6cd5\u5b58\u5728\u6838\u9009\u62e9\u4e0e\u5047\u8bbe\u6307\u5b9a\u987a\u5e8f\u7684\u95ee\u9898\uff1a\u4e00\u65e6\u5047\u8bbe\u88ab\u6307\u5b9a\uff0c\u603b\u80fd\u627e\u5230\u4f7f\u8be5\u5047\u8bbe\u88ab\u62d2\u7edd\u7684\u6838\u51fd\u6570\uff0c\u8fd9\u4f7f\u5f97\u76f8\u5bf9\u76f8\u4f3c\u6027\u6d4b\u8bd5\u5728\u5047\u8bbe\u6307\u5b9a\u540e\u9009\u62e9\u5408\u9002\u6838\u51fd\u6570\u65f6\u53d8\u5f97\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51fa\u951a\u70b9\u6700\u5927\u5dee\u5f02(AMD)\u65b9\u6cd5\uff0c\u5728\u6df1\u5ea6\u6838\u7a7a\u95f4\u4e2d\u5b9a\u4e49\u76f8\u5bf9\u76f8\u4f3c\u6027\u4e3a(U,P)\u548c(U,Q)\u8ddd\u79bb\u7684\u6700\u5927\u5dee\u5f02\u3002\u6d4b\u8bd5\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f30\u8ba1AMD\u5e76\u63a8\u65ad\u6f5c\u5728\u5047\u8bbe\uff0c\u7b2c\u4e8c\u9636\u6bb5\u8bc4\u4f30\u6f5c\u5728\u5047\u8bbe\u7684\u7edf\u8ba1\u663e\u8457\u6027\uff0c\u63d0\u51fa\u7edf\u4e00\u6d4b\u8bd5\u6846\u67b6\u63a8\u5bfc\u4e0d\u540c\u53ef\u80fd\u5047\u8bbe\u7684\u9608\u503c\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u9a8c\u8bc1\u548c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "conclusion": "\u901a\u8fc7\u540c\u65f6\u5b66\u4e60\u5047\u8bbe\u548c\u6838\u51fd\u6570\uff0cAMD\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u76f8\u5bf9\u76f8\u4f3c\u6027\u6d4b\u8bd5\u4e2d\u7684\u6838\u9009\u62e9\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5408\u7406\u7684\u6d4b\u8bd5\u6846\u67b6\u3002"}}
{"id": "2510.11222", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11222", "abs": "https://arxiv.org/abs/2510.11222", "authors": ["Battemuulen Naranbat", "Seyed Sahand Mohammadi Ziabari", "Yousuf Nasser Al Husaini", "Ali Mohammed Mansoor Alsahag"], "title": "Fairness Metric Design Exploration in Multi-Domain Moral Sentiment Classification using Transformer-Based Models", "comment": null, "summary": "Ensuring fairness in natural language processing for moral sentiment\nclassification is challenging, particularly under cross-domain shifts where\ntransformer models are increasingly deployed. Using the Moral Foundations\nTwitter Corpus (MFTC) and Moral Foundations Reddit Corpus (MFRC), this work\nevaluates BERT and DistilBERT in a multi-label setting with in-domain and\ncross-domain protocols. Aggregate performance can mask disparities: we observe\npronounced asymmetry in transfer, with Twitter->Reddit degrading micro-F1 by\n14.9% versus only 1.5% for Reddit->Twitter. Per-label analysis reveals fairness\nviolations hidden by overall scores; notably, the authority label exhibits\nDemographic Parity Differences of 0.22-0.23 and Equalized Odds Differences of\n0.40-0.41. To address this gap, we introduce the Moral Fairness Consistency\n(MFC) metric, which quantifies the cross-domain stability of moral foundation\ndetection. MFC shows strong empirical validity, achieving a perfect negative\ncorrelation with Demographic Parity Difference (rho = -1.000, p < 0.001) while\nremaining independent of standard performance metrics. Across labels, loyalty\ndemonstrates the highest consistency (MFC = 0.96) and authority the lowest (MFC\n= 0.78). These findings establish MFC as a complementary, diagnosis-oriented\nmetric for fairness-aware evaluation of moral reasoning models, enabling more\nreliable deployment across heterogeneous linguistic contexts. .", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86BERT\u548cDistilBERT\u5728\u9053\u5fb7\u60c5\u611f\u5206\u7c7b\u4e2d\u7684\u8de8\u9886\u57df\u516c\u5e73\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u805a\u5408\u6027\u80fd\u4f1a\u63a9\u76d6\u516c\u5e73\u6027\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u9053\u5fb7\u516c\u5e73\u4e00\u81f4\u6027(MFC)\u6307\u6807\u6765\u91cf\u5316\u8de8\u9886\u57df\u7a33\u5b9a\u6027\u3002", "motivation": "\u786e\u4fdd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u9053\u5fb7\u60c5\u611f\u5206\u7c7b\u7684\u516c\u5e73\u6027\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5728\u8de8\u9886\u57df\u8f6c\u79fb\u7684\u60c5\u51b5\u4e0b\uff0c\u5176\u4e2dTransformer\u6a21\u578b\u88ab\u5e7f\u6cdb\u90e8\u7f72\u3002", "method": "\u4f7f\u7528\u9053\u5fb7\u57fa\u7840Twitter\u8bed\u6599\u5e93(MFTC)\u548c\u9053\u5fb7\u57fa\u7840Reddit\u8bed\u6599\u5e93(MFRC)\uff0c\u5728\u9886\u57df\u5185\u548c\u8de8\u9886\u57df\u534f\u8bae\u4e0b\u8bc4\u4f30BERT\u548cDistilBERT\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u89c2\u5bdf\u5230\u663e\u8457\u7684\u8de8\u9886\u57df\u4e0d\u5bf9\u79f0\u6027\uff1aTwitter->Reddit\u8f6c\u79fb\u4f7fmicro-F1\u4e0b\u964d14.9%\uff0c\u800cReddit->Twitter\u4ec5\u4e0b\u964d1.5%\u3002\u6743\u5a01\u6807\u7b7e\u8868\u73b0\u51fa0.22-0.23\u7684\u4eba\u53e3\u7edf\u8ba1\u5747\u7b49\u5dee\u5f02\u548c0.40-0.41\u7684\u5747\u7b49\u673a\u4f1a\u5dee\u5f02\u3002MFC\u4e0e\u4eba\u53e3\u7edf\u8ba1\u5747\u7b49\u5dee\u5f02\u5448\u5b8c\u7f8e\u8d1f\u76f8\u5173(rho = -1.000, p < 0.001)\u3002", "conclusion": "MFC\u4f5c\u4e3a\u8865\u5145\u6027\u3001\u8bca\u65ad\u5bfc\u5411\u7684\u6307\u6807\uff0c\u80fd\u591f\u5bf9\u9053\u5fb7\u63a8\u7406\u6a21\u578b\u8fdb\u884c\u516c\u5e73\u6027\u611f\u77e5\u8bc4\u4f30\uff0c\u5b9e\u73b0\u8de8\u5f02\u6784\u8bed\u8a00\u73af\u5883\u7684\u66f4\u53ef\u9760\u90e8\u7f72\u3002"}}
{"id": "2510.10480", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10480", "abs": "https://arxiv.org/abs/2510.10480", "authors": ["Zishen Zhang", "Xiangzhe Kong", "Wenbing Huang", "Yang Liu"], "title": "Latent Retrieval Augmented Generation of Cross-Domain Protein Binders", "comment": null, "summary": "Designing protein binders targeting specific sites, which requires to\ngenerate realistic and functional interaction patterns, is a fundamental\nchallenge in drug discovery. Current structure-based generative models are\nlimited in generating nterfaces with sufficient rationality and\ninterpretability. In this paper, we propose Retrieval-Augmented Diffusion for\nAligned interface (RADiAnce), a new framework that leverages known interfaces\nto guide the design of novel binders. By unifying retrieval and generation in a\nshared contrastive latent space, our model efficiently identifies relevant\ninterfaces for a given binding site and seamlessly integrates them through a\nconditional latent diffusion generator, enabling cross-domain interface\ntransfer. Extensive exeriments show that RADiAnce significantly outperforms\nbaseline models across multiple metrics, including binding affinity and\nrecovery of geometries and interactions. Additional experimental results\nvalidate cross-domain generalization, demonstrating that retrieving interfaces\nfrom diverse domains, such as peptides, antibodies, and protein fragments,\nenhances the generation performance of binders for other domains. Our work\nestablishes a new paradigm for protein binder design that successfully bridges\nretrieval-based knowledge and generative AI, opening new possibilities for drug\ndiscovery.", "AI": {"tldr": "RADiAnce\u662f\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u6269\u6563\u7684\u86cb\u767d\u8d28\u7ed3\u5408\u4f4d\u70b9\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u68c0\u7d22\u548c\u751f\u6210\u5728\u5171\u4eab\u5bf9\u6bd4\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5229\u7528\u5df2\u77e5\u754c\u9762\u6307\u5bfc\u65b0\u578b\u7ed3\u5408\u5242\u7684\u8bbe\u8ba1\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u7ed3\u6784\u7684\u751f\u6210\u6a21\u578b\u5728\u751f\u6210\u5177\u6709\u8db3\u591f\u5408\u7406\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u754c\u9762\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u800c\u8bbe\u8ba1\u9488\u5bf9\u7279\u5b9a\u4f4d\u70b9\u7684\u86cb\u767d\u8d28\u7ed3\u5408\u5242\u662f\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u57fa\u672c\u6311\u6218\u3002", "method": "\u63d0\u51faRADiAnce\u6846\u67b6\uff0c\u5728\u5171\u4eab\u5bf9\u6bd4\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7edf\u4e00\u68c0\u7d22\u548c\u751f\u6210\uff0c\u901a\u8fc7\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u751f\u6210\u5668\u8bc6\u522b\u76f8\u5173\u754c\u9762\u5e76\u5b9e\u73b0\u8de8\u57df\u754c\u9762\u8f6c\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRADiAnce\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5305\u62ec\u7ed3\u5408\u4eb2\u548c\u529b\u4ee5\u53ca\u51e0\u4f55\u7ed3\u6784\u548c\u76f8\u4e92\u4f5c\u7528\u7684\u6062\u590d\u3002\u8de8\u57df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4ece\u4e0d\u540c\u57df\u68c0\u7d22\u754c\u9762\u80fd\u589e\u5f3a\u5176\u4ed6\u57df\u7ed3\u5408\u5242\u7684\u751f\u6210\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5efa\u7acb\u4e86\u86cb\u767d\u8d28\u7ed3\u5408\u5242\u8bbe\u8ba1\u7684\u65b0\u8303\u5f0f\uff0c\u6210\u529f\u6865\u63a5\u4e86\u57fa\u4e8e\u68c0\u7d22\u7684\u77e5\u8bc6\u548c\u751f\u6210\u5f0fAI\uff0c\u4e3a\u836f\u7269\u53d1\u73b0\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.11225", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11225", "abs": "https://arxiv.org/abs/2510.11225", "authors": ["Hayate Funakura", "Hyunsoo Kim", "Koji Mineshima"], "title": "A Theorem-Proving-Based Evaluation of Neural Semantic Parsing", "comment": "Accepted to BlackboxNLP 2025", "summary": "Graph-matching metrics such as Smatch are the de facto standard for\nevaluating neural semantic parsers, yet they capture surface overlap rather\nthan logical equivalence. We reassess evaluation by pairing graph-matching with\nautomated theorem proving. We compare two approaches to building parsers:\nsupervised fine-tuning (T5-Small/Base) and few-shot in-context learning\n(GPT-4o/4.1/5), under normalized and unnormalized targets. We evaluate outputs\nusing graph-matching, bidirectional entailment between source and target\nformulas with a first-order logic theorem prover, and well-formedness. Across\nsettings, we find that models performing well on graph-matching often fail to\nproduce logically equivalent formulas. Normalization reduces incidental target\nvariability, improves well-formedness, and strengthens logical adequacy. Error\nanalysis shows performance degrades with increasing formula complexity and with\ncoordination, prepositional phrases, and passive voice; the dominant failures\ninvolve variable binding and indexing, and predicate naming. These findings\nhighlight limits of graph-based metrics for reasoning-oriented applications and\nmotivate logic-sensitive evaluation and training objectives together with\nsimplified, normalized target representations. All code and data for our\nexperiments are publicly available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u91cd\u65b0\u8bc4\u4f30\u8bed\u4e49\u89e3\u6790\u5668\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5c06\u56fe\u5339\u914d\u4e0e\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u76f8\u7ed3\u5408\uff0c\u53d1\u73b0\u57fa\u4e8e\u56fe\u5339\u914d\u7684\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u903b\u8f91\u7b49\u4ef7\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u903b\u8f91\u654f\u611f\u8bc4\u4f30\u548c\u7b80\u5316\u76ee\u6807\u8868\u793a\u7684\u5efa\u8bae\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u56fe\u5339\u914d\u7684\u8bc4\u4f30\u6307\u6807\uff08\u5982Smatch\uff09\u53ea\u5173\u6ce8\u8868\u9762\u91cd\u53e0\u800c\u975e\u903b\u8f91\u7b49\u4ef7\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u8bed\u4e49\u89e3\u6790\u5668\u5728\u63a8\u7406\u5bfc\u5411\u5e94\u7528\u4e2d\u7684\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "method": "\u6bd4\u8f83\u76d1\u7763\u5fae\u8c03\uff08T5-Small/Base\uff09\u548c\u5c11\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08GPT-4o/4.1/5\uff09\u4e24\u79cd\u89e3\u6790\u5668\u6784\u5efa\u65b9\u6cd5\uff0c\u4f7f\u7528\u56fe\u5339\u914d\u3001\u53cc\u5411\u8574\u542b\u9a8c\u8bc1\u548c\u683c\u5f0f\u826f\u597d\u6027\u8fdb\u884c\u591a\u7ef4\u5ea6\u8bc4\u4f30\u3002", "result": "\u5728\u56fe\u5339\u914d\u4e0a\u8868\u73b0\u826f\u597d\u7684\u6a21\u578b\u5f80\u5f80\u65e0\u6cd5\u4ea7\u751f\u903b\u8f91\u7b49\u4ef7\u7684\u516c\u5f0f\u3002\u5f52\u4e00\u5316\u51cf\u5c11\u4e86\u76ee\u6807\u53d8\u5f02\u6027\uff0c\u63d0\u9ad8\u4e86\u683c\u5f0f\u826f\u597d\u6027\u548c\u903b\u8f91\u5145\u5206\u6027\u3002\u9519\u8bef\u5206\u6790\u663e\u793a\u6027\u80fd\u968f\u516c\u5f0f\u590d\u6742\u5ea6\u589e\u52a0\u800c\u4e0b\u964d\uff0c\u4e3b\u8981\u5931\u8d25\u6d89\u53ca\u53d8\u91cf\u7ed1\u5b9a\u3001\u7d22\u5f15\u548c\u8c13\u8bcd\u547d\u540d\u3002", "conclusion": "\u57fa\u4e8e\u56fe\u7684\u8bc4\u4f30\u6307\u6807\u5728\u63a8\u7406\u5bfc\u5411\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u903b\u8f91\u654f\u611f\u7684\u8bc4\u4f30\u548c\u8bad\u7ec3\u76ee\u6807\uff0c\u4ee5\u53ca\u7b80\u5316\u3001\u5f52\u4e00\u5316\u7684\u76ee\u6807\u8868\u793a\u3002"}}
{"id": "2510.10483", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.10483", "abs": "https://arxiv.org/abs/2510.10483", "authors": ["Narayan S Iyer", "Bivas Bhaumik", "Ram S Iyer", "Satyasaran Changdar"], "title": "Gradient Enhanced Self-Training Physics-Informed Neural Network (gST-PINN) for Solving Nonlinear Partial Differential Equations", "comment": null, "summary": "Partial differential equations (PDEs) provide a mathematical foundation for\nsimulating and understanding intricate behaviors in both physical sciences and\nengineering. With the growing capabilities of deep learning, data$-$driven\napproaches like Physics$-$Informed Neural Networks (PINNs) have been developed,\noffering a mesh$-$free, analytic type framework for efficiently solving PDEs\nacross a wide range of applications. However, traditional PINNs often struggle\nwith challenges such as limited precision, slow training dynamics, lack of\nlabeled data availability, and inadequate handling of multi$-$physics\ninteractions. To overcome these challenging issues of PINNs, we proposed a\nGradient Enhanced Self$-$Training PINN (gST$-$PINN) method that specifically\nintroduces a gradient based pseudo point self$-$learning algorithm for solving\nPDEs. We tested the proposed method on three different types of PDE problems\nfrom various fields, each representing distinct scenarios. The effectiveness of\nthe proposed method is evident, as the PINN approach for solving the Burgers$'$\nequation attains a mean square error (MSE) on the order of $10^{-3}$, while the\ndiffusion$-$sorption equation achieves an MSE on the order of $10^{-4}$ after\n12,500 iterations, with no further improvement as the iterations increase. In\ncontrast, the MSE for both PDEs in the gST$-$PINN model continues to decrease,\ndemonstrating better generalization and reaching an MSE on the order of\n$10^{-5}$ after 18,500 iterations. Furthermore, the results show that the\nproposed purely semi$-$supervised gST$-$PINN consistently outperforms the\nstandard PINN method in all cases, even when solution of the PDEs are\nunavailable. It generalizes both PINN and Gradient$-$enhanced PINN (gPINN), and\ncan be effectively applied in scenarios prone to low accuracy and convergence\nissues, particularly in the absence of labeled data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u68af\u5ea6\u589e\u5f3a\u81ea\u8bad\u7ec3PINN\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u4f2a\u70b9\u81ea\u5b66\u4e60\u7b97\u6cd5\u89e3\u51b3\u4f20\u7edfPINNs\u7cbe\u5ea6\u4f4e\u3001\u8bad\u7ec3\u6162\u3001\u7f3a\u4e4f\u6807\u7b7e\u6570\u636e\u548c\u591a\u7269\u7406\u573a\u4ea4\u4e92\u5904\u7406\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfPINNs\u5b58\u5728\u7cbe\u5ea6\u6709\u9650\u3001\u8bad\u7ec3\u52a8\u6001\u7f13\u6162\u3001\u7f3a\u4e4f\u6807\u7b7e\u6570\u636e\u4ee5\u53ca\u591a\u7269\u7406\u573a\u4ea4\u4e92\u5904\u7406\u4e0d\u8db3\u7b49\u6311\u6218\uff0c\u9700\u8981\u6539\u8fdb\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u68af\u5ea6\u589e\u5f3a\u81ea\u8bad\u7ec3PINN\u65b9\u6cd5\uff0c\u5f15\u5165\u57fa\u4e8e\u68af\u5ea6\u7684\u4f2a\u70b9\u81ea\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u65e0\u6807\u7b7e\u6570\u636e\u60c5\u51b5\u4e0b\u901a\u8fc7\u4f2a\u70b9\u751f\u6210\u548c\u81ea\u8bad\u7ec3\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u4e09\u79cd\u4e0d\u540c\u7c7b\u578bPDE\u95ee\u9898\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cgST-PINN\u6bd4\u6807\u51c6PINN\u8868\u73b0\u66f4\u597d\uff0cBurgers\u65b9\u7a0b\u548c\u6269\u6563-\u5438\u9644\u65b9\u7a0b\u7684MSE\u5206\u522b\u8fbe\u523010^-5\u91cf\u7ea7\uff0c\u663e\u793a\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "gST-PINN\u65b9\u6cd5\u5728\u65e0\u6807\u7b7e\u6570\u636e\u60c5\u51b5\u4e0b\u663e\u8457\u4f18\u4e8e\u6807\u51c6PINN\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f4e\u7cbe\u5ea6\u548c\u6536\u655b\u95ee\u9898\uff0c\u6cdb\u5316\u4e86PINN\u548cgPINN\u65b9\u6cd5\u3002"}}
{"id": "2510.11233", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11233", "abs": "https://arxiv.org/abs/2510.11233", "authors": ["Jinyuan Xu", "Tian Lan", "Xintao Yu", "Xue He", "Hezhi Zhang", "Ying Wang", "Pierre Magistry", "Mathieu Valette", "Lei Li"], "title": "CNSocialDepress: A Chinese Social Media Dataset for Depression Risk Detection and Structured Analysis", "comment": null, "summary": "Depression is a pressing global public health issue, yet publicly available\nChinese-language resources for risk detection remain scarce and are mostly\nlimited to binary classification. To address this limitation, we release\nCNSocialDepress, a benchmark dataset for depression risk detection from Chinese\nsocial media posts. The dataset contains 44,178 texts from 233 users, within\nwhich psychological experts annotated 10,306 depression-related segments.\nCNSocialDepress provides binary risk labels together with structured\nmulti-dimensional psychological attributes, enabling interpretable and\nfine-grained analysis of depressive signals. Experimental results demonstrate\nits utility across a wide range of NLP tasks, including structured\npsychological profiling and fine-tuning of large language models for depression\ndetection. Comprehensive evaluations highlight the dataset's effectiveness and\npractical value for depression risk identification and psychological analysis,\nthereby providing insights to mental health applications tailored for\nChinese-speaking populations.", "AI": {"tldr": "\u53d1\u5e03\u4e86CNSocialDepress\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u4ece\u4e2d\u6587\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u4e2d\u68c0\u6d4b\u6291\u90c1\u98ce\u9669\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b44,178\u6761\u6587\u672c\u548c10,306\u4e2a\u6291\u90c1\u76f8\u5173\u7247\u6bb5\uff0c\u63d0\u4f9b\u4e8c\u5143\u98ce\u9669\u6807\u7b7e\u548c\u591a\u7ef4\u5fc3\u7406\u5c5e\u6027\u3002", "motivation": "\u6291\u90c1\u75c7\u662f\u5168\u7403\u7d27\u8feb\u7684\u516c\u5171\u536b\u751f\u95ee\u9898\uff0c\u4f46\u516c\u5f00\u53ef\u7528\u7684\u4e2d\u6587\u6291\u90c1\u68c0\u6d4b\u8d44\u6e90\u7a00\u7f3a\u4e14\u5927\u591a\u4ec5\u9650\u4e8e\u4e8c\u5143\u5206\u7c7b\u3002", "method": "\u6784\u5efa\u5305\u542b44,178\u6761\u6587\u672c\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7531\u5fc3\u7406\u5b66\u4e13\u5bb6\u6807\u6ce810,306\u4e2a\u6291\u90c1\u76f8\u5173\u7247\u6bb5\uff0c\u63d0\u4f9b\u4e8c\u5143\u98ce\u9669\u6807\u7b7e\u548c\u7ed3\u6784\u5316\u591a\u7ef4\u5fc3\u7406\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6570\u636e\u96c6\u5728\u591a\u79cdNLP\u4efb\u52a1\u4e2d\u5177\u6709\u5b9e\u7528\u6027\uff0c\u5305\u62ec\u7ed3\u6784\u5316\u5fc3\u7406\u5206\u6790\u548c\u7528\u4e8e\u6291\u90c1\u68c0\u6d4b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u6291\u90c1\u98ce\u9669\u8bc6\u522b\u548c\u5fc3\u7406\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u6027\u548c\u5b9e\u7528\u4ef7\u503c\uff0c\u4e3a\u4e2d\u6587\u4eba\u7fa4\u7684\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2510.10503", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10503", "abs": "https://arxiv.org/abs/2510.10503", "authors": ["Kanishkha Jaisankar", "Sunidhi Tandel"], "title": "Align2Act: Instruction-Tuned Models for Human-Aligned Autonomous Driving", "comment": null, "summary": "Motion planning in complex scenarios is a core challenge in autonomous\ndriving. Conventional methods apply predefined rules or learn from driving data\nto generate trajectories, while recent approaches leverage large language\nmodels (LLMs) for decision-making. However, it remains unclear whether LLMs\ntruly capture human driving logic. We propose Align2Act, a motion planning\nframework that transforms instruction-tuned LLMs into interpretable planners\naligned with human behavior. We derive structured driving instructions based on\nhuman reasoning patterns (e.g., anticipate hazards, yield at intersections) and\ntraffic rules (e.g., stop at red lights, maintain lane boundaries). Our\nAlign2ActChain module guides step-by-step reasoning to produce both an\ninterpretable rationale and a safe trajectory. By fine-tuning LLaMA-2-7B with\nLoRA on one million scenarios from the nuPlan dataset, our method achieves an\nopen-loop score of 85.17 and closed-loop scores of 70.31 (non-reactive) and\n66.96 (reactive) on Test14-random. Unlike prior work focused on synthetic or\nopen-loop settings, we demonstrate improved planning quality and human-likeness\non the real-world nuPlan closed-loop benchmark. Ablation studies confirm that\nstructured reasoning significantly improves performance over baseline LLM\nplanners.", "AI": {"tldr": "Align2Act\u662f\u4e00\u4e2a\u5c06\u6307\u4ee4\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f6c\u5316\u4e3a\u53ef\u89e3\u91ca\u8fd0\u52a8\u89c4\u5212\u5668\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u9a7e\u9a76\u6307\u4ee4\u548c\u9010\u6b65\u63a8\u7406\uff0c\u5728nuPlan\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u5f00\u73af\u548c\u95ed\u73af\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f53\u524dLLM\u5728\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u4e2d\u662f\u5426\u771f\u6b63\u7406\u89e3\u4eba\u7c7b\u9a7e\u9a76\u903b\u8f91\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u5982\u4f55\u5c06LLM\u8f6c\u5316\u4e3a\u4e0e\u4eba\u7c7b\u884c\u4e3a\u5bf9\u9f50\u7684\u53ef\u89e3\u91ca\u89c4\u5212\u5668\u3002", "method": "\u63d0\u51faAlign2Act\u6846\u67b6\uff0c\u57fa\u4e8e\u4eba\u7c7b\u63a8\u7406\u6a21\u5f0f\u548c\u4ea4\u901a\u89c4\u5219\u6784\u5efa\u7ed3\u6784\u5316\u9a7e\u9a76\u6307\u4ee4\uff0c\u901a\u8fc7Align2ActChain\u6a21\u5757\u8fdb\u884c\u9010\u6b65\u63a8\u7406\uff0c\u4f7f\u7528LoRA\u5728nuPlan\u6570\u636e\u96c6\u4e0a\u5fae\u8c03LLaMA-2-7B\u6a21\u578b\u3002", "result": "\u5728nuPlan Test14-random\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u5f00\u73af\u5f97\u520685.17\uff0c\u95ed\u73af\u5f97\u520670.31\uff08\u975e\u53cd\u5e94\u5f0f\uff09\u548c66.96\uff08\u53cd\u5e94\u5f0f\uff09\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u95ed\u73af\u57fa\u51c6\u4e0a\u5c55\u73b0\u51fa\u6539\u8fdb\u7684\u89c4\u5212\u8d28\u91cf\u548c\u4eba\u7c7b\u76f8\u4f3c\u6027\u3002", "conclusion": "\u7ed3\u6784\u5316\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86LLM\u89c4\u5212\u5668\u7684\u6027\u80fd\uff0cAlign2Act\u80fd\u591f\u6709\u6548\u5c06LLM\u8f6c\u5316\u4e3a\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u5bf9\u9f50\u7684\u53ef\u89e3\u91ca\u8fd0\u52a8\u89c4\u5212\u5668\u3002"}}
{"id": "2510.11236", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11236", "abs": "https://arxiv.org/abs/2510.11236", "authors": ["Haoqi Yang", "Yao Yao", "Zuchao Li", "Baoyuan Qi", "Guoming Liu", "Hai Zhao"], "title": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression", "comment": "To be published in The 2025 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP 2025)", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse natural language processing tasks. However, their extensive memory\nrequirements, particularly due to KV cache growth during long-text\nunderstanding and generation, present significant challenges for deployment in\nresource-constrained environments. Quantization has emerged as a promising\nsolution to reduce memory consumption while preserving historical information.\nWe propose XQuant, a training-free and plug-and-play framework that achieves\nultra-low equivalent bit-width KV cache quantization. XQuant introduces two key\ninnovations: a computationally negligible data-free calibration method and\ncross-layer KV cache compression, enabling quantization to sub-1.4 bits.\nExtensive experiments on TruthfulQA and LongBench demonstrate that XQuant\noutperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by\nachieving lower bit-width while maintaining superior performance, establishing\na better trade-off between memory efficiency and model accuracy.", "AI": {"tldr": "XQuant\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u65e0\u5173\u6821\u51c6\u548c\u8de8\u5c42KV\u7f13\u5b58\u538b\u7f29\uff0c\u5b9e\u73b0\u8d85\u4f4e\u6bd4\u7279\u5bbd\u5ea6\uff08\u4f4e\u4e8e1.4\u4f4d\uff09\u7684KV\u7f13\u5b58\u91cf\u5316\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u6587\u672c\u65f6KV\u7f13\u5b58\u5185\u5b58\u9700\u6c42\u5de8\u5927\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u6709\u6548\u7684\u91cf\u5316\u65b9\u6cd5\u6765\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u3002", "method": "\u63d0\u51faXQuant\u6846\u67b6\uff0c\u5305\u542b\u8ba1\u7b97\u91cf\u53ef\u5ffd\u7565\u7684\u6570\u636e\u65e0\u5173\u6821\u51c6\u65b9\u6cd5\u548c\u8de8\u5c42KV\u7f13\u5b58\u538b\u7f29\u6280\u672f\uff0c\u5b9e\u73b0\u8d85\u4f4e\u6bd4\u7279\u5bbd\u5ea6\u7684KV\u7f13\u5b58\u91cf\u5316\u3002", "result": "\u5728TruthfulQA\u548cLongBench\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cXQuant\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff08\u5982KIVI-2bit\u548cAsymKV-1.5bit\uff09\uff0c\u5728\u66f4\u4f4e\u6bd4\u7279\u5bbd\u5ea6\u4e0b\u4ecd\u4fdd\u6301\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "XQuant\u5728\u5185\u5b58\u6548\u7387\u548c\u6a21\u578b\u7cbe\u5ea6\u4e4b\u95f4\u5efa\u7acb\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10510", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10510", "abs": "https://arxiv.org/abs/2510.10510", "authors": ["Subhodip Panda", "Dhruv Tarsadiya", "Shashwat Sourav", "Prathosh A. P", "Sai Praneeth Karimireddy"], "title": "f-INE: A Hypothesis Testing Framework for Estimating Influence under Training Randomness", "comment": null, "summary": "Influence estimation methods promise to explain and debug machine learning by\nestimating the impact of individual samples on the final model. Yet, existing\nmethods collapse under training randomness: the same example may appear\ncritical in one run and irrelevant in the next. Such instability undermines\ntheir use in data curation or cleanup since it is unclear if we indeed\ndeleted/kept the correct datapoints. To overcome this, we introduce\n*f-influence* -- a new influence estimation framework grounded in hypothesis\ntesting that explicitly accounts for training randomness, and establish\ndesirable properties that make it suitable for reliable influence estimation.\nWe also design a highly efficient algorithm **f**-**IN**fluence **E**stimation\n(**f-INE**) that computes f-influence **in a single training run**. Finally, we\nscale up f-INE to estimate influence of instruction tuning data on Llama-3.1-8B\nand show it can reliably detect poisoned samples that steer model opinions,\ndemonstrating its utility for data cleanup and attributing model behavior.", "AI": {"tldr": "\u63d0\u51fa\u4e86f-influence\u6846\u67b6\uff0c\u901a\u8fc7\u5047\u8bbe\u68c0\u9a8c\u8003\u8651\u8bad\u7ec3\u968f\u673a\u6027\uff0c\u63d0\u4f9b\u7a33\u5b9a\u7684\u6837\u672c\u5f71\u54cd\u529b\u4f30\u8ba1\uff0c\u5e76\u5f00\u53d1\u4e86\u5355\u6b21\u8bad\u7ec3\u5373\u53ef\u8ba1\u7b97\u7684\u9ad8\u6548\u7b97\u6cd5f-INE", "motivation": "\u73b0\u6709\u5f71\u54cd\u529b\u4f30\u8ba1\u65b9\u6cd5\u5728\u8bad\u7ec3\u968f\u673a\u6027\u4e0b\u4e0d\u7a33\u5b9a\uff0c\u540c\u4e00\u6837\u672c\u5728\u4e0d\u540c\u8fd0\u884c\u4e2d\u53ef\u80fd\u88ab\u8bc4\u4f30\u4e3a\u5173\u952e\u6216\u65e0\u5173\uff0c\u8fd9\u5f71\u54cd\u4e86\u6570\u636e\u6e05\u7406\u548c\u8c03\u8bd5\u7684\u53ef\u9760\u6027", "method": "\u57fa\u4e8e\u5047\u8bbe\u68c0\u9a8c\u7684f-influence\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7b97\u6cd5f-INE\uff0c\u53ef\u5728\u5355\u6b21\u8bad\u7ec3\u4e2d\u8ba1\u7b97\u5f71\u54cd\u529b\u4f30\u8ba1", "result": "\u6210\u529f\u5c06f-INE\u6269\u5c55\u5230Llama-3.1-8B\u6a21\u578b\uff0c\u53ef\u9760\u68c0\u6d4b\u51fa\u64cd\u7eb5\u6a21\u578b\u610f\u89c1\u7684\u4e2d\u6bd2\u6837\u672c\uff0c\u9a8c\u8bc1\u4e86\u5728\u6570\u636e\u6e05\u7406\u548c\u884c\u4e3a\u5f52\u56e0\u4e2d\u7684\u5b9e\u7528\u6027", "conclusion": "f-influence\u6846\u67b6\u89e3\u51b3\u4e86\u8bad\u7ec3\u968f\u673a\u6027\u5bfc\u81f4\u7684\u5f71\u54cd\u529b\u4f30\u8ba1\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6570\u636e\u6e05\u7406\u548c\u8c03\u8bd5\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177"}}
{"id": "2510.11238", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11238", "abs": "https://arxiv.org/abs/2510.11238", "authors": ["Michael Schlichtkrull"], "title": "Attacks by Content: Automated Fact-checking is an AI Security Issue", "comment": "Accepted to EMNLP 2025", "summary": "When AI agents retrieve and reason over external documents, adversaries can\nmanipulate the data they receive to subvert their behaviour. Previous research\nhas studied indirect prompt injection, where the attacker injects malicious\ninstructions. We argue that injection of instructions is not necessary to\nmanipulate agents - attackers could instead supply biased, misleading, or false\ninformation. We term this an attack by content. Existing defenses, which focus\non detecting hidden commands, are ineffective against attacks by content. To\ndefend themselves and their users, agents must critically evaluate retrieved\ninformation, corroborating claims with external evidence and evaluating source\ntrustworthiness. We argue that this is analogous to an existing NLP task,\nautomated fact-checking, which we propose to repurpose as a cognitive\nself-defense tool for agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AI\u4ee3\u7406\u653b\u51fb\u65b9\u5f0f\u2014\u2014\u5185\u5bb9\u653b\u51fb\uff0c\u533a\u522b\u4e8e\u4f20\u7edf\u7684\u6307\u4ee4\u6ce8\u5165\u653b\u51fb\uff0c\u653b\u51fb\u8005\u901a\u8fc7\u63d0\u4f9b\u6709\u504f\u89c1\u3001\u8bef\u5bfc\u6027\u6216\u865a\u5047\u4fe1\u606f\u6765\u64cd\u7eb5\u4ee3\u7406\u884c\u4e3a\u3002\u4f5c\u8005\u5efa\u8bae\u5c06\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u91cd\u65b0\u7528\u4f5c\u4ee3\u7406\u7684\u8ba4\u77e5\u81ea\u536b\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u673a\u5236\u4e3b\u8981\u5173\u6ce8\u68c0\u6d4b\u9690\u85cf\u6307\u4ee4\uff0c\u4f46\u5bf9\u57fa\u4e8e\u5185\u5bb9\u64cd\u7eb5\u7684\u653b\u51fb\u65e0\u6548\u3002AI\u4ee3\u7406\u5728\u68c0\u7d22\u5916\u90e8\u6587\u6863\u65f6\u9700\u8981\u80fd\u591f\u6279\u5224\u6027\u8bc4\u4f30\u4fe1\u606f\uff0c\u4ee5\u4fdd\u62a4\u81ea\u8eab\u548c\u7528\u6237\u514d\u53d7\u5185\u5bb9\u653b\u51fb\u3002", "method": "\u63d0\u51fa\u5c06\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u91cd\u65b0\u7528\u4f5cAI\u4ee3\u7406\u7684\u8ba4\u77e5\u81ea\u536b\u5de5\u5177\uff0c\u901a\u8fc7\u5916\u90e8\u8bc1\u636e\u9a8c\u8bc1\u58f0\u660e\u5e76\u8bc4\u4f30\u6765\u6e90\u53ef\u4fe1\u5ea6\u3002", "result": "\u8bba\u8bc1\u4e86\u4ec5\u68c0\u6d4b\u9690\u85cf\u6307\u4ee4\u7684\u9632\u5fa1\u673a\u5236\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u5185\u5bb9\u653b\u51fb\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u4fe1\u606f\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "AI\u4ee3\u7406\u5fc5\u987b\u53d1\u5c55\u6279\u5224\u6027\u8bc4\u4f30\u80fd\u529b\uff0c\u5c06\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u4f5c\u4e3a\u8ba4\u77e5\u81ea\u536b\u5de5\u5177\uff0c\u4ee5\u6709\u6548\u9632\u5fa1\u5185\u5bb9\u653b\u51fb\u3002"}}
{"id": "2510.10513", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10513", "abs": "https://arxiv.org/abs/2510.10513", "authors": ["Md Ibrahim Shikder Mahin", "Md Shamsul Arefin", "Md Tanvir Hasan"], "title": "A Hybrid Machine Learning Approach for Synthetic Data Generation with Post Hoc Calibration for Clinical Tabular Datasets", "comment": null, "summary": "Healthcare research and development face significant obstacles due to data\nscarcity and stringent privacy regulations, such as HIPAA and the GDPR,\nrestricting access to essential real-world medical data. These limitations\nimpede innovation, delay robust AI model creation, and hinder advancements in\npatient-centered care. Synthetic data generation offers a transformative\nsolution by producing artificial datasets that emulate real data statistics\nwhile safeguarding patient privacy. We introduce a novel hybrid framework for\nhigh-fidelity healthcare data synthesis integrating five augmentation methods:\nnoise injection, interpolation, Gaussian Mixture Model (GMM) sampling,\nConditional Variational Autoencoder (CVAE) sampling, and SMOTE, combined via a\nreinforcement learning-based dynamic weight selection mechanism. Its key\ninnovations include advanced calibration techniques -- moment matching, full\nhistogram matching, soft and adaptive soft histogram matching, and iterative\nrefinement -- that align marginal distributions and preserve joint feature\ndependencies. Evaluated on the Breast Cancer Wisconsin (UCI Repository) and\nKhulna Medical College cardiology datasets, our calibrated hybrid achieves\nWasserstein distances as low as 0.001 and Kolmogorov-Smirnov statistics around\n0.01, demonstrating near-zero marginal discrepancy. Pairwise trend scores\nsurpass 90%, and Nearest Neighbor Adversarial Accuracy approaches 50%,\nconfirming robust privacy protection. Downstream classifiers trained on\nsynthetic data achieve up to 94% accuracy and F1 scores above 93%, comparable\nto models trained on real data. This scalable, privacy-preserving approach\nmatches state-of-the-art methods, sets new benchmarks for joint-distribution\nfidelity in healthcare, and supports sensitive AI applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u6846\u67b6\u7528\u4e8e\u533b\u7597\u6570\u636e\u5408\u6210\uff0c\u901a\u8fc7\u4e94\u79cd\u589e\u5f3a\u65b9\u6cd5\u548c\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u6743\u91cd\u9009\u62e9\uff0c\u7ed3\u5408\u5148\u8fdb\u7684\u6821\u51c6\u6280\u672f\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u5408\u6210\u533b\u7597\u6570\u636e\u3002", "motivation": "\u533b\u7597\u7814\u53d1\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u6cd5\u89c4\u9650\u5236\uff0c\u963b\u788d\u4e86AI\u6a21\u578b\u521b\u65b0\u548c\u60a3\u8005\u62a4\u7406\u8fdb\u6b65\u3002\u5408\u6210\u6570\u636e\u751f\u6210\u53ef\u4ee5\u6a21\u62df\u771f\u5b9e\u6570\u636e\u7edf\u8ba1\u7279\u6027\u540c\u65f6\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u3002", "method": "\u6df7\u5408\u6846\u67b6\u6574\u5408\u566a\u58f0\u6ce8\u5165\u3001\u63d2\u503c\u3001GMM\u91c7\u6837\u3001CVAE\u91c7\u6837\u548cSMOTE\u4e94\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u9009\u62e9\u6743\u91cd\uff0c\u91c7\u7528\u77e9\u5339\u914d\u3001\u5168\u76f4\u65b9\u56fe\u5339\u914d\u3001\u8f6f\u76f4\u65b9\u56fe\u5339\u914d\u548c\u8fed\u4ee3\u7cbe\u70bc\u7b49\u6821\u51c6\u6280\u672f\u3002", "result": "\u5728\u4e73\u817a\u764c\u548c\u5fc3\u810f\u75c5\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cWasserstein\u8ddd\u79bb\u4f4e\u81f30.001\uff0cKS\u7edf\u8ba1\u7ea60.01\uff0c\u8fb9\u9645\u5dee\u5f02\u63a5\u8fd1\u96f6\u3002\u4e0b\u6e38\u5206\u7c7b\u5668\u51c6\u786e\u7387\u8fbe94%\uff0cF1\u5206\u6570\u8d8593%\uff0c\u4e0e\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u7ed3\u679c\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u3001\u4fdd\u62a4\u9690\u79c1\uff0c\u5728\u533b\u7597\u6570\u636e\u5408\u6210\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4e3a\u654f\u611fAI\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2510.11254", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11254", "abs": "https://arxiv.org/abs/2510.11254", "authors": ["Jana Jung", "Marlene Lutz", "Indira Sen", "Markus Strohmaier"], "title": "Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality", "comment": null, "summary": "Psychometric tests are increasingly used to assess psychological constructs\nin large language models (LLMs). However, it remains unclear whether these\ntests -- originally developed for humans -- yield meaningful results when\napplied to LLMs. In this study, we systematically evaluate the reliability and\nvalidity of human psychometric tests for three constructs: sexism, racism, and\nmorality. We find moderate reliability across multiple item and prompt\nvariations. Validity is evaluated through both convergent (i.e., testing\ntheory-based inter-test correlations) and ecological approaches (i.e., testing\nthe alignment between tests scores and behavior in real-world downstream\ntasks). Crucially, we find that psychometric test scores do not align, and in\nsome cases even negatively correlate with, model behavior in downstream tasks,\nindicating low ecological validity. Our results highlight that systematic\nevaluations of psychometric tests is essential before interpreting their\nscores. They also suggest that psychometric tests designed for humans cannot be\napplied directly to LLMs without adaptation.", "AI": {"tldr": "\u8bc4\u4f30\u4eba\u7c7b\u5fc3\u7406\u6d4b\u91cf\u6d4b\u8bd5\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u53ef\u9760\u6027\u548c\u6709\u6548\u6027\uff0c\u53d1\u73b0\u6d4b\u8bd5\u5206\u6570\u4e0e\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u884c\u4e3a\u4e0d\u4e00\u81f4\uff0c\u8868\u660e\u751f\u6001\u6548\u5ea6\u8f83\u4f4e\u3002", "motivation": "\u5fc3\u7406\u6d4b\u91cf\u6d4b\u8bd5\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5fc3\u7406\u7ed3\u6784\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u4e3a\u4eba\u7c7b\u8bbe\u8ba1\u7684\u6d4b\u8bd5\u662f\u5426\u80fd\u5bf9LLMs\u4ea7\u751f\u6709\u610f\u4e49\u7684\u7ed3\u679c\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e09\u79cd\u5fc3\u7406\u7ed3\u6784\uff08\u6027\u522b\u6b67\u89c6\u3001\u79cd\u65cf\u6b67\u89c6\u548c\u9053\u5fb7\uff09\u7684\u4eba\u7c7b\u5fc3\u7406\u6d4b\u91cf\u6d4b\u8bd5\u7684\u53ef\u9760\u6027\u548c\u6709\u6548\u6027\uff0c\u5305\u62ec\u6536\u655b\u6548\u5ea6\u548c\u751f\u6001\u6548\u5ea6\u8bc4\u4f30\u3002", "result": "\u6d4b\u8bd5\u5728\u591a\u4e2a\u9879\u76ee\u548c\u63d0\u793a\u53d8\u4f53\u4e2d\u663e\u793a\u51fa\u4e2d\u7b49\u53ef\u9760\u6027\uff0c\u4f46\u6d4b\u8bd5\u5206\u6570\u4e0e\u4e0b\u6e38\u4efb\u52a1\u884c\u4e3a\u4e0d\u4e00\u81f4\uff0c\u6709\u65f6\u751a\u81f3\u5448\u8d1f\u76f8\u5173\uff0c\u8868\u660e\u751f\u6001\u6548\u5ea6\u4f4e\u3002", "conclusion": "\u5728\u89e3\u91ca\u5fc3\u7406\u6d4b\u91cf\u6d4b\u8bd5\u5206\u6570\u4e4b\u524d\u5fc5\u987b\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u4e3a\u4eba\u7c7b\u8bbe\u8ba1\u7684\u6d4b\u8bd5\u4e0d\u80fd\u76f4\u63a5\u5e94\u7528\u4e8eLLMs\u800c\u9700\u8981\u8c03\u6574\u3002"}}
{"id": "2510.10530", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10530", "abs": "https://arxiv.org/abs/2510.10530", "authors": ["Hanbing Liu", "Huaze Tang", "Yanru Wu", "Yang Li", "Xiao-Ping Zhang"], "title": "Reinforced Domain Selection for Continuous Domain Adaptation", "comment": null, "summary": "Continuous Domain Adaptation (CDA) effectively bridges significant domain\nshifts by progressively adapting from the source domain through intermediate\ndomains to the target domain. However, selecting intermediate domains without\nexplicit metadata remains a substantial challenge that has not been extensively\nexplored in existing studies. To tackle this issue, we propose a novel\nframework that combines reinforcement learning with feature disentanglement to\nconduct domain path selection in an unsupervised CDA setting. Our approach\nintroduces an innovative unsupervised reward mechanism that leverages the\ndistances between latent domain embeddings to facilitate the identification of\noptimal transfer paths. Furthermore, by disentangling features, our method\nfacilitates the calculation of unsupervised rewards using domain-specific\nfeatures and promotes domain adaptation by aligning domain-invariant features.\nThis integrated strategy is designed to simultaneously optimize transfer paths\nand target task performance, enhancing the effectiveness of domain adaptation\nprocesses. Extensive empirical evaluations on datasets such as Rotated MNIST\nand ADNI demonstrate substantial improvements in prediction accuracy and domain\nselection efficiency, establishing our method's superiority over traditional\nCDA approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u7279\u5f81\u89e3\u8026\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u65e0\u76d1\u7763\u8fde\u7eed\u57df\u81ea\u9002\u5e94\u4e2d\u81ea\u52a8\u9009\u62e9\u6700\u4f18\u57df\u8f6c\u79fb\u8def\u5f84\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5956\u52b1\u673a\u5236\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u548c\u57df\u9009\u62e9\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u8fde\u7eed\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u7f3a\u4e4f\u660e\u786e\u5143\u6570\u636e\u65f6\u96be\u4ee5\u6709\u6548\u9009\u62e9\u4e2d\u95f4\u57df\uff0c\u8fd9\u9650\u5236\u4e86\u57df\u81ea\u9002\u5e94\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u57df\u8def\u5f84\u9009\u62e9\uff0c\u7ed3\u5408\u7279\u5f81\u89e3\u8026\u6280\u672f\uff0c\u5229\u7528\u6f5c\u5728\u57df\u5d4c\u5165\u8ddd\u79bb\u8bbe\u8ba1\u65e0\u76d1\u7763\u5956\u52b1\u673a\u5236\uff0c\u540c\u65f6\u5bf9\u9f50\u57df\u4e0d\u53d8\u7279\u5f81\u4ee5\u4fc3\u8fdb\u57df\u9002\u5e94\u3002", "result": "\u5728Rotated MNIST\u548cADNI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u57df\u9009\u62e9\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u8fde\u7eed\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u5f3a\u5316\u5b66\u4e60\u548c\u7279\u5f81\u89e3\u8026\uff0c\u80fd\u591f\u540c\u65f6\u4f18\u5316\u8f6c\u79fb\u8def\u5f84\u548c\u76ee\u6807\u4efb\u52a1\u6027\u80fd\uff0c\u6709\u6548\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u8fde\u7eed\u57df\u81ea\u9002\u5e94\u7684\u6548\u679c\u3002"}}
{"id": "2510.11277", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11277", "abs": "https://arxiv.org/abs/2510.11277", "authors": ["Guangyu Wei", "Ke Han", "Yueming Lyu", "Yu Luo", "Yue Jiang", "Caifeng Shan", "Nicu Sebe"], "title": "Towards Real-Time Fake News Detection under Evidence Scarcity", "comment": null, "summary": "Fake news detection becomes particularly challenging in real-time scenarios,\nwhere emerging events often lack sufficient supporting evidence. Existing\napproaches often rely heavily on external evidence and therefore struggle to\ngeneralize under evidence scarcity. To address this issue, we propose\nEvaluation-Aware Selection of Experts (EASE), a novel framework for real-time\nfake news detection that dynamically adapts its decision-making process\naccording to the assessed sufficiency of available evidence. EASE introduces a\nsequential evaluation mechanism comprising three independent perspectives: (1)\nEvidence-based evaluation, which assesses evidence and incorporates it into\ndecision-making only when the evidence is sufficiently supportive; (2)\nReasoning-based evaluation, which leverages the world knowledge of large\nlanguage models (LLMs) and applies them only when their reliability is\nadequately established; and (3) Sentiment-based fallback, which integrates\nsentiment cues when neither evidence nor reasoning is reliable. To enhance the\naccuracy of evaluation processes, EASE employs instruction tuning with pseudo\nlabels to guide each evaluator in justifying its perspective-specific knowledge\nthrough interpretable reasoning. Furthermore, the expert modules integrate the\nevaluators' justified assessments with the news content to enable\nevaluation-aware decision-making, thereby enhancing overall detection accuracy.\nMoreover, we introduce RealTimeNews-25, a new benchmark comprising recent news\nfor evaluating model generalization on emerging news with limited evidence.\nExtensive experiments demonstrate that EASE not only achieves state-of-the-art\nperformance across multiple benchmarks, but also significantly improves\ngeneralization to real-time news. The code and dataset are available:\nhttps://github.com/wgyhhhh/EASE.", "AI": {"tldr": "EASE\u662f\u4e00\u4e2a\u7528\u4e8e\u5b9e\u65f6\u5047\u65b0\u95fb\u68c0\u6d4b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8bc4\u4f30\u8bc1\u636e\u5145\u8db3\u6027\u6765\u81ea\u9002\u5e94\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5305\u542b\u8bc1\u636e\u8bc4\u4f30\u3001\u63a8\u7406\u8bc4\u4f30\u548c\u60c5\u611f\u56de\u9000\u4e09\u4e2a\u89c6\u89d2\uff0c\u5728\u8bc1\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5b9e\u65f6\u573a\u666f\u4e2d\u65b0\u5174\u4e8b\u4ef6\u7f3a\u4e4f\u8db3\u591f\u652f\u6301\u8bc1\u636e\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u5916\u90e8\u8bc1\u636e\uff0c\u5728\u8bc1\u636e\u7a00\u7f3a\u65f6\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51faEASE\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u72ec\u7acb\u8bc4\u4f30\u89c6\u89d2\uff1a\u8bc1\u636e\u8bc4\u4f30\u3001\u63a8\u7406\u8bc4\u4f30\u548c\u60c5\u611f\u56de\u9000\uff0c\u4f7f\u7528\u6307\u4ee4\u8c03\u4f18\u548c\u4f2a\u6807\u7b7e\u589e\u5f3a\u8bc4\u4f30\u51c6\u786e\u6027\uff0c\u4e13\u5bb6\u6a21\u5757\u6574\u5408\u8bc4\u4f30\u7ed3\u679c\u8fdb\u884c\u51b3\u7b56\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u5b9e\u65f6\u65b0\u95fb\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u53d1\u5e03\u4e86RealTimeNews-25\u65b0\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "EASE\u901a\u8fc7\u8bc4\u4f30\u611f\u77e5\u7684\u51b3\u7b56\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u8bc1\u636e\u7a00\u7f3a\u4e0b\u7684\u5047\u65b0\u95fb\u68c0\u6d4b\u95ee\u9898\uff0c\u5728\u5b9e\u65f6\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.10541", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10541", "abs": "https://arxiv.org/abs/2510.10541", "authors": ["Zihan Chen", "Yiming Zhang", "Hengguang Zhou", "Zenghui Ding", "Yining Sun", "Cho-Jui Hsieh"], "title": "Rethinking RL Evaluation: Can Benchmarks Truly Reveal Failures of RL Methods?", "comment": null, "summary": "Current benchmarks are inadequate for evaluating progress in reinforcement\nlearning (RL) for large language models (LLMs).Despite recent benchmark gains\nreported for RL, we find that training on these benchmarks' training sets\nachieves nearly the same performance as training directly on the test sets,\nsuggesting that the benchmarks cannot reliably separate further progress.To\nstudy this phenomenon, we introduce a diagnostic suite and the Oracle\nPerformance Gap (OPG) metric that quantifies the performance difference between\ntraining on the train split versus the test split of a benchmark. We further\nanalyze this phenomenon with stress tests and find that, despite strong\nbenchmark scores, existing RL methods struggle to generalize across\ndistribution shifts, varying levels of difficulty, and counterfactual\nscenarios: shortcomings that current benchmarks fail to reveal.We conclude that\ncurrent benchmarks are insufficient for evaluating generalization and propose\nthree core principles for designing more faithful benchmarks: sufficient\ndifficulty, balanced evaluation, and distributional robustness.", "AI": {"tldr": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u5c55\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86Oracle\u6027\u80fd\u5dee\u8ddd\u6307\u6807\u548c\u8bca\u65ad\u5957\u4ef6\u6765\u91cf\u5316\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u5efa\u8bae\u8bbe\u8ba1\u66f4\u53ef\u9760\u7684\u57fa\u51c6\u5e94\u9075\u5faa\u4e09\u4e2a\u6838\u5fc3\u539f\u5219\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u65e0\u6cd5\u53ef\u9760\u533a\u5206\u6a21\u578b\u6027\u80fd\u7684\u771f\u6b63\u8fdb\u6b65\uff0c\u56e0\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u7684\u8868\u73b0\u51e0\u4e4e\u76f8\u540c\uff0c\u8fd9\u63a9\u76d6\u4e86\u6a21\u578b\u5728\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u771f\u5b9e\u7f3a\u9677\u3002", "method": "\u5f15\u5165\u8bca\u65ad\u5957\u4ef6\u548cOracle\u6027\u80fd\u5dee\u8ddd(OPG)\u6307\u6807\u6765\u91cf\u5316\u8bad\u7ec3\u96c6\u4e0e\u6d4b\u8bd5\u96c6\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u538b\u529b\u6d4b\u8bd5\u5206\u6790\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u3001\u96be\u5ea6\u53d8\u5316\u548c\u53cd\u4e8b\u5b9e\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5c3d\u7ba1\u73b0\u6709RL\u65b9\u6cd5\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5206\u5f88\u9ad8\uff0c\u4f46\u5728\u5206\u5e03\u504f\u79fb\u3001\u4e0d\u540c\u96be\u5ea6\u7ea7\u522b\u548c\u53cd\u4e8b\u5b9e\u573a\u666f\u4e0b\u6cdb\u5316\u80fd\u529b\u5f88\u5dee\uff0c\u8fd9\u4e9b\u7f3a\u9677\u88ab\u5f53\u524d\u57fa\u51c6\u6240\u63a9\u76d6\u3002", "conclusion": "\u5f53\u524d\u57fa\u51c6\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u6cdb\u5316\u80fd\u529b\uff0c\u5efa\u8bae\u8bbe\u8ba1\u66f4\u53ef\u9760\u7684\u57fa\u51c6\u5e94\u9075\u5faa\u4e09\u4e2a\u6838\u5fc3\u539f\u5219\uff1a\u8db3\u591f\u7684\u96be\u5ea6\u3001\u5e73\u8861\u7684\u8bc4\u4f30\u548c\u5206\u5e03\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.11288", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11288", "abs": "https://arxiv.org/abs/2510.11288", "authors": ["Nikita Afonin", "Nikita Andriyanov", "Nikhil Bageshpura", "Kyle Liu", "Kevin Zhu", "Sunishchal Dev", "Ashwinee Panda", "Alexander Panchenko", "Oleg Rogov", "Elena Tutubalina", "Mikhail Seleznyov"], "title": "Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs", "comment": null, "summary": "Recent work has shown that narrow finetuning can produce broadly misaligned\nLLMs, a phenomenon termed emergent misalignment (EM). While concerning, these\nfindings were limited to finetuning and activation steering, leaving out\nin-context learning (ICL). We therefore ask: does EM emerge in ICL? We find\nthat it does: across three datasets, three frontier models produce broadly\nmisaligned responses at rates between 2% and 17% given 64 narrow in-context\nexamples, and up to 58% with 256 examples. We also examine mechanisms of EM by\neliciting step-by-step reasoning (while leaving in-context examples unchanged).\nManual analysis of the resulting chain-of-thought shows that 67.5% of\nmisaligned traces explicitly rationalize harmful outputs by adopting a reckless\nor dangerous ''persona'', echoing prior results on finetuning-induced EM.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u4e2d\u4e5f\u4f1a\u51fa\u73b0\u65b0\u5174\u9519\u4f4d(EM)\u73b0\u8c61\uff0c\u5373\u7a84\u9886\u57df\u4e0a\u4e0b\u6587\u793a\u4f8b\u4f1a\u5bfc\u81f4\u5927\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u5e7f\u6cdb\u9519\u4f4d\u7684\u56de\u7b54\uff0c\u9519\u8bef\u7387\u57282%-58%\u4e4b\u95f4\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u663e\u793a\u7a84\u9886\u57df\u5fae\u8c03\u4f1a\u5bfc\u81f4\u6a21\u578b\u9519\u4f4d\uff0c\u4f46\u672a\u8003\u5bdf\u4e0a\u4e0b\u6587\u5b66\u4e60\u662f\u5426\u4e5f\u4f1a\u4ea7\u751f\u7c7b\u4f3c\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a\u524d\u6cbf\u6a21\u578b\u548c\u4e09\u4e2a\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u63d0\u4f9b64-256\u4e2a\u7a84\u9886\u57df\u4e0a\u4e0b\u6587\u793a\u4f8b\uff0c\u5e76\u5206\u6790\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u6a21\u578b\u5728\u7a84\u9886\u57df\u4e0a\u4e0b\u6587\u793a\u4f8b\u4e0b\u4ea7\u751f\u9519\u4f4d\u56de\u7b54\u7684\u6bd4\u4f8b\u4e3a2%-17%(64\u793a\u4f8b)\u548c\u6700\u9ad858%(256\u793a\u4f8b)\uff0c67.5%\u7684\u9519\u4f4d\u63a8\u7406\u8f68\u8ff9\u663e\u793a\u6a21\u578b\u91c7\u7528\u4e86\u5371\u9669\"\u89d2\u8272\"\u6765\u5408\u7406\u5316\u6709\u5bb3\u8f93\u51fa\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u540c\u6837\u4f1a\u5bfc\u81f4\u65b0\u5174\u9519\u4f4d\u73b0\u8c61\uff0c\u5176\u673a\u5236\u4e0e\u5fae\u8c03\u8bf1\u5bfc\u7684\u9519\u4f4d\u76f8\u4f3c\uff0c\u6a21\u578b\u4f1a\u901a\u8fc7\u91c7\u7eb3\u5371\u9669\u89d2\u8272\u6765\u5408\u7406\u5316\u6709\u5bb3\u884c\u4e3a\u3002"}}
{"id": "2510.11297", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11297", "abs": "https://arxiv.org/abs/2510.11297", "authors": ["Ruirui Chen", "Weifeng Jiang", "Chengwei Qin", "Bo Xiong", "Fiona Liausvia", "Dongkyu Choi", "Boon Kiat Quek"], "title": "Are Large Language Models Effective Knowledge Graph Constructors?", "comment": null, "summary": "Knowledge graphs (KGs) are vital for knowledge-intensive tasks and have shown\npromise in reducing hallucinations in large language models (LLMs). However,\nconstructing high-quality KGs remains difficult, requiring accurate information\nextraction and structured representations that support interpretability and\ndownstream utility. Existing LLM-based approaches often focus narrowly on\nentity and relation extraction, limiting coverage to sentence-level contexts or\nrelying on predefined schemas. We propose a hierarchical extraction framework\nthat organizes information at multiple levels, enabling the creation of\nsemantically rich and well-structured KGs. Using state-of-the-art LLMs, we\nextract and construct knowledge graphs and evaluate them comprehensively from\nboth structural and semantic perspectives. Our results highlight the strengths\nand shortcomings of current LLMs in KG construction and identify key challenges\nfor future work. To advance research in this area, we also release a curated\ndataset of LLM-generated KGs derived from research papers on children's mental\nwell-being. This resource aims to foster more transparent, reliable, and\nimpactful applications in high-stakes domains such as healthcare.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u63d0\u53d6\u6846\u67b6\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff0c\u8bc4\u4f30LLM\u5728KG\u6784\u5efa\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u53d1\u5e03\u513f\u7ae5\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u6570\u636e\u96c6", "motivation": "\u73b0\u6709LLM\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5b9e\u4f53\u5173\u7cfb\u63d0\u53d6\uff0c\u5c40\u9650\u4e8e\u53e5\u5b50\u7ea7\u4e0a\u4e0b\u6587\u6216\u9884\u5b9a\u4e49\u6a21\u5f0f\uff0c\u96be\u4ee5\u6784\u5efa\u9ad8\u8d28\u91cf\u3001\u8bed\u4e49\u4e30\u5bcc\u7684\u77e5\u8bc6\u56fe\u8c31", "method": "\u4f7f\u7528\u5206\u5c42\u63d0\u53d6\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u5c42\u6b21\u7ec4\u7ec7\u4fe1\u606f\uff0c\u5229\u7528\u6700\u5148\u8fdb\u7684LLM\u63d0\u53d6\u548c\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31", "result": "\u4ece\u7ed3\u6784\u548c\u8bed\u4e49\u89d2\u5ea6\u5168\u9762\u8bc4\u4f30\u4e86\u6784\u5efa\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u5728KG\u6784\u5efa\u4e2d\u7684\u4f18\u52bf\u548c\u4e0d\u8db3", "conclusion": "\u8bc6\u522b\u4e86\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u53d1\u5e03\u4e86\u513f\u7ae5\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684LLM\u751f\u6210KG\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u9ad8\u98ce\u9669\u9886\u57df\u66f4\u900f\u660e\u53ef\u9760\u7684\u5e94\u7528"}}
{"id": "2510.10558", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10558", "abs": "https://arxiv.org/abs/2510.10558", "authors": ["Weiming Zhao", "Xulong Wang", "Jun Qi", "Yun Yang", "Po Yang"], "title": "Multi-scale Frequency-Aware Adversarial Network for Parkinson's Disease Assessment Using Wearable Sensors", "comment": null, "summary": "Severity assessment of Parkinson's disease (PD) using wearable sensors offers\nan effective, objective basis for clinical management. However, general-purpose\ntime series models often lack pathological specificity in feature extraction,\nmaking it difficult to capture subtle signals highly correlated with\nPD.Furthermore, the temporal sparsity of PD symptoms causes key diagnostic\nfeatures to be easily \"diluted\" by traditional aggregation methods, further\ncomplicating assessment. To address these issues, we propose the Multi-scale\nFrequency-Aware Adversarial Multi-Instance Network (MFAM). This model enhances\nfeature specificity through a frequency decomposition module guided by medical\nprior knowledge. Furthermore, by introducing an attention-based multi-instance\nlearning (MIL) framework, the model can adaptively focus on the most\ndiagnostically valuable sparse segments.We comprehensively validated MFAM on\nboth the public PADS dataset for PD versus differential diagnosis (DD) binary\nclassification and a private dataset for four-class severity assessment.\nExperimental results demonstrate that MFAM outperforms general-purpose time\nseries models in handling complex clinical time series with specificity,\nproviding a promising solution for automated assessment of PD severity.", "AI": {"tldr": "\u63d0\u51fa\u4e86MFAM\u6a21\u578b\uff0c\u901a\u8fc7\u9891\u7387\u5206\u89e3\u6a21\u5757\u548c\u6ce8\u610f\u529b\u591a\u5b9e\u4f8b\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u5e15\u91d1\u68ee\u75c5\u4e25\u91cd\u7a0b\u5ea6\u8bc4\u4f30\u4e2d\u7279\u5f81\u7279\u5f02\u6027\u4e0d\u8db3\u548c\u65f6\u95f4\u7a00\u758f\u6027\u95ee\u9898\u3002", "motivation": "\u901a\u7528\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u7f3a\u4e4f\u75c5\u7406\u7279\u5f02\u6027\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u4e14\u5e15\u91d1\u68ee\u75c5\u75c7\u72b6\u7684\u65f6\u95f4\u7a00\u758f\u6027\u5bfc\u81f4\u5173\u952e\u8bca\u65ad\u7279\u5f81\u88ab\u4f20\u7edf\u805a\u5408\u65b9\u6cd5\u7a00\u91ca\uff0c\u5f71\u54cd\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u9891\u7387\u5206\u89e3\u6a21\u5757\u7ed3\u5408\u533b\u5b66\u5148\u9a8c\u77e5\u8bc6\u589e\u5f3a\u7279\u5f81\u7279\u5f02\u6027\uff0c\u5f15\u5165\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\u6846\u67b6\u81ea\u9002\u5e94\u5173\u6ce8\u6700\u5177\u8bca\u65ad\u4ef7\u503c\u7684\u7a00\u758f\u7247\u6bb5\u3002", "result": "\u5728\u516c\u5f00PADS\u6570\u636e\u96c6\u548c\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMFAM\u5728\u5904\u7406\u590d\u6742\u4e34\u5e8a\u65f6\u95f4\u5e8f\u5217\u65b9\u9762\u4f18\u4e8e\u901a\u7528\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u3002", "conclusion": "MFAM\u4e3a\u5e15\u91d1\u68ee\u75c5\u4e25\u91cd\u7a0b\u5ea6\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u4e34\u5e8a\u65f6\u95f4\u5e8f\u5217\u7684\u590d\u6742\u6027\u548c\u7279\u5f02\u6027\u9700\u6c42\u3002"}}
{"id": "2510.11307", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11307", "abs": "https://arxiv.org/abs/2510.11307", "authors": ["Sabrina McCallum", "Amit Parekh", "Alessandro Suglia"], "title": "FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks", "comment": "EMNLP 2025 Findings", "summary": "Current approaches to embodied AI tend to learn policies from expert\ndemonstrations. However, without a mechanism to evaluate the quality of\ndemonstrated actions, they are limited to learning from optimal behaviour, or\nthey risk replicating errors and inefficiencies. While reinforcement learning\noffers one alternative, the associated exploration typically results in\nsacrificing data efficiency. This work explores how agents trained with\nimitation learning can learn robust representations from both optimal and\nsuboptimal demonstrations when given access to constructive language feedback\nas a means to contextualise different modes of behaviour. We directly provide\nlanguage feedback embeddings as part of the input sequence into a\nTransformer-based policy, and optionally complement the traditional next action\nprediction objective with auxiliary self-supervised learning objectives for\nfeedback prediction. We test our approach on a range of embodied\nVision-and-Language tasks in our custom BabyAI-XGen environment and show\nsignificant improvements in agents' compositional generalisation abilities and\nrobustness, suggesting that our data-efficient method allows models to\nsuccessfully convert suboptimal behaviour into learning opportunities. Overall,\nour results suggest that language feedback is a competitive and intuitive\nalternative to intermediate scalar rewards for language-specified embodied\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8bed\u8a00\u53cd\u9988\u6765\u4ece\u6700\u4f18\u548c\u6b21\u4f18\u6f14\u793a\u4e2d\u5b66\u4e60\u9c81\u68d2\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8bed\u8a00\u53cd\u9988\u5d4c\u5165\u4f5c\u4e3aTransformer\u7b56\u7565\u7684\u8f93\u5165\uff0c\u5e76\u8f85\u4ee5\u53cd\u9988\u9884\u6d4b\u7684\u8f85\u52a9\u5b66\u4e60\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u7684\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u5177\u8eabAI\u65b9\u6cd5\u901a\u5e38\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\u7b56\u7565\uff0c\u4f46\u7f3a\u4e4f\u8bc4\u4f30\u6f14\u793a\u52a8\u4f5c\u8d28\u91cf\u7684\u673a\u5236\uff0c\u8981\u4e48\u53ea\u80fd\u5b66\u4e60\u6700\u4f18\u884c\u4e3a\uff0c\u8981\u4e48\u53ef\u80fd\u590d\u5236\u9519\u8bef\u548c\u4f4e\u6548\u884c\u4e3a\u3002\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u63d0\u4f9b\u4e86\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u63a2\u7d22\u8fc7\u7a0b\u901a\u5e38\u727a\u7272\u6570\u636e\u6548\u7387\u3002", "method": "\u5c06\u8bed\u8a00\u53cd\u9988\u5d4c\u5165\u76f4\u63a5\u4f5c\u4e3aTransformer\u7b56\u7565\u8f93\u5165\u5e8f\u5217\u7684\u4e00\u90e8\u5206\uff0c\u5e76\u53ef\u9009\u5730\u901a\u8fc7\u8f85\u52a9\u81ea\u76d1\u7763\u5b66\u4e60\u76ee\u6807\uff08\u53cd\u9988\u9884\u6d4b\uff09\u6765\u8865\u5145\u4f20\u7edf\u7684\u4e0b\u4e00\u52a8\u4f5c\u9884\u6d4b\u76ee\u6807\u3002\u5728\u81ea\u5b9a\u4e49\u7684BabyAI-XGen\u73af\u5883\u4e2d\u6d4b\u8bd5\u4e86\u8be5\u65b9\u6cd5\u3002", "result": "\u5728\u5177\u8eab\u89c6\u89c9\u4e0e\u8bed\u8a00\u4efb\u52a1\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u667a\u80fd\u4f53\u7684\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u8868\u660e\u8fd9\u79cd\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u80fd\u591f\u6210\u529f\u5c06\u6b21\u4f18\u884c\u4e3a\u8f6c\u5316\u4e3a\u5b66\u4e60\u673a\u4f1a\u3002", "conclusion": "\u8bed\u8a00\u53cd\u9988\u662f\u8bed\u8a00\u6307\u5b9a\u5177\u8eab\u4efb\u52a1\u4e2d\u4e2d\u95f4\u6807\u91cf\u5956\u52b1\u7684\u7ade\u4e89\u6027\u548c\u76f4\u89c2\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2510.10570", "categories": ["cs.LG", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.10570", "abs": "https://arxiv.org/abs/2510.10570", "authors": ["Zirui Wan", "Stefan Vlaski"], "title": "Multitask Learning with Learned Task Relationships", "comment": null, "summary": "Classical consensus-based strategies for federated and decentralized learning\nare statistically suboptimal in the presence of heterogeneous local data or\ntask distributions. As a result, in recent years, there has been growing\ninterest in multitask or personalized strategies, which allow individual agents\nto benefit from one another in pursuing locally optimal models without\nenforcing consensus. Existing strategies require either precise prior knowledge\nof the underlying task relationships or are fully non-parametric and instead\nrely on meta-learning or proximal constructions. In this work, we introduce an\nalgorithmic framework that strikes a balance between these extremes. By\nmodeling task relationships through a Gaussian Markov Random Field with an\nunknown precision matrix, we develop a strategy that jointly learns both the\ntask relationships and the local models, allowing agents to self-organize in a\nway consistent with their individual data distributions. Our theoretical\nanalysis quantifies the quality of the learned relationship, and our numerical\nexperiments demonstrate its practical effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u548c\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7684\u65b0\u7b97\u6cd5\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u4efb\u52a1\u5173\u7cfb\u4e3a\u9ad8\u65af\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u573a\uff0c\u8054\u5408\u5b66\u4e60\u4efb\u52a1\u5173\u7cfb\u548c\u5c40\u90e8\u6a21\u578b\uff0c\u5728\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e0b\u5b9e\u73b0\u66f4\u4f18\u7684\u4e2a\u6027\u5316\u5b66\u4e60\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u5171\u8bc6\u7684\u8054\u90a6\u548c\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7b56\u7565\u5728\u5f02\u6784\u672c\u5730\u6570\u636e\u6216\u4efb\u52a1\u5206\u5e03\u4e0b\u7edf\u8ba1\u4e0a\u4e0d\u6700\u4f18\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u7cbe\u786e\u7684\u5148\u9a8c\u4efb\u52a1\u5173\u7cfb\u77e5\u8bc6\uff0c\u8981\u4e48\u5b8c\u5168\u975e\u53c2\u6570\u5316\uff0c\u9700\u8981\u5728\u8fd9\u4e24\u4e2a\u6781\u7aef\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "method": "\u901a\u8fc7\u5c06\u4efb\u52a1\u5173\u7cfb\u5efa\u6a21\u4e3a\u5177\u6709\u672a\u77e5\u7cbe\u5ea6\u77e9\u9635\u7684\u9ad8\u65af\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u573a\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u8054\u5408\u5b66\u4e60\u4efb\u52a1\u5173\u7cfb\u548c\u5c40\u90e8\u6a21\u578b\u7684\u7b56\u7565\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u4ee5\u4e0e\u5176\u4e2a\u4f53\u6570\u636e\u5206\u5e03\u4e00\u81f4\u7684\u65b9\u5f0f\u81ea\u7ec4\u7ec7\u3002", "result": "\u7406\u8bba\u5206\u6790\u91cf\u5316\u4e86\u5b66\u4e60\u5173\u7cfb\u7684\u8d28\u91cf\uff0c\u6570\u503c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5b9e\u9645\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e0b\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\uff0c\u5e73\u8861\u4e86\u5148\u9a8c\u77e5\u8bc6\u548c\u975e\u53c2\u6570\u65b9\u6cd5\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2510.11314", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11314", "abs": "https://arxiv.org/abs/2510.11314", "authors": ["Belkiss Souayed", "Sarah Ebling", "Yingqiang Gao"], "title": "Template-Based Text-to-Image Alignment for Language Accessibility: A Study on Visualizing Text Simplifications", "comment": null, "summary": "Individuals with intellectual disabilities often have difficulties in\ncomprehending complex texts. While many text-to-image models prioritize\naesthetics over accessibility, it is not clear how visual illustrations relate\nto text simplifications (TS) generated from them. This paper presents a\nstructured vision-language model (VLM) prompting framework for generating\naccessible images from simplified texts. We designed five prompt templates,\ni.e., Basic Object Focus, Contextual Scene, Educational Layout, Multi-Level\nDetail, and Grid Layout, each following distinct spatial arrangements while\nadhering to accessibility constraints such as object count limits, spatial\nseparation, and content restrictions. Using 400 sentence-level simplifications\nfrom four established TS datasets (OneStopEnglish, SimPA, Wikipedia, and\nASSET), we conducted a two-phase evaluation: Phase 1 assessed prompt template\neffectiveness with CLIPScores, and Phase 2 involved human annotation of\ngenerated images across ten visual styles by four accessibility experts.\nResults show that the Basic Object Focus prompt template achieved the highest\nsemantic alignment, indicating that visual minimalism enhances language\naccessibility. Expert evaluation further identified Retro style as the most\naccessible and Wikipedia as the most effective data source. Inter-annotator\nagreement varied across dimensions, with Text Simplicity showing strong\nreliability and Image Quality proving more subjective. Overall, our framework\noffers practical guidelines for accessible content generation and underscores\nthe importance of structured prompting in AI-generated visual accessibility\ntools.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u793a\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7b80\u5316\u6587\u672c\u751f\u6210\u53ef\u8bbf\u95ee\u56fe\u50cf\uff0c\u901a\u8fc7\u4e94\u79cd\u63d0\u793a\u6a21\u677f\u548c\u4e13\u5bb6\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u57fa\u7840\u5bf9\u8c61\u7126\u70b9\u6a21\u677f\u5728\u8bed\u4e49\u5bf9\u9f50\u65b9\u9762\u7684\u6700\u4f73\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u667a\u529b\u969c\u788d\u4eba\u58eb\u7406\u89e3\u590d\u6742\u6587\u672c\u7684\u56f0\u96be\uff0c\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u8fc7\u4e8e\u6ce8\u91cd\u7f8e\u5b66\u800c\u5ffd\u89c6\u53ef\u8bbf\u95ee\u6027\uff0c\u9700\u8981\u63a2\u7d22\u89c6\u89c9\u63d2\u56fe\u4e0e\u6587\u672c\u7b80\u5316\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e94\u79cd\u63d0\u793a\u6a21\u677f\uff08\u57fa\u7840\u5bf9\u8c61\u7126\u70b9\u3001\u4e0a\u4e0b\u6587\u573a\u666f\u3001\u6559\u80b2\u5e03\u5c40\u3001\u591a\u7ea7\u7ec6\u8282\u3001\u7f51\u683c\u5e03\u5c40\uff09\uff0c\u4f7f\u7528400\u4e2a\u6765\u81ea\u56db\u4e2aTS\u6570\u636e\u96c6\u7684\u53e5\u5b50\u7ea7\u7b80\u5316\u6587\u672c\uff0c\u8fdb\u884c\u4e24\u9636\u6bb5\u8bc4\u4f30\uff1aCLIPScore\u8bc4\u4f30\u548c\u4e13\u5bb6\u4eba\u5de5\u6807\u6ce8\u3002", "result": "\u57fa\u7840\u5bf9\u8c61\u7126\u70b9\u63d0\u793a\u6a21\u677f\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u590d\u53e4\u98ce\u683c\u88ab\u8ba4\u4e3a\u6700\u53ef\u8bbf\u95ee\uff0cWikipedia\u662f\u6700\u6709\u6548\u7684\u6570\u636e\u6e90\uff0c\u6587\u672c\u7b80\u5355\u6027\u7ef4\u5ea6\u663e\u793a\u51fa\u5f3a\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53ef\u8bbf\u95ee\u5185\u5bb9\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff0c\u5f3a\u8c03\u4e86\u7ed3\u6784\u5316\u63d0\u793a\u5728AI\u751f\u6210\u89c6\u89c9\u53ef\u8bbf\u95ee\u6027\u5de5\u5177\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u89c6\u89c9\u6781\u7b80\u4e3b\u4e49\u80fd\u591f\u589e\u5f3a\u8bed\u8a00\u53ef\u8bbf\u95ee\u6027\u3002"}}
{"id": "2510.10572", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10572", "abs": "https://arxiv.org/abs/2510.10572", "authors": ["Byeongchan Lee"], "title": "Understanding Self-supervised Contrastive Learning through Supervised Objectives", "comment": "Accepted at TMLR 2025", "summary": "Self-supervised representation learning has achieved impressive empirical\nsuccess, yet its theoretical understanding remains limited. In this work, we\nprovide a theoretical perspective by formulating self-supervised representation\nlearning as an approximation to supervised representation learning objectives.\nBased on this formulation, we derive a loss function closely related to popular\ncontrastive losses such as InfoNCE, offering insight into their underlying\nprinciples. Our derivation naturally introduces the concepts of prototype\nrepresentation bias and a balanced contrastive loss, which help explain and\nimprove the behavior of self-supervised learning algorithms. We further show\nhow components of our theoretical framework correspond to established practices\nin contrastive learning. Finally, we empirically validate the effect of\nbalancing positive and negative pair interactions. All theoretical proofs are\nprovided in the appendix, and our code is included in the supplementary\nmaterial.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5c06\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u5f62\u5f0f\u5316\u4e3a\u76d1\u7763\u8868\u793a\u5b66\u4e60\u76ee\u6807\u7684\u8fd1\u4f3c\uff0c\u63a8\u5bfc\u51fa\u4e0eInfoNCE\u7b49\u5bf9\u6bd4\u635f\u5931\u5bc6\u5207\u76f8\u5173\u7684\u635f\u5931\u51fd\u6570\uff0c\u5f15\u5165\u539f\u578b\u8868\u793a\u504f\u5dee\u548c\u5e73\u8861\u5bf9\u6bd4\u635f\u5931\u6982\u5ff5\uff0c\u5e76\u5b9e\u8bc1\u9a8c\u8bc1\u6b63\u8d1f\u5bf9\u4ea4\u4e92\u5e73\u8861\u7684\u6548\u679c\u3002", "motivation": "\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u5728\u5b9e\u8bc1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5176\u7406\u8bba\u7406\u89e3\u4ecd\u7136\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u4ece\u7406\u8bba\u89d2\u5ea6\u7406\u89e3\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u7684\u539f\u7406\u3002", "method": "\u5c06\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u5f62\u5f0f\u5316\u4e3a\u76d1\u7763\u8868\u793a\u5b66\u4e60\u76ee\u6807\u7684\u8fd1\u4f3c\uff0c\u63a8\u5bfc\u51fa\u76f8\u5173\u635f\u5931\u51fd\u6570\uff0c\u5f15\u5165\u539f\u578b\u8868\u793a\u504f\u5dee\u548c\u5e73\u8861\u5bf9\u6bd4\u635f\u5931\u6982\u5ff5\uff0c\u5e76\u4e0e\u73b0\u6709\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u8df5\u5efa\u7acb\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u63a8\u5bfc\u51fa\u4e0e\u6d41\u884c\u5bf9\u6bd4\u635f\u5931\uff08\u5982InfoNCE\uff09\u5bc6\u5207\u76f8\u5173\u7684\u635f\u5931\u51fd\u6570\uff0c\u63d0\u4f9b\u4e86\u5bf9\u5176\u5e95\u5c42\u539f\u7406\u7684\u89c1\u89e3\uff0c\u5e76\u901a\u8fc7\u5e73\u8861\u6b63\u8d1f\u5bf9\u4ea4\u4e92\u6539\u8fdb\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\u7684\u884c\u4e3a\u3002", "conclusion": "\u672c\u6587\u4e3a\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u91ca\u4e86\u5bf9\u6bd4\u635f\u5931\u7684\u539f\u7406\uff0c\u5e76\u901a\u8fc7\u5e73\u8861\u673a\u5236\u6539\u8fdb\u4e86\u7b97\u6cd5\u6027\u80fd\uff0c\u6240\u6709\u7406\u8bba\u8bc1\u660e\u548c\u4ee3\u7801\u5747\u5df2\u63d0\u4f9b\u3002"}}
{"id": "2510.11328", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11328", "abs": "https://arxiv.org/abs/2510.11328", "authors": ["Chenxi Wang", "Yixuan Zhang", "Ruiji Yu", "Yufei Zheng", "Lang Gao", "Zirui Song", "Zixiang Xu", "Gus Xia", "Huishuai Zhang", "Dongyan Zhao", "Xiuying Chen"], "title": "Do LLMs \"Feel\"? Emotion Circuits Discovery and Control", "comment": "19 pages, 8 figures, 8 tables. Code and dataset available at\n  https://github.com/Aurora-cx/EmotionCircuits-LLM", "summary": "As the demand for emotional intelligence in large language models (LLMs)\ngrows, a key challenge lies in understanding the internal mechanisms that give\nrise to emotional expression and in controlling emotions in generated text.\nThis study addresses three core questions: (1) Do LLMs contain context-agnostic\nmechanisms shaping emotional expression? (2) What form do these mechanisms\ntake? (3) Can they be harnessed for universal emotion control? We first\nconstruct a controlled dataset, SEV (Scenario-Event with Valence), to elicit\ncomparable internal states across emotions. Subsequently, we extract\ncontext-agnostic emotion directions that reveal consistent, cross-context\nencoding of emotion (Q1). We identify neurons and attention heads that locally\nimplement emotional computation through analytical decomposition and causal\nanalysis, and validate their causal roles via ablation and enhancement\ninterventions. Next, we quantify each sublayer's causal influence on the\nmodel's final emotion representation and integrate the identified local\ncomponents into coherent global emotion circuits that drive emotional\nexpression (Q2). Directly modulating these circuits achieves 99.65%\nemotion-expression accuracy on the test set, surpassing prompting- and\nsteering-based methods (Q3). To our knowledge, this is the first systematic\nstudy to uncover and validate emotion circuits in LLMs, offering new insights\ninto interpretability and controllable emotional intelligence.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u60c5\u611f\u7535\u8def\u673a\u5236\uff0c\u901a\u8fc7\u8c03\u63a7\u8fd9\u4e9b\u7535\u8def\u53ef\u4ee5\u5b9e\u73b099.65%\u7684\u60c5\u611f\u8868\u8fbe\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86\u57fa\u4e8e\u63d0\u793a\u548c\u5f15\u5bfc\u7684\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u60c5\u611f\u667a\u80fd\u9700\u6c42\u7684\u589e\u957f\uff0c\u9700\u8981\u7406\u89e3\u60c5\u611f\u8868\u8fbe\u7684\u5185\u90e8\u673a\u5236\u5e76\u63a7\u5236\u751f\u6210\u6587\u672c\u4e2d\u7684\u60c5\u611f\u3002", "method": "\u6784\u5efaSEV\u6570\u636e\u96c6\u63d0\u53d6\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\u60c5\u611f\u65b9\u5411\uff0c\u901a\u8fc7\u5206\u6790\u5206\u89e3\u548c\u56e0\u679c\u5206\u6790\u8bc6\u522b\u795e\u7ecf\u5143\u548c\u6ce8\u610f\u529b\u5934\uff0c\u91cf\u5316\u5404\u5b50\u5c42\u5bf9\u60c5\u611f\u8868\u793a\u7684\u56e0\u679c\u5f71\u54cd\uff0c\u6574\u5408\u5c40\u90e8\u7ec4\u4ef6\u5f62\u6210\u5168\u5c40\u60c5\u611f\u7535\u8def\u3002", "result": "\u8bc6\u522b\u51fa\u60c5\u611f\u7535\u8def\uff0c\u76f4\u63a5\u8c03\u63a7\u8fd9\u4e9b\u7535\u8def\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523099.65%\u7684\u60c5\u611f\u8868\u8fbe\u51c6\u786e\u7387\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u7cfb\u7edf\u53d1\u73b0\u548c\u9a8c\u8bc1\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u60c5\u611f\u7535\u8def\u7684\u7814\u7a76\uff0c\u4e3a\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u60c5\u611f\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2510.10586", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.10586", "abs": "https://arxiv.org/abs/2510.10586", "authors": ["Giulio Ruffini"], "title": "Compositional Symmetry as Compression: Lie Pseudogroup Structure in Algorithmic Agents", "comment": "Submitted to NeurReps 2025 (https://www.neurreps.org)", "summary": "In the algorithmic (Kolmogorov) view, agents are programs that track and\ncompress sensory streams using generative programs. We propose a framework\nwhere the relevant structural prior is simplicity (Solomonoff) understood as\n\\emph{compositional symmetry}: natural streams are well described by (local)\nactions of finite-parameter Lie pseudogroups on geometrically and topologically\ncomplex low-dimensional configuration manifolds (latent spaces). Modeling the\nagent as a generic neural dynamical system coupled to such streams, we show\nthat accurate world-tracking imposes (i) \\emph{structural constraints} --\nequivariance of the agent's constitutive equations and readouts -- and (ii)\n\\emph{dynamical constraints}: under static inputs, symmetry induces conserved\nquantities (Noether-style labels) in the agent dynamics and confines\ntrajectories to reduced invariant manifolds; under slow drift, these manifolds\nmove but remain low-dimensional. This yields a hierarchy of reduced manifolds\naligned with the compositional factorization of the pseudogroup, providing a\ngeometric account of the ``blessing of compositionality'' in deep models. We\nconnect these ideas to the Spencer formalism for Lie pseudogroups and formulate\na symmetry-based, self-contained version of predictive coding in which higher\nlayers receive only \\emph{coarse-grained residual transformations}\n(prediction-error coordinates) along symmetry directions unresolved at lower\nlayers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7ec4\u5408\u5bf9\u79f0\u6027\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u81ea\u7136\u6570\u636e\u6d41\u5efa\u6a21\u4e3a\u6709\u9650\u53c2\u6570\u674e\u4f2a\u7fa4\u5728\u4f4e\u7ef4\u6d41\u5f62\u4e0a\u7684\u5c40\u90e8\u4f5c\u7528\uff0c\u5e76\u8bc1\u660e\u51c6\u786e\u7684\u4e16\u754c\u8ddf\u8e2a\u9700\u8981\u7ed3\u6784\u7ea6\u675f\uff08\u7b49\u53d8\u6027\uff09\u548c\u52a8\u529b\u5b66\u7ea6\u675f\uff08\u5b88\u6052\u91cf\u3001\u4e0d\u53d8\u6d41\u5f62\uff09\u3002", "motivation": "\u4ece\u7b97\u6cd5\u89c6\u89d2\u7406\u89e3\u667a\u80fd\u4f53\u5982\u4f55\u538b\u7f29\u611f\u77e5\u6d41\uff0c\u5229\u7528\u7ec4\u5408\u5bf9\u79f0\u6027\u4f5c\u4e3a\u7ed3\u6784\u5148\u9a8c\u6765\u89e3\u91ca\u81ea\u7136\u6570\u636e\u6d41\u7684\u89c4\u5f8b\u6027\u3002", "method": "\u5c06\u667a\u80fd\u4f53\u5efa\u6a21\u4e3a\u4e0e\u5bf9\u79f0\u6570\u636e\u6d41\u8026\u5408\u7684\u795e\u7ecf\u52a8\u529b\u7cfb\u7edf\uff0c\u5206\u6790\u7ed3\u6784\u7ea6\u675f\uff08\u7b49\u53d8\u65b9\u7a0b\uff09\u548c\u52a8\u529b\u5b66\u7ea6\u675f\uff08Noether\u5b88\u6052\u91cf\u3001\u4e0d\u53d8\u6d41\u5f62\uff09\uff0c\u8fde\u63a5Spencer\u5f62\u5f0f\u4f53\u7cfb\u5e76\u63d0\u51fa\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u9884\u6d4b\u7f16\u7801\u6846\u67b6\u3002", "result": "\u8bc1\u660e\u4e86\u5bf9\u79f0\u6027\u8bf1\u5bfc\u5b88\u6052\u91cf\u548c\u4f4e\u7ef4\u4e0d\u53d8\u6d41\u5f62\uff0c\u5efa\u7acb\u4e86\u4e0e\u674e\u4f2a\u7fa4\u7ec4\u5408\u5206\u89e3\u5bf9\u9f50\u7684\u5c42\u6b21\u5316\u6d41\u5f62\u7ed3\u6784\uff0c\u4e3a\u6df1\u5ea6\u6a21\u578b\u7684\u7ec4\u5408\u6027\u63d0\u4f9b\u51e0\u4f55\u89e3\u91ca\u3002", "conclusion": "\u7ec4\u5408\u5bf9\u79f0\u6027\u4e3a\u667a\u80fd\u4f53\u7684\u4e16\u754c\u5efa\u6a21\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u901a\u8fc7\u5bf9\u79f0\u6027\u7ea6\u675f\u5b9e\u73b0\u9ad8\u6548\u7684\u6570\u636e\u538b\u7f29\u548c\u5c42\u6b21\u5316\u8868\u793a\u5b66\u4e60\u3002"}}
{"id": "2510.11358", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11358", "abs": "https://arxiv.org/abs/2510.11358", "authors": ["Hengran Zhang", "Keping Bi", "Jiafeng Guo", "Jiaming Zhang", "Shuaiqiang Wang", "Dawei Yin", "Xueqi Cheng"], "title": "LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation", "comment": "13 pages, 9 figures", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge. While traditional retrieval focuses on\nrelevance, RAG's effectiveness depends on the utility of retrieved passages,\ni.e., the usefulness in facilitating the generation of an accurate and\ncomprehensive answer. Existing studies often treat utility as a generic\nattribute, ignoring the fact that different LLMs may benefit differently from\nthe same passage due to variations in internal knowledge and comprehension\nability. In this work, we introduce and systematically investigate the notion\nof LLM-specific utility. Through large-scale experiments across multiple\ndatasets and LLMs, we demonstrate that human-annotated passages are not optimal\nfor LLMs and that ground-truth utilitarian passages are not transferable across\ndifferent LLMs. These findings highlight the necessity of adopting the\nLLM-specific utility in RAG research. Our findings indicate that some\nhuman-annotated passages are not ground-truth utilitarian passages for specific\nLLMs, partially due to the varying readability of queries and passages for\nLLMs, a tendency for which perplexity is a key metric. Based on these findings,\nwe propose a benchmarking procedure for LLM-specific utility judgments. We\nevaluate existing utility judgment methods on six datasets and find that while\nverbalized methods using pseudo-answers perform robustly, LLMs struggle to\nassess utility effectively-failing to reject all passages for known queries and\nto select truly useful ones for unknown queries.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86LLM\u7279\u5b9a\u6548\u7528\u7684\u6982\u5ff5\uff0c\u53d1\u73b0\u4eba\u7c7b\u6807\u6ce8\u7684\u6bb5\u843d\u5bf9\u7279\u5b9aLLM\u5e76\u975e\u6700\u4f18\uff0c\u4e14\u4e0d\u540cLLM\u4e4b\u95f4\u7684\u6548\u7528\u6bb5\u843d\u4e0d\u53ef\u8f6c\u79fb\uff0c\u5f3a\u8c03\u5728RAG\u7814\u7a76\u4e2d\u91c7\u7528LLM\u7279\u5b9a\u6548\u7528\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u4f20\u7edf\u68c0\u7d22\u5173\u6ce8\u76f8\u5173\u6027\uff0c\u4f46RAG\u7684\u6709\u6548\u6027\u53d6\u51b3\u4e8e\u68c0\u7d22\u6bb5\u843d\u7684\u6548\u7528\u3002\u73b0\u6709\u7814\u7a76\u5c06\u6548\u7528\u89c6\u4e3a\u901a\u7528\u5c5e\u6027\uff0c\u5ffd\u7565\u4e86\u4e0d\u540cLLM\u7531\u4e8e\u5185\u90e8\u77e5\u8bc6\u548c\u7406\u89e3\u80fd\u529b\u5dee\u5f02\uff0c\u53ef\u80fd\u4ece\u540c\u4e00\u6bb5\u843d\u4e2d\u83b7\u76ca\u4e0d\u540c\u3002", "method": "\u901a\u8fc7\u8de8\u591a\u4e2a\u6570\u636e\u96c6\u548cLLM\u7684\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u5206\u6790LLM\u7279\u5b9a\u6548\u7528\uff1b\u63d0\u51fa\u57fa\u4e8e\u56f0\u60d1\u5ea6\u7684LLM\u7279\u5b9a\u6548\u7528\u8bc4\u4f30\u57fa\u51c6\u7a0b\u5e8f\uff1b\u8bc4\u4f30\u73b0\u6709\u6548\u7528\u5224\u65ad\u65b9\u6cd5\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u4eba\u7c7b\u6807\u6ce8\u7684\u6bb5\u843d\u5bf9\u7279\u5b9aLLM\u5e76\u975e\u6700\u4f18\uff1b\u771f\u5b9e\u6548\u7528\u6bb5\u843d\u5728\u4e0d\u540cLLM\u95f4\u4e0d\u53ef\u8f6c\u79fb\uff1b\u57fa\u4e8e\u4f2a\u7b54\u6848\u7684\u8a00\u8bed\u5316\u65b9\u6cd5\u8868\u73b0\u7a33\u5065\uff0c\u4f46LLM\u5728\u8bc4\u4f30\u6548\u7528\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "RAG\u7814\u7a76\u5fc5\u987b\u8003\u8651LLM\u7279\u5b9a\u6548\u7528\uff0c\u73b0\u6709LLM\u5728\u6548\u7528\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684LLM\u7279\u5b9a\u6548\u7528\u5224\u65ad\u65b9\u6cd5\u3002"}}
{"id": "2510.10604", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10604", "abs": "https://arxiv.org/abs/2510.10604", "authors": ["Yuheng Chen", "Dingkun Liu", "Xinyao Yang", "Xinping Xu", "Baicheng Chen", "Dongrui Wu"], "title": "FusionGen: Feature Fusion-Based Few-Shot EEG Data Generation", "comment": null, "summary": "Brain-computer interfaces (BCIs) provide potential for applications ranging\nfrom medical rehabilitation to cognitive state assessment by establishing\ndirect communication pathways between the brain and external devices via\nelectroencephalography (EEG). However, EEG-based BCIs are severely constrained\nby data scarcity and significant inter-subject variability, which hinder the\ngeneralization and applicability of EEG decoding models in practical settings.\nTo address these challenges, we propose FusionGen, a novel EEG data generation\nframework based on disentangled representation learning and feature fusion. By\nintegrating features across trials through a feature matching fusion module and\ncombining them with a lightweight feature extraction and reconstruction\npipeline, FusionGen ensures both data diversity and trainability under limited\ndata constraints. Extensive experiments on multiple publicly available EEG\ndatasets demonstrate that FusionGen significantly outperforms existing\naugmentation techniques, yielding notable improvements in classification\naccuracy.", "AI": {"tldr": "FusionGen\u662f\u4e00\u4e2a\u57fa\u4e8e\u89e3\u8026\u8868\u793a\u5b66\u4e60\u548c\u7279\u5f81\u878d\u5408\u7684\u65b0\u578bEEG\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u8111\u673a\u63a5\u53e3\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u4e2a\u4f53\u5dee\u5f02\u95ee\u9898\u3002", "motivation": "EEG\u8111\u673a\u63a5\u53e3\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u663e\u8457\u7684\u4e2a\u4f53\u95f4\u5dee\u5f02\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86EEG\u89e3\u7801\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u7279\u5f81\u5339\u914d\u878d\u5408\u6a21\u5757\u6574\u5408\u8de8\u8bd5\u9a8c\u7684\u7279\u5f81\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u7279\u5f81\u63d0\u53d6\u548c\u91cd\u5efa\u6d41\u7a0b\uff0c\u786e\u4fdd\u5728\u6709\u9650\u6570\u636e\u7ea6\u675f\u4e0b\u7684\u6570\u636e\u591a\u6837\u6027\u548c\u53ef\u8bad\u7ec3\u6027\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00EEG\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFusionGen\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5728\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "FusionGen\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u8868\u793a\u5b66\u4e60\u548c\u7279\u5f81\u878d\u5408\u6709\u6548\u89e3\u51b3\u4e86EEG\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3a\u8111\u673a\u63a5\u53e3\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2510.11370", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11370", "abs": "https://arxiv.org/abs/2510.11370", "authors": ["Wenhan Ma", "Hailin Zhang", "Liang Zhao", "Yifan Song", "Yudong Wang", "Zhifang Sui", "Fuli Luo"], "title": "Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing\nthe capabilities of large language models. However, in Mixture-of-Experts (MoE)\nmodels, the routing mechanism often introduces instability, even leading to\ncatastrophic RL training collapse. We analyze the training-inference\nconsistency of MoE models and identify a notable discrepancy in routing\nbehaviors between the two phases. Moreover, even under identical conditions,\nthe routing framework can yield divergent expert selections across repeated\nforward passes. To address this foundational inconsistency, we propose Rollout\nRouting Replay (R3), a method that records routing distributions from the\ninference engine and replays them during training. R3 significantly reduces\ntraining-inference policy KL divergence and mitigates extreme discrepancies\nwithout compromising training speed. Extensive experiments on various settings\nconfirm that R3 succeeds in stabilizing RL training, preventing collapse and\noutperforming methods such as GSPO and TIS. We believe this work can offer a\nnew solution for stabilizing RL in MoE models.", "AI": {"tldr": "\u63d0\u51fa\u4e86Rollout Routing Replay (R3)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bb0\u5f55\u63a8\u7406\u9636\u6bb5\u7684\u8def\u7531\u5206\u5e03\u5e76\u5728\u8bad\u7ec3\u65f6\u91cd\u653e\uff0c\u89e3\u51b3MoE\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u8def\u7531\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "MoE\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u8def\u7531\u673a\u5236\u4f1a\u5f15\u5165\u4e0d\u7a33\u5b9a\u6027\uff0c\u751a\u81f3\u5bfc\u81f4\u707e\u96be\u6027\u8bad\u7ec3\u5d29\u6e83\u3002\u7814\u7a76\u53d1\u73b0\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u5b58\u5728\u8def\u7531\u884c\u4e3a\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faR3\u65b9\u6cd5\uff1a\u8bb0\u5f55\u63a8\u7406\u5f15\u64ce\u7684\u8def\u7531\u5206\u5e03\uff0c\u5728\u8bad\u7ec3\u65f6\u91cd\u653e\u8fd9\u4e9b\u5206\u5e03\uff0c\u51cf\u5c11\u8bad\u7ec3-\u63a8\u7406\u7b56\u7565KL\u6563\u5ea6\uff0c\u7f13\u89e3\u6781\u7aef\u5dee\u5f02\u3002", "result": "R3\u6210\u529f\u7a33\u5b9a\u4e86RL\u8bad\u7ec3\uff0c\u9632\u6b62\u4e86\u5d29\u6e83\uff0c\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8eGSPO\u548cTIS\u7b49\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7a33\u5b9aMoE\u6a21\u578b\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10605", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10605", "abs": "https://arxiv.org/abs/2510.10605", "authors": ["MohammadHossein Bateni", "Hossein Esfandiari", "Samira HosseinGhorban", "Alireza Mirrokni", "Radin Shahdaei"], "title": "Budget Allocation for Unknown Value Functions in a Lipschitz Space", "comment": null, "summary": "Building learning models frequently requires evaluating numerous intermediate\nmodels. Examples include models considered during feature selection, model\nstructure search, and parameter tunings. The evaluation of an intermediate\nmodel influences subsequent model exploration decisions. Although prior\nknowledge can provide initial quality estimates, true performance is only\nrevealed after evaluation. In this work, we address the challenge of optimally\nallocating a bounded budget to explore the space of intermediate models. We\nformalize this as a general budget allocation problem over unknown-value\nfunctions within a Lipschitz space.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6709\u9650\u9884\u7b97\u4e0b\u4f18\u5316\u63a2\u7d22\u4e2d\u95f4\u6a21\u578b\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u5c06\u5176\u5f62\u5f0f\u5316\u4e3aLipschitz\u7a7a\u95f4\u4e2d\u672a\u77e5\u503c\u51fd\u6570\u7684\u9884\u7b97\u5206\u914d\u95ee\u9898\u3002", "motivation": "\u6784\u5efa\u5b66\u4e60\u6a21\u578b\u65f6\u7ecf\u5e38\u9700\u8981\u8bc4\u4f30\u5927\u91cf\u4e2d\u95f4\u6a21\u578b\uff08\u5982\u7279\u5f81\u9009\u62e9\u3001\u6a21\u578b\u7ed3\u6784\u641c\u7d22\u3001\u53c2\u6570\u8c03\u4f18\u7b49\uff09\uff0c\u8fd9\u4e9b\u8bc4\u4f30\u7ed3\u679c\u4f1a\u5f71\u54cd\u540e\u7eed\u7684\u6a21\u578b\u63a2\u7d22\u51b3\u7b56\u3002\u867d\u7136\u5148\u9a8c\u77e5\u8bc6\u53ef\u4ee5\u63d0\u4f9b\u521d\u59cb\u8d28\u91cf\u4f30\u8ba1\uff0c\u4f46\u771f\u5b9e\u6027\u80fd\u53ea\u6709\u5728\u8bc4\u4f30\u540e\u624d\u80fd\u63ed\u793a\u3002", "method": "\u5c06\u4e2d\u95f4\u6a21\u578b\u63a2\u7d22\u95ee\u9898\u5f62\u5f0f\u5316\u4e3aLipschitz\u7a7a\u95f4\u4e2d\u672a\u77e5\u503c\u51fd\u6570\u7684\u4e00\u822c\u9884\u7b97\u5206\u914d\u95ee\u9898\u3002", "result": "\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e\u5177\u4f53\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u89e3\u51b3\u4e86\u5728\u6709\u9650\u9884\u7b97\u4e0b\u6700\u4f18\u5206\u914d\u8d44\u6e90\u6765\u63a2\u7d22\u4e2d\u95f4\u6a21\u578b\u7a7a\u95f4\u7684\u6311\u6218\u3002"}}
{"id": "2510.11372", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11372", "abs": "https://arxiv.org/abs/2510.11372", "authors": ["Dean L. Slack", "Noura Al Moubayed"], "title": "Early Detection and Reduction of Memorisation for Domain Adaptation and Instruction Tuning", "comment": "Accepted to Transactions of the ACL (TACL), 2025. 15 pages, 6\n  figures, 3 tables", "summary": "Although large language models excel across many tasks, they can memorise\ntraining data and thereby expose private or copyrighted text. Most defences\ntarget the pre-training stage, leaving memorisation during fine-tuning,\nespecially for domain adaptation and instruction tuning, poorly understood. We\nfine-tune Pythia, Llama3, and Mistral models spanning 1.4B-70B parameters on\ncommon evaluation datasets and track verbatim memorisation throughout training.\nWe find that memorisation increases dramatically in the first few epochs, often\nsignificantly before either validation perplexity or evaluation performance is\noptimised. We use a simple but effective n-gram memorisation score which\nreliably precedes verbatim memorisation; using it as an early-stopping\ncriterion mitigates memorisation with minimal performance loss. Further, we\nintroduce an n-gram-aware loss regulariser and show that it reduces\nmemorisation across all model families tested by up to 40% while minimising\nevaluation performance trade-offs when compared to an existing memorisation\nmitigation strategy. These results yield practical, scalable insights into\nmemorisation dynamics during language model fine-tuning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u8bb0\u5fc6\u95ee\u9898\uff0c\u53d1\u73b0\u8bb0\u5fc6\u73b0\u8c61\u5728\u8bad\u7ec3\u65e9\u671f\u6025\u5267\u589e\u52a0\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8en-gram\u7684\u65e9\u671f\u505c\u6b62\u51c6\u5219\u548c\u635f\u5931\u6b63\u5219\u5316\u65b9\u6cd5\u6765\u7f13\u89e3\u8bb0\u5fc6\u95ee\u9898\u3002", "motivation": "\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bb8\u591a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u4f1a\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\uff0c\u4ece\u800c\u66b4\u9732\u79c1\u6709\u6216\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u6587\u672c\u3002\u5927\u591a\u6570\u9632\u5fa1\u63aa\u65bd\u9488\u5bf9\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u800c\u5bf9\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u8bb0\u5fc6\u95ee\u9898\uff0c\u7279\u522b\u662f\u9886\u57df\u9002\u5e94\u548c\u6307\u4ee4\u5fae\u8c03\u4e2d\u7684\u8bb0\u5fc6\u95ee\u9898\u4e86\u89e3\u4e0d\u8db3\u3002", "method": "\u5728Pythia\u3001Llama3\u548cMistral\u6a21\u578b\u4e0a\u5fae\u8c03\uff0c\u8ddf\u8e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u9010\u5b57\u8bb0\u5fc6\u3002\u4f7f\u7528\u7b80\u5355\u7684n-gram\u8bb0\u5fc6\u8bc4\u5206\u4f5c\u4e3a\u65e9\u671f\u505c\u6b62\u51c6\u5219\uff0c\u5e76\u5f15\u5165n-gram\u611f\u77e5\u7684\u635f\u5931\u6b63\u5219\u5316\u5668\u3002", "result": "\u53d1\u73b0\u8bb0\u5fc6\u5728\u524d\u51e0\u4e2a\u8bad\u7ec3\u5468\u671f\u4e2d\u6025\u5267\u589e\u52a0\uff0c\u901a\u5e38\u663e\u8457\u65e9\u4e8e\u9a8c\u8bc1\u56f0\u60d1\u5ea6\u6216\u8bc4\u4f30\u6027\u80fd\u7684\u4f18\u5316\u3002\u63d0\u51fa\u7684n-gram\u8bb0\u5fc6\u8bc4\u5206\u53ef\u9760\u5730\u9884\u6d4b\u9010\u5b57\u8bb0\u5fc6\uff0c\u65e9\u671f\u505c\u6b62\u51c6\u5219\u80fd\u6709\u6548\u7f13\u89e3\u8bb0\u5fc6\u95ee\u9898\u4e14\u6027\u80fd\u635f\u5931\u6700\u5c0f\u3002\u635f\u5931\u6b63\u5219\u5316\u5668\u5728\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u7cfb\u5217\u4e2d\u51cf\u5c11\u8bb0\u5fc6\u8fbe40%\uff0c\u4e0e\u73b0\u6709\u8bb0\u5fc6\u7f13\u89e3\u7b56\u7565\u76f8\u6bd4\u6700\u5c0f\u5316\u6027\u80fd\u6743\u8861\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u8bb0\u5fc6\u52a8\u6001\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u89c1\u89e3\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u8bb0\u5fc6\u95ee\u9898\u3002"}}
{"id": "2510.11389", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11389", "abs": "https://arxiv.org/abs/2510.11389", "authors": ["Zirui Song", "Yuan Huang", "Junchang Liu", "Haozhe Luo", "Chenxi Wang", "Lang Gao", "Zixiang Xu", "Mingfei Han", "Xiaojun Chang", "Xiuying Chen"], "title": "Beyond Survival: Evaluating LLMs in Social Deduction Games with Human-Aligned Strategies", "comment": "34 pages, 32figures", "summary": "Social deduction games like Werewolf combine language, reasoning, and\nstrategy, providing a testbed for studying natural language and social\nintelligence. However, most studies reduce the game to LLM-based self-play,\nyielding templated utterances and anecdotal cases that overlook the richness of\nsocial gameplay. Evaluation further relies on coarse metrics such as survival\ntime or subjective scoring due to the lack of quality reference data. To\naddress these gaps, we curate a high-quality, human-verified multimodal\nWerewolf dataset containing over 100 hours of video, 32.4M utterance tokens,\nand 15 rule variants. Based on this dataset, we propose a novel\nstrategy-alignment evaluation that leverages the winning faction's strategies\nas ground truth in two stages: 1) Speech evaluation, formulated as\nmultiple-choice-style tasks that assess whether the model can adopt appropriate\nstances across five dimensions of social ability; and 2) Decision evaluation,\nwhich assesses the model's voting choices and opponent-role inferences. This\nframework enables a fine-grained evaluation of models' linguistic and reasoning\ncapabilities, while capturing their ability to generate strategically coherent\ngameplay. Our experiments show that state-of-the-art LLMs show diverse\nperformance, with roughly half remain below 0.50, revealing clear gaps in\ndeception and counterfactual reasoning. We hope our dataset further inspires\nresearch on language, reasoning, and strategy in multi-agent interaction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u72fc\u4eba\u6740\u6570\u636e\u96c6\u548c\u7b56\u7565\u5bf9\u9f50\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4ea4\u63a8\u7406\u548c\u7b56\u7565\u6e38\u620f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u7814\u7a76\u5927\u591a\u7b80\u5316\u4e3aLLM\u81ea\u5bf9\u5f08\uff0c\u4ea7\u751f\u6a21\u677f\u5316\u5bf9\u8bdd\u5e76\u5ffd\u89c6\u793e\u4ea4\u6e38\u620f\u7684\u4e30\u5bcc\u6027\uff0c\u4e14\u7f3a\u4e4f\u8d28\u91cf\u53c2\u8003\u6570\u636e\u548c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u5305\u542b100+\u5c0f\u65f6\u89c6\u9891\u300132.4M\u8bdd\u8bed\u6807\u8bb0\u548c15\u79cd\u89c4\u5219\u53d8\u4f53\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e24\u9636\u6bb5\u7b56\u7565\u5bf9\u9f50\u8bc4\u4f30\uff1a\u8a00\u8bed\u8bc4\u4f30\uff08\u591a\u9009\u62e9\u4efb\u52a1\u8bc4\u4f30\u793e\u4ea4\u80fd\u529b\uff09\u548c\u51b3\u7b56\u8bc4\u4f30\uff08\u8bc4\u4f30\u6295\u7968\u9009\u62e9\u548c\u5bf9\u624b\u89d2\u8272\u63a8\u7406\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6700\u5148\u8fdb\u7684LLMs\u8868\u73b0\u591a\u6837\uff0c\u7ea6\u4e00\u534a\u6a21\u578b\u5f97\u5206\u4f4e\u4e8e0.50\uff0c\u5728\u6b3a\u9a97\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u65b9\u9762\u5b58\u5728\u660e\u663e\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u4e3a\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4e2d\u7684\u8bed\u8a00\u3001\u63a8\u7406\u548c\u7b56\u7565\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u548c\u542f\u53d1\u3002"}}
{"id": "2510.10621", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10621", "abs": "https://arxiv.org/abs/2510.10621", "authors": ["Hanbing Liu", "Yanru Wu", "Yang Li", "Ercan E. Kuruoglu", "Xuan Zhang"], "title": "SDG-L: A Semiparametric Deep Gaussian Process based Framework for Battery Capacity Prediction", "comment": null, "summary": "Lithium-ion batteries are becoming increasingly omnipresent in energy supply.\nHowever, the durability of energy storage using lithium-ion batteries is\nthreatened by their dropping capacity with the growing number of\ncharging/discharging cycles. An accurate capacity prediction is the key to\nensure system efficiency and reliability, where the exploitation of battery\nstate information in each cycle has been largely undervalued. In this paper, we\npropose a semiparametric deep Gaussian process regression framework named SDG-L\nto give predictions based on the modeling of time series battery state data. By\nintroducing an LSTM feature extractor, the SDG-L is specially designed to\nbetter utilize the auxiliary profiling information during charging/discharging\nprocess. In experimental studies based on NASA dataset, our proposed method\nobtains an average test MSE error of 1.2%. We also show that SDG-L achieves\nbetter performance compared to existing works and validate the framework using\nablation studies.", "AI": {"tldr": "\u63d0\u51faSDG-L\u6846\u67b6\uff0c\u57fa\u4e8e\u534a\u53c2\u6570\u6df1\u5ea6\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff0c\u5229\u7528LSTM\u7279\u5f81\u63d0\u53d6\u5668\u5efa\u6a21\u7535\u6c60\u72b6\u6001\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u51c6\u786e\u9884\u6d4b\u9502\u79bb\u5b50\u7535\u6c60\u5bb9\u91cf\u8870\u51cf\u3002", "motivation": "\u9502\u79bb\u5b50\u7535\u6c60\u5728\u5145\u653e\u7535\u5faa\u73af\u4e2d\u5bb9\u91cf\u4e0b\u964d\u5a01\u80c1\u5176\u8010\u4e45\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u7535\u6c60\u72b6\u6001\u4fe1\u606f\u7684\u5229\u7528\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u5bb9\u91cf\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSDG-L\u6846\u67b6\uff0c\u7ed3\u5408\u534a\u53c2\u6570\u6df1\u5ea6\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u548cLSTM\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u66f4\u597d\u5229\u7528\u5145\u653e\u7535\u8fc7\u7a0b\u4e2d\u7684\u8f85\u52a9\u5256\u9762\u4fe1\u606f\u3002", "result": "\u5728NASA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5e73\u5747\u6d4b\u8bd5MSE\u8bef\u5dee\u4e3a1.2%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\u3002", "conclusion": "SDG-L\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528\u7535\u6c60\u72b6\u6001\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u63d0\u4f9b\u51c6\u786e\u7684\u5bb9\u91cf\u9884\u6d4b\uff0c\u63d0\u5347\u7cfb\u7edf\u6548\u7387\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.11407", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11407", "abs": "https://arxiv.org/abs/2510.11407", "authors": ["Sahil Kale", "Devendra Singh Dhami"], "title": "KnowRL: Teaching Language Models to Know What They Know", "comment": "14 pages, 7 figures", "summary": "Truly reliable AI requires more than simply scaling up knowledge; it demands\nthe ability to know what it knows and when it does not. Yet recent research\nshows that even the best LLMs misjudge their own competence in more than one in\nfive cases, making any response born of such internal uncertainty impossible to\nfully trust. Inspired by self-improvement reinforcement learning techniques\nthat require minimal data, we present a simple but powerful framework KnowRL\nthat strengthens a model's internal understanding of its own feasibility\nboundaries, enabling safer and more responsible behaviour. Our framework\ncombines two components: (i) introspection, where the model generates and\nclassifies tasks it judges feasible or infeasible, and (ii) consensus-based\nrewarding, where stability of self-knowledge assessment is reinforced through\ninternal agreement. By using internally generated data, this design strengthens\nconsistency in self-knowledge and entirely avoids costly external supervision.\nIn experiments on LLaMA-3.1-8B and Qwen-2.5-7B, KnowRL steadily improved\nself-knowledge, validated by both intrinsic self-consistency and extrinsic\nbenchmarking. With nothing more than a small seed set and no external\nsupervision, our method drove gains as high as 28% in accuracy and 12% in F1,\noutperforming baselines in just a few iterations. Our framework essentially\nunlocks the untapped capacity of LLMs to self-improve their knowledge\nawareness, opening the door to reliable, more accountable AI and safer\ndeployment in critical applications. Owing to its simplicity and independence\nfrom external effort, we encourage applying this reliability-enhancing process\nto all future models.", "AI": {"tldr": "KnowRL\u6846\u67b6\u901a\u8fc7\u81ea\u7701\u548c\u5171\u8bc6\u5956\u52b1\u673a\u5236\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u5373\u53ef\u63d0\u5347LLM\u7684\u81ea\u6211\u8ba4\u77e5\u80fd\u529b\uff0c\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u548cF1\u5206\u6570\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684LLM\u5728\u8d85\u8fc7\u4e94\u5206\u4e4b\u4e00\u7684\u60c5\u51b5\u4e0b\u4f1a\u9519\u8bef\u5224\u65ad\u81ea\u8eab\u80fd\u529b\uff0c\u5bfc\u81f4\u57fa\u4e8e\u8fd9\u79cd\u5185\u90e8\u4e0d\u786e\u5b9a\u6027\u7684\u54cd\u5e94\u65e0\u6cd5\u5b8c\u5168\u4fe1\u4efb\uff0c\u8fd9\u963b\u788d\u4e86\u53ef\u9760AI\u7684\u53d1\u5c55\u3002", "method": "\u7ed3\u5408\u4e24\u4e2a\u7ec4\u4ef6\uff1a(i)\u81ea\u7701\uff1a\u6a21\u578b\u751f\u6210\u5e76\u5206\u7c7b\u5176\u5224\u65ad\u4e3a\u53ef\u884c\u6216\u4e0d\u53ef\u884c\u7684\u4efb\u52a1\uff1b(ii)\u5171\u8bc6\u5956\u52b1\uff1a\u901a\u8fc7\u5185\u90e8\u4e00\u81f4\u6027\u6765\u5f3a\u5316\u81ea\u6211\u77e5\u8bc6\u8bc4\u4f30\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5728LLaMA-3.1-8B\u548cQwen-2.5-7B\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cKnowRL\u663e\u8457\u63d0\u5347\u4e86\u81ea\u6211\u8ba4\u77e5\u80fd\u529b\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u9ad8\u8fbe28%\uff0cF1\u5206\u6570\u63d0\u9ad812%\uff0c\u4ec5\u9700\u5c11\u91cf\u79cd\u5b50\u96c6\u4e14\u65e0\u9700\u5916\u90e8\u76d1\u7763\u3002", "conclusion": "\u8be5\u6846\u67b6\u89e3\u9501\u4e86LLM\u81ea\u6211\u63d0\u5347\u77e5\u8bc6\u610f\u8bc6\u7684\u6f5c\u529b\uff0c\u4e3a\u5b9e\u73b0\u66f4\u53ef\u9760\u3001\u66f4\u8d1f\u8d23\u4efb\u7684AI\u4ee5\u53ca\u5728\u5173\u952e\u5e94\u7528\u4e2d\u66f4\u5b89\u5168\u7684\u90e8\u7f72\u6253\u5f00\u4e86\u5927\u95e8\u3002"}}
{"id": "2510.10625", "categories": ["cs.LG", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10625", "abs": "https://arxiv.org/abs/2510.10625", "authors": ["Yuval Golbari", "Navve Wasserman", "Gal Vardi", "Michal Irani"], "title": "ImpMIA: Leveraging Implicit Bias for Membership Inference Attack under Realistic Scenarios", "comment": null, "summary": "Determining which data samples were used to train a model-known as Membership\nInference Attack (MIA)-is a well-studied and important problem with\nimplications for data privacy. Black-box methods presume access only to the\nmodel's outputs and often rely on training auxiliary reference models. While\nthey have shown strong empirical performance, they rely on assumptions that\nrarely hold in real-world settings: (i) the attacker knows the training\nhyperparameters; (ii) all available non-training samples come from the same\ndistribution as the training data; and (iii) the fraction of training data in\nthe evaluation set is known. In this paper, we demonstrate that removing these\nassumptions leads to a significant drop in the performance of black-box\nattacks. We introduce ImpMIA, a Membership Inference Attack that exploits the\nImplicit Bias of neural networks, hence removes the need to rely on any\nreference models and their assumptions. ImpMIA is a white-box attack -- a\nsetting which assumes access to model weights and is becoming increasingly\nrealistic given that many models are publicly available (e.g., via Hugging\nFace). Building on maximum-margin implicit bias theory, ImpMIA uses the\nKarush-Kuhn-Tucker (KKT) optimality conditions to identify training samples.\nThis is done by finding the samples whose gradients most strongly reconstruct\nthe trained model's parameters. As a result, ImpMIA achieves state-of-the-art\nperformance compared to both black and white box attacks in realistic settings\nwhere only the model weights and a superset of the training data are available.", "AI": {"tldr": "ImpMIA\u662f\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u9690\u5f0f\u504f\u7f6e\u7684\u767d\u76d2\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u53c2\u8003\u6a21\u578b\uff0c\u5728\u4ec5\u77e5\u9053\u6a21\u578b\u6743\u91cd\u548c\u8bad\u7ec3\u6570\u636e\u8d85\u96c6\u7684\u73b0\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u9ed1\u76d2\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\u4f9d\u8d56\u4e0d\u73b0\u5b9e\u7684\u5047\u8bbe\uff1a\u653b\u51fb\u8005\u77e5\u9053\u8bad\u7ec3\u8d85\u53c2\u6570\u3001\u975e\u8bad\u7ec3\u6837\u672c\u4e0e\u8bad\u7ec3\u6570\u636e\u540c\u5206\u5e03\u3001\u8bc4\u4f30\u96c6\u4e2d\u8bad\u7ec3\u6570\u636e\u6bd4\u4f8b\u5df2\u77e5\u3002\u79fb\u9664\u8fd9\u4e9b\u5047\u8bbe\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u57fa\u4e8e\u6700\u5927\u95f4\u9694\u9690\u5f0f\u504f\u7f6e\u7406\u8bba\uff0c\u5229\u7528KKT\u6700\u4f18\u6027\u6761\u4ef6\u8bc6\u522b\u8bad\u7ec3\u6837\u672c\uff0c\u901a\u8fc7\u5bfb\u627e\u68af\u5ea6\u6700\u80fd\u91cd\u6784\u6a21\u578b\u53c2\u6570\u7684\u6837\u672c\u6765\u5b9e\u73b0\u653b\u51fb\u3002", "result": "\u5728\u4ec5\u80fd\u83b7\u53d6\u6a21\u578b\u6743\u91cd\u548c\u8bad\u7ec3\u6570\u636e\u8d85\u96c6\u7684\u73b0\u5b9e\u8bbe\u7f6e\u4e2d\uff0cImpMIA\u76f8\u6bd4\u9ed1\u76d2\u548c\u767d\u76d2\u653b\u51fb\u65b9\u6cd5\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "ImpMIA\u901a\u8fc7\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u7684\u9690\u5f0f\u504f\u7f6e\uff0c\u6446\u8131\u4e86\u5bf9\u53c2\u8003\u6a21\u578b\u53ca\u5176\u5047\u8bbe\u7684\u4f9d\u8d56\uff0c\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u3002"}}
{"id": "2510.11408", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11408", "abs": "https://arxiv.org/abs/2510.11408", "authors": ["Stefan Krsteski", "Giuseppe Russo", "Serina Chang", "Robert West", "Kristina Gligori\u0107"], "title": "Valid Survey Simulations with Limited Human Data: The Roles of Prompting, Fine-Tuning, and Rectification", "comment": "19 pages, 4 figures, 9 tables", "summary": "Surveys provide valuable insights into public opinion and behavior, but their\nexecution is costly and slow. Large language models (LLMs) have been proposed\nas a scalable, low-cost substitute for human respondents, but their outputs are\noften biased and yield invalid estimates. We study the interplay between\nsynthesis methods that use LLMs to generate survey responses and rectification\nmethods that debias population estimates, and explore how human responses are\nbest allocated between them. Using two panel surveys with questions on\nnutrition, politics, and economics, we find that synthesis alone introduces\nsubstantial bias (24-86%), whereas combining it with rectification reduces bias\nbelow 5% and increases effective sample size by up to 14%. Overall, we\nchallenge the common practice of using all human responses for fine-tuning,\nshowing that under a fixed budget, allocating most to rectification results in\nfar more effective estimation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7ed3\u5408LLM\u751f\u6210\u8c03\u67e5\u54cd\u5e94\u548c\u53bb\u504f\u65b9\u6cd5\u7684\u6700\u4f73\u7b56\u7565\uff0c\u53d1\u73b0\u5728\u56fa\u5b9a\u9884\u7b97\u4e0b\uff0c\u5c06\u5927\u90e8\u5206\u4eba\u7c7b\u54cd\u5e94\u5206\u914d\u7ed9\u53bb\u504f\u5904\u7406\u6bd4\u7528\u4e8e\u5fae\u8c03\u80fd\u66f4\u6709\u6548\u5730\u51cf\u5c11\u504f\u5dee\u5e76\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u8c03\u67e5\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u867d\u7136LLMs\u88ab\u63d0\u8bae\u4f5c\u4e3a\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u8f93\u51fa\u5b58\u5728\u504f\u5dee\u4e14\u4f30\u8ba1\u65e0\u6548\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u6700\u4f73\u5206\u914d\u4eba\u7c7b\u54cd\u5e94\u5728\u5408\u6210\u548c\u53bb\u504f\u65b9\u6cd5\u4e4b\u95f4\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u5305\u542b\u8425\u517b\u3001\u653f\u6cbb\u548c\u7ecf\u6d4e\u95ee\u9898\u7684\u9762\u677f\u8c03\u67e5\uff0c\u7814\u7a76\u5408\u6210\u65b9\u6cd5\uff08\u7528LLM\u751f\u6210\u54cd\u5e94\uff09\u548c\u53bb\u504f\u65b9\u6cd5\uff08\u6821\u6b63\u4eba\u53e3\u4f30\u8ba1\u504f\u5dee\uff09\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4ee5\u53ca\u4eba\u7c7b\u54cd\u5e94\u7684\u6700\u4f73\u5206\u914d\u7b56\u7565\u3002", "result": "\u5355\u72ec\u4f7f\u7528\u5408\u6210\u65b9\u6cd5\u4f1a\u4ea7\u751f\u663e\u8457\u504f\u5dee\uff0824-86%\uff09\uff0c\u800c\u7ed3\u5408\u53bb\u504f\u65b9\u6cd5\u53ef\u5c06\u504f\u5dee\u964d\u81f35%\u4ee5\u4e0b\uff0c\u5e76\u5c06\u6709\u6548\u6837\u672c\u91cf\u63d0\u9ad8\u8fbe14%\u3002\u5728\u56fa\u5b9a\u9884\u7b97\u4e0b\uff0c\u5c06\u5927\u90e8\u5206\u4eba\u7c7b\u54cd\u5e94\u5206\u914d\u7ed9\u53bb\u504f\u5904\u7406\u6bd4\u7528\u4e8e\u5fae\u8c03\u6548\u679c\u66f4\u597d\u3002", "conclusion": "\u6311\u6218\u4e86\u5c06\u6240\u6709\u4eba\u7c7b\u54cd\u5e94\u7528\u4e8e\u5fae\u8c03\u7684\u5e38\u89c1\u505a\u6cd5\uff0c\u8bc1\u660e\u5728\u56fa\u5b9a\u9884\u7b97\u4e0b\uff0c\u5c06\u5927\u90e8\u5206\u54cd\u5e94\u5206\u914d\u7ed9\u53bb\u504f\u5904\u7406\u80fd\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u4f30\u8ba1\u3002"}}
{"id": "2510.10634", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10634", "abs": "https://arxiv.org/abs/2510.10634", "authors": ["Shaoning Li", "Le Zhuo", "Yusong Wang", "Mingyu Li", "Xinheng He", "Fandi Wu", "Hongsheng Li", "Pheng-Ann Heng"], "title": "ProteinAE: Protein Diffusion Autoencoders for Structure Encoding", "comment": null, "summary": "Developing effective representations of protein structures is essential for\nadvancing protein science, particularly for protein generative modeling.\nCurrent approaches often grapple with the complexities of the SE(3) manifold,\nrely on discrete tokenization, or the need for multiple training objectives,\nall of which can hinder the model optimization and generalization. We introduce\nProteinAE, a novel and streamlined protein diffusion autoencoder designed to\novercome these challenges by directly mapping protein backbone coordinates from\nE(3) into a continuous, compact latent space. ProteinAE employs a\nnon-equivariant Diffusion Transformer with a bottleneck design for efficient\ncompression and is trained end-to-end with a single flow matching objective,\nsubstantially simplifying the optimization pipeline. We demonstrate that\nProteinAE achieves state-of-the-art reconstruction quality, outperforming\nexisting autoencoders. The resulting latent space serves as a powerful\nfoundation for a latent diffusion model that bypasses the need for explicit\nequivariance. This enables efficient, high-quality structure generation that is\ncompetitive with leading structure-based approaches and significantly\noutperforms prior latent-based methods. Code is available at\nhttps://github.com/OnlyLoveKFC/ProteinAE_v1.", "AI": {"tldr": "ProteinAE\u662f\u4e00\u79cd\u65b0\u9896\u7684\u86cb\u767d\u8d28\u6269\u6563\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5c06\u86cb\u767d\u8d28\u4e3b\u5e72\u5750\u6807\u76f4\u63a5\u6620\u5c04\u5230\u8fde\u7eed\u7d27\u51d1\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u7b80\u5316\u4e86\u86cb\u767d\u8d28\u7ed3\u6784\u8868\u793a\u5b66\u4e60\uff0c\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u7ed3\u6784\u751f\u6210\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u5f53\u524d\u86cb\u767d\u8d28\u7ed3\u6784\u8868\u793a\u65b9\u6cd5\u9762\u4e34SE(3)\u6d41\u5f62\u590d\u6742\u6027\u3001\u79bb\u6563\u6807\u8bb0\u5316\u6216\u591a\u8bad\u7ec3\u76ee\u6807\u7b49\u6311\u6218\uff0c\u8fd9\u4e9b\u56e0\u7d20\u963b\u788d\u4e86\u6a21\u578b\u4f18\u5316\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5177\u6709\u74f6\u9888\u8bbe\u8ba1\u7684\u975e\u7b49\u53d8\u6269\u6563\u53d8\u6362\u5668\uff0c\u901a\u8fc7\u5355\u4e00\u6d41\u5339\u914d\u76ee\u6807\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u5c06\u86cb\u767d\u8d28\u4e3b\u5e72\u5750\u6807\u4eceE(3)\u6620\u5c04\u5230\u8fde\u7eed\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "ProteinAE\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u81ea\u7f16\u7801\u5668\uff1b\u5176\u6f5c\u5728\u7a7a\u95f4\u4e3a\u6f5c\u5728\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u5f3a\u5927\u57fa\u7840\uff0c\u65e0\u9700\u663e\u5f0f\u7b49\u53d8\u6027\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u9ad8\u8d28\u91cf\u7ed3\u6784\u751f\u6210\u3002", "conclusion": "ProteinAE\u901a\u8fc7\u7b80\u5316\u7684\u4f18\u5316\u6d41\u7a0b\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u86cb\u767d\u8d28\u7ed3\u6784\u8868\u793a\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u86cb\u767d\u8d28\u79d1\u5b66\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2510.11434", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11434", "abs": "https://arxiv.org/abs/2510.11434", "authors": ["Dana Sotto Porat", "Ella Rabinovich"], "title": "Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated Content", "comment": "ECAI2025 (Identity-Aware AI workshop)", "summary": "Generative large language models (LLMs) have become central to everyday life,\nproducing human-like text across diverse domains. A growing body of research\ninvestigates whether these models also exhibit personality- and\ndemographic-like characteristics in their language. In this work, we introduce\na novel, data-driven methodology for assessing LLM personality without relying\non self-report questionnaires, applying instead automatic personality and\ngender classifiers to model replies on open-ended questions collected from\nReddit. Comparing six widely used models to human-authored responses, we find\nthat LLMs systematically express higher Agreeableness and lower Neuroticism,\nreflecting cooperative and stable conversational tendencies. Gendered language\npatterns in model text broadly resemble those of human writers, though with\nreduced variation, echoing prior findings on automated agents. We contribute a\nnew dataset of human and model responses, along with large-scale comparative\nanalyses, shedding new light on the topic of personality and demographic\npatterns of generative AI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u81ea\u6211\u62a5\u544a\u95ee\u5377\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e2a\u6027\u7279\u5f81\uff0c\u53d1\u73b0LLM\u5728\u8bed\u8a00\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5b9c\u4eba\u6027\u548c\u66f4\u4f4e\u7684\u795e\u7ecf\u8d28\uff0c\u6027\u522b\u8bed\u8a00\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u4f46\u53d8\u5316\u8f83\u5c0f\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5728\u5176\u8bed\u8a00\u4e2d\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u683c\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u7684\u7279\u5f81\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u751f\u6210\u5f0fAI\u7684\u4e2a\u6027\u6a21\u5f0f\u3002", "method": "\u4f7f\u7528\u81ea\u52a8\u4e2a\u6027\u548c\u6027\u522b\u5206\u7c7b\u5668\u5206\u6790\u6a21\u578b\u5728Reddit\u5f00\u653e\u6027\u95ee\u9898\u4e0a\u7684\u56de\u590d\uff0c\u6bd4\u8f83\u516d\u79cd\u5e38\u7528\u6a21\u578b\u4e0e\u4eba\u7c7b\u64b0\u5199\u7684\u56de\u7b54\u3002", "result": "LLM\u7cfb\u7edf\u6027\u5730\u8868\u8fbe\u66f4\u9ad8\u7684\u5b9c\u4eba\u6027\u548c\u66f4\u4f4e\u7684\u795e\u7ecf\u8d28\uff0c\u53cd\u6620\u5408\u4f5c\u7a33\u5b9a\u7684\u5bf9\u8bdd\u503e\u5411\uff1b\u6027\u522b\u8bed\u8a00\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u4f5c\u8005\u76f8\u4f3c\u4f46\u53d8\u5316\u8f83\u5c0f\u3002", "conclusion": "\u8d21\u732e\u4e86\u65b0\u7684\u4eba\u7c7b\u548c\u6a21\u578b\u56de\u590d\u6570\u636e\u96c6\u53ca\u5927\u89c4\u6a21\u6bd4\u8f83\u5206\u6790\uff0c\u4e3a\u751f\u6210\u5f0fAI\u7684\u4e2a\u6027\u548c\u4eba\u53e3\u7edf\u8ba1\u6a21\u5f0f\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2510.10645", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10645", "abs": "https://arxiv.org/abs/2510.10645", "authors": ["Michal Sadowski", "Maria Wyrzykowska", "Lukasz Sztukiewicz", "Tadija Radusinovi\u0107", "Jan Rzymkowski", "Pawe\u0142 W\u0142odarczyk-Pruszy\u0144ski", "Miko\u0142aj Sacha", "Piotr Kozakowski", "Ruard van Workum", "Stanislaw Kamil Jastrzebski"], "title": "Trustworthy Retrosynthesis: Eliminating Hallucinations with a Diverse Ensemble of Reaction Scorers", "comment": null, "summary": "Retrosynthesis is one of the domains transformed by the rise of generative\nmodels, and it is one where the problem of nonsensical or erroneous outputs\n(hallucinations) is particularly insidious: reliable assessment of synthetic\nplans is time-consuming, with automatic methods lacking. In this work, we\npresent RetroTrim, a retrosynthesis system that successfully avoids nonsensical\nplans on a set of challenging drug-like targets. Compared to common baselines\nin the field, our system is not only the sole method that succeeds in filtering\nout hallucinated reactions, but it also results in the highest number of\nhigh-quality paths overall. The key insight behind RetroTrim is the combination\nof diverse reaction scoring strategies, based on machine learning models and\nexisting chemical databases. We show that our scoring strategies capture\ndifferent classes of hallucinations by analyzing them on a dataset of labeled\nretrosynthetic intermediates. To measure the performance of retrosynthesis\nsystems, we propose a novel evaluation protocol for reactions and synthetic\npaths based on a structured review by expert chemists. Using this protocol, we\ncompare systems on a set of 32 novel targets, curated to reflect recent trends\nin drug structures. While the insights behind our methodology are broadly\napplicable to retrosynthesis, our focus is on targets in the drug-like domain.\nBy releasing our benchmark targets and the details of our evaluation protocol,\nwe hope to inspire further research into reliable retrosynthesis.", "AI": {"tldr": "RetroTrim\u662f\u4e00\u4e2a\u9006\u5408\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u53cd\u5e94\u8bc4\u5206\u7b56\u7565\u6210\u529f\u907f\u514d\u4e86\u65e0\u610f\u4e49\u7684\u5408\u6210\u8def\u5f84\uff0c\u5728\u836f\u7269\u7c7b\u76ee\u6807\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u9006\u5408\u6210\u9886\u57df\u5b58\u5728\u751f\u6210\u6a21\u578b\u4ea7\u751f\u65e0\u610f\u4e49\u6216\u9519\u8bef\u8f93\u51fa\uff08\u5e7b\u89c9\uff09\u7684\u95ee\u9898\uff0c\u800c\u53ef\u9760\u7684\u5408\u6210\u8ba1\u5212\u8bc4\u4f30\u8017\u65f6\u4e14\u7f3a\u4e4f\u81ea\u52a8\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u73b0\u6709\u5316\u5b66\u6570\u636e\u5e93\u7684\u591a\u6837\u5316\u53cd\u5e94\u8bc4\u5206\u7b56\u7565\uff0c\u6355\u83b7\u4e0d\u540c\u7c7b\u578b\u7684\u5e7b\u89c9\u53cd\u5e94\u3002", "result": "RetroTrim\u662f\u552f\u4e00\u80fd\u6210\u529f\u8fc7\u6ee4\u6389\u5e7b\u89c9\u53cd\u5e94\u7684\u65b9\u6cd5\uff0c\u572832\u4e2a\u65b0\u578b\u836f\u7269\u76ee\u6807\u4e0a\u4ea7\u751f\u6700\u591a\u9ad8\u8d28\u91cf\u5408\u6210\u8def\u5f84\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03\u57fa\u51c6\u76ee\u6807\u548c\u8bc4\u4f30\u534f\u8bae\u7ec6\u8282\uff0c\u5e0c\u671b\u63a8\u52a8\u53ef\u9760\u9006\u5408\u6210\u7814\u7a76\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2510.11444", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11444", "abs": "https://arxiv.org/abs/2510.11444", "authors": ["Yawen Yang", "Fukun Ma", "Shiao Meng", "Aiwei Liu", "Lijie Wen"], "title": "GenCNER: A Generative Framework for Continual Named Entity Recognition", "comment": "Accepted by IJCNN 2025", "summary": "Traditional named entity recognition (NER) aims to identify text mentions\ninto pre-defined entity types. Continual Named Entity Recognition (CNER) is\nintroduced since entity categories are continuously increasing in various\nreal-world scenarios. However, existing continual learning (CL) methods for NER\nface challenges of catastrophic forgetting and semantic shift of non-entity\ntype. In this paper, we propose GenCNER, a simple but effective Generative\nframework for CNER to mitigate the above drawbacks. Specifically, we skillfully\nconvert the CNER task into sustained entity triplet sequence generation problem\nand utilize a powerful pre-trained seq2seq model to solve it. Additionally, we\ndesign a type-specific confidence-based pseudo labeling strategy along with\nknowledge distillation (KD) to preserve learned knowledge and alleviate the\nimpact of label noise at the triplet level. Experimental results on two\nbenchmark datasets show that our framework outperforms previous\nstate-of-the-art methods in multiple CNER settings, and achieves the smallest\ngap compared with non-CL results.", "AI": {"tldr": "\u63d0\u51faGenCNER\u751f\u6210\u5f0f\u6846\u67b6\u89e3\u51b3\u6301\u7eed\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u8bed\u4e49\u504f\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u5c06NER\u4efb\u52a1\u8f6c\u6362\u4e3a\u5b9e\u4f53\u4e09\u5143\u7ec4\u5e8f\u5217\u751f\u6210\uff0c\u7ed3\u5408\u4f2a\u6807\u7b7e\u7b56\u7565\u548c\u77e5\u8bc6\u84b8\u998f\u6765\u4fdd\u6301\u5b66\u4e60\u77e5\u8bc6\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u5b9e\u4f53\u7c7b\u522b\u6301\u7eed\u589e\u52a0\uff0c\u4f20\u7edfNER\u65e0\u6cd5\u9002\u5e94\uff0c\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u548c\u975e\u5b9e\u4f53\u7c7b\u578b\u8bed\u4e49\u504f\u79fb\u7684\u6311\u6218\u3002", "method": "\u5c06CNER\u4efb\u52a1\u8f6c\u6362\u4e3a\u6301\u7eed\u5b9e\u4f53\u4e09\u5143\u7ec4\u5e8f\u5217\u751f\u6210\u95ee\u9898\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3seq2seq\u6a21\u578b\uff0c\u8bbe\u8ba1\u7c7b\u578b\u7279\u5b9a\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u7b56\u7565\u548c\u77e5\u8bc6\u84b8\u998f\u6765\u7f13\u89e3\u6807\u7b7e\u566a\u58f0\u5f71\u54cd\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2aCNER\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e0e\u975e\u6301\u7eed\u5b66\u4e60\u7ed3\u679c\u7684\u5dee\u8ddd\u6700\u5c0f\u3002", "conclusion": "GenCNER\u662f\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u751f\u6210\u5f0f\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6301\u7eed\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.10694", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10694", "abs": "https://arxiv.org/abs/2510.10694", "authors": ["Ying-Kuan Tsai", "Vispi Karkaria", "Yi-Ping Chen", "Wei Chen"], "title": "Digital Twin-enabled Multi-generation Control Co-Design with Deep Reinforcement Learning", "comment": "to be published in Journal of Mechanical Design", "summary": "Control Co-Design (CCD) integrates physical and control system design to\nimprove the performance of dynamic and autonomous systems. Despite advances in\nuncertainty-aware CCD methods, real-world uncertainties remain highly\nunpredictable. Multi-generation design addresses this challenge by considering\nthe full lifecycle of a product: data collected from each generation informs\nthe design of subsequent generations, enabling progressive improvements in\nrobustness and efficiency. Digital Twin (DT) technology further strengthens\nthis paradigm by creating virtual representations that evolve over the\nlifecycle through real-time sensing, model updating, and adaptive\nre-optimization. This paper presents a DT-enabled CCD framework that integrates\nDeep Reinforcement Learning (DRL) to jointly optimize physical design and\ncontroller. DRL accelerates real-time decision-making by allowing controllers\nto continuously learn from data and adapt to uncertain environments. Extending\nthis approach, the framework employs a multi-generation paradigm, where each\ncycle of deployment, operation, and redesign uses collected data to refine DT\nmodels, improve uncertainty quantification through quantile regression, and\ninform next-generation designs of both physical components and controllers. The\nframework is demonstrated on an active suspension system, where DT-enabled\nlearning from road conditions and driving behaviors yields smoother and more\nstable control trajectories. Results show that the method significantly\nenhances dynamic performance, robustness, and efficiency. Contributions of this\nwork include: (1) extending CCD into a lifecycle-oriented multi-generation\nframework, (2) leveraging DTs for continuous model updating and informed\ndesign, and (3) employing DRL to accelerate adaptive real-time decision-making.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u5b57\u5b6a\u751f\u9a71\u52a8\u7684\u63a7\u5236\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u4ee3\u8bbe\u8ba1\u8303\u5f0f\uff0c\u901a\u8fc7\u751f\u547d\u5468\u671f\u6570\u636e\u6536\u96c6\u548c\u6a21\u578b\u66f4\u65b0\u6765\u63d0\u5347\u52a8\u6001\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u4e0d\u786e\u5b9a\u6027\u9ad8\u5ea6\u4e0d\u53ef\u9884\u6d4b\uff0c\u4f20\u7edf\u63a7\u5236\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002\u591a\u4ee3\u8bbe\u8ba1\u901a\u8fc7\u4ea7\u54c1\u5168\u751f\u547d\u5468\u671f\u6570\u636e\u6536\u96c6\u6765\u9010\u6b65\u6539\u8fdb\u7cfb\u7edf\u6027\u80fd\uff0c\u6570\u5b57\u5b6a\u751f\u6280\u672f\u901a\u8fc7\u5b9e\u65f6\u611f\u77e5\u548c\u6a21\u578b\u66f4\u65b0\u8fdb\u4e00\u6b65\u52a0\u5f3a\u8fd9\u4e00\u8303\u5f0f\u3002", "method": "\u96c6\u6210\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6765\u8054\u5408\u4f18\u5316\u7269\u7406\u8bbe\u8ba1\u548c\u63a7\u5236\u5668\uff0c\u91c7\u7528\u591a\u4ee3\u8bbe\u8ba1\u8303\u5f0f\uff0c\u6bcf\u4e2a\u90e8\u7f72-\u64cd\u4f5c-\u91cd\u65b0\u8bbe\u8ba1\u5468\u671f\u90fd\u4f7f\u7528\u6536\u96c6\u7684\u6570\u636e\u6765\u7cbe\u70bc\u6570\u5b57\u5b6a\u751f\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u4f4d\u6570\u56de\u5f52\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5e76\u4e3a\u4e0b\u4e00\u4ee3\u7269\u7406\u7ec4\u4ef6\u548c\u63a7\u5236\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4fe1\u606f\u3002", "result": "\u5728\u4e3b\u52a8\u60ac\u67b6\u7cfb\u7edf\u4e0a\u7684\u6f14\u793a\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ece\u9053\u8def\u6761\u4ef6\u548c\u9a7e\u9a76\u884c\u4e3a\u4e2d\u5b66\u4e60\uff0c\u4ea7\u751f\u4e86\u66f4\u5e73\u6ed1\u548c\u66f4\u7a33\u5b9a\u7684\u63a7\u5236\u8f68\u8ff9\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u52a8\u6001\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u7684\u8d21\u732e\u5305\u62ec\uff1a(1)\u5c06\u63a7\u5236\u534f\u540c\u8bbe\u8ba1\u6269\u5c55\u5230\u9762\u5411\u751f\u547d\u5468\u671f\u7684\u591a\u4ee3\u6846\u67b6\uff0c(2)\u5229\u7528\u6570\u5b57\u5b6a\u751f\u8fdb\u884c\u8fde\u7eed\u6a21\u578b\u66f4\u65b0\u548c\u77e5\u60c5\u8bbe\u8ba1\uff0c(3)\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u52a0\u901f\u81ea\u9002\u5e94\u5b9e\u65f6\u51b3\u7b56\u3002"}}
{"id": "2510.11482", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11482", "abs": "https://arxiv.org/abs/2510.11482", "authors": ["Marco Braga", "Gian Carlo Milanese", "Gabriella Pasi"], "title": "Investigating Large Language Models' Linguistic Abilities for Text Preprocessing", "comment": "Accepted in WI-IAT 2025. Pre-camera-ready version", "summary": "Text preprocessing is a fundamental component of Natural Language Processing,\ninvolving techniques such as stopword removal, stemming, and lemmatization to\nprepare text as input for further processing and analysis. Despite the\ncontext-dependent nature of the above techniques, traditional methods usually\nignore contextual information. In this paper, we investigate the idea of using\nLarge Language Models (LLMs) to perform various preprocessing tasks, due to\ntheir ability to take context into account without requiring extensive\nlanguage-specific annotated resources. Through a comprehensive evaluation on\nweb-sourced data, we compare LLM-based preprocessing (specifically stopword\nremoval, lemmatization and stemming) to traditional algorithms across multiple\ntext classification tasks in six European languages. Our analysis indicates\nthat LLMs are capable of replicating traditional stopword removal,\nlemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%,\nrespectively. Additionally, we show that ML algorithms trained on texts\npreprocessed by LLMs achieve an improvement of up to 6% with respect to the\n$F_1$ measure compared to traditional techniques. Our code, prompts, and\nresults are publicly available at\nhttps://github.com/GianCarloMilanese/llm_pipeline_wi-iat.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6587\u672c\u9884\u5904\u7406\uff0c\u5728\u516d\u79cd\u6b27\u6d32\u8bed\u8a00\u7684\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728F1\u5206\u6570\u4e0a\u63d0\u5347\u9ad8\u8fbe6%\u3002", "motivation": "\u4f20\u7edf\u6587\u672c\u9884\u5904\u7406\u65b9\u6cd5\uff08\u5982\u505c\u7528\u8bcd\u53bb\u9664\u3001\u8bcd\u5f62\u8fd8\u539f\u548c\u8bcd\u5e72\u63d0\u53d6\uff09\u901a\u5e38\u5ffd\u7565\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u8003\u8651\u4e0a\u4e0b\u6587\u4e14\u4e0d\u9700\u8981\u5927\u91cf\u8bed\u8a00\u7279\u5b9a\u7684\u6807\u6ce8\u8d44\u6e90\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6267\u884c\u591a\u79cd\u9884\u5904\u7406\u4efb\u52a1\uff0c\u5e76\u5728\u516d\u79cd\u6b27\u6d32\u8bed\u8a00\u7684\u7f51\u7edc\u6570\u636e\u4e0a\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u6bd4\u8f83LLM\u9884\u5904\u7406\u4e0e\u4f20\u7edf\u7b97\u6cd5\u5728\u591a\u4e2a\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "LLM\u80fd\u591f\u590d\u5236\u4f20\u7edf\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u505c\u7528\u8bcd\u53bb\u9664\u3001\u8bcd\u5f62\u8fd8\u539f\u548c\u8bcd\u5e72\u63d0\u53d6\u7684\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523097%\u300182%\u548c74%\u3002\u4f7f\u7528LLM\u9884\u5904\u7406\u7684\u6587\u672c\u8bad\u7ec3\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728F1\u5206\u6570\u4e0a\u6bd4\u4f20\u7edf\u6280\u672f\u63d0\u5347\u9ad8\u8fbe6%\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u6267\u884c\u6587\u672c\u9884\u5904\u7406\u4efb\u52a1\uff0c\u5728\u8003\u8651\u4e0a\u4e0b\u6587\u7684\u540c\u65f6\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2510.10695", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10695", "abs": "https://arxiv.org/abs/2510.10695", "authors": ["Long Chen", "Huixin Bai", "Mingxin Wang", "Xiaohua Huang", "Ying Liu", "Jie Zhao", "Ziyu Guan"], "title": "Stock Prediction via a Dual Relation Fusion Network incorporating Static and Dynamic Relations", "comment": "11 pages", "summary": "Accurate modeling of inter-stock relationships is critical for stock price\nforecasting. However, existing methods predominantly focus on single-state\nrelationships, neglecting the essential complementarity between dynamic and\nstatic inter-stock relations. To solve this problem, we propose a Dual Relation\nFusion Network (DRFN) to capture the long-term relative stability of stock\nrelation structures while retaining the flexibility to respond to sudden market\nshifts. Our approach features a novel relative static relation component that\nmodels time-varying long-term patterns and incorporates overnight informational\ninfluences. We capture dynamic inter-stock relationships through distance-aware\nmechanisms, while evolving long-term structures via recurrent fusion of dynamic\nrelations from the prior day with the pre-defined static relations. Experiments\ndemonstrate that our method significantly outperforms the baselines across\ndifferent markets, with high sensitivity to the co-movement of relational\nstrength and stock price.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u5173\u7cfb\u878d\u5408\u7f51\u7edc\uff08DRFN\uff09\uff0c\u540c\u65f6\u6355\u6349\u80a1\u7968\u95f4\u7684\u52a8\u6001\u548c\u9759\u6001\u5173\u7cfb\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u53ea\u5173\u6ce8\u5355\u4e00\u72b6\u6001\u5173\u7cfb\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u80a1\u4ef7\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u80a1\u7968\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u72b6\u6001\u5173\u7cfb\uff0c\u5ffd\u7565\u4e86\u52a8\u6001\u548c\u9759\u6001\u80a1\u7968\u5173\u7cfb\u4e4b\u95f4\u7684\u4e92\u8865\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u5e02\u573a\u53d8\u5316\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u53cc\u5173\u7cfb\u878d\u5408\u7f51\u7edc\uff0c\u5305\u542b\u76f8\u5bf9\u9759\u6001\u5173\u7cfb\u7ec4\u4ef6\u5efa\u6a21\u957f\u671f\u6a21\u5f0f\uff0c\u901a\u8fc7\u8ddd\u79bb\u611f\u77e5\u673a\u5236\u6355\u6349\u52a8\u6001\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5faa\u73af\u878d\u5408\u524d\u4e00\u5929\u52a8\u6001\u5173\u7cfb\u4e0e\u9884\u5b9a\u4e49\u9759\u6001\u5173\u7cfb\u6765\u6f14\u5316\u957f\u671f\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u5e02\u573a\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5bf9\u5173\u7cfb\u5f3a\u5ea6\u4e0e\u80a1\u4ef7\u5171\u540c\u53d8\u52a8\u7684\u654f\u611f\u6027\u5f88\u9ad8\u3002", "conclusion": "\u540c\u65f6\u8003\u8651\u52a8\u6001\u548c\u9759\u6001\u80a1\u7968\u5173\u7cfb\u80fd\u591f\u6709\u6548\u63d0\u5347\u80a1\u4ef7\u9884\u6d4b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6355\u6349\u5e02\u573a\u957f\u671f\u7a33\u5b9a\u6027\u548c\u77ed\u671f\u7a81\u53d8\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.11529", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11529", "abs": "https://arxiv.org/abs/2510.11529", "authors": ["Yusheng Song", "Lirong Qiu", "Xi Zhang", "Zhihao Tang"], "title": "Hallucination Detection via Internal States and Structured Reasoning Consistency in Large Language Models", "comment": null, "summary": "The detection of sophisticated hallucinations in Large Language Models (LLMs)\nis hampered by a ``Detection Dilemma'': methods probing internal states\n(Internal State Probing) excel at identifying factual inconsistencies but fail\non logical fallacies, while those verifying externalized reasoning\n(Chain-of-Thought Verification) show the opposite behavior. This schism creates\na task-dependent blind spot: Chain-of-Thought Verification fails on\nfact-intensive tasks like open-domain QA where reasoning is ungrounded, while\nInternal State Probing is ineffective on logic-intensive tasks like\nmathematical reasoning where models are confidently wrong. We resolve this with\na unified framework that bridges this critical gap. However, unification is\nhindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse\nsymbolic reasoning chains lack signals directly comparable to fine-grained\ninternal states, and the Representational Alignment Barrier, a deep-seated\nmismatch between their underlying semantic spaces. To overcome these, we\nintroduce a multi-path reasoning mechanism to obtain more comparable,\nfine-grained signals, and a segment-aware temporalized cross-attention module\nto adaptively fuse these now-aligned representations, pinpointing subtle\ndissonances. Extensive experiments on three diverse benchmarks and two leading\nLLMs demonstrate that our framework consistently and significantly outperforms\nstrong baselines. Our code is available: https://github.com/peach918/HalluDet.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u89e3\u51b3LLM\u5e7b\u89c9\u68c0\u6d4b\u7684\u4e24\u96be\u56f0\u5883\uff1a\u5185\u90e8\u72b6\u6001\u63a2\u6d4b\u548c\u601d\u7ef4\u94fe\u9a8c\u8bc1\u5404\u6709\u76f2\u533a\uff0c\u901a\u8fc7\u591a\u8def\u5f84\u63a8\u7406\u673a\u5236\u548c\u5206\u6bb5\u611f\u77e5\u65f6\u5e8f\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u5b9e\u73b0\u6709\u6548\u878d\u5408\u3002", "motivation": "\u89e3\u51b3LLM\u5e7b\u89c9\u68c0\u6d4b\u4e2d\u7684\"\u68c0\u6d4b\u56f0\u5883\"\uff1a\u5185\u90e8\u72b6\u6001\u63a2\u6d4b\u64c5\u957f\u8bc6\u522b\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u4f46\u5728\u903b\u8f91\u8c2c\u8bef\u4e0a\u5931\u8d25\uff0c\u800c\u601d\u7ef4\u94fe\u9a8c\u8bc1\u5219\u76f8\u53cd\uff0c\u8fd9\u5bfc\u81f4\u4efb\u52a1\u4f9d\u8d56\u7684\u76f2\u70b9\u3002", "method": "\u5f15\u5165\u591a\u8def\u5f84\u63a8\u7406\u673a\u5236\u83b7\u53d6\u66f4\u53ef\u6bd4\u8f83\u7684\u7ec6\u7c92\u5ea6\u4fe1\u53f7\uff0c\u4ee5\u53ca\u5206\u6bb5\u611f\u77e5\u65f6\u5e8f\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u81ea\u9002\u5e94\u878d\u5408\u5bf9\u9f50\u540e\u7684\u8868\u793a\uff0c\u7cbe\u786e\u5b9a\u4f4d\u7ec6\u5fae\u7684\u4e0d\u4e00\u81f4\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e24\u4e2a\u9886\u5148LLM\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u6301\u7eed\u4e14\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86LLM\u5e7b\u89c9\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u514b\u670d\u4fe1\u53f7\u7a00\u7f3a\u548c\u8868\u793a\u5bf9\u9f50\u969c\u788d\uff0c\u5b9e\u73b0\u4e86\u66f4\u5168\u9762\u7684\u5e7b\u89c9\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2510.10702", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10702", "abs": "https://arxiv.org/abs/2510.10702", "authors": ["Usman Gani Joy", "Shahadat kabir", "Tasnim Niger"], "title": "Attention-Enhanced LSTM Modeling for Improved Temperature and Rainfall Forecasting in Bangladesh", "comment": null, "summary": "Accurate climate forecasting is vital for Bangladesh, a region highly\nsusceptible to climate change impacts on temperature and rainfall. Existing\nmodels often struggle to capture long-range dependencies and complex temporal\npatterns in climate data. This study introduces an advanced Long Short-Term\nMemory (LSTM) model integrated with an attention mechanism to enhance the\nprediction of temperature and rainfall dynamics. Utilizing comprehensive\ndatasets from 1901-2023, sourced from NASA's POWER Project for temperature and\nthe Humanitarian Data Exchange for rainfall, the model effectively captures\nseasonal and long-term trends. It outperforms baseline models, including\nXGBoost, Simple LSTM, and GRU, achieving a test MSE of 0.2411 (normalized\nunits), MAE of 0.3860 degrees C, R^2 of 0.9834, and NRMSE of 0.0370 for\ntemperature, and MSE of 1283.67 mm^2, MAE of 22.91 mm, R^2 of 0.9639, and NRMSE\nof 0.0354 for rainfall on monthly forecasts. The model demonstrates improved\nrobustness with only a 20 percent increase in MSE under simulated climate\ntrends (compared to an approximately 2.2-fold increase in baseline models\nwithout trend features) and a 50 percent degradation under regional variations\n(compared to an approximately 4.8-fold increase in baseline models without\nenhancements). These results highlight the model's ability to improve\nforecasting precision and offer potential insights into the physical processes\ngoverning climate variability in Bangladesh, supporting applications in\nclimate-sensitive sectors.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u7684LSTM\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u5b5f\u52a0\u62c9\u56fd\u6e29\u5ea6\u548c\u964d\u96e8\u91cf\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002\u8be5\u6a21\u578b\u57281901-2023\u5e74\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86XGBoost\u3001\u7b80\u5355LSTM\u548cGRU\u7b49\u57fa\u51c6\u6a21\u578b\u3002", "motivation": "\u5b5f\u52a0\u62c9\u56fd\u662f\u6c14\u5019\u53d8\u5316\u5f71\u54cd\u4e25\u91cd\u7684\u5730\u533a\uff0c\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u6355\u6349\u6c14\u5019\u6570\u636e\u4e2d\u7684\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u548c\u590d\u6742\u65f6\u95f4\u6a21\u5f0f\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u65b9\u6cd5\u6765\u652f\u6301\u6c14\u5019\u654f\u611f\u90e8\u95e8\u3002", "method": "\u91c7\u7528\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u7684\u9ad8\u7ea7LSTM\u6a21\u578b\uff0c\u5229\u7528NASA POWER\u9879\u76ee\u548cHDX\u76841901-2023\u5e74\u6e29\u5ea6\u548c\u964d\u96e8\u6570\u636e\uff0c\u6709\u6548\u6355\u6349\u5b63\u8282\u6027\u548c\u957f\u671f\u8d8b\u52bf\u3002", "result": "\u6a21\u578b\u5728\u6e29\u5ea6\u9884\u6d4b\u4e0a\u8fbe\u5230MSE 0.2411\u3001MAE 0.3860\u00b0C\u3001R\u00b2 0.9834\u3001NRMSE 0.0370\uff1b\u5728\u964d\u96e8\u9884\u6d4b\u4e0a\u8fbe\u5230MSE 1283.67 mm\u00b2\u3001MAE 22.91 mm\u3001R\u00b2 0.9639\u3001NRMSE 0.0354\u3002\u5728\u6a21\u62df\u6c14\u5019\u53d8\u5316\u8d8b\u52bf\u4e0b\uff0cMSE\u4ec5\u589e\u52a020%\uff0c\u800c\u57fa\u51c6\u6a21\u578b\u589e\u52a0\u7ea62.2\u500d\u3002", "conclusion": "\u8be5\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5bf9\u7406\u89e3\u5b5f\u52a0\u62c9\u56fd\u6c14\u5019\u53d8\u5316\u7684\u7269\u7406\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u6f5c\u5728\u89c1\u89e3\uff0c\u80fd\u591f\u652f\u6301\u6c14\u5019\u654f\u611f\u90e8\u95e8\u7684\u51b3\u7b56\u5e94\u7528\u3002"}}
{"id": "2510.11537", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11537", "abs": "https://arxiv.org/abs/2510.11537", "authors": ["Ba-Quang Nguyen"], "title": "An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese Token-Level Classification", "comment": "11 pages, 1 figure. Submitted to VLSP 2025 and reviewed", "summary": "We propose a novel neural architecture named TextGraphFuseGAT, which\nintegrates a pretrained transformer encoder (PhoBERT) with Graph Attention\nNetworks for token-level classification tasks. The proposed model constructs a\nfully connected graph over the token embeddings produced by PhoBERT, enabling\nthe GAT layer to capture rich inter-token dependencies beyond those modeled by\nsequential context alone. To further enhance contextualization, a\nTransformer-style self-attention layer is applied on top of the graph-enhanced\nembeddings. The final token representations are passed through a classification\nhead to perform sequence labeling. We evaluate our approach on three Vietnamese\nbenchmark datasets: PhoNER-COVID19 for named entity recognition in the COVID-19\ndomain, PhoDisfluency for speech disfluency detection, and VietMed-NER for\nmedical-domain NER. VietMed-NER is the first Vietnamese medical spoken NER\ndataset, featuring 18 entity types collected from real-world medical speech\ntranscripts and annotated with the BIO tagging scheme. Its specialized\nvocabulary and domain-specific expressions make it a challenging benchmark for\ntoken-level classification models. Experimental results show that our method\nconsistently outperforms strong baselines, including transformer-only and\nhybrid neural models such as BiLSTM + CNN + CRF, confirming the effectiveness\nof combining pretrained semantic features with graph-based relational modeling\nfor improved token classification across multiple domains.", "AI": {"tldr": "\u63d0\u51faTextGraphFuseGAT\u67b6\u6784\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3Transformer\u7f16\u7801\u5668(PhoBERT)\u548c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u7528\u4e8e\u8d8a\u5357\u8bed\u6807\u8bb0\u7ea7\u5206\u7c7b\u4efb\u52a1\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u5e8f\u5217\u6a21\u578b\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u6355\u6349\u6807\u8bb0\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u9700\u8981\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u6765\u5efa\u6a21\u8d85\u8d8a\u5e8f\u5217\u4e0a\u4e0b\u6587\u7684\u4e30\u5bcc\u6807\u8bb0\u95f4\u5173\u7cfb\u3002", "method": "\u6784\u5efa\u57fa\u4e8ePhoBERT\u6807\u8bb0\u5d4c\u5165\u7684\u5b8c\u5168\u8fde\u63a5\u56fe\uff0c\u4f7f\u7528GAT\u5c42\u6355\u83b7\u6807\u8bb0\u95f4\u4f9d\u8d56\uff0c\u518d\u5e94\u7528Transformer\u5f0f\u81ea\u6ce8\u610f\u529b\u5c42\u589e\u5f3a\u4e0a\u4e0b\u6587\u8868\u793a\uff0c\u6700\u540e\u901a\u8fc7\u5206\u7c7b\u5934\u8fdb\u884c\u5e8f\u5217\u6807\u6ce8\u3002", "result": "\u5728\u4e09\u4e2a\u8d8a\u5357\u8bed\u57fa\u51c6\u6570\u636e\u96c6(PhoNER-COVID19\u3001PhoDisfluency\u3001VietMed-NER)\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5305\u62ec\u4ec5\u4f7f\u7528Transformer\u7684\u6a21\u578b\u548cBiLSTM+CNN+CRF\u6df7\u5408\u6a21\u578b\u3002", "conclusion": "\u5c06\u9884\u8bad\u7ec3\u8bed\u4e49\u7279\u5f81\u4e0e\u57fa\u4e8e\u56fe\u7684\u5173\u7cfb\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u80fd\u6709\u6548\u63d0\u5347\u8de8\u591a\u4e2a\u9886\u57df\u7684\u6807\u8bb0\u5206\u7c7b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u5728\u6355\u83b7\u590d\u6742\u6807\u8bb0\u4f9d\u8d56\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.10706", "categories": ["cs.LG", "cs.DM"], "pdf": "https://arxiv.org/pdf/2510.10706", "abs": "https://arxiv.org/abs/2510.10706", "authors": ["Mamoona Ghafoor", "Tatsuya Akutsu"], "title": "Designing ReLU Generative Networks to Enumerate Trees with a Given Tree Edit Distance", "comment": null, "summary": "The generation of trees with a specified tree edit distance has significant\napplications across various fields, including computational biology, structured\ndata analysis, and image processing. Recently, generative networks have been\nincreasingly employed to synthesize new data that closely resembles the\noriginal datasets. However, the appropriate size and depth of generative\nnetworks required to generate data with a specified tree edit distance remain\nunclear. In this paper, we theoretically establish the existence and\nconstruction of generative networks capable of producing trees similar to a\ngiven tree with respect to the tree edit distance. Specifically, for a given\nrooted, ordered, and vertex-labeled tree T of size n + 1 with labels from an\nalphabet \\Sigma, and a non-negative integer d, we prove that all rooted,\nordered, and vertex-labeled trees over \\Sigma with tree edit distance at most d\nfrom T can be generated using a ReLU-based generative network with size O(n^3 )\nand constant depth. The proposed networks were implemented and evaluated for\ngenerating trees with up to 21 nodes. Due to their deterministic architecture,\nthe networks successfully generated all valid trees within the specified tree\nedit distance. In contrast, state-of-the-art graph generative models GraphRNN\nand GraphGDP, which rely on non-deterministic mechanisms, produced\nsignificantly fewer valid trees, achieving validation rates of only up to 35%\nand 48%, respectively. These findings provide a theoretical foundation towards\nconstruction of compact generative models and open new directions for exact and\nvalid tree-structured data generation. An implementation of the proposed\nnetworks is available at https://github.com/MGANN-KU/TreeGen_ReLUNetworks.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u4f7f\u7528O(n^3)\u5927\u5c0f\u7684\u5e38\u6570\u6df1\u5ea6ReLU\u751f\u6210\u7f51\u7edc\u53ef\u4ee5\u751f\u6210\u4e0e\u7ed9\u5b9a\u6811\u5177\u6709\u6307\u5b9a\u6811\u7f16\u8f91\u8ddd\u79bb\u7684\u6240\u6709\u6811\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u6709\u6548\u6027\u3002", "motivation": "\u6811\u7f16\u8f91\u8ddd\u79bb\u5728\u8ba1\u7b97\u751f\u7269\u5b66\u3001\u7ed3\u6784\u5316\u6570\u636e\u5206\u6790\u548c\u56fe\u50cf\u5904\u7406\u4e2d\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u751f\u6210\u7f51\u7edc\u5728\u751f\u6210\u5177\u6709\u6307\u5b9a\u6811\u7f16\u8f91\u8ddd\u79bb\u7684\u6570\u636e\u65f6\uff0c\u5176\u6240\u9700\u89c4\u6a21\u548c\u6df1\u5ea6\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4e3a\u7ed9\u5b9a\u7684\u5927\u5c0f\u4e3an+1\u7684\u6839\u6709\u5e8f\u9876\u70b9\u6807\u8bb0\u6811T\u548c\u975e\u8d1f\u6574\u6570d\uff0c\u6784\u5efaReLU\u57fa\u7840\u7684\u751f\u6210\u7f51\u7edc\uff0c\u80fd\u591f\u751f\u6210\u6240\u6709\u4e0eT\u7684\u6811\u7f16\u8f91\u8ddd\u79bb\u4e0d\u8d85\u8fc7d\u7684\u6811\u3002", "result": "\u5b9e\u73b0\u7684\u7f51\u7edc\u6210\u529f\u751f\u6210\u4e86\u6700\u591a21\u4e2a\u8282\u70b9\u7684\u6240\u6709\u6709\u6548\u6811\uff0c\u800c\u6700\u5148\u8fdb\u7684\u56fe\u751f\u6210\u6a21\u578bGraphRNN\u548cGraphGDP\u4ec5\u5206\u522b\u8fbe\u523035%\u548c48%\u7684\u6709\u6548\u751f\u6210\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u7d27\u51d1\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u4e3a\u7cbe\u786e\u6709\u6548\u7684\u6811\u7ed3\u6784\u6570\u636e\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.11545", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11545", "abs": "https://arxiv.org/abs/2510.11545", "authors": ["Jiayu Ding", "Lei Cui", "Li Dong", "Nanning Zheng", "Furu Wei"], "title": "Information-Preserving Reformulation of Reasoning Traces for Antidistillation", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) show that extending the\nlength of reasoning chains significantly improves performance on complex tasks.\nWhile revealing these reasoning traces helps users better follow, verify, and\nlearn from the model's problem-solving process, it also makes them highly\nvulnerable to unauthorized distillation. To mitigate this risk, proprietary\nmodel providers often adopt aggressive protection strategies, such as replacing\ndetailed reasoning with brief summaries, which deprive users of valuable\nintermediate information. To address this trade-off, we propose PART, an\ninformation-preserving antidistillation reformulation of reasoning traces.\nMotivated by the difference between how humans understand reasoning traces and\nhow LLMs exploit them for supervised fine-tuning, we design a simple but\neffective two-step reformulation: removing self-talk behaviors and reordering\nsub-conclusions. A small auxiliary model is trained to perform this\nreformulation, incurring minimal computational overhead. Extensive experiments\ndemonstrate that PART consistently disrupts distillation across student models\nof different sizes and types on various reasoning benchmarks. For instance,\nwhen training on reformulated traces, even the performance of a large 32B\nstudent model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a\n13.5% degradation.", "AI": {"tldr": "\u63d0\u51faPART\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u8868\u8ff0\u63a8\u7406\u8f68\u8ff9\u6765\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u84b8\u998f\uff0c\u540c\u65f6\u4fdd\u7559\u5bf9\u4eba\u7c7b\u7528\u6237\u6709\u7528\u7684\u4e2d\u95f4\u4fe1\u606f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8be6\u7ec6\u63a8\u7406\u8f68\u8ff9\u867d\u7136\u6709\u52a9\u4e8e\u7528\u6237\u7406\u89e3\u548c\u9a8c\u8bc1\uff0c\u4f46\u4e5f\u5bb9\u6613\u88ab\u7528\u4e8e\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u84b8\u998f\u3002\u73b0\u6709\u4fdd\u62a4\u7b56\u7565\u5f80\u5f80\u8fc7\u5ea6\u7b80\u5316\u63a8\u7406\u8fc7\u7a0b\uff0c\u5265\u593a\u4e86\u7528\u6237\u7684\u5b9d\u8d35\u4fe1\u606f\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u6b65\u91cd\u65b0\u8868\u8ff0\u65b9\u6cd5\uff1a\u79fb\u9664\u81ea\u5bf9\u8bdd\u884c\u4e3a\u5e76\u91cd\u65b0\u6392\u5e8f\u5b50\u7ed3\u8bba\u3002\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u578b\u8f85\u52a9\u6a21\u578b\u6765\u6267\u884c\u8fd9\u79cd\u91cd\u65b0\u8868\u8ff0\uff0c\u8ba1\u7b97\u5f00\u9500\u5f88\u5c0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePART\u80fd\u6709\u6548\u5e72\u6270\u5404\u79cd\u89c4\u6a21\u548c\u7c7b\u578b\u7684\u5b66\u751f\u6a21\u578b\u7684\u84b8\u998f\u8fc7\u7a0b\u3002\u4f8b\u5982\uff0c\u5728AIME 2024\u4e0a\uff0c32B\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u4ece54.17\u4e0b\u964d\u523046.88\uff0c\u4e0b\u964d\u4e8613.5%\u3002", "conclusion": "PART\u65b9\u6cd5\u5728\u4fdd\u62a4\u6a21\u578b\u77e5\u8bc6\u4ea7\u6743\u7684\u540c\u65f6\uff0c\u4fdd\u7559\u4e86\u63a8\u7406\u8f68\u8ff9\u5bf9\u4eba\u7c7b\u7528\u6237\u7684\u4ef7\u503c\uff0c\u5b9e\u73b0\u4e86\u4fe1\u606f\u4fdd\u62a4\u4e0e\u53ef\u7528\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u3002"}}
{"id": "2510.11557", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11557", "abs": "https://arxiv.org/abs/2510.11557", "authors": ["Saurabh Khanna", "Xinxu Li"], "title": "Invisible Languages of the LLM Universe", "comment": null, "summary": "Large Language Models are trained on massive multilingual corpora, yet this\nabundance masks a profound crisis: of the world's 7,613 living languages,\napproximately 2,000 languages with millions of speakers remain effectively\ninvisible in digital ecosystems. We propose a critical framework connecting\nempirical measurements of language vitality (real world demographic strength)\nand digitality (online presence) with postcolonial theory and epistemic\ninjustice to explain why linguistic inequality in AI systems is not incidental\nbut structural. Analyzing data across all documented human languages, we\nidentify four categories: Strongholds (33%, high vitality and digitality),\nDigital Echoes (6%, high digitality despite declining vitality), Fading Voices\n(36%, low on both dimensions), and critically, Invisible Giants (27%, high\nvitality but near-zero digitality) - languages spoken by millions yet absent\nfrom the LLM universe. We demonstrate that these patterns reflect continuities\nfrom colonial-era linguistic hierarchies to contemporary AI development,\nconstituting what we term digital epistemic injustice. Our analysis reveals\nthat English dominance in AI is not a technical necessity but an artifact of\npower structures that systematically exclude marginalized linguistic knowledge.\nWe conclude with implications for decolonizing language technology and\ndemocratizing access to AI benefits.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u5168\u7403\u8bed\u8a00\u5728AI\u7cfb\u7edf\u4e2d\u7684\u4e0d\u5e73\u7b49\u73b0\u8c61\uff0c\u63d0\u51fa\u4e86\u8fde\u63a5\u8bed\u8a00\u6d3b\u529b\u4e0e\u6570\u5b57\u5b58\u5728\u7684\u5206\u6790\u6846\u67b6\uff0c\u8bc6\u522b\u51fa\u56db\u79cd\u8bed\u8a00\u7c7b\u522b\uff0c\u63ed\u793a\u4e86\u6b96\u6c11\u65f6\u4ee3\u8bed\u8a00\u7b49\u7ea7\u5236\u5ea6\u5728\u5f53\u4ee3AI\u53d1\u5c55\u4e2d\u7684\u5ef6\u7eed\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5927\u91cf\u591a\u8bed\u8a00\u8bed\u6599\u4e0a\u8bad\u7ec3\uff0c\u4f46\u5168\u7403\u7ea62000\u79cd\u8bed\u8a00\u3001\u6570\u767e\u4e07\u4f7f\u7528\u8005\u4ecd\u88ab\u6570\u5b57\u751f\u6001\u7cfb\u7edf\u5ffd\u89c6\uff0c\u8fd9\u79cd\u8bed\u8a00\u4e0d\u5e73\u7b49\u5e76\u975e\u5076\u7136\u800c\u662f\u7ed3\u6784\u6027\u7684\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6240\u6709\u8bb0\u5f55\u7684\u4eba\u7c7b\u8bed\u8a00\u6570\u636e\uff0c\u5c06\u8bed\u8a00\u6309\u6d3b\u529b\uff08\u73b0\u5b9e\u4e16\u754c\u4eba\u53e3\u5f3a\u5ea6\uff09\u548c\u6570\u5b57\u6027\uff08\u5728\u7ebf\u5b58\u5728\uff09\u5206\u4e3a\u56db\u7c7b\uff1aStrongholds\u3001Digital Echoes\u3001Fading Voices\u548cInvisible Giants\u3002", "result": "\u53d1\u73b027%\u7684\u8bed\u8a00\u5c5e\u4e8e\"\u9690\u5f62\u5de8\u4eba\"\u7c7b\u522b\uff0c\u5373\u62e5\u6709\u9ad8\u6d3b\u529b\u4f46\u6570\u5b57\u5b58\u5728\u51e0\u4e4e\u4e3a\u96f6\uff0c\u8fd9\u4e9b\u8bed\u8a00\u6709\u6570\u767e\u4e07\u4f7f\u7528\u8005\u5374\u88ab\u6392\u9664\u5728LLM\u751f\u6001\u7cfb\u7edf\u4e4b\u5916\u3002", "conclusion": "\u82f1\u8bed\u5728AI\u4e2d\u7684\u4e3b\u5bfc\u5730\u4f4d\u4e0d\u662f\u6280\u672f\u5fc5\u7136\uff0c\u800c\u662f\u6743\u529b\u7ed3\u6784\u7684\u4ea7\u7269\uff0c\u7cfb\u7edf\u6027\u5730\u6392\u9664\u4e86\u8fb9\u7f18\u5316\u8bed\u8a00\u77e5\u8bc6\uff0c\u9700\u8981\u53bb\u6b96\u6c11\u5316\u8bed\u8a00\u6280\u672f\u5e76\u6c11\u4e3b\u5316AI\u5229\u76ca\u83b7\u53d6\u3002"}}
{"id": "2510.10739", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10739", "abs": "https://arxiv.org/abs/2510.10739", "authors": ["Shivani Shukla", "Himanshu Joshi"], "title": "A Stochastic Differential Equation Framework for Multi-Objective LLM Interactions: Dynamical Systems Analysis with Code Generation Applications", "comment": "Peer-reviewed and accepted to the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025) DynaFront 2025 Workshop\n  (https://sites.google.com/view/dynafrontneurips25)", "summary": "We introduce a general stochastic differential equation framework for\nmodelling multiobjective optimization dynamics in iterative Large Language\nModel (LLM) interactions. Our framework captures the inherent stochasticity of\nLLM responses through explicit diffusion terms and reveals systematic\ninterference patterns between competing objectives via an interference matrix\nformulation. We validate our theoretical framework using iterative code\ngeneration as a proof-of-concept application, analyzing 400 sessions across\nsecurity, efficiency, and functionality objectives. Our results demonstrate\nstrategy-dependent convergence behaviors with rates ranging from 0.33 to 1.29,\nand predictive accuracy achieving R2 = 0.74 for balanced approaches. This work\nproposes the feasibility of dynamical systems analysis for multi-objective LLM\ninteractions, with code generation serving as an initial validation domain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u6846\u67b6\u6765\u5efa\u6a21\u5927\u8bed\u8a00\u6a21\u578b\u591a\u76ee\u6807\u4f18\u5316\u52a8\u6001\uff0c\u901a\u8fc7\u6269\u6563\u9879\u6355\u6349LLM\u54cd\u5e94\u7684\u968f\u673a\u6027\uff0c\u5e76\u7528\u5e72\u6270\u77e9\u9635\u63ed\u793a\u76ee\u6807\u95f4\u7684\u7cfb\u7edf\u6027\u5e72\u6270\u6a21\u5f0f\u3002", "motivation": "\u9700\u8981\u5efa\u6a21LLM\u8fed\u4ee3\u4ea4\u4e92\u4e2d\u7684\u591a\u76ee\u6807\u4f18\u5316\u52a8\u6001\uff0c\u6355\u6349\u5176\u968f\u673a\u6027\u548c\u76ee\u6807\u95f4\u7684\u5e72\u6270\u6548\u5e94\u3002", "method": "\u4f7f\u7528\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u6846\u67b6\uff0c\u5305\u542b\u663e\u5f0f\u6269\u6563\u9879\u548c\u5e72\u6270\u77e9\u9635\uff0c\u4ee5\u4ee3\u7801\u751f\u6210\u4e3a\u9a8c\u8bc1\u5e94\u7528\uff0c\u5206\u6790400\u4e2a\u4f1a\u8bdd\u7684\u5b89\u5168\u3001\u6548\u7387\u548c\u529f\u80fd\u76ee\u6807\u3002", "result": "\u5c55\u793a\u4e86\u7b56\u7565\u4f9d\u8d56\u7684\u6536\u655b\u884c\u4e3a\uff08\u6536\u655b\u73870.33-1.29\uff09\uff0c\u5e73\u8861\u65b9\u6cd5\u7684\u9884\u6d4b\u51c6\u786e\u7387\u8fbe\u5230R\u00b2=0.74\u3002", "conclusion": "\u8bc1\u660e\u4e86\u52a8\u6001\u7cfb\u7edf\u5206\u6790\u5728\u591a\u76ee\u6807LLM\u4ea4\u4e92\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4ee3\u7801\u751f\u6210\u4e3a\u521d\u6b65\u9a8c\u8bc1\u9886\u57df\u3002"}}
{"id": "2510.11563", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11563", "abs": "https://arxiv.org/abs/2510.11563", "authors": ["Shreya Havaldar", "Sunny Rai", "Young-Min Cho", "Lyle Ungar"], "title": "Culturally-Aware Conversations: A Framework & Benchmark for LLMs", "comment": "To appear at the 4th HCI + NLP Workshop @ EMNLP", "summary": "Existing benchmarks that measure cultural adaptation in LLMs are misaligned\nwith the actual challenges these models face when interacting with users from\ndiverse cultural backgrounds. In this work, we introduce the first framework\nand benchmark designed to evaluate LLMs in realistic, multicultural\nconversational settings. Grounded in sociocultural theory, our framework\nformalizes how linguistic style - a key element of cultural communication - is\nshaped by situational, relational, and cultural context. We construct a\nbenchmark dataset based on this framework, annotated by culturally diverse\nraters, and propose a new set of desiderata for cross-cultural evaluation in\nNLP: conversational framing, stylistic sensitivity, and subjective correctness.\nWe evaluate today's top LLMs on our benchmark and show that these models\nstruggle with cultural adaptation in a conversational setting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u8bc4\u4f30LLMs\u5728\u591a\u6587\u5316\u5bf9\u8bdd\u573a\u666f\u4e2d\u6587\u5316\u9002\u5e94\u80fd\u529b\u7684\u6846\u67b6\u548c\u57fa\u51c6\uff0c\u57fa\u4e8e\u793e\u4f1a\u6587\u5316\u7406\u8bba\uff0c\u91cd\u70b9\u5173\u6ce8\u8bed\u8a00\u98ce\u683c\u5728\u6587\u5316\u6c9f\u901a\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u73b0\u6709\u8861\u91cfLLMs\u6587\u5316\u9002\u5e94\u80fd\u529b\u7684\u57fa\u51c6\u4e0e\u6a21\u578b\u5728\u5b9e\u9645\u591a\u6837\u5316\u6587\u5316\u80cc\u666f\u7528\u6237\u4ea4\u4e92\u4e2d\u9762\u4e34\u7684\u6311\u6218\u4e0d\u5339\u914d\u3002", "method": "\u57fa\u4e8e\u793e\u4f1a\u6587\u5316\u7406\u8bba\u6784\u5efa\u6846\u67b6\uff0c\u5f62\u5f0f\u5316\u8bed\u8a00\u98ce\u683c\u5982\u4f55\u53d7\u60c5\u5883\u3001\u5173\u7cfb\u548c\u6587\ufffd\ufffd\ufffd\u80cc\u666f\u5f71\u54cd\uff0c\u521b\u5efa\u7531\u591a\u6837\u5316\u6587\u5316\u8bc4\u5206\u8005\u6807\u6ce8\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u8de8\u6587\u5316\u8bc4\u4f30\u7684\u65b0\u6807\u51c6\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5f53\u524d\u9876\u7ea7LLMs\u5728\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u6587\u5316\u9002\u5e94\u80fd\u529b\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u9700\u8981\u66f4\u597d\u7684\u6846\u67b6\u548c\u57fa\u51c6\u6765\u8bc4\u4f30LLMs\u5728\u591a\u6587\u5316\u5bf9\u8bdd\u73af\u5883\u4e2d\u7684\u6587\u5316\u9002\u5e94\u80fd\u529b\uff0c\u73b0\u6709\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002"}}
{"id": "2510.10764", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10764", "abs": "https://arxiv.org/abs/2510.10764", "authors": ["Shaharyar Ahmed Khan Tareen", "Filza Khan Tareen"], "title": "Optimally Deep Networks -- Adapting Model Depth to Datasets for Superior Efficiency", "comment": "6 pages, 3 figures, 1 table", "summary": "Deep neural networks (DNNs) have provided brilliant performance across\nvarious tasks. However, this success often comes at the cost of unnecessarily\nlarge model sizes, high computational demands, and substantial memory\nfootprints. Typically, powerful architectures are trained at full depths but\nnot all datasets or tasks require such high model capacity. Training very deep\narchitectures on relatively low-complexity datasets frequently leads to wasted\ncomputation, unnecessary energy consumption, and excessive memory usage, which\nin turn makes deployment of models on resource-constrained devices impractical.\nTo address this problem, we introduce Optimally Deep Networks (ODNs), which\nprovide a balance between model depth and task complexity. Specifically, we\npropose a NAS like training strategy called progressive depth expansion, which\nbegins by training deep networks at shallower depths and incrementally\nincreases their depth as the earlier blocks converge, continuing this process\nuntil the target accuracy is reached. ODNs use only the optimal depth for the\ngiven datasets, removing redundant layers. This cuts down future training and\ninference costs, lowers the memory footprint, enhances computational\nefficiency, and facilitates deployment on edge devices. Empirical results show\nthat the optimal depths of ResNet-18 and ResNet-34 for MNIST and SVHN, achieve\nup to 98.64 % and 96.44 % reduction in memory footprint, while maintaining a\ncompetitive accuracy of 99.31 % and 96.08 %, respectively.", "AI": {"tldr": "\u63d0\u51faOptimally Deep Networks (ODNs)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e10\u8fdb\u6df1\u5ea6\u6269\u5c55\u8bad\u7ec3\u7b56\u7565\uff0c\u4e3a\u4e0d\u540c\u590d\u6742\u5ea6\u6570\u636e\u96c6\u81ea\u52a8\u9009\u62e9\u6700\u4f18\u7f51\u7edc\u6df1\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u76f8\u5bf9\u7b80\u5355\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u65f6\u5b58\u5728\u7684\u6a21\u578b\u8fc7\u5927\u3001\u8ba1\u7b97\u9700\u6c42\u9ad8\u3001\u5185\u5b58\u5360\u7528\u591a\u7684\u95ee\u9898\uff0c\u4f7f\u6a21\u578b\u66f4\u9002\u5408\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u7c7b\u4f3c\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u7684\u8bad\u7ec3\u7b56\u7565\u2014\u2014\u6e10\u8fdb\u6df1\u5ea6\u6269\u5c55\uff0c\u4ece\u8f83\u6d45\u6df1\u5ea6\u5f00\u59cb\u8bad\u7ec3\uff0c\u968f\u7740\u65e9\u671f\u6a21\u5757\u6536\u655b\u9010\u6b65\u589e\u52a0\u7f51\u7edc\u6df1\u5ea6\uff0c\u76f4\u5230\u8fbe\u5230\u76ee\u6807\u7cbe\u5ea6\uff0c\u4ec5\u4fdd\u7559\u6700\u4f18\u6df1\u5ea6\u3002", "result": "\u5728MNIST\u548cSVHN\u6570\u636e\u96c6\u4e0a\uff0cResNet-18\u548cResNet-34\u7684\u6700\u4f18\u6df1\u5ea6\u5206\u522b\u5b9e\u73b0\u4e8698.64%\u548c96.44%\u7684\u5185\u5b58\u5360\u7528\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u630199.31%\u548c96.08%\u7684\u7ade\u4e89\u6027\u51c6\u786e\u7387\u3002", "conclusion": "ODNs\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u6570\u636e\u96c6\u590d\u6742\u5ea6\u81ea\u52a8\u786e\u5b9a\u6700\u4f18\u7f51\u7edc\u6df1\u5ea6\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u4fbf\u4e8e\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002"}}
{"id": "2510.11584", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11584", "abs": "https://arxiv.org/abs/2510.11584", "authors": ["Ting Li", "Yang Yang", "Yipeng Yu", "Liang Yao", "Guoqing Chao", "Ruifeng Xu"], "title": "LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings", "comment": "13 pages", "summary": "Adversarial attacks on knowledge graph embeddings (KGE) aim to disrupt the\nmodel's ability of link prediction by removing or inserting triples. A recent\nblack-box method has attempted to incorporate textual and structural\ninformation to enhance attack performance. However, it is unable to generate\nhuman-readable explanations, and exhibits poor generalizability. In the past\nfew years, large language models (LLMs) have demonstrated powerful capabilities\nin text comprehension, generation, and reasoning. In this paper, we propose\nLLMAtKGE, a novel LLM-based framework that selects attack targets and generates\nhuman-readable explanations. To provide the LLM with sufficient factual context\nunder limited input constraints, we design a structured prompting scheme that\nexplicitly formulates the attack as multiple-choice questions while\nincorporating KG factual evidence. To address the context-window limitation and\nhesitation issues, we introduce semantics-based and centrality-based filters,\nwhich compress the candidate set while preserving high recall of\nattack-relevant information. Furthermore, to efficiently integrate both\nsemantic and structural information into the filter, we precompute high-order\nadjacency and fine-tune the LLM with a triple classification task to enhance\nfiltering performance. Experiments on two widely used knowledge graph datasets\ndemonstrate that our attack outperforms the strongest black-box baselines and\nprovides explanations via reasoning, and showing competitive performance\ncompared with white-box methods. Comprehensive ablation and case studies\nfurther validate its capability to generate explanations.", "AI": {"tldr": "LLMAtKGE\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u6297\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u548c\u591a\u9009\u62e9\u95ee\u9898\u8bbe\u8ba1\uff0c\u5728\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u653b\u51fb\u4e2d\u5b9e\u73b0\u76ee\u6807\u9009\u62e9\u548c\u53ef\u89e3\u91ca\u653b\u51fb\u3002", "motivation": "\u73b0\u6709\u7684\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\u65e0\u6cd5\u751f\u6210\u4eba\u7c7b\u53ef\u8bfb\u7684\u89e3\u91ca\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u7406\u89e3\u3001\u751f\u6210\u548c\u63a8\u7406\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u53ef\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u7ed3\u6784\u5316\u63d0\u793a\u65b9\u6848\u5c06\u653b\u51fb\u8868\u8ff0\u4e3a\u591a\u9009\u62e9\u95ee\u9898\uff0c\u5f15\u5165\u57fa\u4e8e\u8bed\u4e49\u548c\u4e2d\u5fc3\u6027\u7684\u8fc7\u6ee4\u5668\u538b\u7f29\u5019\u9009\u96c6\uff0c\u9884\u8ba1\u7b97\u9ad8\u9636\u90bb\u63a5\u77e9\u9635\u5e76\u5fae\u8c03LLM\u8fdb\u884c\u4e09\u5143\u7ec4\u5206\u7c7b\u4ee5\u63d0\u5347\u8fc7\u6ee4\u6027\u80fd\u3002", "result": "\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u77e5\u8bc6\u56fe\u8c31\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u6700\u5f3a\u7684\u9ed1\u76d2\u57fa\u7ebf\uff0c\u63d0\u4f9b\u4e86\u63a8\u7406\u89e3\u91ca\uff0c\u5e76\u4e0e\u767d\u76d2\u65b9\u6cd5\u76f8\u6bd4\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "LLMAtKGE\u6846\u67b6\u80fd\u6709\u6548\u751f\u6210\u53ef\u89e3\u91ca\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u5728\u653b\u51fb\u6027\u80fd\u548c\u89e3\u91ca\u80fd\u529b\u65b9\u9762\u5747\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.10775", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10775", "abs": "https://arxiv.org/abs/2510.10775", "authors": ["Amber Li", "Aruzhan Abil", "Juno Marques Oda"], "title": "Structure Over Signal: A Globalized Approach to Multi-relational GNNs for Stock Prediction", "comment": null, "summary": "In financial markets, Graph Neural Networks have been successfully applied to\nmodeling relational data, effectively capturing nonlinear inter-stock\ndependencies. Yet, existing models often fail to efficiently propagate messages\nduring macroeconomic shocks. In this paper, we propose OmniGNN, an\nattention-based multi-relational dynamic GNN that integrates macroeconomic\ncontext via heterogeneous node and edge types for robust message passing.\nCentral to OmniGNN is a sector node acting as a global intermediary, enabling\nrapid shock propagation across the graph without relying on long-range\nmulti-hop diffusion. The model leverages Graph Attention Networks (GAT) to\nweigh neighbor contributions and employs Transformers to capture temporal\ndynamics across multiplex relations. Experiments show that OmniGNN outperforms\nexisting stock prediction models on public datasets, particularly demonstrating\nstrong robustness during the COVID-19 period.", "AI": {"tldr": "\u63d0\u51faOmniGNN\uff0c\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u591a\u5173\u7cfb\u52a8\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5f15\u5165\u884c\u4e1a\u8282\u70b9\u4f5c\u4e3a\u5168\u5c40\u4e2d\u4ecb\u6765\u6539\u5584\u5b8f\u89c2\u7ecf\u6d4e\u51b2\u51fb\u4e0b\u7684\u6d88\u606f\u4f20\u64ad\u6548\u7387\uff0c\u5728\u80a1\u7968\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5b8f\u89c2\u7ecf\u6d4e\u51b2\u51fb\u671f\u95f4\u65e0\u6cd5\u6709\u6548\u4f20\u64ad\u6d88\u606f\uff0c\u9700\u8981\u6539\u8fdb\u6a21\u578b\u4ee5\u66f4\u597d\u5730\u6355\u6349\u975e\u7ebf\u6027\u80a1\u7968\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u7684\u591a\u5173\u7cfb\u52a8\u6001GNN\uff0c\u96c6\u6210\u5b8f\u89c2\u7ecf\u6d4e\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u884c\u4e1a\u8282\u70b9\u5b9e\u73b0\u5feb\u901f\u51b2\u51fb\u4f20\u64ad\uff0c\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548cTransformer\u6355\u6349\u65f6\u7a7a\u52a8\u6001\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u80a1\u7968\u9884\u6d4b\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728COVID-19\u671f\u95f4\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "OmniGNN\u901a\u8fc7\u521b\u65b0\u7684\u884c\u4e1a\u8282\u70b9\u8bbe\u8ba1\u548c\u591a\u5173\u7cfb\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5b8f\u89c2\u7ecf\u6d4e\u51b2\u51fb\u4e0b\u7684\u6d88\u606f\u4f20\u64ad\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u80a1\u7968\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2510.11598", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11598", "abs": "https://arxiv.org/abs/2510.11598", "authors": ["Bo Cheng", "Xu Wang", "Jinda Liu", "Yi Chang", "Yuan Wu"], "title": "MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language Models", "comment": null, "summary": "Low-Rank Adaptation (LoRA) has emerged as one of the most widely used\nparameter-efficient fine-tuning (PEFT) methods for adapting large language\nmodels (LLMs) to downstream tasks. While highly effective in single-task\nsettings, it struggles to efficiently leverage inter-task knowledge in complex\nmulti-task learning scenarios, often requiring substantial task-specific data\nto achieve optimal performance. To address this limitation, we introduce\nMeTA-LoRA, a two-stage optimization framework that significantly improves data\nefficiency in multi-task adaptation. In the first stage, task-specific LoRA\nadapters are learned using only a few samples from each involved dataset,\nenabling rapid adaptation without large-scale supervision. In the second stage,\nthe shared LoRA adapter is updated by aggregating gradients from multiple tasks\nto promote knowledge transfer across tasks, further reducing data usage by\nleveraging common patterns. In both multi-task learning and multilingual\nlearning scenarios, our method matches or surpasses the performance of\ntraditional full-data LoRA fine-tuning approaches, while using significantly\nless task-specific data.", "AI": {"tldr": "MeTA-LoRA\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u9002\u914d\u5668\uff0c\u7136\u540e\u805a\u5408\u591a\u4efb\u52a1\u68af\u5ea6\u6765\u63d0\u5347\u77e5\u8bc6\u8fc1\u79fb\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u4efb\u52a1\u9002\u5e94\u4e2d\u7684\u6570\u636e\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684LoRA\u65b9\u6cd5\u5728\u5355\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u96be\u4ee5\u9ad8\u6548\u5229\u7528\u4efb\u52a1\u95f4\u77e5\u8bc6\uff0c\u9700\u8981\u5927\u91cf\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u624d\u80fd\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5c11\u91cf\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u7279\u5b9aLoRA\u9002\u914d\u5668\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u805a\u5408\u591a\u4efb\u52a1\u68af\u5ea6\u66f4\u65b0\u5171\u4eabLoRA\u9002\u914d\u5668\uff0c\u4fc3\u8fdb\u8de8\u4efb\u52a1\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u591a\u8bed\u8a00\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u4f20\u7edf\u5168\u6570\u636eLoRA\u5fae\u8c03\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4f7f\u7528\u4e86\u663e\u8457\u66f4\u5c11\u7684\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u3002", "conclusion": "MeTA-LoRA\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u9002\u5e94\u4e2d\u7684\u6570\u636e\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u4f18\u5316\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u77e5\u8bc6\u8fc1\u79fb\u548c\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.11599", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11599", "abs": "https://arxiv.org/abs/2510.11599", "authors": ["Marc Brinner", "Sina Zarrie\u00df"], "title": "SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific and Interpretable Scientific Domain Mapping", "comment": null, "summary": "We propose SemCSE-Multi, a novel unsupervised framework for generating\nmultifaceted embeddings of scientific abstracts, evaluated in the domains of\ninvasion biology and medicine. These embeddings capture distinct, individually\nspecifiable aspects in isolation, thus enabling fine-grained and controllable\nsimilarity assessments as well as adaptive, user-driven visualizations of\nscientific domains. Our approach relies on an unsupervised procedure that\nproduces aspect-specific summarizing sentences and trains embedding models to\nmap semantically related summaries to nearby positions in the embedding space.\nWe then distill these aspect-specific embedding capabilities into a unified\nembedding model that directly predicts multiple aspect embeddings from a\nscientific abstract in a single, efficient forward pass. In addition, we\nintroduce an embedding decoding pipeline that decodes embeddings back into\nnatural language descriptions of their associated aspects. Notably, we show\nthat this decoding remains effective even for unoccupied regions in\nlow-dimensional visualizations, thus offering vastly improved interpretability\nin user-centric settings.", "AI": {"tldr": "\u63d0\u51faSemCSE-Multi\u6846\u67b6\uff0c\u4e3a\u79d1\u5b66\u6458\u8981\u751f\u6210\u591a\u65b9\u9762\u7684\u5d4c\u5165\u5411\u91cf\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u76f8\u4f3c\u5ea6\u8bc4\u4f30\u548c\u7528\u6237\u9a71\u52a8\u7684\u53ef\u89c6\u5316\u3002", "motivation": "\u73b0\u6709\u5d4c\u5165\u65b9\u6cd5\u901a\u5e38\u53ea\u80fd\u751f\u6210\u5355\u4e00\u8868\u793a\uff0c\u65e0\u6cd5\u6355\u6349\u79d1\u5b66\u6587\u732e\u7684\u4e0d\u540c\u65b9\u9762\uff0c\u9650\u5236\u4e86\u7ec6\u7c92\u5ea6\u5206\u6790\u548c\u53ef\u89c6\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u65e0\u76d1\u7763\u65b9\u6cd5\u751f\u6210\u65b9\u9762\u7279\u5b9a\u7684\u6458\u8981\u53e5\uff0c\u8bad\u7ec3\u5d4c\u5165\u6a21\u578b\u5c06\u8bed\u4e49\u76f8\u5173\u6458\u8981\u6620\u5c04\u5230\u5d4c\u5165\u7a7a\u95f4\u90bb\u8fd1\u4f4d\u7f6e\uff0c\u7136\u540e\u84b8\u998f\u5230\u7edf\u4e00\u6a21\u578b\u4e2d\u5b9e\u73b0\u5355\u6b21\u524d\u5411\u9884\u6d4b\u3002", "result": "\u5728\u5165\u4fb5\u751f\u7269\u5b66\u548c\u533b\u5b66\u9886\u57df\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0c\u80fd\u591f\u751f\u6210\u591a\u65b9\u9762\u5d4c\u5165\u5e76\u5b9e\u73b0\u5d4c\u5165\u5230\u81ea\u7136\u8bed\u8a00\u7684\u89e3\u7801\uff0c\u5373\u4f7f\u5728\u4f4e\u7ef4\u53ef\u89c6\u5316\u7684\u672a\u5360\u636e\u533a\u57df\u4e5f\u4fdd\u6301\u6709\u6548\u6027\u3002", "conclusion": "SemCSE-Multi\u63d0\u4f9b\u4e86\u53ef\u63a7\u7684\u76f8\u4f3c\u5ea6\u8bc4\u4f30\u548c\u81ea\u9002\u5e94\u53ef\u89c6\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u4e2d\u5fc3\u8bbe\u7f6e\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.10790", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10790", "abs": "https://arxiv.org/abs/2510.10790", "authors": ["Zhongju Yuan", "Geraint Wiggins", "Dick Botteldooren"], "title": "BioOSS: A Bio-Inspired Oscillatory State System with Spatio-Temporal Dynamics", "comment": null, "summary": "Today's deep learning architectures are primarily based on perceptron models,\nwhich do not capture the oscillatory dynamics characteristic of biological\nneurons. Although oscillatory systems have recently gained attention for their\ncloser resemblance to neural behavior, they still fall short of modeling the\nintricate spatio-temporal interactions observed in natural neural circuits. In\nthis paper, we propose a bio-inspired oscillatory state system (BioOSS)\ndesigned to emulate the wave-like propagation dynamics critical to neural\nprocessing, particularly in the prefrontal cortex (PFC), where complex activity\npatterns emerge. BioOSS comprises two interacting populations of neurons: p\nneurons, which represent simplified membrane-potential-like units inspired by\npyramidal cells in cortical columns, and o neurons, which govern propagation\nvelocities and modulate the lateral spread of activity. Through local\ninteractions, these neurons produce wave-like propagation patterns. The model\nincorporates trainable parameters for damping and propagation speed, enabling\nflexible adaptation to task-specific spatio-temporal structures. We evaluate\nBioOSS on both synthetic and real-world tasks, demonstrating superior\nperformance and enhanced interpretability compared to alternative\narchitectures.", "AI": {"tldr": "\u63d0\u51faBioOSS\u751f\u7269\u542f\u53d1\u7684\u632f\u8361\u72b6\u6001\u7cfb\u7edf\uff0c\u6a21\u62df\u524d\u989d\u53f6\u76ae\u5c42\u4e2d\u6ce2\u72b6\u4f20\u64ad\u52a8\u6001\uff0c\u5305\u542bp\u795e\u7ecf\u5143\u548co\u795e\u7ecf\u5143\u4e24\u4e2a\u76f8\u4e92\u4f5c\u7528\u7fa4\u4f53\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u57fa\u4e8e\u611f\u77e5\u5668\u6a21\u578b\uff0c\u65e0\u6cd5\u6355\u6349\u751f\u7269\u795e\u7ecf\u5143\u7684\u632f\u8361\u52a8\u6001\u7279\u6027\uff0c\u800c\u73b0\u6709\u632f\u8361\u7cfb\u7edf\u4ecd\u65e0\u6cd5\u5145\u5206\u6a21\u62df\u81ea\u7136\u795e\u7ecf\u56de\u8def\u4e2d\u590d\u6742\u7684\u65f6\u7a7a\u76f8\u4e92\u4f5c\u7528\u3002", "method": "BioOSS\u7cfb\u7edf\u5305\u542bp\u795e\u7ecf\u5143\uff08\u6a21\u62df\u9525\u4f53\u7ec6\u80de\u819c\u7535\u4f4d\uff09\u548co\u795e\u7ecf\u5143\uff08\u63a7\u5236\u4f20\u64ad\u901f\u5ea6\uff09\uff0c\u901a\u8fc7\u5c40\u90e8\u76f8\u4e92\u4f5c\u7528\u4ea7\u751f\u6ce2\u72b6\u4f20\u64ad\u6a21\u5f0f\uff0c\u5305\u542b\u53ef\u8bad\u7ec3\u963b\u5c3c\u548c\u4f20\u64ad\u901f\u5ea6\u53c2\u6570\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4efb\u52a1\u4e2d\u8bc4\u4f30BioOSS\uff0c\u76f8\u6bd4\u5176\u4ed6\u67b6\u6784\u5c55\u73b0\u51fa\u66f4\u4f18\u6027\u80fd\u548c\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "BioOSS\u6210\u529f\u6a21\u62df\u4e86\u795e\u7ecf\u5904\u7406\u4e2d\u7684\u6ce2\u72b6\u4f20\u64ad\u52a8\u6001\uff0c\u4e3a\u751f\u7269\u542f\u53d1\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.11602", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11602", "abs": "https://arxiv.org/abs/2510.11602", "authors": ["Huiyin Xue", "Nafise Sadat Moosavi", "Nikolaos Aletras"], "title": "Deconstructing Attention: Investigating Design Principles for Effective Language Modeling", "comment": null, "summary": "The success of Transformer language models is widely credited to their\ndot-product attention mechanism, which interweaves a set of key design\nprinciples: mixing information across positions (enabling multi-token\ninteractions), sequence-dependent activations (where attention weights adapt to\neach input), a specific mathematical form (dot-product similarities plus\nsoftmax weighting), and coupling of queries and keys to evolving hidden states\n(grounding attention in the current layer). However, the necessity of each of\nthese principles remains largely untested. In this work, we systematically\ndeconstruct attention by designing controlled variants that selectively relax\nthese principles, applied both uniformly across all layers and in hybrid\narchitectures where only some layers retain standard attention. Our empirical\nanalysis reveals that mechanisms for mixing tokens are indispensable, as their\nabsence collapses models to near-random behavior, while the exact mathematical\nform and sequence dependency can be substantially relaxed, especially when\npreserved in just a subset of layers. Surprisingly, even variants that fail in\nisolation can achieve robust performance when interleaved with standard\nattention, highlighting a cooperative effect. These findings deepen our\nunderstanding of what truly underpins attention's effectiveness and open new\navenues for simplifying language models without sacrificing performance.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u89e3\u6784\u4e86Transformer\u6ce8\u610f\u529b\u673a\u5236\uff0c\u53d1\u73b0token\u6df7\u5408\u673a\u5236\u4e0d\u53ef\u6216\u7f3a\uff0c\u800c\u6570\u5b66\u5f62\u5f0f\u548c\u5e8f\u5217\u4f9d\u8d56\u6027\u53ef\u4ee5\u5927\u5e45\u7b80\u5316\uff0c\u7279\u522b\u662f\u5728\u90e8\u5206\u5c42\u4fdd\u7559\u6807\u51c6\u6ce8\u610f\u529b\u65f6\u3002", "motivation": "Transformer\u8bed\u8a00\u6a21\u578b\u7684\u6210\u529f\u901a\u5e38\u5f52\u529f\u4e8e\u5176\u70b9\u79ef\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f46\u8be5\u673a\u5236\u4e2d\u5404\u4e2a\u8bbe\u8ba1\u539f\u5219\u7684\u5fc5\u8981\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u6d4b\u8bd5\u3002", "method": "\u8bbe\u8ba1\u53d7\u63a7\u53d8\u4f53\uff0c\u9009\u62e9\u6027\u5730\u653e\u677e\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e0d\u540c\u539f\u5219\uff0c\u5305\u62ec\u5728\u5168\u90e8\u5c42\u7edf\u4e00\u5e94\u7528\u548c\u5728\u6df7\u5408\u67b6\u6784\u4e2d\u4ec5\u90e8\u5206\u5c42\u4fdd\u7559\u6807\u51c6\u6ce8\u610f\u529b\u3002", "result": "token\u6df7\u5408\u673a\u5236\u662f\u5fc5\u9700\u7684\uff0c\u5176\u7f3a\u5931\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u63a5\u8fd1\u968f\u673a\uff1b\u800c\u6570\u5b66\u5f62\u5f0f\u548c\u5e8f\u5217\u4f9d\u8d56\u6027\u53ef\u4ee5\u5927\u5e45\u653e\u677e\uff0c\u7279\u522b\u662f\u5728\u90e8\u5206\u5c42\u4fdd\u7559\u6807\u51c6\u6ce8\u610f\u529b\u65f6\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u5355\u72ec\u5931\u8d25\u7684\u53d8\u4f53\u5728\u4e0e\u6807\u51c6\u6ce8\u610f\u529b\u4ea4\u9519\u65f6\u4e5f\u80fd\u83b7\u5f97\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u6df1\u5316\u4e86\u5bf9\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u6027\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u7b80\u5316\u8bed\u8a00\u6a21\u578b\u800c\u4e0d\u727a\u7272\u6027\u80fd\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.10799", "categories": ["cs.LG", "physics.ao-ph", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2510.10799", "abs": "https://arxiv.org/abs/2510.10799", "authors": ["Wanshu Nie", "Sujay V. Kumar", "Junyu Chen", "Long Zhao", "Olya Skulovich", "Jinwoong Yoo", "Justin Pflug", "Shahryar Khalique Ahmad", "Goutam Konapala"], "title": "Rethinking deep learning: linear regression remains a key benchmark in predicting terrestrial water storage", "comment": null, "summary": "Recent advances in machine learning such as Long Short-Term Memory (LSTM)\nmodels and Transformers have been widely adopted in hydrological applications,\ndemonstrating impressive performance amongst deep learning models and\noutperforming physical models in various tasks. However, their superiority in\npredicting land surface states such as terrestrial water storage (TWS) that are\ndominated by many factors such as natural variability and human driven\nmodifications remains unclear. Here, using the open-access, globally\nrepresentative HydroGlobe dataset - comprising a baseline version derived\nsolely from a land surface model simulation and an advanced version\nincorporating multi-source remote sensing data assimilation - we show that\nlinear regression is a robust benchmark, outperforming the more complex LSTM\nand Temporal Fusion Transformer for TWS prediction. Our findings highlight the\nimportance of including traditional statistical models as benchmarks when\ndeveloping and evaluating deep learning models. Additionally, we emphasize the\ncritical need to establish globally representative benchmark datasets that\ncapture the combined impact of natural variability and human interventions.", "AI": {"tldr": "\u7ebf\u6027\u56de\u5f52\u5728\u9646\u5730\u6c34\u50a8\u91cf\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u590d\u6742\u7684LSTM\u548cTransformer\u6a21\u578b\uff0c\u5f3a\u8c03\u9700\u8981\u5c06\u4f20\u7edf\u7edf\u8ba1\u6a21\u578b\u4f5c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u57fa\u51c6\u3002", "motivation": "\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u53d7\u81ea\u7136\u53d8\u5f02\u548c\u4eba\u7c7b\u6d3b\u52a8\u5f71\u54cd\u7684\u9646\u5730\u6c34\u50a8\u91cf\u65b9\u9762\u7684\u6027\u80fd\uff0c\u76ee\u524d\u5176\u4f18\u52bf\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u7528HydroGlobe\u6570\u636e\u96c6\uff08\u5305\u542b\u4ec5\u6765\u81ea\u9646\u9762\u6a21\u578b\u6a21\u62df\u7684\u57fa\u51c6\u7248\u672c\u548c\u878d\u5408\u591a\u6e90\u9065\u611f\u6570\u636e\u540c\u5316\u7684\u9ad8\u7ea7\u7248\u672c\uff09\uff0c\u6bd4\u8f83\u7ebf\u6027\u56de\u5f52\u3001LSTM\u548c\u65f6\u5e8f\u878d\u5408Transformer\u5728TWS\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7ebf\u6027\u56de\u5f52\u662f\u4e00\u4e2a\u7a33\u5065\u7684\u57fa\u51c6\uff0c\u5728TWS\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u66f4\u590d\u6742\u7684LSTM\u548c\u65f6\u5e8f\u878d\u5408Transformer\u6a21\u578b\u3002", "conclusion": "\u5728\u5f00\u53d1\u548c\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u65f6\uff0c\u9700\u8981\u5305\u542b\u4f20\u7edf\u7edf\u8ba1\u6a21\u578b\u4f5c\u4e3a\u57fa\u51c6\uff0c\u5e76\u5efa\u7acb\u80fd\u591f\u6355\u6349\u81ea\u7136\u53d8\u5f02\u548c\u4eba\u7c7b\u5e72\u9884\u7efc\u5408\u5f71\u54cd\u7684\u5168\u7403\u4ee3\u8868\u6027\u57fa\u51c6\u6570\u636e\u96c6\u3002"}}
{"id": "2510.11615", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11615", "abs": "https://arxiv.org/abs/2510.11615", "authors": ["Xurong Xie", "Zhucun Xue", "Jiafu Wu", "Jian Li", "Yabiao Wang", "Xiaobin Hu", "Yong Liu", "Jiangning Zhang"], "title": "LLM-Oriented Token-Adaptive Knowledge Distillation", "comment": "15 pages, 4 figures", "summary": "Knowledge distillation (KD) is a key technique for compressing large-scale\nlanguage models (LLMs), yet prevailing logit-based methods typically employ\nstatic strategies that are misaligned with the dynamic learning process of\nstudent models. These methods typically treat all tokens indiscriminately and\napply a single, fixed temperature, resulting in suboptimal knowledge transfer.\nTo address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge\nDistillation (AdaKD), a novel framework that adapts the distillation process to\nthe real-time learning state of each token. AdaKD consists of two synergistic\nmodules driven by a unified token difficulty metric. First, our Loss-Driven\nAdaptive Token Focusing (LATF) module dynamically adjusts the distillation\nfocus by monitoring the student's learning stability, concentrating\ncomputational resources on the most valuable tokens at each training phase.\nSecond, we introduce Inverse Difficulty Temperature Scaling (IDTS), a\ncounterintuitive yet effective token-level temperature strategy. It employs low\ntemperatures for difficult tokens for targeted error correction, and high\ntemperatures for easy tokens to encourage students to learn from the teacher's\ncomplete and smooth output distribution, thereby enhancing generalization. As a\nplug-and-play framework, AdaKD can consistently improve the performance of\nvarious distillation methods on multiple model architectures and benchmarks.", "AI": {"tldr": "\u63d0\u51faAdaKD\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u84b8\u998f\u8fc7\u7a0b\u6765\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u5305\u542b\u635f\u5931\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u4ee4\u724c\u805a\u7126\u548c\u53cd\u96be\u5ea6\u6e29\u5ea6\u7f29\u653e\u4e24\u4e2a\u6a21\u5757\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8elogit\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u91c7\u7528\u9759\u6001\u7b56\u7565\uff0c\u4e0e\u5b66\u751f\u5b66\u4e60\u8fc7\u7a0b\u7684\u52a8\u6001\u7279\u6027\u4e0d\u5339\u914d\uff0c\u5bf9\u6240\u6709\u4ee4\u724c\u91c7\u7528\u5355\u4e00\u56fa\u5b9a\u6e29\u5ea6\uff0c\u5bfc\u81f4\u77e5\u8bc6\u8f6c\u79fb\u6548\u679c\u4e0d\u4f73\u3002", "method": "AdaKD\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u534f\u540c\u6a21\u5757\uff1aLATF\u6a21\u5757\u901a\u8fc7\u76d1\u63a7\u5b66\u751f\u5b66\u4e60\u7a33\u5b9a\u6027\u52a8\u6001\u8c03\u6574\u84b8\u998f\u7126\u70b9\uff1bIDTS\u6a21\u5757\u91c7\u7528\u53cd\u96be\u5ea6\u6e29\u5ea6\u7f29\u653e\u7b56\u7565\uff0c\u5bf9\u56f0\u96be\u4ee4\u724c\u4f7f\u7528\u4f4e\u6e29\u8fdb\u884c\u9488\u5bf9\u6027\u7ea0\u9519\uff0c\u5bf9\u7b80\u5355\u4ee4\u724c\u4f7f\u7528\u9ad8\u6e29\u9f13\u52b1\u5b66\u4e60\u5b8c\u6574\u5e73\u6ed1\u7684\u8f93\u51fa\u5206\u5e03\u3002", "result": "AdaKD\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u80fd\u591f\u5728\u591a\u79cd\u6a21\u578b\u67b6\u6784\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u63d0\u5347\u5404\u79cd\u84b8\u998f\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "AdaKD\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u6bcf\u4e2a\u4ee4\u724c\u7684\u5b9e\u65f6\u5b66\u4e60\u72b6\u6001\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u8f6c\u79fb\u6548\u679c\u3002"}}
{"id": "2510.10803", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10803", "abs": "https://arxiv.org/abs/2510.10803", "authors": ["Javier Garc\u00eda-Sig\u00fcenza", "Mirco Nanni", "Fara\u00f3n Llorens-Largo", "Jos\u00e9 F. Vicent"], "title": "PruneGCRN: Minimizing and explaining spatio-temporal problems through node pruning", "comment": null, "summary": "This work addresses the challenge of using a deep learning model to prune\ngraphs and the ability of this method to integrate explainability into\nspatio-temporal problems through a new approach. Instead of applying\nexplainability to the model's behavior, we seek to gain a better understanding\nof the problem itself. To this end, we propose a novel model that integrates an\noptimized pruning mechanism capable of removing nodes from the graph during the\ntraining process, rather than doing so as a separate procedure. This\nintegration allows the architecture to learn how to minimize prediction error\nwhile selecting the most relevant nodes. Thus, during training, the model\nsearches for the most relevant subset of nodes, obtaining the most important\nelements of the problem, facilitating its analysis. To evaluate the proposed\napproach, we used several widely used traffic datasets, comparing the accuracy\nobtained by pruning with the model and with other methods. The experiments\ndemonstrate that our method is capable of retaining a greater amount of\ninformation as the graph reduces in size compared to the other methods used.\nThese results highlight the potential of pruning as a tool for developing\nmodels capable of simplifying spatio-temporal problems, thereby obtaining their\nmost important elements.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u96c6\u6210\u4f18\u5316\u526a\u679d\u673a\u5236\u7684\u65b0\u6a21\u578b\uff0c\u80fd\u591f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u79fb\u9664\u56fe\u8282\u70b9\uff0c\u4ece\u800c\u9009\u62e9\u6700\u76f8\u5173\u8282\u70b9\u5e76\u6700\u5c0f\u5316\u9884\u6d4b\u8bef\u5dee\uff0c\u7528\u4e8e\u7b80\u5316\u65f6\u7a7a\u95ee\u9898\u5e76\u83b7\u53d6\u5173\u952e\u5143\u7d20\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u56fe\u526a\u679d\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5c06\u53ef\u89e3\u91ca\u6027\u878d\u5165\u65f6\u7a7a\u95ee\u9898\u6765\u66f4\u597d\u5730\u7406\u89e3\u95ee\u9898\u672c\u8eab\uff0c\u800c\u4e0d\u662f\u4ec5\u4ec5\u89e3\u91ca\u6a21\u578b\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u6a21\u578b\uff0c\u96c6\u6210\u4f18\u5316\u7684\u526a\u679d\u673a\u5236\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u79fb\u9664\u56fe\u8282\u70b9\u800c\u975e\u5355\u72ec\u5904\u7406\uff0c\u4f7f\u67b6\u6784\u80fd\u591f\u5b66\u4e60\u5982\u4f55\u6700\u5c0f\u5316\u9884\u6d4b\u8bef\u5dee\u540c\u65f6\u9009\u62e9\u6700\u76f8\u5173\u8282\u70b9\u3002", "result": "\u5728\u591a\u4e2a\u4ea4\u901a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u5f62\u5c3a\u5bf8\u51cf\u5c0f\u65f6\u6bd4\u5176\u4ed6\u65b9\u6cd5\u80fd\u4fdd\u7559\u66f4\u591a\u4fe1\u606f\uff0c\u8bc1\u660e\u4e86\u526a\u679d\u4f5c\u4e3a\u7b80\u5316\u65f6\u7a7a\u95ee\u9898\u5de5\u5177\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u526a\u679d\u4f5c\u4e3a\u5f00\u53d1\u80fd\u591f\u7b80\u5316\u65f6\u7a7a\u95ee\u9898\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u83b7\u53d6\u95ee\u9898\u7684\u6700\u91cd\u8981\u5143\u7d20\uff0c\u4e3a\u65f6\u7a7a\u95ee\u9898\u7684\u5206\u6790\u548c\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.11618", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.11618", "abs": "https://arxiv.org/abs/2510.11618", "authors": ["Zehao Chen", "Rong Pan", "Haoran Li"], "title": "StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up Long-Form Story Generation Using Large Language Models", "comment": "Project: https://storyboxproject.github.io", "summary": "Human writers often begin their stories with an overarching mental scene,\nwhere they envision the interactions between characters and their environment.\nInspired by this creative process, we propose a novel approach to long-form\nstory generation, termed hybrid bottom-up long-form story generation, using\nmulti-agent simulations. In our method, agents interact within a dynamic\nsandbox environment, where their behaviors and interactions with one another\nand the environment generate emergent events. These events form the foundation\nfor the story, enabling organic character development and plot progression.\nUnlike traditional top-down approaches that impose rigid structures, our hybrid\nbottom-up approach allows for the natural unfolding of events, fostering more\nspontaneous and engaging storytelling. The system is capable of generating\nstories exceeding 10,000 words while maintaining coherence and consistency,\naddressing some of the key challenges faced by current story generation models.\nWe achieve state-of-the-art performance across several metrics. This approach\noffers a scalable and innovative solution for creating dynamic, immersive\nlong-form stories that evolve organically from agent-driven interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6a21\u62df\u7684\u6df7\u5408\u81ea\u5e95\u5411\u4e0a\u957f\u7bc7\u5c0f\u8bf4\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u5728\u52a8\u6001\u6c99\u76d2\u73af\u5883\u4e2d\u7684\u4e92\u52a8\u4ea7\u751f\u6d8c\u73b0\u4e8b\u4ef6\uff0c\u5f62\u6210\u6545\u4e8b\u57fa\u7840\uff0c\u80fd\u591f\u751f\u6210\u8d85\u8fc710,000\u5b57\u7684\u8fde\u8d2f\u957f\u7bc7\u5c0f\u8bf4\u3002", "motivation": "\u53d7\u4eba\u7c7b\u4f5c\u5bb6\u521b\u4f5c\u8fc7\u7a0b\u7684\u542f\u53d1\uff0c\u4f20\u7edf\u81ea\u4e0a\u800c\u4e0b\u7684\u65b9\u6cd5\u5f3a\u52a0\u521a\u6027\u7ed3\u6784\uff0c\u800c\u672c\u65b9\u6cd5\u65e8\u5728\u5b9e\u73b0\u66f4\u81ea\u7136\u3001\u81ea\u53d1\u7684\u6545\u4e8b\u53d1\u5c55\uff0c\u89e3\u51b3\u5f53\u524d\u6545\u4e8b\u751f\u6210\u6a21\u578b\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u6a21\u62df\uff0c\u8ba9\u667a\u80fd\u4f53\u5728\u52a8\u6001\u6c99\u76d2\u73af\u5883\u4e2d\u4e92\u52a8\uff0c\u901a\u8fc7\u884c\u4e3a\u548c\u73af\u5883\u4ea4\u4e92\u4ea7\u751f\u6d8c\u73b0\u4e8b\u4ef6\uff0c\u5f62\u6210\u6545\u4e8b\u7684\u57fa\u7840\uff0c\u5b9e\u73b0\u6709\u673a\u7684\u89d2\u8272\u53d1\u5c55\u548c\u60c5\u8282\u63a8\u8fdb\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u8d85\u8fc710,000\u5b57\u7684\u957f\u7bc7\u5c0f\u8bf4\uff0c\u540c\u65f6\u4fdd\u6301\u8fde\u8d2f\u6027\u548c\u4e00\u81f4\u6027\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u521b\u5efa\u52a8\u6001\u3001\u6c89\u6d78\u5f0f\u7684\u957f\u7bc7\u5c0f\u8bf4\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u6545\u4e8b\u4ece\u667a\u80fd\u4f53\u9a71\u52a8\u7684\u4e92\u52a8\u4e2d\u6709\u673a\u6f14\u5316\u800c\u6765\u3002"}}
{"id": "2510.11620", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11620", "abs": "https://arxiv.org/abs/2510.11620", "authors": ["Siheng Xiong", "Ali Payani", "Faramarz Fekri"], "title": "Enhancing Long Chain-of-Thought Reasoning through Multi-Path Plan Aggregation", "comment": null, "summary": "Inference-time scaling enhances the reasoning ability of a language model\n(LM) by extending its chain-of-thought (CoT). However, existing approaches\ntypically generate the entire reasoning chain in a single forward pass, which\noften leads to CoT derailment, i.e., the reasoning trajectory drifting off\ncourse due to compounding errors. This problem is particularly severe for\nsmaller LMs with long CoTs due to their limited capacity. To address this, we\nanalyze raw long CoTs and uncover a reasoning hierarchy consisting of planning\nand execution steps. Our analysis reveals that most reasoning errors stem from\nincorrect planning. Motivated by this observation, we propose Multi-Path Plan\nAggregation (MPPA), a framework that augments single-pass reasoning with plan\nexploration and aggregation. Following a variable interval schedule based on\nthe token position, MPPA generates multiple candidate plans and aggregates them\ninto a refined planning step. To maintain efficiency, we adopt a minimal design\nin which the base LM serves as the primary policy, while a lightweight LoRA\nmodule implements the plan aggregation policy. We further observe that\noutcome-reward RL is inefficient for long trajectories (e.g., exceeding 4K\ntokens). To overcome this, we introduce online Step-DPO, a process-level\npreference optimization scheme that leverages Twisted Sequential Monte Carlo\n(TSMC) to provide scalable stepwise supervision using small LMs. This yields\nmore efficient training, improved stability, and higher accuracy. Extensive\nexperiments on challenging math, science, and logical reasoning benchmarks\ndemonstrate that, with only 10% SFT data and 5% of preference pairs, our method\noutperforms both the DeepSeek-R1 distillation baseline and the outcome-reward\nRL baseline across multiple base models and tasks.", "AI": {"tldr": "\u63d0\u51faMPPA\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8def\u5f84\u8ba1\u5212\u805a\u5408\u548c\u5728\u7ebfStep-DPO\u65b9\u6cd5\u89e3\u51b3\u957f\u63a8\u7406\u94fe\u4e2d\u7684\u89c4\u5212\u9519\u8bef\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5c0f\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u65f6\u6269\u5c55\u65b9\u6cd5\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u751f\u6210\u6574\u4e2a\u63a8\u7406\u94fe\uff0c\u5bb9\u6613\u56e0\u7d2f\u79ef\u9519\u8bef\u5bfc\u81f4\u63a8\u7406\u8f68\u8ff9\u504f\u79bb\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u80fd\u529b\u6709\u9650\u7684\u5c0f\u6a21\u578b\u548c\u957f\u63a8\u7406\u94fe\u3002\u7814\u7a76\u53d1\u73b0\u5927\u591a\u6570\u63a8\u7406\u9519\u8bef\u6e90\u4e8e\u9519\u8bef\u7684\u89c4\u5212\u6b65\u9aa4\u3002", "method": "\u63d0\u51fa\u591a\u8def\u5f84\u8ba1\u5212\u805a\u5408(MPPA)\u6846\u67b6\uff0c\u57fa\u4e8etoken\u4f4d\u7f6e\u7684\u53d8\u95f4\u9694\u8c03\u5ea6\u751f\u6210\u591a\u4e2a\u5019\u9009\u8ba1\u5212\u5e76\u805a\u5408\u4e3a\u7cbe\u70bc\u89c4\u5212\u6b65\u9aa4\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7LoRA\u6a21\u5757\u5b9e\u73b0\u8ba1\u5212\u805a\u5408\u7b56\u7565\u3002\u5f15\u5165\u5728\u7ebfStep-DPO\uff0c\u5229\u7528TSMC\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u9010\u6b65\u76d1\u7763\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u3002", "result": "\u5728\u6570\u5b66\u3001\u79d1\u5b66\u548c\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u4f7f\u752810%\u7684SFT\u6570\u636e\u548c5%\u7684\u504f\u597d\u5bf9\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8eDeepSeek-R1\u84b8\u998f\u57fa\u7ebf\u548c\u7ed3\u679c\u5956\u52b1RL\u57fa\u7ebf\u3002", "conclusion": "MPPA\u6846\u67b6\u901a\u8fc7\u8ba1\u5212\u63a2\u7d22\u548c\u805a\u5408\u6709\u6548\u89e3\u51b3\u4e86\u957f\u63a8\u7406\u94fe\u4e2d\u7684\u89c4\u5212\u9519\u8bef\u95ee\u9898\uff0c\u7ed3\u5408Step-DPO\u5b9e\u73b0\u4e86\u9ad8\u6548\u8bad\u7ec3\u548c\u66f4\u9ad8\u51c6\u786e\u6027\uff0c\u4e3a\u5c0f\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10810", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10810", "abs": "https://arxiv.org/abs/2510.10810", "authors": ["Omar Islam Laskar", "Fatemeh Ramezani Khozestani", "Ishika Nankani", "Sohrab Namazi Nia", "Senjuti Basu Roy", "Kaustubh Beedkar"], "title": "Aegis: A Correlation-Based Data Masking Advisor for Data Sharing Ecosystems", "comment": "Accepted at SIGMOD 2026", "summary": "Data-sharing ecosystems enable entities -- such as providers, consumers, and\nintermediaries -- to access, exchange, and utilize data for various downstream\ntasks and applications. Due to privacy concerns, data providers typically\nanonymize datasets before sharing them; however, the existence of multiple\nmasking configurations results in masked datasets with varying utility.\nConsequently, a key challenge lies in efficiently determining the optimal\nmasking configuration that maximizes a dataset's utility. This paper presents\nAEGIS, a middleware framework for identifying the optimal masking configuration\nfor machine learning datasets that consist of features and a class label. We\nintroduce a utility optimizer that minimizes predictive utility deviation -- a\nmetric based on the changes in feature-label correlations before and after\nmasking. Our framework leverages limited data summaries (such as 1D histograms)\nor none to estimate the feature-label joint distribution, making it suitable\nfor scenarios where raw data is inaccessible due to privacy restrictions. To\nachieve this, we propose a joint distribution estimator based on iterative\nproportional fitting, which allows supporting various feature-label correlation\nquantification methods such as g3, mutual information, or chi-square. Our\nexperimental evaluation on real-world datasets shows that AEGIS identifies\noptimal masking configurations over an order of magnitude faster, while the\nresulting masked datasets achieve predictive performance on downstream ML tasks\nthat is on par with baseline approaches.", "AI": {"tldr": "AEGIS\u662f\u4e00\u4e2a\u4e2d\u95f4\u4ef6\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u5305\u542b\u7279\u5f81\u548c\u7c7b\u522b\u6807\u7b7e\u7684\u673a\u5668\u5b66\u4e60\u6570\u636e\u96c6\u8bc6\u522b\u6700\u4f18\u7684\u63a9\u7801\u914d\u7f6e\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u9884\u6d4b\u6548\u7528\u504f\u5dee\u6765\u4f18\u5316\u6570\u636e\u6548\u7528\u3002", "motivation": "\u6570\u636e\u5171\u4eab\u751f\u6001\u7cfb\u7edf\u4e2d\uff0c\u7531\u4e8e\u9690\u79c1\u95ee\u9898\uff0c\u6570\u636e\u63d0\u4f9b\u8005\u901a\u5e38\u4f1a\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u533f\u540d\u5316\u5904\u7406\uff0c\u4f46\u4e0d\u540c\u7684\u63a9\u7801\u914d\u7f6e\u4f1a\u5bfc\u81f4\u6570\u636e\u6548\u7528\u5dee\u5f02\u3002\u5173\u952e\u6311\u6218\u5728\u4e8e\u9ad8\u6548\u786e\u5b9a\u80fd\u6700\u5927\u5316\u6570\u636e\u96c6\u6548\u7528\u7684\u6700\u4f18\u63a9\u7801\u914d\u7f6e\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8fed\u4ee3\u6bd4\u4f8b\u62df\u5408\u7684\u8054\u5408\u5206\u5e03\u4f30\u8ba1\u5668\uff0c\u5229\u7528\u6709\u9650\u7684\u6570\u636e\u6458\u8981\uff08\u59821D\u76f4\u65b9\u56fe\uff09\u6216\u65e0\u6458\u8981\u6765\u4f30\u8ba1\u7279\u5f81-\u6807\u7b7e\u8054\u5408\u5206\u5e03\uff0c\u652f\u6301\u591a\u79cd\u7279\u5f81-\u6807\u7b7e\u76f8\u5173\u6027\u91cf\u5316\u65b9\u6cd5\uff08g3\u3001\u4e92\u4fe1\u606f\u3001\u5361\u65b9\u68c0\u9a8c\uff09\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0cAEGIS\u8bc6\u522b\u6700\u4f18\u63a9\u7801\u914d\u7f6e\u7684\u901f\u5ea6\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u751f\u6210\u7684\u63a9\u7801\u6570\u636e\u96c6\u5728\u4e0b\u6e38\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "AEGIS\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u8bc6\u522b\u6700\u4f18\u63a9\u7801\u914d\u7f6e\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u6570\u636e\u6548\u7528\uff0c\u9002\u7528\u4e8e\u539f\u59cb\u6570\u636e\u56e0\u9690\u79c1\u9650\u5236\u800c\u65e0\u6cd5\u8bbf\u95ee\u7684\u573a\u666f\u3002"}}
{"id": "2510.11652", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11652", "abs": "https://arxiv.org/abs/2510.11652", "authors": ["Xin Gui", "King Zhu", "JinCheng Ren", "Qianben Chen", "Zekun Moore Wang", "Yizhi LI", "Xinpeng Liu", "Xiaowan Li", "Wenli Ren", "Linyu Miao", "Tianrui Qin", "Ziqi Shu", "He Zhu", "Xiangru Tang", "Dingfeng Shi", "Jiaheng Liu", "Yuchen Eleanor Jiang", "Minghao Liu", "Ge Zhang", "Wangchunshu Zhou"], "title": "ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems", "comment": null, "summary": "In recent years, the research focus of large language models (LLMs) and\nagents has shifted increasingly from demonstrating novel capabilities to\ncomplex reasoning and tackling challenging tasks. However, existing evaluations\nfocus mainly on math/code contests or general tasks, while existing\nmulti-domain academic benchmarks lack sufficient reasoning depth, leaving the\nfield without a rigorous benchmark for high-level reasoning. To fill this gap,\nwe introduce the Acadreason benchmark, designed to evaluate the ability of LLMs\nand agents to acquire and reason over academic knowledge. It consists of 50\nexpert-annotated academic problems across five high-reasoning domains,\nincluding computer science, economics, law, mathematics, and philosophy. All\nquestions are sourced from top-tier publications in recent years and undergo\nrigorous annotation and quality control to ensure they are both challenging and\nanswerable. We conduct systematic evaluations of over 10 mainstream LLMs and\nagents. The results show that most LLMs scored below 20 points, with even the\ncutting-edge GPT-5 achieving only 16 points. While agents achieved higher\nscores, none exceeded 40 points. This demonstrates the current capability gap\nbetween LLMs and agents in super-intelligent academic research tasks and\nhighlights the challenges of Acadreason.", "AI": {"tldr": "\u63d0\u51fa\u4e86Acadreason\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u548c\u667a\u80fd\u4f53\u5728\u5b66\u672f\u77e5\u8bc6\u83b7\u53d6\u4e0e\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5305\u542b5\u4e2a\u9ad8\u63a8\u7406\u9886\u57df\u768450\u4e2a\u4e13\u5bb6\u6807\u6ce8\u95ee\u9898\u3002\u4e3b\u6d41\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u9ad8\u5206\u4e0d\u8d85\u8fc740\u5206\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u6570\u5b66/\u7f16\u7a0b\u7ade\u8d5b\u6216\u4e00\u822c\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u9ad8\u6c34\u5e73\u5b66\u672f\u63a8\u7406\u80fd\u529b\u7684\u4e25\u683c\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u6784\u5efa\u5305\u542b\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u7ecf\u6d4e\u5b66\u3001\u6cd5\u5b66\u3001\u6570\u5b66\u548c\u54f2\u5b665\u4e2a\u9886\u57df\u768450\u4e2a\u4e13\u5bb6\u6807\u6ce8\u5b66\u672f\u95ee\u9898\uff0c\u6240\u6709\u95ee\u9898\u6765\u81ea\u9876\u7ea7\u671f\u520a\u5e76\u7ecf\u8fc7\u4e25\u683c\u8d28\u91cf\u63a7\u5236\u3002", "result": "\u8bc4\u4f3010\u591a\u4e2a\u4e3b\u6d41LLM\u548c\u667a\u80fd\u4f53\uff0c\u5927\u591a\u6570LLM\u5f97\u5206\u4f4e\u4e8e20\u5206\uff0c\u6700\u5148\u8fdb\u7684GPT-5\u4ec5\u5f9716\u5206\u3002\u667a\u80fd\u4f53\u8868\u73b0\u66f4\u597d\u4f46\u65e0\u4eba\u8d85\u8fc740\u5206\u3002", "conclusion": "\u5f53\u524dLLM\u548c\u667a\u80fd\u4f53\u5728\u8d85\u7ea7\u667a\u80fd\u5b66\u672f\u7814\u7a76\u4efb\u52a1\u4e2d\u5b58\u5728\u660e\u663e\u80fd\u529b\u5dee\u8ddd\uff0cAcadreason\u57fa\u51c6\u51f8\u663e\u4e86\u8fd9\u4e00\u6311\u6218\u3002"}}
{"id": "2510.10849", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10849", "abs": "https://arxiv.org/abs/2510.10849", "authors": ["Donald Loveland", "Yao-An Yang", "Danai Koutra"], "title": "Glance for Context: Learning When to Leverage LLMs for Node-Aware GNN-LLM Fusion", "comment": null, "summary": "Learning on text-attributed graphs has motivated the use of Large Language\nModels (LLMs) for graph learning. However, most fusion strategies are applied\nuniformly across all nodes and attain only small overall performance gains. We\nargue this result stems from aggregate metrics that obscure when LLMs provide\nbenefit, inhibiting actionable signals for new strategies. In this work, we\nreframe LLM-GNN fusion around nodes where GNNs typically falter. We first show\nthat performance can significantly differ between GNNs and LLMs, with each\nexcelling on distinct structural patterns, such as local homophily. To leverage\nthis finding, we propose GLANCE (GNN with LLM Assistance for Neighbor- and\nContext-aware Embeddings), a framework that invokes an LLM to refine a GNN's\nprediction. GLANCE employs a lightweight router that, given inexpensive\nper-node signals, decides whether to query the LLM. Since the LLM calls are\nnon-differentiable, the router is trained with an advantage-based objective\nthat compares the utility of querying the LLM against relying solely on the\nGNN. Across multiple benchmarks, GLANCE achieves the best performance balance\nacross node subgroups, achieving significant gains on heterophilous nodes (up\nto $+13\\%$) while simultaneously achieving top overall performance. Our\nfindings highlight the value of adaptive, node-aware GNN-LLM architectures,\nwhere selectively invoking the LLM enables scalable deployment on large graphs\nwithout incurring high computational costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86GLANCE\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u9009\u62e9\u6027\u8c03\u7528LLM\u6765\u4f18\u5316GNN\u9884\u6d4b\uff0c\u5728\u5f02\u6784\u8282\u70b9\u4e0a\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u6574\u4f53\u6027\u80fd\u6700\u4f73\u3002", "motivation": "\u73b0\u6709LLM-GNN\u878d\u5408\u7b56\u7565\u5bf9\u6240\u6709\u8282\u70b9\u7edf\u4e00\u5e94\u7528\uff0c\u6027\u80fd\u63d0\u5347\u6709\u9650\u3002\u7814\u7a76\u53d1\u73b0GNN\u548cLLM\u5728\u4e0d\u540c\u7ed3\u6784\u6a21\u5f0f\u4e0a\u8868\u73b0\u5404\u5f02\uff0c\u9700\u8981\u9488\u5bf9GNN\u6613\u51fa\u9519\u8282\u70b9\u8fdb\u884c\u9009\u62e9\u6027\u878d\u5408\u3002", "method": "GLANCE\u6846\u67b6\u5305\u542b\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\uff0c\u57fa\u4e8e\u4f4e\u6210\u672c\u8282\u70b9\u4fe1\u53f7\u51b3\u5b9a\u662f\u5426\u67e5\u8be2LLM\u3002\u8def\u7531\u5668\u901a\u8fc7\u4f18\u52bf\u76ee\u6807\u8bad\u7ec3\uff0c\u6bd4\u8f83\u8c03\u7528LLM\u4e0e\u4ec5\u4f9d\u8d56GNN\u7684\u6548\u7528\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGLANCE\u5728\u8282\u70b9\u5b50\u7ec4\u95f4\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u5e73\u8861\uff0c\u5f02\u6784\u8282\u70b9\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe13%\uff0c\u540c\u65f6\u4fdd\u6301\u6574\u4f53\u6027\u80fd\u9886\u5148\u3002", "conclusion": "\u81ea\u9002\u5e94\u3001\u8282\u70b9\u611f\u77e5\u7684GNN-LLM\u67b6\u6784\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u9009\u62e9\u6027\u8c03\u7528LLM\u53ef\u5728\u4e0d\u589e\u52a0\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5927\u89c4\u6a21\u56fe\u4e0a\u7684\u53ef\u6269\u5c55\u90e8\u7f72\u3002"}}
{"id": "2510.11693", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11693", "abs": "https://arxiv.org/abs/2510.11693", "authors": ["Chenghao Xiao", "Hou Pong Chan", "Hao Zhang", "Weiwen Xu", "Mahani Aljunied", "Yu Rong"], "title": "Scaling Language-Centric Omnimodal Representation Learning", "comment": "NeurIPS 2025", "summary": "Recent multimodal embedding approaches leveraging multimodal large language\nmodels (MLLMs) fine-tuned with contrastive learning (CL) have shown promising\nresults, yet the underlying reasons behind their superiority remain\nunderexplored. This work argues that a crucial advantage of MLLM-based\napproaches stems from implicit cross-modal alignment achieved during generative\npretraining, where the language decoder learns to exploit multimodal signals\nwithin a shared representation space for generating unimodal outputs. Through\nanalysis of anisotropy and kernel similarity structure, we empirically confirm\nthat latent alignment emerges within MLLM representations, allowing CL to serve\nas a lightweight refinement stage. Leveraging this insight, we propose a\nLanguage-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive\nexperiments across diverse backbones and benchmarks demonstrate its\neffectiveness, achieving state-of-the-art performance across modalities.\nFurthermore, we identify a Generation-Representation Scaling Law (GRSL),\nshowing that the representational capabilities gained through contrastive\nrefinement scales positively with the MLLM's generative capabilities. This\nsuggests that improving generative abilities evolves as an effective paradigm\nfor enhancing representation quality. We provide a theoretical explanation of\nGRSL, which formally links the MLLM's generative quality to the upper bound on\nits representation performance, and validate it on a challenging, low-resource\nvisual-document retrieval task, showing that continual generative pretraining\nbefore CL can further enhance the potential of a model's embedding\ncapabilities. Codes, models, and resources are available at\nhttps://github.com/LCO-Embedding/LCO-Embedding.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLM)\u7684\u5d4c\u5165\u65b9\u6cd5\u4e4b\u6240\u4ee5\u4f18\u8d8a\uff0c\u662f\u56e0\u4e3a\u5728\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u4e86\u9690\u5f0f\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u4f7f\u5f97\u5bf9\u6bd4\u5b66\u4e60\u53ea\u9700\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u7cbe\u70bc\u9636\u6bb5\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u8bed\u8a00\u4e2d\u5fc3\u7684\u5168\u6a21\u6001\u5d4c\u5165\u6846\u67b6LCO-Emb\uff0c\u5e76\u53d1\u73b0\u4e86\u751f\u6210-\u8868\u793a\u7f29\u653e\u5b9a\u5f8b\u3002", "motivation": "\u63a2\u7d22\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u539f\u56e0\uff0c\u7814\u7a76\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u4e2d\u5f62\u6210\u7684\u9690\u5f0f\u8de8\u6a21\u6001\u5bf9\u9f50\u5982\u4f55\u63d0\u5347\u8868\u793a\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5404\u5411\u5f02\u6027\u548c\u6838\u76f8\u4f3c\u6027\u7ed3\u6784\u9a8c\u8bc1\u9690\u5f0f\u5bf9\u9f50\u7684\u5b58\u5728\uff0c\u63d0\u51fa\u8bed\u8a00\u4e2d\u5fc3\u7684\u5168\u6a21\u6001\u5d4c\u5165\u6846\u67b6LCO-Emb\uff0c\u5e76\u5efa\u7acb\u751f\u6210-\u8868\u793a\u7f29\u653e\u5b9a\u5f8b\u7684\u7406\u8bba\u89e3\u91ca\u3002", "result": "\u5728\u591a\u6837\u5316\u9aa8\u5e72\u7f51\u7edc\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u751f\u6210-\u8868\u793a\u7f29\u653e\u5b9a\u5f8b\uff0c\u8868\u660e\u751f\u6210\u80fd\u529b\u4e0e\u8868\u793a\u6027\u80fd\u5448\u6b63\u76f8\u5173\u3002", "conclusion": "\u63d0\u5347\u751f\u6210\u80fd\u529b\u662f\u589e\u5f3a\u8868\u793a\u8d28\u91cf\u7684\u6709\u6548\u8303\u5f0f\uff0c\u901a\u8fc7\u6301\u7eed\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u7684\u5d4c\u5165\u6f5c\u529b\u3002"}}
{"id": "2510.11695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11695", "abs": "https://arxiv.org/abs/2510.11695", "authors": ["Lingfei Qian", "Xueqing Peng", "Yan Wang", "Vincent Jim Zhang", "Huan He", "Hanley Smith", "Yi Han", "Yueru He", "Haohang Li", "Yupeng Cao", "Yangyang Yu", "Alejandro Lopez-Lira", "Peng Lu", "Jian-Yun Nie", "Guojun Xiong", "Jimin Huang", "Sophia Ananiadou"], "title": "When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents", "comment": null, "summary": "Although Large Language Model (LLM)-based agents are increasingly used in\nfinancial trading, it remains unclear whether they can reason and adapt in live\nmarkets, as most studies test models instead of agents, cover limited periods\nand assets, and rely on unverified data. To address these gaps, we introduce\nAgent Market Arena (AMA), the first lifelong, real-time benchmark for\nevaluating LLM-based trading agents across multiple markets. AMA integrates\nverified trading data, expert-checked news, and diverse agent architectures\nwithin a unified trading framework, enabling fair and continuous comparison\nunder real conditions. It implements four agents, including InvestorAgent as a\nsingle-agent baseline, TradeAgent and HedgeFundAgent with different risk\nstyles, and DeepFundAgent with memory-based reasoning, and evaluates them\nacross GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and\nGemini-2.0-flash. Live experiments on both cryptocurrency and stock markets\ndemonstrate that agent frameworks display markedly distinct behavioral\npatterns, spanning from aggressive risk-taking to conservative decision-making,\nwhereas model backbones contribute less to outcome variation. AMA thus\nestablishes a foundation for rigorous, reproducible, and continuously evolving\nevaluation of financial reasoning and trading intelligence in LLM-based agents.", "AI": {"tldr": "AMA\u662f\u9996\u4e2a\u7ec8\u8eab\u5b9e\u65f6\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u4ea4\u6613\u4ee3\u7406\u5728\u591a\u4e2a\u5e02\u573a\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u6d4b\u8bd5\u6a21\u578b\u800c\u975e\u4ee3\u7406\u3001\u8986\u76d6\u671f\u6709\u9650\u3001\u4f9d\u8d56\u672a\u9a8c\u8bc1\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u91d1\u878d\u4ea4\u6613\u4ee3\u7406\u7814\u7a76\u5b58\u5728\u6d4b\u8bd5\u5bf9\u8c61\u9519\u8bef\uff08\u6d4b\u8bd5\u6a21\u578b\u800c\u975e\u4ee3\u7406\uff09\u3001\u8986\u76d6\u671f\u6709\u9650\u3001\u6570\u636e\u672a\u9a8c\u8bc1\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u4e25\u8c28\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f15\u5165Agent Market Arena (AMA)\u6846\u67b6\uff0c\u6574\u5408\u9a8c\u8bc1\u4ea4\u6613\u6570\u636e\u3001\u4e13\u5bb6\u68c0\u67e5\u65b0\u95fb\u548c\u591a\u6837\u5316\u4ee3\u7406\u67b6\u6784\uff0c\u5b9e\u73b0\u56db\u79cd\u4ee3\u7406\uff08InvestorAgent\u3001TradeAgent\u3001HedgeFundAgent\u3001DeepFundAgent\uff09\u5728\u591a\u4e2aLLM\u6a21\u578b\u4e0a\u7684\u5b9e\u65f6\u8bc4\u4f30\u3002", "result": "\u5b9e\u76d8\u5b9e\u9a8c\u663e\u793a\u4ee3\u7406\u6846\u67b6\u8868\u73b0\u51fa\u660e\u663e\u4e0d\u540c\u7684\u884c\u4e3a\u6a21\u5f0f\uff08\u4ece\u6fc0\u8fdb\u98ce\u9669\u627f\u62c5\u5230\u4fdd\u5b88\u51b3\u7b56\uff09\uff0c\u800c\u6a21\u578b\u9aa8\u5e72\u5bf9\u7ed3\u679c\u53d8\u5f02\u8d21\u732e\u8f83\u5c0f\u3002", "conclusion": "AMA\u4e3a\u57fa\u4e8eLLM\u7684\u91d1\u878d\u63a8\u7406\u548c\u4ea4\u6613\u667a\u80fd\u7684\u4e25\u8c28\u3001\u53ef\u590d\u73b0\u548c\u6301\u7eed\u6f14\u5316\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.10862", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10862", "abs": "https://arxiv.org/abs/2510.10862", "authors": ["Samuel Yuan", "Divyanshu Saxena", "Jiayi Chen", "Nihal Sharma", "Aditya Akella"], "title": "A Joint Learning Approach to Hardware Caching and Prefetching", "comment": "Accepted at ML for Systems Workshop at the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)", "summary": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u786c\u4ef6\u7f13\u5b58\u4e2d\u7684\u66ff\u6362\u7b56\u7565\u548c\u9884\u53d6\u7b56\u7565\uff0c\u901a\u8fc7\u5f00\u53d1\u5171\u4eab\u7279\u5f81\u8868\u793a\u6765\u89e3\u51b3\u5355\u72ec\u8bad\u7ec3\u7b56\u7565\u65f6\u7684\u6b21\u4f18\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5b66\u4e60\u7b56\u7565\u5728\u5355\u72ec\u8bad\u7ec3\u65f6\uff0c\u5373\u4f7f\u5404\u81ea\u8868\u73b0\u826f\u597d\uff0c\u7ec4\u5408\u4f7f\u7528\u65f6\u4ecd\u53ef\u80fd\u8fbe\u5230\u6b21\u4f18\u6027\u80fd\u3002\u7f13\u5b58\u66ff\u6362\u548c\u9884\u53d6\u7b56\u7565\u4e4b\u95f4\u5b58\u5728\u53cc\u5411\u4f9d\u8d56\u5173\u7cfb\uff0c\u9700\u8981\u8054\u5408\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5171\u4eab\u7279\u5f81\u8868\u793a\u7684\u8054\u5408\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u4e24\u79cd\u5177\u4f53\u65b9\u6cd5\uff1a\u57fa\u4e8e\u8054\u5408\u7f16\u7801\u5668\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u5d4c\u5165\u65b9\u6cd5\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u8054\u5408\u5b66\u4e60\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8bba\u6587\u4e3a\u786c\u4ef6\u7f13\u5b58\u7b56\u7565\u7684\u8054\u5408\u5b66\u4e60\u65b9\u5411\u5236\u5b9a\u4e86\u672a\u6765\u7814\u7a76\u8bae\u7a0b\uff0c\u5f3a\u8c03\u4e86\u5171\u4eab\u8868\u793a\u5728\u89e3\u51b3\u7b56\u7565\u95f4\u76f8\u4e92\u4f9d\u8d56\u95ee\u9898\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.11701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11701", "abs": "https://arxiv.org/abs/2510.11701", "authors": ["Zhaochen Yu", "Ling Yang", "Jiaru Zou", "Shuicheng Yan", "Mengdi Wang"], "title": "Demystifying Reinforcement Learning in Agentic Reasoning", "comment": "Code and models: https://github.com/Gen-Verse/Open-AgentRL", "summary": "Recently, the emergence of agentic RL has showcased that RL could also\neffectively improve the agentic reasoning ability of LLMs, yet the key design\nprinciples and optimal practices remain unclear. In this work, we conduct a\ncomprehensive and systematic investigation to demystify reinforcement learning\nin agentic reasoning from three key perspectives: data, algorithm, and\nreasoning mode. We highlight our key insights: (i) Replacing stitched synthetic\ntrajectories with real end-to-end tool-use trajectories yields a far stronger\nSFT initialization; high-diversity, model-aware datasets sustain exploration\nand markedly improve RL performance. (ii) Exploration-friendly techniques are\ncrucial for agentic RL, such as clip higher, overlong reward shaping, and\nmaintaining adequate policy entropy could improve the training efficiency.\n(iii) A deliberative strategy with fewer tool calls outperforms frequent tool\ncalls or verbose self-reasoning, improving tool efficiency and final accuracy.\nTogether, these simple practices consistently enhance agentic reasoning and\ntraining efficiency, achieving strong results on challenging benchmarks with\nsmaller models, and establishing a practical baseline for future agentic RL\nresearch. Beyond these empirical insights, we further contribute a\nhigh-quality, real end-to-end agentic SFT dataset along with a high-quality RL\ndataset, and demonstrate the effectiveness of our insights in boosting the\nagentic reasoning ability of LLMs across four challenging benchmarks, including\nAIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,\n4B-sized models could also achieve superior agentic reasoning performance\ncompared to 32B-sized models. Code and models:\nhttps://github.com/Gen-Verse/Open-AgentRL", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u667a\u80fd\u4f53\u63a8\u7406\u4e2d\u7684\u5173\u952e\u8bbe\u8ba1\u539f\u5219\uff0c\u53d1\u73b0\u771f\u5b9e\u7aef\u5230\u7aef\u5de5\u5177\u4f7f\u7528\u8f68\u8ff9\u3001\u63a2\u7d22\u53cb\u597d\u6280\u672f\u3001\u4ee5\u53ca\u6df1\u601d\u719f\u8651\u7684\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u867d\u7136\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5df2\u5c55\u793a\u51fa\u63d0\u5347LLMs\u63a8\u7406\u80fd\u529b\u7684\u6f5c\u529b\uff0c\u4f46\u5176\u5173\u952e\u8bbe\u8ba1\u539f\u5219\u548c\u6700\u4f73\u5b9e\u8df5\u4ecd\u4e0d\u660e\u786e\uff0c\u9700\u8981\u8fdb\u884c\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u4ece\u6570\u636e\u3001\u7b97\u6cd5\u548c\u63a8\u7406\u6a21\u5f0f\u4e09\u4e2a\u5173\u952e\u89d2\u5ea6\u8fdb\u884c\u7efc\u5408\u7cfb\u7edf\u8c03\u67e5\uff0c\u5305\u62ec\u4f7f\u7528\u771f\u5b9e\u7aef\u5230\u7aef\u5de5\u5177\u4f7f\u7528\u8f68\u8ff9\u3001\u63a2\u7d22\u53cb\u597d\u6280\u672f\uff08\u5982clip higher\u3001\u5956\u52b1\u5851\u5f62\u3001\u4fdd\u6301\u7b56\u7565\u71b5\uff09\u4ee5\u53ca\u6df1\u601d\u719f\u8651\u7684\u7b56\u7565\u3002", "result": "\u8fd9\u4e9b\u7b80\u5355\u5b9e\u8df5\u80fd\u6301\u7eed\u589e\u5f3a\u667a\u80fd\u4f53\u63a8\u7406\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u5f3a\u52b2\u7ed3\u679c\uff0c4B\u89c4\u6a21\u6a21\u578b\u4e5f\u80fd\u8fbe\u5230\u4f18\u4e8e32B\u89c4\u6a21\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u5efa\u7acb\u4e86\u5b9e\u7528\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u9002\u5f53\u8bbe\u8ba1\uff0c\u5c0f\u578b\u6a21\u578b\u4e5f\u80fd\u5b9e\u73b0\u5353\u8d8a\u7684\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.10864", "categories": ["cs.LG", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.10864", "abs": "https://arxiv.org/abs/2510.10864", "authors": ["Shuaicheng Zhang", "Haohui Wang", "Junhong Lin", "Xiaojie Guo", "Yada Zhu", "Si Zhang", "Dongqi Fu", "Dawei Zhou"], "title": "HeroFilter: Adaptive Spectral Graph Filter for Varying Heterophilic Relations", "comment": null, "summary": "Graph heterophily, where connected nodes have different labels, has attracted\nsignificant interest recently. Most existing works adopt a simplified approach\n- using low-pass filters for homophilic graphs and high-pass filters for\nheterophilic graphs. However, we discover that the relationship between graph\nheterophily and spectral filters is more complex - the optimal filter response\nvaries across frequency components and does not follow a strict monotonic\ncorrelation with heterophily degree. This finding challenges conventional fixed\nfilter designs and suggests the need for adaptive filtering to preserve\nexpressiveness in graph embeddings. Formally, natural questions arise: Given a\nheterophilic graph G, how and to what extent will the varying heterophily\ndegree of G affect the performance of GNNs? How can we design adaptive filters\nto fit those varying heterophilic connections? Our theoretical analysis reveals\nthat the average frequency response of GNNs and graph heterophily degree do not\nfollow a strict monotonic correlation, necessitating adaptive graph filters to\nguarantee good generalization performance. Hence, we propose [METHOD NAME], a\nsimple yet powerful GNN, which extracts information across the heterophily\nspectrum and combines salient representations through adaptive mixing. [METHOD\nNAME]'s superior performance achieves up to 9.2% accuracy improvement over\nleading baselines across homophilic and heterophilic graphs.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u5173\u4e8e\u5f02\u914d\u6027\u56fe\u7684\u4f20\u7edf\u89c2\u70b9\uff0c\u53d1\u73b0\u9891\u8c31\u6ee4\u6ce2\u5668\u4e0e\u56fe\u5f02\u914d\u6027\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\u6bd4\u9884\u60f3\u7684\u66f4\u590d\u6742\uff0c\u9700\u8981\u81ea\u9002\u5e94\u6ee4\u6ce2\u5668\u6765\u4fdd\u6301\u56fe\u5d4c\u5165\u7684\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u540c\u914d\u6027\u56fe\u4f7f\u7528\u4f4e\u901a\u6ee4\u6ce2\u5668\u3001\u5bf9\u5f02\u914d\u6027\u56fe\u4f7f\u7528\u9ad8\u901a\u6ee4\u6ce2\u5668\u7684\u7b80\u5316\u65b9\u6cd5\u5b58\u5728\u95ee\u9898\uff0c\u4f5c\u8005\u53d1\u73b0\u56fe\u5f02\u914d\u6027\u4e0e\u9891\u8c31\u6ee4\u6ce2\u5668\u4e4b\u95f4\u7684\u5173\u7cfb\u66f4\u4e3a\u590d\u6742\uff0c\u9700\u8981\u81ea\u9002\u5e94\u65b9\u6cd5\u6765\u9002\u5e94\u53d8\u5316\u7684\u5f02\u914d\u6027\u8fde\u63a5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u5f3a\u5927\u7684GNN\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u5f02\u914d\u6027\u9891\u8c31\u4e0a\u63d0\u53d6\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u6df7\u5408\u7ed3\u5408\u663e\u8457\u8868\u793a\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u540c\u7c7b\u548c\u5f02\u914d\u6027\u56fe\u4e0a\u76f8\u6bd4\u9886\u5148\u57fa\u7ebf\u5b9e\u73b0\u4e86\u9ad8\u8fbe9.2%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u56fe\u5f02\u914d\u6027\u5ea6\u4e0eGNNs\u7684\u5e73\u5747\u9891\u7387\u54cd\u5e94\u4e4b\u95f4\u4e0d\u5b58\u5728\u4e25\u683c\u7684\u5355\u8c03\u76f8\u5173\u6027\uff0c\u9700\u8981\u81ea\u9002\u5e94\u56fe\u6ee4\u6ce2\u5668\u6765\u4fdd\u8bc1\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2510.11713", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11713", "abs": "https://arxiv.org/abs/2510.11713", "authors": ["Tsung-Han Wu", "Mihran Miroyan", "David M. Chan", "Trevor Darrell", "Narges Norouzi", "Joseph E. Gonzalez"], "title": "Are Large Reasoning Models Interruptible?", "comment": "Project Page: https://dynamic-lm.github.io/", "summary": "Large Reasoning Models (LRMs) excel at complex reasoning but are\ntraditionally evaluated in static, \"frozen world\" settings: model responses are\nassumed to be instantaneous, and the context of a request is presumed to be\nimmutable over the duration of the response. While generally true for\nshort-term tasks, the \"frozen world\" assumption breaks down in modern reasoning\ntasks such as assistive programming, where models may take hours to think\nthrough problems and code may change dramatically from the time the model\nstarts thinking to the model's final output. In this work, we challenge the\nfrozen world assumption and evaluate LRM robustness under two realistic dynamic\nscenarios: interruptions, which test the quality of the model's partial outputs\non a limited budget, and dynamic context, which tests model adaptation to\nin-flight changes. Across mathematics and programming benchmarks that require\nlong-form reasoning, static evaluations consistently overestimate robustness:\neven state-of-the-art LRMs, which achieve high accuracy in static settings, can\nfail unpredictably when interrupted or exposed to changing context, with\nperformance dropping by up to 60% when updates are introduced late in the\nreasoning process. Our analysis further reveals several novel failure modes,\nincluding reasoning leakage, where models fold the reasoning into their final\nanswer when interrupted; panic, where under time pressure models abandon\nreasoning entirely and return incorrect answers; and self-doubt, where\nperformance degrades while incorporating updated information.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6311\u6218\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u9759\u6001\u73af\u5883\u4e0b\u7684\u8bc4\u4f30\u5047\u8bbe\uff0c\u63d0\u51fa\u4e86\u52a8\u6001\u573a\u666f\u4e0b\u7684\u4e2d\u65ad\u548c\u4e0a\u4e0b\u6587\u53d8\u5316\u6d4b\u8bd5\uff0c\u53d1\u73b0\u9759\u6001\u8bc4\u4f30\u9ad8\u4f30\u4e86\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u6027\u80fd\u4e0b\u964d\u53ef\u8fbe60%\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u5047\u8bbe\u6a21\u578b\u54cd\u5e94\u662f\u77ac\u65f6\u4e14\u4e0a\u4e0b\u6587\u4e0d\u53d8\u7684\uff0c\u4f46\u5728\u73b0\u4ee3\u63a8\u7406\u4efb\u52a1\u4e2d\u8fd9\u79cd\"\u51bb\u7ed3\u4e16\u754c\"\u5047\u8bbe\u4e0d\u6210\u7acb\uff0c\u56e0\u4e3a\u6a21\u578b\u53ef\u80fd\u9700\u8981\u6570\u5c0f\u65f6\u601d\u8003\u4e14\u4e0a\u4e0b\u6587\u4f1a\u52a8\u6001\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u4e24\u79cd\u73b0\u5b9e\u52a8\u6001\u573a\u666f\u8bc4\u4f30LRM\u9c81\u68d2\u6027\uff1a\u4e2d\u65ad\u6d4b\u8bd5\u6a21\u578b\u5728\u6709\u9650\u9884\u7b97\u4e0b\u7684\u90e8\u5206\u8f93\u51fa\u8d28\u91cf\uff0c\u52a8\u6001\u4e0a\u4e0b\u6587\u6d4b\u8bd5\u6a21\u578b\u5bf9\u98de\u884c\u4e2d\u53d8\u5316\u7684\u9002\u5e94\u80fd\u529b\u3002", "result": "\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u9759\u6001\u8bc4\u4f30\u4e00\u81f4\u9ad8\u4f30\u9c81\u68d2\u6027\uff1a\u5373\u4f7f\u5728\u9759\u6001\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u9ad8\u7cbe\u5ea6\u7684\u6700\u5148\u8fdbLRM\uff0c\u5728\u4e2d\u65ad\u6216\u4e0a\u4e0b\u6587\u53d8\u5316\u65f6\u4e5f\u4f1a\u4e0d\u53ef\u9884\u6d4b\u5730\u5931\u8d25\u3002", "conclusion": "\u63ed\u793a\u4e86\u65b0\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5305\u62ec\u63a8\u7406\u6cc4\u6f0f\u3001\u6050\u614c\u548c\u81ea\u6211\u6000\u7591\uff0c\u8868\u660e\u9700\u8981\u5f00\u53d1\u5728\u52a8\u6001\u73af\u5883\u4e2d\u66f4\u9c81\u68d2\u7684\u63a8\u7406\u6a21\u578b\u3002"}}
{"id": "2510.10915", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10915", "abs": "https://arxiv.org/abs/2510.10915", "authors": ["Hanchang Cheng", "Weimin Mu", "Fan Liu", "Weilin Zhu", "Can Ma"], "title": "LPCVAE: A Conditional VAE with Long-Term Dependency and Probabilistic Time-Frequency Fusion for Time Series Anomaly Detection", "comment": null, "summary": "Time series anomaly detection(TSAD) is a critical task in signal processing\nfield, ensuring the reliability of complex systems. Reconstruction-based\nmethods dominate in TSAD. Among these methods, VAE-based methods have achieved\npromising results. Existing VAE-based methods suffer from the limitation of\nsingle-window feature and insufficient leveraging of long-term time and\nfrequency information. We propose a Conditional Variational AutoEncoder with\nLong-term dependency and Probabilistic time-frequency fusion, named LPCVAE.\nLPCVAE introduces LSTM to capture long-term dependencies beyond windows. It\nfurther incorporates a Product-of-Experts (PoE) mechanism for adaptive and\ndistribution-level probabilistic fusion. This design effectively mitigates\ntime-frequency information loss. Extensive experiments on four public datasets\ndemonstrate it outperforms state-of-the-art methods. The results confirm that\nintegrating long-term time and frequency representations with adaptive fusion\nyields a robust and efficient solution for TSAD.", "AI": {"tldr": "\u63d0\u51faLPCVAE\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165LSTM\u6355\u83b7\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u4f7f\u7528\u4e13\u5bb6\u4e58\u79ef\u673a\u5236\u8fdb\u884c\u6982\u7387\u6027\u65f6\u9891\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfVAE\u65b9\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5355\u7a97\u53e3\u7279\u5f81\u548c\u65f6\u9891\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eVAE\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u5355\u7a97\u53e3\u7279\u5f81\u9650\u5236\u548c\u957f\u671f\u65f6\u9891\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51faLPCVAE\u6a21\u578b\uff1a1\uff09\u5f15\u5165LSTM\u6355\u83b7\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff1b2\uff09\u4f7f\u7528\u4e13\u5bb6\u4e58\u79ef\u673a\u5236\u8fdb\u884c\u81ea\u9002\u5e94\u3001\u5206\u5e03\u7ea7\u7684\u6982\u7387\u6027\u65f6\u9891\u878d\u5408\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLPCVAE\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u957f\u671f\u65f6\u9891\u8868\u793a\u4e0e\u81ea\u9002\u5e94\u878d\u5408\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u96c6\u6210\u957f\u671f\u65f6\u95f4\u548c\u9891\u7387\u8868\u793a\u4e0e\u81ea\u9002\u5e94\u878d\u5408\u4e3a\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10925", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10925", "abs": "https://arxiv.org/abs/2510.10925", "authors": ["Hengyuan Zhang", "Shiping Yang", "Xiao Liang", "Chenming Shang", "Yuxuan Jiang", "Chaofan Tao", "Jing Xiong", "Hayden Kwok-Hay So", "Ruobing Xie", "Angel X. Chang", "Ngai Wong"], "title": "Find Your Optimal Teacher: Personalized Data Synthesis via Router-Guided Multi-Teacher Distillation", "comment": "19 pages, 10 figures", "summary": "Training student models on synthetic data generated by strong teacher models\nis a promising way to distilling the capabilities of teachers. However, recent\nstudies show that stronger models are not always optimal teachers, revealing a\nmismatch between teacher outputs and student learnability. To address this\nissue, we propose PerSyn (Personalized data Synthesis), a novel synthesis\nstrategy that operates under a new ``Route then Generate'' paradigm to create\ndata tailored to each student model, enabling it to learn more effectively.\nSpecifically, PerSyn first assigns each prompt to its optimal teacher via a\nquery-level router that jointly considers student learnability and teacher\nresponse quality. Each teacher then synthesizes data only for its assigned\nprompts, making the process more efficient than the conventional ``Generate\nthen Select'' paradigm, where all teachers must generate parallel responses for\nthe entire prompt set before constructing the final dataset. Extensive\nexperiments across different model families and scales demonstrate that PerSyn\nconsistently achieves superior or comparable performance to all baselines in\ninstruct tuning and math reasoning settings. Further analysis verifies the\neffectiveness of PerSyn and offers extra insights to propel future research.", "AI": {"tldr": "\u63d0\u51faPerSyn\u65b9\u6cd5\uff0c\u901a\u8fc7\"\u8def\u7531\u540e\u751f\u6210\"\u8303\u5f0f\u4e3a\u6bcf\u4e2a\u5b66\u751f\u6a21\u578b\u4e2a\u6027\u5316\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u89e3\u51b3\u5f3a\u6559\u5e08\u6a21\u578b\u8f93\u51fa\u4e0e\u5b66\u751f\u53ef\u5b66\u4e60\u6027\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\uff0c\u66f4\u5f3a\u7684\u6a21\u578b\u5e76\u4e0d\u603b\u662f\u6700\u4f18\u7684\u6559\u5e08\uff0c\u5b58\u5728\u6559\u5e08\u8f93\u51fa\u4e0e\u5b66\u751f\u53ef\u5b66\u4e60\u6027\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "PerSyn\u91c7\u7528\"\u8def\u7531\u540e\u751f\u6210\"\u8303\u5f0f\uff0c\u9996\u5148\u901a\u8fc7\u67e5\u8be2\u7ea7\u8def\u7531\u5668\u4e3a\u6bcf\u4e2a\u63d0\u793a\u5206\u914d\u6700\u4f18\u6559\u5e08\uff0c\u8003\u8651\u5b66\u751f\u53ef\u5b66\u4e60\u6027\u548c\u6559\u5e08\u54cd\u5e94\u8d28\u91cf\uff0c\u7136\u540e\u5404\u6559\u5e08\u4ec5\u4e3a\u5206\u914d\u7684\u63d0\u793a\u751f\u6210\u6570\u636e\u3002", "result": "\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u548c\u89c4\u6a21\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPerSyn\u5728\u6307\u4ee4\u8c03\u4f18\u548c\u6570\u5b66\u63a8\u7406\u8bbe\u7f6e\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6216\u4e0e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "PerSyn\u901a\u8fc7\u4e2a\u6027\u5316\u6570\u636e\u5408\u6210\u6709\u6548\u89e3\u51b3\u4e86\u6559\u5e08-\u5b66\u751f\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u989d\u5916\u89c1\u89e3\u3002"}}
{"id": "2510.10937", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.10937", "abs": "https://arxiv.org/abs/2510.10937", "authors": ["Qizhou Peng", "Yang Zheng", "Yu Wen", "Yanna Wu", "Yingying Du"], "title": "Neutral Agent-based Adversarial Policy Learning against Deep Reinforcement Learning in Multi-party Open Systems", "comment": null, "summary": "Reinforcement learning (RL) has been an important machine learning paradigm\nfor solving long-horizon sequential decision-making problems under uncertainty.\nBy integrating deep neural networks (DNNs) into the RL framework, deep\nreinforcement learning (DRL) has emerged, which achieved significant success in\nvarious domains. However, the integration of DNNs also makes it vulnerable to\nadversarial attacks. Existing adversarial attack techniques mainly focus on\neither directly manipulating the environment with which a victim agent\ninteracts or deploying an adversarial agent that interacts with the victim\nagent to induce abnormal behaviors. While these techniques achieve promising\nresults, their adoption in multi-party open systems remains limited due to two\nmajor reasons: impractical assumption of full control over the environment and\ndependent on interactions with victim agents.\n  To enable adversarial attacks in multi-party open systems, in this paper, we\nredesigned an adversarial policy learning approach that can mislead\nwell-trained victim agents without requiring direct interactions with these\nagents or full control over their environments. Particularly, we propose a\nneutral agent-based approach across various task scenarios in multi-party open\nsystems. While the neutral agents seemingly are detached from the victim\nagents, indirectly influence them through the shared environment. We evaluate\nour proposed method on the SMAC platform based on Starcraft II and the\nautonomous driving simulation platform Highway-env. The experimental results\ndemonstrate that our method can launch general and effective adversarial\nattacks in multi-party open systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u591a\u53c2\u4e0e\u8005\u5f00\u653e\u7cfb\u7edf\u4e2d\u65e0\u9700\u4e0e\u53d7\u5bb3\u8005\u667a\u80fd\u4f53\u76f4\u63a5\u4ea4\u4e92\u6216\u5b8c\u5168\u63a7\u5236\u73af\u5883\u5373\u53ef\u8bef\u5bfc\u8bad\u7ec3\u6709\u7d20\u667a\u80fd\u4f53\u7684\u5bf9\u6297\u6027\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5", "motivation": "\u73b0\u6709\u7684\u5bf9\u6297\u653b\u51fb\u6280\u672f\u8981\u4e48\u9700\u8981\u5b8c\u5168\u63a7\u5236\u73af\u5883\uff0c\u8981\u4e48\u4f9d\u8d56\u4e0e\u53d7\u5bb3\u8005\u667a\u80fd\u4f53\u7684\u4ea4\u4e92\uff0c\u8fd9\u5728\u591a\u53c2\u4e0e\u8005\u5f00\u653e\u7cfb\u7edf\u4e2d\u96be\u4ee5\u5b9e\u73b0\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u4e2d\u7acb\u667a\u80fd\u4f53\u7684\u5bf9\u6297\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u4e2d\u7acb\u667a\u80fd\u4f53\u770b\u4f3c\u4e0e\u53d7\u5bb3\u8005\u667a\u80fd\u4f53\u65e0\u5173\uff0c\u4f46\u901a\u8fc7\u5171\u4eab\u73af\u5883\u95f4\u63a5\u5f71\u54cd\u5b83\u4eec\u3002", "result": "\u5728SMAC\u548cHighway-env\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u591a\u53c2\u4e0e\u8005\u5f00\u653e\u7cfb\u7edf\u4e2d\u53d1\u8d77\u901a\u7528\u4e14\u6709\u6548\u7684\u5bf9\u6297\u653b\u51fb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u53c2\u4e0e\u8005\u5f00\u653e\u7cfb\u7edf\u4e2d\u7684\u5bf9\u6297\u653b\u51fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.10952", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.10952", "abs": "https://arxiv.org/abs/2510.10952", "authors": ["Xi Mao", "Zhendong Wang", "Jingyu Li", "Lingchao Mao", "Utibe Essien", "Hairong Wang", "Xuelei Sherry Ni"], "title": "Interpretable Machine Learning for Cognitive Aging: Handling Missing Data and Uncovering Social Determinant", "comment": null, "summary": "Early detection of Alzheimer's disease (AD) is crucial because its\nneurodegenerative effects are irreversible, and neuropathologic and\nsocial-behavioral risk factors accumulate years before diagnosis. Identifying\nhigher-risk individuals earlier enables prevention, timely care, and equitable\nresource allocation. We predict cognitive performance from social determinants\nof health (SDOH) using the NIH NIA-supported PREPARE Challenge Phase 2 dataset\nderived from the nationally representative Mex-Cog cohort of the 2003 and 2012\nMexican Health and Aging Study (MHAS).\n  Data: The target is a validated composite cognitive score across seven\ndomains-orientation, memory, attention, language, constructional praxis, and\nexecutive function-derived from the 2016 and 2021 MHAS waves. Predictors span\ndemographic, socioeconomic, health, lifestyle, psychosocial, and healthcare\naccess factors.\n  Methodology: Missingness was addressed with a singular value decomposition\n(SVD)-based imputation pipeline treating continuous and categorical variables\nseparately. This approach leverages latent feature correlations to recover\nmissing values while balancing reliability and scalability. After evaluating\nmultiple methods, XGBoost was chosen for its superior predictive performance.\n  Results and Discussion: The framework outperformed existing methods and the\ndata challenge leaderboard, demonstrating high accuracy, robustness, and\ninterpretability. SHAP-based post hoc analysis identified top contributing SDOH\nfactors and age-specific feature patterns. Notably, flooring material emerged\nas a strong predictor, reflecting socioeconomic and environmental disparities.\nOther influential factors, age, SES, lifestyle, social interaction, sleep,\nstress, and BMI, underscore the multifactorial nature of cognitive aging and\nthe value of interpretable, data-driven SDOH modeling.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u793e\u4f1a\u5065\u5eb7\u51b3\u5b9a\u56e0\u7d20\u9884\u6d4b\u8ba4\u77e5\u8868\u73b0\uff0c\u57fa\u4e8e\u58a8\u897f\u54e5\u5065\u5eb7\u4e0e\u8001\u9f84\u5316\u7814\u7a76\u6570\u636e\uff0c\u901a\u8fc7SVD\u63d2\u8865\u548cXGBoost\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u8ba4\u77e5\u8bc4\u5206\u9884\u6d4b\uff0c\u5e76\u8bc6\u522b\u51fa\u5730\u677f\u6750\u6599\u7b49\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u65e9\u671f\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5176\u795e\u7ecf\u9000\u884c\u6027\u6548\u5e94\u4e0d\u53ef\u9006\u8f6c\uff0c\u4e14\u795e\u7ecf\u75c5\u7406\u548c\u793e\u4f1a\u884c\u4e3a\u98ce\u9669\u56e0\u7d20\u5728\u8bca\u65ad\u524d\u591a\u5e74\u5c31\u5df2\u7d2f\u79ef\u3002\u65e9\u671f\u8bc6\u522b\u9ad8\u98ce\u9669\u4e2a\u4f53\u6709\u52a9\u4e8e\u9884\u9632\u3001\u53ca\u65f6\u62a4\u7406\u548c\u516c\u5e73\u8d44\u6e90\u5206\u914d\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5947\u5f02\u503c\u5206\u89e3\u7684\u63d2\u8865\u7ba1\u9053\u5206\u522b\u5904\u7406\u8fde\u7eed\u548c\u5206\u7c7b\u53d8\u91cf\u7684\u7f3a\u5931\u503c\uff0c\u5229\u7528\u6f5c\u5728\u7279\u5f81\u76f8\u5173\u6027\u6062\u590d\u7f3a\u5931\u503c\u3002\u8bc4\u4f30\u591a\u79cd\u65b9\u6cd5\u540e\u9009\u62e9XGBoost\u8fdb\u884c\u9884\u6d4b\u5efa\u6a21\u3002", "result": "\u8be5\u6846\u67b6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u548c\u6570\u636e\u6311\u6218\u6392\u884c\u699c\uff0c\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002SHAP\u540e\u9a8c\u5206\u6790\u8bc6\u522b\u51fa\u5730\u677f\u6750\u6599\u3001\u5e74\u9f84\u3001\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\u3001\u751f\u6d3b\u65b9\u5f0f\u3001\u793e\u4f1a\u4e92\u52a8\u3001\u7761\u7720\u3001\u538b\u529b\u548cBMI\u7b49\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u793e\u4f1a\u5065\u5eb7\u51b3\u5b9a\u56e0\u7d20\u5728\u8ba4\u77e5\u8001\u5316\u4e2d\u7684\u591a\u56e0\u7d20\u6027\u8d28\uff0c\u4ee5\u53ca\u53ef\u89e3\u91ca\u3001\u6570\u636e\u9a71\u52a8\u7684SDOH\u5efa\u6a21\u7684\u4ef7\u503c\uff0c\u7279\u522b\u5f3a\u8c03\u4e86\u5730\u677f\u6750\u6599\u4f5c\u4e3a\u793e\u4f1a\u7ecf\u6d4e\u548c\u73af\u5883\u5dee\u5f02\u53cd\u6620\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.10962", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10962", "abs": "https://arxiv.org/abs/2510.10962", "authors": ["Wei Huang", "Yue Liao", "Yukang Chen", "Jianhui Liu", "Haoru Tan", "Si Liu", "Shiming Zhang", "Shuicheng Yan", "Xiaojuan Qi"], "title": "MC#: Mixture Compressor for Mixture-of-Experts Large Models", "comment": "15 pages, 13 figures", "summary": "Mixture-of-Experts (MoE) effectively scales large language models (LLMs) and\nvision-language models (VLMs) by increasing capacity through sparse activation.\nHowever, preloading all experts into memory and activating multiple experts per\ninput introduces significant computational and memory overhead, making the\nexpert module a major contributor to model size and inference cost. To address\nthis, we propose MC# (Mixture-Compressor-sharp), a framework that combines\nstatic quantization and dynamic expert pruning by leveraging the significance\nof experts and tokens for aggressive compression of MoE-LLMs/VLMs. To reduce\nstorage and loading costs, we introduce Pre-Loading Mixed-Precision\nQuantization (PMQ), which optimizes bit allocation via linear programming,\nbalancing expert importance and quantization error for a Pareto-optimal\ntrade-off between size and performance. To reduce runtime computation, Online\nTop-any Pruning (OTP) uses Gumbel-Softmax sampling to dynamically select a\nsubset of experts per token, enabling fine-grained control over activation. By\ncombining PMQ's static bit-width optimization with OTP's dynamic routing, MC#\nachieves extreme compression with minimal accuracy loss. On DeepSeek-VL2, MC#\nachieves a 6.2 times weight reduction at 2.57 average bits with only a 1.7%\naccuracy drop across five multimodal benchmarks. Additionally, OTP reduces\nexpert activation over 20% with less than 1% performance degradation,\ndemonstrating strong potential for efficient MoE-based model deployment.", "AI": {"tldr": "MC#\u662f\u4e00\u4e2a\u7ed3\u5408\u9759\u6001\u91cf\u5316\u548c\u52a8\u6001\u4e13\u5bb6\u526a\u679d\u7684\u6846\u67b6\uff0c\u901a\u8fc7PMQ\u4f18\u5316\u4f4d\u5206\u914d\u548cOTP\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u5b50\u96c6\uff0c\u5b9e\u73b0MoE\u6a21\u578b\u7684\u9ad8\u6548\u538b\u7f29\u90e8\u7f72\u3002", "motivation": "\u89e3\u51b3MoE\u6a21\u578b\u4e2d\u4e13\u5bb6\u6a21\u5757\u5e26\u6765\u7684\u663e\u8457\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u95ee\u9898\uff0c\u8fd9\u4e9b\u6a21\u5757\u662f\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u6210\u672c\u7684\u4e3b\u8981\u8d21\u732e\u8005\u3002", "method": "\u7ed3\u5408Pre-Loading Mixed-Precision Quantization (PMQ)\u8fdb\u884c\u9759\u6001\u4f4d\u5bbd\u4f18\u5316\uff0c\u4f7f\u7528\u7ebf\u6027\u7f16\u7a0b\u5e73\u8861\u4e13\u5bb6\u91cd\u8981\u6027\u548c\u91cf\u5316\u8bef\u5dee\uff1b\u901a\u8fc7Online Top-any Pruning (OTP)\u4f7f\u7528Gumbel-Softmax\u91c7\u6837\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u5b50\u96c6\u3002", "result": "\u5728DeepSeek-VL2\u4e0a\u5b9e\u73b06.2\u500d\u6743\u91cd\u538b\u7f29\uff0c\u5e73\u57472.57\u4f4d\uff0c\u4e94\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u635f\u59311.7%\u51c6\u786e\u7387\uff1bOTP\u51cf\u5c1120%\u4ee5\u4e0a\u4e13\u5bb6\u6fc0\u6d3b\uff0c\u6027\u80fd\u4e0b\u964d\u5c0f\u4e8e1%\u3002", "conclusion": "MC#\u901a\u8fc7\u9759\u6001\u4f4d\u5bbd\u4f18\u5316\u548c\u52a8\u6001\u8def\u7531\u7684\u534f\u540c\uff0c\u5b9e\u73b0\u4e86\u6781\u7aef\u538b\u7f29\u4e14\u4fdd\u6301\u6700\u5c0f\u7cbe\u5ea6\u635f\u5931\uff0c\u4e3a\u57fa\u4e8eMoE\u7684\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10963", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10963", "abs": "https://arxiv.org/abs/2510.10963", "authors": ["Zhuo Li", "Yuege Feng", "Dandan Guo", "Jinpeng Hu", "Anningzhe Gao", "Xiang Wan"], "title": "APLOT: Robust Reward Modeling via Adaptive Preference Learning with Optimal Transport", "comment": "EMNLP2025", "summary": "The reward model (RM) plays a crucial role in aligning Large Language Models\n(LLMs) with human preferences through Reinforcement Learning, where the\nBradley-Terry (BT) objective has been recognized as simple yet powerful,\nspecifically for pairwise preference learning. However, BT-based RMs often\nstruggle to effectively distinguish between similar preference responses,\nleading to insufficient separation between preferred and non-preferred outputs.\nConsequently, they may easily overfit easy samples and cannot generalize well\nto Out-Of-Distribution (OOD) samples, resulting in suboptimal performance. To\naddress these challenges, this paper introduces an effective enhancement to\nBT-based RMs through an adaptive margin mechanism. Specifically, we design to\ndynamically adjust the RM focus on more challenging samples through margins,\nbased on both semantic similarity and model-predicted reward differences, which\nis approached from a distributional perspective solvable with Optimal Transport\n(OT). By incorporating these factors into a principled OT cost matrix design,\nour adaptive margin enables the RM to better capture distributional differences\nbetween chosen and rejected responses, yielding significant improvements in\nperformance, convergence speed, and generalization capabilities. Experimental\nresults across multiple benchmarks demonstrate that our method outperforms\nseveral existing RM techniques, showcasing enhanced performance in both\nIn-Distribution (ID) and OOD settings. Moreover, RLHF experiments support our\npractical effectiveness in better aligning LLMs with human preferences. Our\ncode is available at https://github.com/BIRlz/APLOT", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u8fb9\u754c\u7684\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u52a8\u6001\u8c03\u6574\u5956\u52b1\u6a21\u578b\u5bf9\u56f0\u96be\u6837\u672c\u7684\u5173\u6ce8\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfBradley-Terry\u6a21\u578b\u5728\u533a\u5206\u76f8\u4f3c\u504f\u597d\u54cd\u5e94\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u7684Bradley-Terry\u5956\u52b1\u6a21\u578b\u5728\u5904\u7406\u76f8\u4f3c\u504f\u597d\u54cd\u5e94\u65f6\u533a\u5206\u80fd\u529b\u4e0d\u8db3\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\u7b80\u5355\u6837\u672c\uff0c\u4e14\u5728\u5206\u5e03\u5916\u6837\u672c\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5bfc\u81f4\u5bf9\u9f50\u6548\u679c\u4e0d\u7406\u60f3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u548c\u6a21\u578b\u9884\u6d4b\u5956\u52b1\u5dee\u5f02\u7684\u81ea\u9002\u5e94\u8fb9\u754c\u673a\u5236\uff0c\u4ece\u5206\u5e03\u89d2\u5ea6\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u8bbe\u8ba1\u6210\u672c\u77e9\u9635\uff0c\u52a8\u6001\u8c03\u6574\u6a21\u578b\u5bf9\u6311\u6218\u6027\u6837\u672c\u7684\u5173\u6ce8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u5956\u52b1\u6a21\u578b\u6280\u672f\uff0c\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3001\u6536\u655b\u901f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u81ea\u9002\u5e94\u8fb9\u754c\u673a\u5236\u589e\u5f3a\u7684\u5956\u52b1\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u9009\u62e9\u4e0e\u62d2\u7edd\u54cd\u5e94\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u6548\u679c\u3002"}}
{"id": "2510.10964", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10964", "abs": "https://arxiv.org/abs/2510.10964", "authors": ["Junhyuck Kim", "Ethan Ewer", "Taehong Moon", "Jongho Park", "Dimitris Papailiopoulos"], "title": "Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies for Reasoning Models", "comment": "20 pages, 12 figures", "summary": "While 4-bit quantization has emerged as a memory-optimal choice for\nnon-reasoning models and zero-shot tasks across scales, we show that this\nuniversal prescription fails for reasoning models, where the KV cache rather\nthan model size can dominate memory. Through systematic experiments across\n1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent\ntrade-off: models with an effective size below 8-bit 4B parameters achieve\nbetter accuracy by allocating memory to more weights rather than longer\ngeneration, while larger models achieve better accuracy by allocating memory to\nlonger generations. This scale threshold also determines when parallel scaling\nbecomes memory-efficient and whether KV cache eviction outperforms KV\nquantization. Our findings show that memory optimization for LLMs cannot be\nscale-agnostic, while providing principled guidelines: for small reasoning\nmodels, prioritize model capacity over test-time compute, while for larger\nones, maximize test-time compute. Our results suggest that optimizing reasoning\nmodels for deployment requires fundamentally different strategies from those\nestablished for non-reasoning models.", "AI": {"tldr": "4-bit\u91cf\u5316\u5bf9\u4e8e\u63a8\u7406\u6a21\u578b\u5e76\u975e\u6700\u4f18\u9009\u62e9\uff0cKV\u7f13\u5b58\u800c\u975e\u6a21\u578b\u5927\u5c0f\u4e3b\u5bfc\u5185\u5b58\u4f7f\u7528\u3002\u7814\u7a76\u53d1\u73b0\u5b58\u5728\u89c4\u6a21\u4f9d\u8d56\u7684\u6743\u8861\uff1a8B\u53c2\u6570\u4ee5\u4e0b\u7684\u5c0f\u6a21\u578b\u5e94\u4f18\u5148\u5206\u914d\u5185\u5b58\u7ed9\u6743\u91cd\uff0c\u800c\u5927\u6a21\u578b\u5e94\u4f18\u5148\u7ed9\u66f4\u957f\u7684\u751f\u6210\u5e8f\u5217\u3002", "motivation": "\u7814\u7a76\u63a8\u7406\u6a21\u578b\u7684\u5185\u5b58\u4f18\u5316\u7b56\u7565\uff0c\u53d1\u73b04-bit\u91cf\u5316\u7684\u901a\u7528\u65b9\u6848\u5728\u63a8\u7406\u6a21\u578b\u4e2d\u5931\u6548\uff0c\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u5236\u5b9a\u4e0d\u540c\u7b56\u7565\u3002", "method": "\u5728AIME25\u548cGPQA-Diamond\u6570\u636e\u96c6\u4e0a\u8fdb\u884c1700\u4e2a\u63a8\u7406\u573a\u666f\u7684\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u5206\u6790\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u5728\u5185\u5b58\u5206\u914d\u4e0a\u7684\u6743\u8861\u3002", "result": "\u53d1\u73b08B\u53c2\u6570\u662f\u4e34\u754c\u70b9\uff1a\u5c0f\u6a21\u578b\uff08<8B\uff09\u901a\u8fc7\u5206\u914d\u66f4\u591a\u5185\u5b58\u7ed9\u6743\u91cd\u83b7\u5f97\u66f4\u597d\u7cbe\u5ea6\uff0c\u5927\u6a21\u578b\u901a\u8fc7\u5206\u914d\u5185\u5b58\u7ed9\u66f4\u957f\u751f\u6210\u5e8f\u5217\u83b7\u5f97\u66f4\u597d\u7cbe\u5ea6\u3002", "conclusion": "\u63a8\u7406\u6a21\u578b\u7684\u90e8\u7f72\u4f18\u5316\u9700\u8981\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u5b8c\u5168\u4e0d\u540c\u7684\u7b56\u7565\uff0c\u5fc5\u987b\u8003\u8651\u89c4\u6a21\u4f9d\u8d56\u6027\uff0c\u5c0f\u6a21\u578b\u4f18\u5148\u6a21\u578b\u5bb9\u91cf\uff0c\u5927\u6a21\u578b\u4f18\u5148\u63a8\u7406\u65f6\u8ba1\u7b97\u8d44\u6e90\u3002"}}
{"id": "2510.10982", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10982", "abs": "https://arxiv.org/abs/2510.10982", "authors": ["Zihan Wang", "Zhiyong Ma", "Zhongkui Ma", "Shuofeng Liu", "Akide Liu", "Derui Wang", "Minhui Xue", "Guangdong Bai"], "title": "Catch-Only-One: Non-Transferable Examples for Model-Specific Authorization", "comment": null, "summary": "Recent AI regulations call for data that remain useful for innovation while\nresistant to misuse, balancing utility with protection at the model level.\nExisting approaches either perturb data to make it unlearnable or retrain\nmodels to suppress transfer, but neither governs inference by unknown models,\nand both typically require control over training. We propose non-transferable\nexamples (NEs), a training-free and data-agnostic input-side usage-control\nmechanism. We recode inputs within a model-specific low-sensitivity subspace,\npreserving outputs for the authorized model while reducing performance on\nunauthorized models through subspace misalignment. We establish formal bounds\nthat guarantee utility for the authorized model and quantify deviation for\nunauthorized ones, with the Hoffman-Wielandt inequality linking degradation to\nspectral differences. Empirically, NEs retain performance on diverse vision\nbackbones and state-of-the-art vision-language models under common\npreprocessing, whereas non-target models collapse even with reconstruction\nattempts. These results establish NEs as a practical means to preserve intended\ndata utility while preventing unauthorized exploitation. Our project is\navailable at https://trusted-system-lab.github.io/model-specificity", "AI": {"tldr": "\u63d0\u51fa\u975e\u53ef\u8f6c\u79fb\u793a\u4f8b\uff08NEs\uff09\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u6570\u636e\u4e0d\u53ef\u77e5\u7684\u8f93\u5165\u4fa7\u4f7f\u7528\u63a7\u5236\u673a\u5236\uff0c\u901a\u8fc7\u6a21\u578b\u7279\u5b9a\u7684\u4f4e\u654f\u611f\u5b50\u7a7a\u95f4\u91cd\u65b0\u7f16\u7801\u8f93\u5165\uff0c\u4fdd\u6301\u6388\u6743\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u975e\u6388\u6743\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u6270\u52a8\u6570\u636e\u4f7f\u5176\u4e0d\u53ef\u5b66\u4e60\uff0c\u8981\u4e48\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u6291\u5236\u8fc1\u79fb\uff0c\u4f46\u90fd\u65e0\u6cd5\u63a7\u5236\u672a\u77e5\u6a21\u578b\u7684\u63a8\u7406\uff0c\u4e14\u901a\u5e38\u9700\u8981\u63a7\u5236\u8bad\u7ec3\u8fc7\u7a0b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5e73\u8861\u6548\u7528\u4e0e\u4fdd\u62a4\u3001\u5728\u6a21\u578b\u5c42\u9762\u8fdb\u884c\u63a7\u5236\u7684\u673a\u5236\u3002", "method": "\u5728\u6a21\u578b\u7279\u5b9a\u7684\u4f4e\u654f\u611f\u5b50\u7a7a\u95f4\u5185\u91cd\u65b0\u7f16\u7801\u8f93\u5165\uff0c\u5229\u7528\u5b50\u7a7a\u95f4\u4e0d\u5bf9\u9f50\u6765\u4fdd\u6301\u6388\u6743\u6a21\u578b\u8f93\u51fa\uff0c\u540c\u65f6\u964d\u4f4e\u975e\u6388\u6743\u6a21\u578b\u6027\u80fd\u3002\u57fa\u4e8eHoffman-Wielandt\u4e0d\u7b49\u5f0f\u5efa\u7acb\u7406\u8bba\u8fb9\u754c\u3002", "result": "\u7ecf\u9a8c\u9a8c\u8bc1\u663e\u793aNEs\u5728\u591a\u79cd\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u548c\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u4fdd\u6301\u6027\u80fd\uff0c\u800c\u975e\u76ee\u6807\u6a21\u578b\u5373\u4f7f\u5c1d\u8bd5\u91cd\u5efa\u4e5f\u4f1a\u6027\u80fd\u5d29\u6e83\u3002", "conclusion": "NEs\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u65e2\u80fd\u4fdd\u6301\u9884\u671f\u6570\u636e\u6548\u7528\uff0c\u53c8\u80fd\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u5229\u7528\uff0c\u4e3a\u6a21\u578b\u7ea7\u522b\u7684\u4f7f\u7528\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.11016", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11016", "abs": "https://arxiv.org/abs/2510.11016", "authors": ["Ziyi Gao", "Yike Xu", "Jiahao Yuan", "Baokun Wang", "Jinyong Wen", "Xiaotong Lin", "Yun Liu", "Xing Fu", "Yu Cheng", "Yongchao Liu", "Weiqiang Wang", "Zhongle Xie"], "title": "Instruction-aware User Embedding via Synergistic Language and Representation Modeling", "comment": null, "summary": "User representation modeling has become increasingly crucial for personalized\napplications, yet existing approaches struggle with generalizability across\ndomains and sensitivity to noisy behavioral signals. We present InstructUE, an\ninstruction-aware user embedding foundation model that leverages large language\nmodels (LLMs) to generate general and instruction-aware user representations.\nInstructUE introduces a multi-encoder architecture with a lightweight adapter\nthat efficiently processes heterogeneous data from six different sources while\npreserving their structural characteristics. Additionally, it proposes a novel\ncontrastive-autoregressive training framework that bridges language and\nrepresentation spaces through a curated UserQA dataset. The\ncontrastive-autoregressive training framework simultaneously leverages\nautoregressive learning to capture domain knowledge in language space and\ncontrastive learning to align user-text embeddings in representation space,\nthereby enhancing the instruction-awareness and noise-robustness of user\nembeddings. Through extensive experiments on real-world applications, we\ndemonstrate that InstructUE significantly outperforms existing methods across\nmultiple domains including user prediction, marketing, and recommendation\nscenarios. Our results show that instruction-aware user modeling can\neffectively achieve instruction-guided denoising of user information in\nspecific scenarios, paving the way for more generalizable and robust user\nrepresentation learning.", "AI": {"tldr": "InstructUE\u662f\u4e00\u4e2a\u57fa\u4e8e\u6307\u4ee4\u611f\u77e5\u7684\u7528\u6237\u5d4c\u5165\u57fa\u7840\u6a21\u578b\uff0c\u5229\u7528LLM\u751f\u6210\u901a\u7528\u4e14\u6307\u4ee4\u611f\u77e5\u7684\u7528\u6237\u8868\u793a\uff0c\u901a\u8fc7\u591a\u7f16\u7801\u5668\u67b6\u6784\u548c\u5bf9\u6bd4-\u81ea\u56de\u5f52\u8bad\u7ec3\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u9886\u57df\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7528\u6237\u8868\u793a\u5efa\u6a21\u65b9\u6cd5\u5728\u8de8\u9886\u57df\u6cdb\u5316\u6027\u548c\u5bf9\u566a\u58f0\u884c\u4e3a\u4fe1\u53f7\u7684\u654f\u611f\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u901a\u7528\u548c\u9c81\u68d2\u7684\u7528\u6237\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u591a\u7f16\u7801\u5668\u67b6\u6784\u5904\u7406\u5f02\u6784\u6570\u636e\uff0c\u5f15\u5165\u5bf9\u6bd4-\u81ea\u56de\u5f52\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7UserQA\u6570\u636e\u96c6\u8fde\u63a5\u8bed\u8a00\u548c\u8868\u793a\u7a7a\u95f4\uff0c\u540c\u65f6\u5229\u7528\u81ea\u56de\u5f52\u5b66\u4e60\u6355\u83b7\u9886\u57df\u77e5\u8bc6\u548c\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u5d4c\u5165\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\uff0cInstructUE\u5728\u7528\u6237\u9884\u6d4b\u3001\u8425\u9500\u548c\u63a8\u8350\u7b49\u591a\u4e2a\u9886\u57df\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u7279\u5b9a\u573a\u666f\u4e0b\u7684\u6307\u4ee4\u5f15\u5bfc\u53bb\u566a\u3002", "conclusion": "\u6307\u4ee4\u611f\u77e5\u7684\u7528\u6237\u5efa\u6a21\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u7528\u6237\u4fe1\u606f\u7684\u6307\u4ee4\u5f15\u5bfc\u53bb\u566a\uff0c\u4e3a\u66f4\u901a\u7528\u548c\u9c81\u68d2\u7684\u7528\u6237\u8868\u793a\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.11018", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11018", "abs": "https://arxiv.org/abs/2510.11018", "authors": ["Pranav Ramesh", "Arjun Roy", "Deepak Ravikumar", "Kaushik Roy", "Gopalakrishnan Srinivasan"], "title": "The Easy Path to Robustness: Coreset Selection using Sample Hardness", "comment": null, "summary": "Designing adversarially robust models from a data-centric perspective\nrequires understanding which input samples are most crucial for learning\nresilient features. While coreset selection provides a mechanism for efficient\ntraining on data subsets, current algorithms are designed for clean accuracy\nand fall short in preserving robustness. To address this, we propose a\nframework linking a sample's adversarial vulnerability to its\n\\textit{hardness}, which we quantify using the average input gradient norm\n(AIGN) over training. We demonstrate that \\textit{easy} samples (with low AIGN)\nare less vulnerable and occupy regions further from the decision boundary.\nLeveraging this insight, we present EasyCore, a coreset selection algorithm\nthat retains only the samples with low AIGN for training. We empirically show\nthat models trained on EasyCore-selected data achieve significantly higher\nadversarial accuracy than those trained with competing coreset methods under\nboth standard and adversarial training. As AIGN is a model-agnostic dataset\nproperty, EasyCore is an efficient and widely applicable data-centric method\nfor improving adversarial robustness. We show that EasyCore achieves up to 7\\%\nand 5\\% improvement in adversarial accuracy under standard training and TRADES\nadversarial training, respectively, compared to existing coreset methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86EasyCore\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u4f4e\u5e73\u5747\u8f93\u5165\u68af\u5ea6\u8303\u6570(AIGN)\u7684\u7b80\u5355\u6837\u672c\u8fdb\u884c\u6838\u5fc3\u96c6\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u6297\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u6838\u5fc3\u96c6\u9009\u62e9\u7b97\u6cd5\u4e3b\u8981\u5173\u6ce8\u6e05\u6d01\u51c6\u786e\u7387\uff0c\u65e0\u6cd5\u6709\u6548\u4fdd\u6301\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u9700\u8981\u4ece\u6570\u636e\u4e2d\u5fc3\u89c6\u89d2\u7406\u89e3\u54ea\u4e9b\u6837\u672c\u5bf9\u5b66\u4e60\u9c81\u68d2\u7279\u5f81\u6700\u5173\u952e", "method": "\u5c06\u6837\u672c\u7684\u5bf9\u6297\u8106\u5f31\u6027\u4e0e\u5176\u786c\u5ea6\u5173\u8054\uff0c\u7528AIGN\u91cf\u5316\u6837\u672c\u96be\u5ea6\uff0c\u9009\u62e9\u4f4eAIGN\u7684\u7b80\u5355\u6837\u672c\u6784\u5efa\u6838\u5fc3\u96c6\u8fdb\u884c\u8bad\u7ec3", "result": "EasyCore\u5728\u6807\u51c6\u8bad\u7ec3\u548cTRADES\u5bf9\u6297\u8bad\u7ec3\u4e0b\uff0c\u76f8\u6bd4\u73b0\u6709\u6838\u5fc3\u96c6\u65b9\u6cd5\u5206\u522b\u5b9e\u73b0\u4e867%\u548c5%\u7684\u5bf9\u6297\u51c6\u786e\u7387\u63d0\u5347", "conclusion": "AIGN\u662f\u6a21\u578b\u65e0\u5173\u7684\u6570\u636e\u96c6\u5c5e\u6027\uff0cEasyCore\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u6570\u636e\u4e2d\u5fc3\u65b9\u6cd5\uff0c\u53ef\u663e\u8457\u6539\u5584\u5bf9\u6297\u9c81\u68d2\u6027"}}
{"id": "2510.11049", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11049", "abs": "https://arxiv.org/abs/2510.11049", "authors": ["Sonakshi Dua", "Gonzalo Mateos", "Sundeep Prabhakar Chepuri"], "title": "Conformal Inference for Time Series over Graphs", "comment": null, "summary": "Trustworthy decision making in networked, dynamic environments calls for\ninnovative uncertainty quantification substrates in predictive models for graph\ntime series. Existing conformal prediction (CP) methods have been applied\nseparately to multivariate time series and static graphs, but they either\nignore the underlying graph topology or neglect temporal dynamics. To bridge\nthis gap, here we develop a CP-based sequential prediction region framework\ntailored for graph time series. A key technical innovation is to leverage the\ngraph structure and thus capture pairwise dependencies across nodes, while\nproviding user-specified coverage guarantees on the predictive outcomes. We\nformally establish that our scheme yields an exponential shrinkage in the\nvolume of the ellipsoidal prediction set relative to its graph-agnostic\ncounterpart. Using real-world datasets, we demonstrate that the novel\nuncertainty quantification framework maintains desired empirical coverage while\nachieving markedly smaller (up to 80% reduction) prediction regions than\nexisting approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u56fe\u65f6\u95f4\u5e8f\u5217\u7684\u4fdd\u5f62\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u56fe\u7ed3\u6784\u6355\u83b7\u8282\u70b9\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u4fdd\u8bc1\u6307\u5b9a\u8986\u76d6\u6982\u7387\u7684\u540c\u65f6\u663e\u8457\u7f29\u5c0f\u9884\u6d4b\u533a\u57df\u4f53\u79ef\u3002", "motivation": "\u73b0\u6709\u4fdd\u5f62\u9884\u6d4b\u65b9\u6cd5\u5206\u522b\u5e94\u7528\u4e8e\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u548c\u9759\u6001\u56fe\uff0c\u4f46\u8981\u4e48\u5ffd\u7565\u56fe\u62d3\u6251\u7ed3\u6784\uff0c\u8981\u4e48\u5ffd\u89c6\u65f6\u95f4\u52a8\u6001\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u540c\u65f6\u5904\u7406\u56fe\u7ed3\u6784\u548c\u65f6\u95f4\u52a8\u6001\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u4fdd\u5f62\u9884\u6d4b\u7684\u5e8f\u5217\u9884\u6d4b\u533a\u57df\u6846\u67b6\uff0c\u5229\u7528\u56fe\u7ed3\u6784\u6355\u83b7\u8282\u70b9\u95f4\u6210\u5bf9\u4f9d\u8d56\u5173\u7cfb\uff0c\u63d0\u4f9b\u7528\u6237\u6307\u5b9a\u7684\u8986\u76d6\u4fdd\u8bc1\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u76f8\u6bd4\u56fe\u65e0\u5173\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u4f7f\u692d\u7403\u9884\u6d4b\u96c6\u4f53\u79ef\u6307\u6570\u7ea7\u7f29\u5c0f\uff1b\u5b9e\u8bc1\u663e\u793a\u5728\u4fdd\u6301\u671f\u671b\u8986\u76d6\u7684\u540c\u65f6\uff0c\u9884\u6d4b\u533a\u57df\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u51cf\u5c0f\uff08\u6700\u591a\u51cf\u5c1180%\uff09\u3002", "conclusion": "\u8be5\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\u6210\u529f\u7ed3\u5408\u56fe\u7ed3\u6784\u548c\u65f6\u95f4\u52a8\u6001\uff0c\u5728\u4fdd\u8bc1\u9884\u6d4b\u53ef\u9760\u6027\u7684\u540c\u65f6\u5927\u5e45\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2510.11057", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11057", "abs": "https://arxiv.org/abs/2510.11057", "authors": ["Youngrok Park", "Hojung Jung", "Sangmin Bae", "Se-Young Yun"], "title": "Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models", "comment": "54 pages, 17 figures, 18 tables", "summary": "Diffusion models have achieved remarkable success as generative models.\nHowever, even a well-trained model can accumulate errors throughout the\ngeneration process. These errors become particularly problematic when arbitrary\nguidance is applied to steer samples toward desired properties, which often\nbreaks sample fidelity. In this paper, we propose a general solution to address\nthe off-manifold phenomenon observed in diffusion models. Our approach\nleverages a time predictor to estimate deviations from the desired data\nmanifold at each timestep, identifying that a larger time gap is associated\nwith reduced generation quality. We then design a novel guidance mechanism,\n`Temporal Alignment Guidance' (TAG), attracting the samples back to the desired\nmanifold at every timestep during generation. Through extensive experiments, we\ndemonstrate that TAG consistently produces samples closely aligned with the\ndesired manifold at each timestep, leading to significant improvements in\ngeneration quality across various downstream tasks.", "AI": {"tldr": "\u63d0\u51faTemporal Alignment Guidance (TAG)\u65b9\u6cd5\u89e3\u51b3\u6269\u6563\u6a21\u578b\u4e2d\u7684\u79bb\u6d41\u5f62\u95ee\u9898\uff0c\u901a\u8fc7\u65f6\u95f4\u9884\u6d4b\u5668\u4f30\u8ba1\u504f\u5dee\u5e76\u5f15\u5bfc\u6837\u672c\u56de\u5230\u671f\u671b\u6d41\u5f62\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u4f1a\u7d2f\u79ef\u8bef\u5dee\uff0c\u7279\u522b\u662f\u5728\u5e94\u7528\u4efb\u610f\u5f15\u5bfc\u65f6\uff0c\u8fd9\u4f1a\u7834\u574f\u6837\u672c\u4fdd\u771f\u5ea6\uff0c\u51fa\u73b0\u79bb\u6d41\u5f62\u73b0\u8c61\u3002", "method": "\u4f7f\u7528\u65f6\u95f4\u9884\u6d4b\u5668\u4f30\u8ba1\u6bcf\u4e2a\u65f6\u95f4\u6b65\u4e0e\u671f\u671b\u6570\u636e\u6d41\u5f62\u7684\u504f\u5dee\uff0c\u8bbe\u8ba1TAG\u5f15\u5bfc\u673a\u5236\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5c06\u6837\u672c\u5438\u5f15\u56de\u671f\u671b\u6d41\u5f62\u3002", "result": "TAG\u80fd\u6301\u7eed\u4ea7\u751f\u4e0e\u671f\u671b\u6d41\u5f62\u7d27\u5bc6\u5bf9\u9f50\u7684\u6837\u672c\uff0c\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u663e\u8457\u6539\u5584\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "TAG\u662f\u4e00\u79cd\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u6269\u6563\u6a21\u578b\u4e2d\u7684\u79bb\u6d41\u5f62\u95ee\u9898\uff0c\u63d0\u5347\u5f15\u5bfc\u751f\u6210\u7684\u8d28\u91cf\u548c\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2510.11168", "categories": ["cs.LG", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11168", "abs": "https://arxiv.org/abs/2510.11168", "authors": ["Jinbin Zhang", "Nasib Ullah", "Erik Schultheis", "Rohit Babbar"], "title": "ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large Output Spaces", "comment": "Accepted to ICML 2025", "summary": "Large output spaces, also referred to as Extreme multilabel classification\n(XMC), is a setting that arises, e.g., in large-scale tagging and\nproduct-to-product recommendation, and is characterized by the number of labels\nranging from hundreds of thousands to millions. This means that the linear\nclassification head, usually only a tiny fraction of the overall model, turns\ninto the main driver for compute and memory demand. Current state-of-the-art\nXMC methods predominantly rely on FP16-FP32 mixed-precision training, which we\nshow can be unstable, and inefficient in terms of memory usage and\ncomputational overhead. Meanwhile, existing low-precision methods typically\nretain higher precision for the classification layer. In this work, we propose\nELMO, a pure low-precision training framework for XMC models using BFloat16 and\nFloat8 data types. By leveraging Kahan summation and stochastic rounding, we\ndemonstrate that XMC models can be effectively trained entirely in Float8,\nwithout relying on single-precision master weights or tensor scaling.\nLow-precision training, combined with our proposed memory optimizations --\ngradient fusion and chunking -- enables significant reductions in GPU memory\nusage. For example, we train a 3-million-label XMC model with only 6.6 GiB of\nGPU memory, compared to the 39.7 GiB required by the optimized SOTA method,\nRenee without compromising accuracy.", "AI": {"tldr": "ELMO\u662f\u4e00\u4e2a\u7eaf\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7f\u7528BFloat16\u548cFloat8\u6570\u636e\u7c7b\u578b\u6765\u8bad\u7ec3\u6781\u7aef\u591a\u6807\u7b7e\u5206\u7c7b\u6a21\u578b\uff0c\u901a\u8fc7Kahan\u6c42\u548c\u548c\u968f\u673a\u820d\u5165\u6280\u672f\u5b9e\u73b0\u5168Float8\u8bad\u7ec3\uff0c\u663e\u8457\u964d\u4f4eGPU\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u5f53\u524dXMC\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56FP16-FP32\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\uff0c\u5b58\u5728\u4e0d\u7a33\u5b9a\u3001\u5185\u5b58\u4f7f\u7528\u6548\u7387\u4f4e\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002\u73b0\u6709\u4f4e\u7cbe\u5ea6\u65b9\u6cd5\u901a\u5e38\u4e3a\u5206\u7c7b\u5c42\u4fdd\u7559\u66f4\u9ad8\u7cbe\u5ea6\uff0c\u9650\u5236\u4e86\u5185\u5b58\u8282\u7701\u6548\u679c\u3002", "method": "\u63d0\u51faELMO\u6846\u67b6\uff0c\u4f7f\u7528BFloat16\u548cFloat8\u6570\u636e\u7c7b\u578b\uff0c\u7ed3\u5408Kahan\u6c42\u548c\u548c\u968f\u673a\u820d\u5165\u6280\u672f\uff0c\u5b9e\u73b0\u5168Float8\u8bad\u7ec3\u3002\u91c7\u7528\u68af\u5ea6\u878d\u5408\u548c\u5206\u5757\u7b49\u5185\u5b58\u4f18\u5316\u6280\u672f\u3002", "result": "\u5728300\u4e07\u4e2a\u6807\u7b7e\u7684XMC\u6a21\u578b\u4e0a\uff0c\u4ec5\u4f7f\u75286.6 GiB GPU\u5185\u5b58\uff0c\u76f8\u6bd4\u4f18\u5316SOTA\u65b9\u6cd5Renee\u6240\u9700\u768439.7 GiB\u663e\u8457\u964d\u4f4e\uff0c\u4e14\u4e0d\u635f\u5931\u51c6\u786e\u6027\u3002", "conclusion": "ELMO\u8bc1\u660eXMC\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u5728\u5168Float8\u7cbe\u5ea6\u4e0b\u8bad\u7ec3\uff0c\u65e0\u9700\u4f9d\u8d56\u5355\u7cbe\u5ea6\u4e3b\u6743\u91cd\u6216\u5f20\u91cf\u7f29\u653e\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5185\u5b58\u8282\u7701\u3002"}}
{"id": "2510.11058", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11058", "abs": "https://arxiv.org/abs/2510.11058", "authors": ["I Chiu", "Yu-Tung Liu", "Kuan-Chen Wang", "Hung-Yu Wei", "Yu Tsao"], "title": "Robust Photoplethysmography Signal Denoising via Mamba Networks", "comment": "5 pages, 2 figures", "summary": "Photoplethysmography (PPG) is widely used in wearable health monitoring, but\nits reliability is often degraded by noise and motion artifacts, limiting\ndownstream applications such as heart rate (HR) estimation. This paper presents\na deep learning framework for PPG denoising with an emphasis on preserving\nphysiological information. In this framework, we propose DPNet, a Mamba-based\ndenoising backbone designed for effective temporal modeling. To further enhance\ndenoising performance, the framework also incorporates a scale-invariant\nsignal-to-distortion ratio (SI-SDR) loss to promote waveform fidelity and an\nauxiliary HR predictor (HRP) that provides physiological consistency through\nHR-based supervision. Experiments on the BIDMC dataset show that our method\nachieves strong robustness against both synthetic noise and real-world motion\nartifacts, outperforming conventional filtering and existing neural models. Our\nmethod can effectively restore PPG signals while maintaining HR accuracy,\nhighlighting the complementary roles of SI-SDR loss and HR-guided supervision.\nThese results demonstrate the potential of our approach for practical\ndeployment in wearable healthcare systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eMamba\u7684PPG\u4fe1\u53f7\u53bb\u566a\u6846\u67b6DPNet\uff0c\u7ed3\u5408SI-SDR\u635f\u5931\u548c\u5fc3\u7387\u9884\u6d4b\u5668\uff0c\u5728BIDMC\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002", "motivation": "\u5149\u7535\u4f53\u79ef\u63cf\u8bb0\u6cd5(PPG)\u5728\u53ef\u7a7f\u6234\u5065\u5eb7\u76d1\u6d4b\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u53ef\u9760\u6027\u5e38\u53d7\u566a\u58f0\u548c\u8fd0\u52a8\u4f2a\u5f71\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u5fc3\u7387\u4f30\u8ba1\u7b49\u4e0b\u6e38\u5e94\u7528\u3002", "method": "\u63d0\u51faDPNet\u4f5c\u4e3a\u57fa\u4e8eMamba\u7684\u53bb\u566a\u4e3b\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408\u5c3a\u5ea6\u4e0d\u53d8\u4fe1\u566a\u6bd4(SI-SDR)\u635f\u5931\u548c\u8f85\u52a9\u5fc3\u7387\u9884\u6d4b\u5668(HRP)\uff0c\u901a\u8fc7\u5fc3\u7387\u76d1\u7763\u63d0\u4f9b\u751f\u7406\u4e00\u81f4\u6027\u3002", "result": "\u5728BIDMC\u6570\u636e\u96c6\u4e0a\u5bf9\u5408\u6210\u566a\u58f0\u548c\u771f\u5b9e\u8fd0\u52a8\u4f2a\u5f71\u5747\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\uff0c\u80fd\u6709\u6548\u6062\u590dPPG\u4fe1\u53f7\u540c\u65f6\u4fdd\u6301\u5fc3\u7387\u51c6\u786e\u6027\u3002", "conclusion": "SI-SDR\u635f\u5931\u548c\u5fc3\u7387\u5f15\u5bfc\u76d1\u7763\u5177\u6709\u4e92\u8865\u4f5c\u7528\uff0c\u8be5\u65b9\u6cd5\u5728\u53ef\u7a7f\u6234\u533b\u7597\u7cfb\u7edf\u4e2d\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2510.11170", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11170", "abs": "https://arxiv.org/abs/2510.11170", "authors": ["Daniel Scalena", "Leonidas Zotos", "Elisabetta Fersini", "Malvina Nissim", "Ahmet \u00dcst\u00fcn"], "title": "EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling", "comment": null, "summary": "With the rise of reasoning language models and test-time scaling methods as a\nparadigm for improving model performance, substantial computation is often\nrequired to generate multiple candidate sequences from the same prompt. This\nenables exploration of different reasoning paths toward the correct solution,\nhowever, allocates the same compute budget for each prompt. Grounded on the\nassumption that different prompts carry different degrees of complexity, and\nthus different computation needs, we propose EAGer, a training-free generation\nmethod that leverages model uncertainty through token-wise entropy distribution\nto reduce redundant computation and concurrently improve overall performance.\nEAGer allows branching to multiple reasoning paths only in the presence of\nhigh-entropy tokens, and then reallocates the saved compute budget to the\ninstances where exploration of alternative paths is most needed. We find that\nacross multiple open-source models on complex reasoning benchmarks such as AIME\n2025, EAGer can reallocate the budget without accessing target labels,\nachieving the best efficiency-performance trade-off in terms of reasoning\nlength and Pass@k. When target labels are accessible, EAGer generates up to 65%\nfewer tokens (hence saving compute) and achieves up to 37% improvement in\nPass@k compared to the Full Parallel Sampling.", "AI": {"tldr": "EAGer\u662f\u4e00\u79cd\u8bad\u7ec3\u81ea\u7531\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8etoken\u71b5\u5206\u5e03\u6765\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u9884\u7b97\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u5e76\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u4e0d\u540c\u63d0\u793a\u5177\u6709\u4e0d\u540c\u7684\u590d\u6742\u5ea6\uff0c\u9700\u8981\u4e0d\u540c\u7684\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5bf9\u6240\u6709\u63d0\u793a\u5206\u914d\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5229\u7528\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7token-wise\u71b5\u5206\u5e03\u6765\u8bc6\u522b\u9ad8\u71b5token\uff0c\u53ea\u5728\u9700\u8981\u65f6\u5206\u652f\u5230\u591a\u4e2a\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u91cd\u65b0\u5206\u914d\u8282\u7701\u7684\u8ba1\u7b97\u9884\u7b97\u3002", "result": "\u5728AIME 2025\u7b49\u590d\u6742\u63a8\u7406\u57fa\u51c6\u4e0a\uff0cEAGer\u65e0\u9700\u8bbf\u95ee\u76ee\u6807\u6807\u7b7e\u5373\u53ef\u91cd\u65b0\u5206\u914d\u9884\u7b97\uff0c\u5728\u63a8\u7406\u957f\u5ea6\u548cPass@k\u65b9\u9762\u8fbe\u5230\u6700\u4f73\u6548\u7387-\u6027\u80fd\u5e73\u8861\u3002\u6709\u76ee\u6807\u6807\u7b7e\u65f6\uff0c\u53ef\u51cf\u5c1165% token\u751f\u6210\u5e76\u63d0\u534737% Pass@k\u3002", "conclusion": "EAGer\u901a\u8fc7\u57fa\u4e8e\u71b5\u7684\u52a8\u6001\u8ba1\u7b97\u5206\u914d\uff0c\u6709\u6548\u63d0\u5347\u4e86\u63a8\u7406\u6a21\u578b\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u3002"}}
{"id": "2510.11062", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.11062", "abs": "https://arxiv.org/abs/2510.11062", "authors": ["Yujie Zhao", "Lanxiang Hu", "Yang Wang", "Minmin Hou", "Hao Zhang", "Ke Ding", "Jishen Zhao"], "title": "Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs", "comment": null, "summary": "Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to\nenhance the agentic capabilities of large language models (LLMs). MAS improves\ntask performance through role-based orchestration, while RL uses environmental\nrewards to learn stronger policies, such as GRPO-style optimization. However,\napplying on-policy RL to MAS remains underexplored and presents unique\nchallenges. Algorithmically, standard GRPO grouping assumptions break down\nbecause prompts vary by role and by turn. System-wise, the training stack must\nsupport MAS-workflow rollouts and on-policy updates for both single-policy and\nmulti-policy models.\n  We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL\nalgorithm tailored to MAS and (ii) a training system that supports both single-\nand multi-policy regimes. Across game, planning, coding, and math tasks,\nAT-GRPO delivers substantial gains. On long-horizon planning, it increases\naccuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5\npercent. It also improves reasoning performance, with average gains of 3.87 to\n7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and\nenvironments are available at: https://github.com/pettingllms-ai/PettingLLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86AT-GRPO\u65b9\u6cd5\uff0c\u9488\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u6311\u6218\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u548c\u8f6e\u6b21\u5206\u7ec4\u7b97\u6cd5\u6539\u8fdb\u8bad\u7ec3\uff0c\u5728\u6e38\u620f\u3001\u89c4\u5212\u3001\u7f16\u7a0b\u548c\u6570\u5b66\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u5f3a\u5316\u5b66\u4e60\u88ab\u5e7f\u6cdb\u7528\u4e8e\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u80fd\u529b\uff0c\u4f46\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5e94\u7528\u5728\u7ebf\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u4ecd\u5b58\u5728\u7b97\u6cd5\u548c\u7cfb\u7edf\u5c42\u9762\u7684\u6311\u6218\u3002", "method": "AT-GRPO\u5305\u62ec\uff1a(i) \u9488\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b9a\u5236\u7684\u667a\u80fd\u4f53\u548c\u8f6e\u6b21\u5206\u7ec4\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff1b(ii) \u652f\u6301\u5355\u7b56\u7565\u548c\u591a\u7b56\u7565\u6a21\u578b\u7684\u8bad\u7ec3\u7cfb\u7edf\u3002", "result": "\u5728\u957f\u671f\u89c4\u5212\u4efb\u52a1\u4e2d\uff0c\u51c6\u786e\u7387\u4ece14.0-47.0%\u63d0\u5347\u81f396.0-99.5%\uff1b\u7f16\u7a0b\u4efb\u52a1\u5e73\u5747\u63d0\u53473.87-7.62%\uff1b\u6570\u5b66\u4efb\u52a1\u5e73\u5747\u63d0\u53479.0-17.93%\u3002", "conclusion": "AT-GRPO\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.11184", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11184", "abs": "https://arxiv.org/abs/2510.11184", "authors": ["Zhengyu Chen", "Jinluan Yang", "Teng Xiao", "Ruochen Zhou", "Luan Zhang", "Xiangyu Xi", "Xiaowei Shi", "Wei Wang", "Jinggang Wang"], "title": "Can Tool-Integrated Reinforcement Learning Generalize Across Diverse Domains?", "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities in reasoning and tool utilization. However, the generalization of\ntool-augmented reinforcement learning (RL) across diverse domains remains\nunderexplored. In this work, we investigate the cross-domain generalization of\nan LLM agent equipped with a code interpreter tool, which is exclusively\ntrained on mathematical problem-solving tasks. Despite the restricted training\ndomain, we evaluate the agent's performance across several distinct reasoning\ndomains. The results reveal that RL-based tool usage learned from mathematical\ntasks can be effectively transferred to complex tasks in other domains,\nenabling great task performance and high token efficiency. To facilitate this\ncross-domain transfer, we propose a Tool Generalization Reinforcement Learning\n(TGRL) framework designed to promote domain-agnostic learning and skill\nmigration, encompassing: (i) a standardized tool interface that abstracts\ndomain-specific nuances through consistent formatting and explicit termination,\nfostering transferable invocation patterns; (ii) a dual-component reward system\nthat decomposes rewards to incentivize generalizable behaviors like tool\nefficiency and reasoning abstraction, ensuring alignment and robustness across\ndomain shifts; and (iii) an XML-based prompt template that separates thinking,\ntool calls, and responses to encourage modular, domain-invariant planning and\ncoherent multi-turn interactions. Extensive experiments across diverse\nbenchmarks validate our approach, achieving state-of-the-art performance and\nhighlighting the cross-domain potential of Tool RL for LLM reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u5177\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u5728\u8de8\u9886\u57df\u6cdb\u5316\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u4e86Tool Generalization Reinforcement Learning (TGRL)\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6570\u5b66\u4efb\u52a1\u4e0a\u8bad\u7ec3\u7684\u4ee3\u7801\u89e3\u91ca\u5668\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86\u5411\u5176\u4ed6\u63a8\u7406\u9886\u57df\u7684\u6709\u6548\u8fc1\u79fb\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5de5\u5177\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u5728\u4e0d\u540c\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u4ece\u6570\u5b66\u4efb\u52a1\u5b66\u4e60\u7684\u5de5\u5177\u4f7f\u7528\u6280\u80fd\u80fd\u5426\u6709\u6548\u8fc1\u79fb\u5230\u5176\u4ed6\u590d\u6742\u63a8\u7406\u9886\u57df\u3002", "method": "\u63d0\u51fa\u4e86TGRL\u6846\u67b6\uff0c\u5305\u62ec\uff1a\u6807\u51c6\u5316\u7684\u5de5\u5177\u63a5\u53e3\u3001\u53cc\u7ec4\u4ef6\u5956\u52b1\u7cfb\u7edf\u548cXML\u63d0\u793a\u6a21\u677f\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u62bd\u8c61\u9886\u57df\u7279\u5b9a\u7ec6\u8282\u3001\u5206\u89e3\u5956\u52b1\u6fc0\u52b1\u53ef\u6cdb\u5316\u884c\u4e3a\uff0c\u4ee5\u53ca\u6a21\u5757\u5316\u89c4\u5212\u6765\u4fc3\u8fdb\u8de8\u9886\u57df\u6280\u80fd\u8fc1\u79fb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u6570\u5b66\u4efb\u52a1\u8bad\u7ec3\u7684RL\u5de5\u5177\u4f7f\u7528\u53ef\u4ee5\u6709\u6548\u5730\u8fc1\u79fb\u5230\u5176\u4ed6\u9886\u57df\u7684\u590d\u6742\u4efb\u52a1\u4e2d\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u4efb\u52a1\u6027\u80fd\u548c\u9ad8\u6548\u7684token\u4f7f\u7528\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u5de5\u5177\u5f3a\u5316\u5b66\u4e60\u5177\u6709\u663e\u8457\u7684\u8de8\u9886\u57df\u6cdb\u5316\u6f5c\u529b\uff0cTGRL\u6846\u67b6\u6210\u529f\u4fc3\u8fdb\u4e86\u9886\u57df\u65e0\u5173\u7684\u5b66\u4e60\u548c\u6280\u80fd\u8fc1\u79fb\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u9886\u57df\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.11068", "categories": ["cs.LG", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.11068", "abs": "https://arxiv.org/abs/2510.11068", "authors": ["Xinyu Luo", "Jie Liu", "Kecheng Chen", "Junyi Yang", "Bo Ding", "Arindam Basu", "Haoliang Li"], "title": "Efficient Edge Test-Time Adaptation via Latent Feature Coordinate Correction", "comment": "Under review", "summary": "Edge devices face significant challenges due to limited computational\nresources and distribution shifts, making efficient and adaptable machine\nlearning essential. Existing test-time adaptation (TTA) methods often rely on\ngradient-based optimization or batch processing, which are inherently\nunsuitable for resource-constrained edge scenarios due to their reliance on\nbackpropagation and high computational demands. Gradient-free alternatives\naddress these issues but often suffer from limited learning capacity, lack\nflexibility, or impose architectural constraints. To overcome these\nlimitations, we propose a novel single-instance TTA method tailored for edge\ndevices (TED), which employs forward-only coordinate optimization in the\nprincipal subspace of latent using the covariance matrix adaptation evolution\nstrategy (CMA-ES). By updating a compact low-dimensional vector, TED not only\nenhances output confidence but also aligns the latent representation closer to\nthe source latent distribution within the latent principal subspace. This is\nachieved without backpropagation, keeping the model parameters frozen, and\nenabling efficient, forgetting-free adaptation with minimal memory and\ncomputational overhead. Experiments on image classification and keyword\nspotting tasks across the ImageNet and Google Speech Commands series datasets\ndemonstrate that TED achieves state-of-the-art performance while\n$\\textit{reducing computational complexity by up to 63 times}$, offering a\npractical and scalable solution for real-world edge applications. Furthermore,\nwe successfully $\\textit{deployed TED on the ZYNQ-7020 platform}$,\ndemonstrating its feasibility and effectiveness for resource-constrained edge\ndevices in real-world deployments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u8fb9\u7f18\u8bbe\u5907\u7684\u5355\u5b9e\u4f8b\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5TED\uff0c\u4f7f\u7528CMA-ES\u5728\u6f5c\u5728\u4e3b\u5b50\u7a7a\u95f4\u8fdb\u884c\u524d\u5411\u5750\u6807\u4f18\u5316\uff0c\u65e0\u9700\u53cd\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u4f4e63\u500d\uff0c\u5728ZYNQ-7020\u5e73\u53f0\u4e0a\u6210\u529f\u90e8\u7f72\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u4e14\u9762\u4e34\u5206\u5e03\u504f\u79fb\uff0c\u73b0\u6709TTA\u65b9\u6cd5\u4f9d\u8d56\u68af\u5ea6\u4f18\u5316\u6216\u6279\u5904\u7406\uff0c\u4e0d\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u573a\u666f\u3002\u68af\u5ea6\u81ea\u7531\u66ff\u4ee3\u65b9\u6848\u5b58\u5728\u5b66\u4e60\u80fd\u529b\u6709\u9650\u3001\u7075\u6d3b\u6027\u4e0d\u8db3\u6216\u67b6\u6784\u7ea6\u675f\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u534f\u65b9\u5dee\u77e9\u9635\u81ea\u9002\u5e94\u8fdb\u5316\u7b56\u7565(CMA-ES)\u5728\u6f5c\u5728\u4e3b\u5b50\u7a7a\u95f4\u8fdb\u884c\u524d\u5411\u5750\u6807\u4f18\u5316\uff0c\u901a\u8fc7\u66f4\u65b0\u7d27\u51d1\u7684\u4f4e\u7ef4\u5411\u91cf\u6765\u589e\u5f3a\u8f93\u51fa\u7f6e\u4fe1\u5ea6\u5e76\u4f7f\u6f5c\u5728\u8868\u793a\u66f4\u63a5\u8fd1\u6e90\u6f5c\u5728\u5206\u5e03\uff0c\u6a21\u578b\u53c2\u6570\u4fdd\u6301\u51bb\u7ed3\u3002", "result": "\u5728ImageNet\u548cGoogle Speech Commands\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTED\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u4f4e\u9ad8\u8fbe63\u500d\uff0c\u5e76\u5728ZYNQ-7020\u5e73\u53f0\u4e0a\u6210\u529f\u90e8\u7f72\u3002", "conclusion": "TED\u4e3a\u8fb9\u7f18\u8bbe\u5907\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u5b9e\u9645\u8fb9\u7f18\u5e94\u7528\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u65e0\u9057\u5fd8\u7684\u81ea\u9002\u5e94\uff0c\u5177\u6709\u6700\u5c0f\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2510.11278", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.11278", "abs": "https://arxiv.org/abs/2510.11278", "authors": ["Gareth Seneque", "Lap-Hang Ho", "Nafise Erfanian Saeedi", "Jeffrey Molendijk", "Ariel Kupermann", "Tim Elson"], "title": "ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models", "comment": "52 pages, 10 figures", "summary": "We present Entropic Mutual-Information Geometry Large-Language Model\nAlignment (ENIGMA), a novel approach to Large-Language Model (LLM) training\nthat jointly improves reasoning, alignment and robustness by treating an\norganisation's policies/principles as directions to move on a model's\ninformation manifold. Our single-loop trainer combines Group-Relative Policy\nOptimisation (GRPO), an on-policy, critic-free RL method with Chain-of-Thought\n(CoT)-format only rewards; a Self-Supervised Alignment with Mutual Information\n(SAMI)-style symmetric InfoNCE auxiliary; and an entropic Sinkhorn\noptimal-transport regulariser on hidden-state distributions to bound geometry\ndrift. We also introduce infoNCE metrics that specialise to a standard MI lower\nbound under matched negatives to measure how strongly a model's CoT encodes\nthese policies. These metrics include a Sufficiency Index (SI) that enables the\nselection and creation of principles that maximise downstream performance prior\nto training. In our experiments using small (1B) LLMs, high-SI principles\npredict steadier training dynamics and improved benchmark performance over GRPO\nablations. Our information-geometry analysis of trained models validates\ndesirable structural change in the manifold. These results support our\nhypothesis that reasoning, alignment, and robustness are projections of a\nsingle informationgeometric objective, and that models trained using ENIGMA\ndemonstrate principled reasoning without the use of a reward model, offering a\npath to trusted capability", "AI": {"tldr": "ENIGMA\u662f\u4e00\u79cd\u65b0\u9896\u7684LLM\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7ec4\u7ec7\u653f\u7b56\u89c6\u4e3a\u4fe1\u606f\u6d41\u5f62\u4e0a\u7684\u65b9\u5411\u6765\u8054\u5408\u6539\u8fdb\u63a8\u7406\u3001\u5bf9\u9f50\u548c\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86GRPO\u5f3a\u5316\u5b66\u4e60\u3001SAMI\u4fe1\u606f\u8f85\u52a9\u548cSinkhorn\u6b63\u5219\u5316\uff0c\u65e0\u9700\u5956\u52b1\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u539f\u5219\u6027\u63a8\u7406\u3002", "motivation": "\u5f53\u524dLLM\u8bad\u7ec3\u4e2d\u63a8\u7406\u3001\u5bf9\u9f50\u548c\u9c81\u68d2\u6027\u901a\u5e38\u662f\u5206\u5f00\u5904\u7406\u7684\uff0c\u4f5c\u8005\u5047\u8bbe\u8fd9\u4e9b\u662f\u5355\u4e00\u4fe1\u606f\u51e0\u4f55\u76ee\u6807\u7684\u4e0d\u540c\u6295\u5f71\uff0c\u5e0c\u671b\u901a\u8fc7\u7edf\u4e00\u7684\u4fe1\u606f\u51e0\u4f55\u65b9\u6cd5\u540c\u65f6\u6539\u8fdb\u8fd9\u4e9b\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5355\u5faa\u73af\u8bad\u7ec3\u5668\u7ed3\u5408\uff1a1) GRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u4ec5\u4f7f\u7528CoT\u683c\u5f0f\u5956\u52b1\uff09\uff1b2) SAMI\u98ce\u683c\u7684\u5bf9\u79f0InfoNCE\u8f85\u52a9\uff1b3) \u9690\u85cf\u72b6\u6001\u5206\u5e03\u4e0a\u7684Sinkhorn\u6700\u4f18\u4f20\u8f93\u6b63\u5219\u5316\uff1b4) \u5f15\u5165infoNCE\u6307\u6807\u548c\u5145\u5206\u6027\u6307\u6570\u6765\u9009\u62e9\u4f18\u5316\u539f\u5219\u3002", "result": "\u57281B\u53c2\u6570LLM\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u9ad8SI\u539f\u5219\u9884\u6d4b\u4e86\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u52a8\u6001\u548c\u66f4\u597d\u7684\u57fa\u51c6\u6027\u80fd\u3002\u4fe1\u606f\u51e0\u4f55\u5206\u6790\u9a8c\u8bc1\u4e86\u6d41\u5f62\u4e0a\u7684\u7406\u60f3\u7ed3\u6784\u53d8\u5316\u3002", "conclusion": "ENIGMA\u652f\u6301\u4e86\u63a8\u7406\u3001\u5bf9\u9f50\u548c\u9c81\u68d2\u6027\u662f\u5355\u4e00\u4fe1\u606f\u51e0\u4f55\u76ee\u6807\u6295\u5f71\u7684\u5047\u8bbe\uff0c\u65e0\u9700\u5956\u52b1\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u539f\u5219\u6027\u63a8\u7406\uff0c\u4e3a\u53ef\u4fe1\u80fd\u529b\u63d0\u4f9b\u4e86\u8def\u5f84\u3002"}}
{"id": "2510.11084", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11084", "abs": "https://arxiv.org/abs/2510.11084", "authors": ["Wonah Kim", "Jeonghyeon Park", "Dongsan Jun", "Jungkyu Han", "Sejin Chun"], "title": "Causal Disentanglement Learning for Accurate Anomaly Detection in Multivariate Time Series", "comment": "20 pages, 4 Figures,", "summary": "Disentangling complex causal relationships is important for accurate\ndetection of anomalies. In multivariate time series analysis, dynamic\ninteractions among data variables over time complicate the interpretation of\ncausal relationships. Traditional approaches assume statistical independence\nbetween variables in unsupervised settings, whereas recent methods capture\nfeature correlations through graph representation learning. However, their\nrepresentations fail to explicitly infer the causal relationships over\ndifferent time periods. To solve the problem, we propose Causally Disentangled\nRepresentation Learning for Anomaly Detection (CDRL4AD) to detect anomalies and\nidentify their causal relationships in multivariate time series. First, we\ndesign the causal process as model input, the temporal heterogeneous graph, and\ncausal relationships. Second, our representation identifies causal\nrelationships over different time periods and disentangles latent variables to\ninfer the corresponding causal factors. Third, our experiments on real-world\ndatasets demonstrate that CDRL4AD outperforms state-of-the-art methods in terms\nof accuracy and root cause analysis. Fourth, our model analysis validates\nhyperparameter sensitivity and the time complexity of CDRL4AD. Last, we conduct\na case study to show how our approach assists human experts in diagnosing the\nroot causes of anomalies.", "AI": {"tldr": "\u63d0\u51faCDRL4AD\u65b9\u6cd5\uff0c\u901a\u8fc7\u56e0\u679c\u89e3\u7f20\u8868\u793a\u5b66\u4e60\u6765\u68c0\u6d4b\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5f02\u5e38\u5e76\u8bc6\u522b\u5176\u56e0\u679c\u5173\u7cfb\uff0c\u5728\u51c6\u786e\u6027\u548c\u6839\u56e0\u5206\u6790\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u4e2d\u53d8\u91cf\u95f4\u7684\u52a8\u6001\u4ea4\u4e92\u4f7f\u5f97\u56e0\u679c\u5173\u7cfb\u89e3\u91ca\u590d\u6742\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u660e\u786e\u63a8\u65ad\u4e0d\u540c\u65f6\u95f4\u6bb5\u7684\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u8bbe\u8ba1\u65f6\u95f4\u5f02\u6784\u56fe\u548c\u56e0\u679c\u5173\u7cfb\u4f5c\u4e3a\u6a21\u578b\u8f93\u5165\uff0c\u901a\u8fc7\u8868\u793a\u5b66\u4e60\u8bc6\u522b\u4e0d\u540c\u65f6\u95f4\u6bb5\u7684\u56e0\u679c\u5173\u7cfb\u5e76\u89e3\u7f20\u6f5c\u5728\u53d8\u91cf\u4ee5\u63a8\u65ad\u56e0\u679c\u56e0\u7d20\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCDRL4AD\u5728\u51c6\u786e\u6027\u548c\u6839\u56e0\u5206\u6790\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u5728\u8f85\u52a9\u4e13\u5bb6\u8bca\u65ad\u5f02\u5e38\u6839\u56e0\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "CDRL4AD\u6210\u529f\u89e3\u51b3\u4e86\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u4e2d\u56e0\u679c\u5173\u7cfb\u63a8\u65ad\u7684\u6311\u6218\uff0c\u4e3a\u5f02\u5e38\u68c0\u6d4b\u548c\u6839\u56e0\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.11498", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11498", "abs": "https://arxiv.org/abs/2510.11498", "authors": ["Yuhang Li", "Chenchen Zhang", "Ruilin Lv", "Ao Liu", "Ken Deng", "Yuanxing Zhang", "Jiaheng Liu", "Wiggin Zhou", "Bo Zhou"], "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding", "comment": null, "summary": "While Large Language Models (LLMs) excel at algorithmic code generation, they\nstruggle with front-end development, where correctness is judged on rendered\npixels and interaction. We present ReLook, an agentic, vision-grounded\nreinforcement learning framework that empowers an agent to close a robust\ngenerate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.\nDuring training, the agent uses the MLLM-in-the-loop both as a visual\ncritic--scoring code with screenshots--and as a source of actionable,\nvision-grounded feedback; a strict zero-reward rule for invalid renders anchors\nrenderability and prevents reward hacking. To prevent behavioral collapse, we\nintroduce Forced Optimization, a strict acceptance rule that admits only\nimproving revisions, yielding monotonically better trajectories. At inference,\nwe decouple the critic and run a lightweight, critic-free self-edit cycle,\nkeeping latency comparable to base decoding while retaining most of the gains.\nAcross three widely used benchmarks, ReLook consistently outperforms strong\nbaselines in vision-grounded front-end code generation, highlighting the\nbenefits of agentic perception, visual rewards, and training-inference\ndecoupling.", "AI": {"tldr": "ReLook\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLM)\u5b9e\u73b0\u524d\u7aef\u4ee3\u7801\u7684\u751f\u6210-\u8bca\u65ad-\u4f18\u5316\u95ed\u73af\uff0c\u663e\u8457\u63d0\u5347\u524d\u7aef\u5f00\u53d1\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7b97\u6cd5\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u524d\u7aef\u5f00\u53d1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u524d\u7aef\u4ee3\u7801\u7684\u6b63\u786e\u6027\u9700\u8981\u901a\u8fc7\u6e32\u67d3\u50cf\u7d20\u548c\u4ea4\u4e92\u6765\u5224\u65ad\u3002", "method": "\u91c7\u7528\u4ee3\u7406\u5f0f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06MLLM\u4f5c\u4e3a\u5de5\u5177\u8fdb\u884c\u89c6\u89c9\u6279\u8bc4\u548c\u53cd\u9988\uff0c\u5f15\u5165\u5f3a\u5236\u4f18\u5316\u89c4\u5219\u786e\u4fdd\u5355\u8c03\u6539\u8fdb\uff0c\u5728\u63a8\u7406\u65f6\u89e3\u8026\u6279\u8bc4\u5668\u8fd0\u884c\u8f7b\u91cf\u7ea7\u81ea\u7f16\u8f91\u5faa\u73af\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReLook\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u524d\u7aef\u4ee3\u7801\u751f\u6210\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ReLook\u5c55\u793a\u4e86\u4ee3\u7406\u611f\u77e5\u3001\u89c6\u89c9\u5956\u52b1\u548c\u8bad\u7ec3-\u63a8\u7406\u89e3\u8026\u5728\u89c6\u89c9\u57fa\u7840\u524d\u7aef\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2510.11110", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11110", "abs": "https://arxiv.org/abs/2510.11110", "authors": ["Cheol-Hui Lee", "Hwa-Yeon Lee", "Min-Kyung Jung", "Dong-Joo Kim"], "title": "PhysioME: A Robust Multimodal Self-Supervised Framework for Physiological Signals with Missing Modalities", "comment": "9 pages, 2 figures", "summary": "Missing or corrupted modalities are common in physiological signal-based\nmedical applications owing to hardware constraints or motion artifacts.\nHowever, most existing methods assume the availability of all modalities,\nresulting in substantial performance degradation in the absence of any\nmodality. To overcome this limitation, this study proposes PhysioME, a robust\nframework designed to ensure reliable performance under missing modality\nconditions. PhysioME adopts: (1) a multimodal self-supervised learning approach\nthat combines contrastive learning with masked prediction; (2) a\nDual-PathNeuroNet backbone tailored to capture the temporal dynamics of each\nphysiological signal modality; and (3) a restoration decoder that reconstructs\nmissing modality tokens, enabling flexible processing of incomplete inputs. The\nexperimental results show that PhysioME achieves high consistency and\ngeneralization performance across various missing modality scenarios. These\nfindings highlight the potential of PhysioME as a reliable tool for supporting\nclinical decision-making in real-world settings with imperfect data\navailability.", "AI": {"tldr": "PhysioME\u662f\u4e00\u4e2a\u7528\u4e8e\u5904\u7406\u751f\u7406\u4fe1\u53f7\u4e2d\u6a21\u6001\u7f3a\u5931\u95ee\u9898\u7684\u9c81\u68d2\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u53cc\u8def\u5f84\u7f51\u7edc\u548c\u6062\u590d\u89e3\u7801\u5668\u6765\u786e\u4fdd\u5728\u6a21\u6001\u7f3a\u5931\u60c5\u51b5\u4e0b\u7684\u53ef\u9760\u6027\u80fd\u3002", "motivation": "\u751f\u7406\u4fe1\u53f7\u533b\u7597\u5e94\u7528\u4e2d\u5e38\u56e0\u786c\u4ef6\u9650\u5236\u6216\u8fd0\u52a8\u4f2a\u5f71\u5bfc\u81f4\u6a21\u6001\u7f3a\u5931\uff0c\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u6240\u6709\u6a21\u6001\u90fd\u53ef\u7528\uff0c\u5728\u6a21\u6001\u7f3a\u5931\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\uff08\u5bf9\u6bd4\u5b66\u4e60+\u63a9\u7801\u9884\u6d4b\uff09\u3001\u53cc\u8def\u5f84\u7f51\u7edc\u4e3b\u5e72\u6355\u83b7\u751f\u7406\u4fe1\u53f7\u65f6\u5e8f\u52a8\u6001\u3001\u6062\u590d\u89e3\u7801\u5668\u91cd\u5efa\u7f3a\u5931\u6a21\u6001\u4ee4\u724c\u3002", "result": "PhysioME\u5728\u5404\u79cd\u6a21\u6001\u7f3a\u5931\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "PhysioME\u6709\u6f5c\u529b\u6210\u4e3a\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u4e0d\u5b8c\u7f8e\u60c5\u51b5\u4e0b\u7684\u53ef\u9760\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5de5\u5177\u3002"}}
{"id": "2510.11683", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11683", "abs": "https://arxiv.org/abs/2510.11683", "authors": ["Nianyi Lin", "Jiajie Zhang", "Lei Hou", "Juanzi Li"], "title": "Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models", "comment": null, "summary": "A key challenge in applying reinforcement learning (RL) to diffusion large\nlanguage models (dLLMs) lies in the intractability of their likelihood\nfunctions, which are essential for the RL objective, necessitating\ncorresponding approximation in each training step. While existing methods\napproximate the log-likelihoods by their evidence lower bounds (ELBOs) via\ncustomized Monte Carlo (MC) sampling, the forward computational graphs of all\nMC samples need to be retained for the gradient computation of non-linear terms\nin the RL objective, resulting in significant memory overhead. This constraint\nrestricts feasible sample sizes, leading to imprecise likelihood approximations\nand ultimately distorting the RL objective. To overcome this limitation, we\npropose \\emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient\nRL algorithm that maximizes a specially constructed lower bound of the\nELBO-based objective. This lower bound is carefully designed to satisfy two key\nproperties: (1) Linearity: it is formulated in a linear sum where each term\ndepends only on a single MC sample, thereby enabling gradient accumulation\nacross samples and ensuring constant memory usage; (2) Equivalence: Both the\nvalue and gradient of this lower bound are equal to those of the ELBO-based\nobjective in on-policy training, making it also an effective approximation for\nthe original RL objective. These properties allow BGPO to adopt a large MC\nsample size, resulting in more accurate likelihood approximations and improved\nRL objective estimation, which in turn leads to enhanced performance.\nExperiments show that BGPO significantly outperforms previous RL algorithms for\ndLLMs in math problem solving, code generation, and planning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86BGPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7279\u6b8a\u7684ELBO\u4e0b\u754c\u6765\u89e3\u51b3\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578bRL\u8bad\u7ec3\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u6052\u5b9a\u5185\u5b58\u4f7f\u7528\u548c\u66f4\u51c6\u786e\u7684\u76ee\u6807\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684RL\u8bad\u7ec3\u4e2d\uff0c\u7531\u4e8e\u9700\u8981\u4fdd\u7559\u6240\u6709\u8499\u7279\u5361\u6d1b\u6837\u672c\u7684\u524d\u5411\u8ba1\u7b97\u56fe\u6765\u8ba1\u7b97\u68af\u5ea6\uff0c\u5bfc\u81f4\u5185\u5b58\u5f00\u9500\u5de8\u5927\uff0c\u9650\u5236\u4e86\u6837\u672c\u6570\u91cf\uff0c\u5f71\u54cd\u4f3c\u7136\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u8fb9\u754c\u5f15\u5bfc\u7b56\u7565\u4f18\u5316(BGPO)\uff0c\u6784\u5efa\u6ee1\u8db3\u7ebf\u6027\u548c\u7b49\u4ef7\u6027\u4e24\u4e2a\u5173\u952e\u7279\u6027\u7684ELBO\u4e0b\u754c\uff0c\u4f7f\u6bcf\u4e2a\u9879\u4ec5\u4f9d\u8d56\u5355\u4e2aMC\u6837\u672c\uff0c\u652f\u6301\u68af\u5ea6\u7d2f\u79ef\u548c\u6052\u5b9a\u5185\u5b58\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660eBGPO\u5728\u6570\u5b66\u95ee\u9898\u6c42\u89e3\u3001\u4ee3\u7801\u751f\u6210\u548c\u89c4\u5212\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684dLLM RL\u7b97\u6cd5\u3002", "conclusion": "BGPO\u901a\u8fc7\u5185\u5b58\u9ad8\u6548\u7684\u8bbe\u8ba1\u89e3\u51b3\u4e86dLLM RL\u8bad\u7ec3\u7684\u5173\u952e\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u4f3c\u7136\u8fd1\u4f3c\u548cRL\u76ee\u6807\u4f30\u8ba1\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2510.11121", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11121", "abs": "https://arxiv.org/abs/2510.11121", "authors": ["Rongjie Zhu", "Cong Zhang", "Zhiguang Cao"], "title": "Refining Hybrid Genetic Search for CVRP via Reinforcement Learning-Finetuned LLM", "comment": null, "summary": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini.", "AI": {"tldr": "\u63d0\u51faRFTHGS\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u5c0f\u578b\u4e13\u7528LLM\uff0c\u4e3a\u6df7\u5408\u9057\u4f20\u641c\u7d22\u6c42\u89e3\u5668\u751f\u6210\u8d85\u8d8a\u4e13\u5bb6\u8bbe\u8ba1\u7684\u4ea4\u53c9\u7b97\u5b50\uff0c\u5728\u5bb9\u91cf\u7ea6\u675f\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6311\u6218\u5f53\u524d\u4f9d\u8d56\u5927\u578b\u901a\u7528LLM\uff08\u5982GPT-4\uff09\u4f5c\u4e3a\u542f\u53d1\u5f0f\u8bbe\u8ba1\u5668\u7684\u8303\u5f0f\uff0c\u8bc1\u660e\u7ecf\u8fc7\u7cbe\u5fc3\u5fae\u8c03\u7684\u4e13\u7528\u5c0f\u578bLLM\u53ef\u4ee5\u751f\u6210\u8d85\u8d8a\u4e13\u5bb6\u624b\u5de5\u8bbe\u8ba1\u7684\u7ec4\u4ef6\u3002", "method": "\u4f7f\u7528\u591a\u5c42\u7ea7\u8bfe\u7a0b\u5f0f\u5956\u52b1\u51fd\u6570\uff0c\u9010\u6b65\u5f15\u5bfcLLM\u751f\u6210\u53ef\u7f16\u8bd1\u3001\u53ef\u6267\u884c\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u4ea4\u53c9\u7b97\u5b50\uff0c\u7ed3\u5408\u7b97\u5b50\u7f13\u5b58\u673a\u5236\u9632\u6b62\u6284\u88ad\u5e76\u4fc3\u8fdb\u591a\u6837\u6027\u3002", "result": "\u5fae\u8c03\u540e\u7684LLM\u751f\u6210\u7684\u4ea4\u53c9\u7b97\u5b50\u5728HGS\u6c42\u89e3\u5668\u4e2d\u663e\u8457\u4f18\u4e8e\u4e13\u5bb6\u8bbe\u8ba1\u7684\u7b97\u5b50\uff0c\u6027\u80fd\u4f18\u52bf\u4ece1000\u8282\u70b9\u7684\u5927\u89c4\u6a21\u95ee\u9898\u4fdd\u6301\u4e00\u81f4\uff0c\u8d85\u8d8a\u795e\u7ecf\u7ec4\u5408\u57fa\u7ebf\u3001\u63d0\u793a\u65b9\u6cd5\u548c\u5546\u4e1aLLM\u3002", "conclusion": "\u4e13\u7528\u5c0f\u578bLLM\u7ecf\u8fc7\u9002\u5f53\u5fae\u8c03\u53ef\u4ee5\u8d85\u8d8a\u5927\u578b\u901a\u7528\u6a21\u578b\uff0c\u4e3a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u9ad8\u6548\u7684\u542f\u53d1\u5f0f\u8bbe\u8ba1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.11696", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11696", "abs": "https://arxiv.org/abs/2510.11696", "authors": ["Wei Huang", "Yi Ge", "Shuai Yang", "Yicheng Xiao", "Huizi Mao", "Yujun Lin", "Hanrong Ye", "Sifei Liu", "Ka Chun Cheung", "Hongxu Yin", "Yao Lu", "Xiaojuan Qi", "Song Han", "Yukang Chen"], "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs", "comment": "Code is available at https://github.com/NVlabs/QeRL", "summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs.", "AI": {"tldr": "QeRL\u662f\u4e00\u4e2a\u91cf\u5316\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408NVFP4\u91cf\u5316\u548cLoRA\u6280\u672f\uff0c\u52a0\u901fLLM\u7684RL\u8bad\u7ec3\u8fc7\u7a0b\u5e76\u964d\u4f4e\u5185\u5b58\u5f00\u9500\uff0c\u540c\u65f6\u5229\u7528\u91cf\u5316\u566a\u58f0\u589e\u5f3a\u63a2\u7d22\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfRL\u8bad\u7ec3LLM\u9700\u8981\u5927\u91cfGPU\u5185\u5b58\u548c\u957f\u65f6\u95f4rollout\uff0c\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u9650\u5236\u4e86\u6a21\u578b\u89c4\u6a21\u548c\u5e94\u7528\u6548\u7387\u3002", "method": "\u7ed3\u5408NVFP4\u91cf\u5316\u548cLoRA\u6280\u672f\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u91cf\u5316\u566a\u58f0\u673a\u5236\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u566a\u58f0\u6c34\u5e73\u3002", "result": "\u5728rollout\u9636\u6bb5\u5b9e\u73b01.5\u500d\u52a0\u901f\uff0c\u9996\u6b21\u5728\u5355\u5f20H100 80GB GPU\u4e0a\u8bad\u7ec332B LLM\uff0c\u5728GSM8K\u548cMATH 500\u6570\u5b66\u57fa\u51c6\u4e0a\u5206\u522b\u8fbe\u523090.8%\u548c77.4%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "QeRL\u4e3aLLM\u7684RL\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u6709\u6548\u7684\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2510.11128", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11128", "abs": "https://arxiv.org/abs/2510.11128", "authors": ["Qiyi Tong", "Olivia Nocentini", "Marta Lagomarsino", "Kuanqi Cai", "Marta Lorenzini", "Arash Ajoudani"], "title": "Lightweight Facial Landmark Detection in Thermal Images via Multi-Level Cross-Modal Knowledge Transfer", "comment": null, "summary": "Facial Landmark Detection (FLD) in thermal imagery is critical for\napplications in challenging lighting conditions, but it is hampered by the lack\nof rich visual cues. Conventional cross-modal solutions, like feature fusion or\nimage translation from RGB data, are often computationally expensive or\nintroduce structural artifacts, limiting their practical deployment. To address\nthis, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a\nnovel framework that decouples high-fidelity RGB-to-thermal knowledge transfer\nfrom model compression to create both accurate and efficient thermal FLD\nmodels. A central challenge during knowledge transfer is the profound modality\ngap between RGB and thermal data, where traditional unidirectional distillation\nfails to enforce semantic consistency across disparate feature spaces. To\novercome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a\nbidirectional mechanism designed specifically for this task. DIKD establishes a\nconnection between modalities: it not only guides the thermal student with rich\nRGB features but also validates the student's learned representations by\nfeeding them back into the frozen teacher's prediction head. This closed-loop\nsupervision forces the student to learn modality-invariant features that are\nsemantically aligned with the teacher, ensuring a robust and profound knowledge\ntransfer. Experiments show that our approach sets a new state-of-the-art on\npublic thermal FLD benchmarks, notably outperforming previous methods while\ndrastically reducing computational overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86MLCM-KD\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u77e5\u8bc6\u84b8\u998f\u89e3\u51b3\u70ed\u6210\u50cf\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u4e2d\u7684\u6a21\u6001\u5dee\u5f02\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u70ed\u6210\u50cfFLD\u6a21\u578b\u3002", "motivation": "\u70ed\u6210\u50cf\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u5728\u6076\u52a3\u5149\u7167\u6761\u4ef6\u4e0b\u5f88\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u4e30\u5bcc\u7684\u89c6\u89c9\u7ebf\u7d22\u3002\u4f20\u7edf\u7684\u8de8\u6a21\u6001\u65b9\u6cd5\u8ba1\u7b97\u6602\u8d35\u6216\u4ea7\u751f\u7ed3\u6784\u4f2a\u5f71\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u591a\u7ea7\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f(MLCM-KD)\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u5411\u6ce8\u5165\u77e5\u8bc6\u84b8\u998f(DIKD)\u673a\u5236\uff0c\u4e0d\u4ec5\u7528RGB\u7279\u5f81\u6307\u5bfc\u5b66\u751f\uff0c\u8fd8\u5c06\u5b66\u751f\u8868\u5f81\u53cd\u9988\u7ed9\u6559\u5e08\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5f3a\u5236\u5b66\u4e60\u6a21\u6001\u4e0d\u53d8\u7279\u5f81\u3002", "result": "\u5728\u516c\u5f00\u70ed\u6210\u50cfFLD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u663e\u8457\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "MLCM-KD\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86RGB\u548c\u70ed\u6210\u50cf\u6570\u636e\u95f4\u7684\u6a21\u6001\u5dee\u5f02\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4e3a\u70ed\u6210\u50cf\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.11133", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11133", "abs": "https://arxiv.org/abs/2510.11133", "authors": ["Yingnan Liu", "Rui Qiao", "Mong Li Lee", "Wynne Hsu"], "title": "Test-Time Adaptation by Causal Trimming", "comment": "Accepted to the Thirty-Ninth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2025); Code is available at\n  https://github.com/NancyQuris/TACT", "summary": "Test-time adaptation aims to improve model robustness under distribution\nshifts by adapting models with access to unlabeled target samples. A primary\ncause of performance degradation under such shifts is the model's reliance on\nfeatures that lack a direct causal relationship with the prediction target. We\nintroduce Test-time Adaptation by Causal Trimming (TACT), a method that\nidentifies and removes non-causal components from representations for test\ndistributions. TACT applies data augmentations that preserve causal features\nwhile varying non-causal ones. By analyzing the changes in the representations\nusing Principal Component Analysis, TACT identifies the highest variance\ndirections associated with non-causal features. It trims the representations by\nremoving their projections on the identified directions, and uses the trimmed\nrepresentations for the predictions. During adaptation, TACT continuously\ntracks and refines these directions to get a better estimate of non-causal\nfeatures. We theoretically analyze the effectiveness of this approach and\nempirically validate TACT on real-world out-of-distribution benchmarks. TACT\nconsistently outperforms state-of-the-art methods by a significant margin.", "AI": {"tldr": "TACT\u662f\u4e00\u79cd\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u53bb\u9664\u8868\u793a\u4e2d\u7684\u975e\u56e0\u679c\u6210\u5206\u6765\u63d0\u5347\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u6a21\u578b\u6027\u80fd\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u4e0b\u964d\u7684\u4e3b\u8981\u539f\u56e0\u662f\u6a21\u578b\u4f9d\u8d56\u4e0e\u9884\u6d4b\u76ee\u6807\u6ca1\u6709\u76f4\u63a5\u56e0\u679c\u5173\u7cfb\u7684\u7279\u5f81\u3002", "method": "\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u4fdd\u7559\u56e0\u679c\u7279\u5f81\u540c\u65f6\u6539\u53d8\u975e\u56e0\u679c\u7279\u5f81\uff0c\u901a\u8fc7\u4e3b\u6210\u5206\u5206\u6790\u8bc6\u522b\u4e0e\u975e\u56e0\u679c\u7279\u5f81\u76f8\u5173\u7684\u9ad8\u65b9\u5dee\u65b9\u5411\uff0c\u5e76\u53bb\u9664\u8fd9\u4e9b\u65b9\u5411\u7684\u6295\u5f71\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTACT\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u56e0\u679c\u4fee\u526a\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u7684\u9002\u5e94\u80fd\u529b\uff0c\u6539\u5584\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.11140", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11140", "abs": "https://arxiv.org/abs/2510.11140", "authors": ["Zhijian Zhou", "Xunye Tian", "Liuhua Peng", "Chao Lei", "Antonin Schrab", "Danica J. Sutherland", "Feng Liu"], "title": "DUAL: Learning Diverse Kernels for Aggregated Two-sample and Independence Testing", "comment": null, "summary": "To adapt kernel two-sample and independence testing to complex structured\ndata, aggregation of multiple kernels is frequently employed to boost testing\npower compared to single-kernel tests. However, we observe a phenomenon that\ndirectly maximizing multiple kernel-based statistics may result in highly\nsimilar kernels that capture highly overlapping information, limiting the\neffectiveness of aggregation. To address this, we propose an aggregated\nstatistic that explicitly incorporates kernel diversity based on the covariance\nbetween different kernels. Moreover, we identify a fundamental challenge: a\ntrade-off between the diversity among kernels and the test power of individual\nkernels, i.e., the selected kernels should be both effective and diverse. This\nmotivates a testing framework with selection inference, which leverages\ninformation from the training phase to select kernels with strong individual\nperformance from the learned diverse kernel pool. We provide rigorous\ntheoretical statements and proofs to show the consistency on the test power and\ncontrol of Type-I error, along with asymptotic analysis of the proposed\nstatistics. Lastly, we conducted extensive empirical experiments demonstrating\nthe superior performance of our proposed approach across various benchmarks for\nboth two-sample and independence testing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6838\u591a\u6837\u6027\u7684\u805a\u5408\u7edf\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u4e0d\u540c\u6838\u4e4b\u95f4\u7684\u534f\u65b9\u5dee\u6765\u89e3\u51b3\u591a\u6838\u6d4b\u8bd5\u4e2d\u6838\u9009\u62e9\u8fc7\u4e8e\u76f8\u4f3c\u7684\u95ee\u9898\uff0c\u5e76\u5efa\u7acb\u4e86\u9009\u62e9\u63a8\u65ad\u6846\u67b6\u6765\u5e73\u8861\u6838\u7684\u591a\u6837\u6027\u548c\u4e2a\u4f53\u6d4b\u8bd5\u80fd\u529b\u3002", "motivation": "\u5728\u591a\u6838\u4e24\u6837\u672c\u548c\u72ec\u7acb\u6027\u6d4b\u8bd5\u4e2d\uff0c\u76f4\u63a5\u6700\u5927\u5316\u57fa\u4e8e\u591a\u6838\u7684\u7edf\u8ba1\u91cf\u53ef\u80fd\u5bfc\u81f4\u9ad8\u5ea6\u76f8\u4f3c\u7684\u6838\uff0c\u8fd9\u4e9b\u6838\u6355\u83b7\u9ad8\u5ea6\u91cd\u53e0\u7684\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u805a\u5408\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u660e\u786e\u5305\u542b\u6838\u591a\u6837\u6027\u7684\u805a\u5408\u7edf\u8ba1\u91cf\uff0c\u57fa\u4e8e\u4e0d\u540c\u6838\u4e4b\u95f4\u7684\u534f\u65b9\u5dee\uff1b\u5efa\u7acb\u9009\u62e9\u63a8\u65ad\u6846\u67b6\uff0c\u4ece\u5b66\u4e60\u7684\u591a\u6837\u5316\u6838\u6c60\u4e2d\u9009\u62e9\u5177\u6709\u5f3a\u4e2a\u4f53\u6027\u80fd\u7684\u6838\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u6d4b\u8bd5\u529f\u6548\u7684\u4e00\u81f4\u6027\u548c\u7b2c\u4e00\u7c7b\u9519\u8bef\u7684\u63a7\u5236\uff0c\u4ee5\u53ca\u6240\u63d0\u51fa\u7edf\u8ba1\u91cf\u7684\u6e10\u8fd1\u5206\u6790\uff1b\u5927\u91cf\u5b9e\u8bc1\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u660e\u786e\u8003\u8651\u6838\u591a\u6837\u6027\u5e76\u5e73\u8861\u591a\u6837\u6027\u4e0e\u4e2a\u4f53\u6d4b\u8bd5\u80fd\u529b\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u6838\u4e24\u6837\u672c\u548c\u72ec\u7acb\u6027\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.11141", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11141", "abs": "https://arxiv.org/abs/2510.11141", "authors": ["Mohammad Karami", "Mostafa Jalali", "Fatemeh Ghassemi"], "title": "A Comprehensive Forecasting-Based Framework for Time Series Anomaly Detection: Benchmarking on the Numenta Anomaly Benchmark (NAB)", "comment": null, "summary": "Time series anomaly detection is critical for modern digital infrastructures,\nyet existing methods lack systematic cross-domain evaluation. We present a\ncomprehensive forecasting-based framework unifying classical methods\n(Holt-Winters, SARIMA) with deep learning architectures (LSTM, Informer) under\na common residual-based detection interface. Our modular pipeline integrates\npreprocessing (normalization, STL decomposition), four forecasting models, four\ndetection methods, and dual evaluation through forecasting metrics (MAE, RMSE,\nPCC) and detection metrics (Precision, Recall, F1, AUC). We conduct the first\ncomplete evaluation on the Numenta Anomaly Benchmark (58 datasets, 7\ncategories) with 232 model training runs and 464 detection evaluations\nachieving 100\\% success rate. LSTM achieves best performance (F1: 0.688,\nranking first or second on 81\\% of datasets) with exceptional correlation on\ncomplex patterns (PCC: 0.999). Informer provides competitive accuracy (F1:\n0.683) with 30\\% faster training. Classical methods achieve perfect predictions\non simple synthetic data with 60 lower cost but show 2-3 worse F1-scores on\nreal-world datasets. Forecasting quality dominates detection performance:\ndifferences between detection methods (F1: 0.621-0.688) are smaller than\nbetween forecasting models (F1: 0.344-0.688). Our findings provide\nevidence-based guidance: use LSTM for complex patterns, Informer for\nefficiency-critical deployments, and classical methods for simple periodic data\nwith resource constraints. The complete implementation and results establish\nbaselines for future forecasting-based anomaly detection research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u9884\u6d4b\u5f0f\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u5bf9\u7ecf\u5178\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u53d1\u73b0LSTM\u5728\u590d\u6742\u6a21\u5f0f\u4e0a\u8868\u73b0\u6700\u4f73\uff0cInformer\u5728\u6548\u7387\u4e0a\u6709\u4f18\u52bf\uff0c\u7ecf\u5178\u65b9\u6cd5\u9002\u7528\u4e8e\u7b80\u5355\u5468\u671f\u6027\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u7684\u8de8\u9886\u57df\u8bc4\u4f30\uff0c\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u6784\u5efa\u6a21\u5757\u5316\u6d41\u6c34\u7ebf\uff0c\u96c6\u6210\u9884\u5904\u7406\u3001\u56db\u79cd\u9884\u6d4b\u6a21\u578b\u548c\u56db\u79cd\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728Numenta\u5f02\u5e38\u57fa\u51c6\u4e0a\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u3002", "result": "LSTM\u83b7\u5f97\u6700\u4f73\u6027\u80fd\uff08F1: 0.688\uff09\uff0c\u572881%\u6570\u636e\u96c6\u4e0a\u6392\u540d\u524d\u4e8c\uff1bInformer\u63d0\u4f9b\u7ade\u4e89\u6027\u7cbe\u5ea6\u4e14\u8bad\u7ec3\u5feb30%\uff1b\u7ecf\u5178\u65b9\u6cd5\u5728\u7b80\u5355\u6570\u636e\u4e0a\u5b8c\u7f8e\u9884\u6d4b\u4f46\u771f\u5b9e\u6570\u636e\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u9884\u6d4b\u8d28\u91cf\u4e3b\u5bfc\u68c0\u6d4b\u6027\u80fd\uff0c\u5efa\u8bae\u6839\u636e\u6570\u636e\u590d\u6742\u5ea6\u9009\u62e9\u6a21\u578b\uff1a\u590d\u6742\u6a21\u5f0f\u7528LSTM\uff0c\u6548\u7387\u4f18\u5148\u7528Informer\uff0c\u7b80\u5355\u5468\u671f\u6570\u636e\u7528\u7ecf\u5178\u65b9\u6cd5\u3002"}}
{"id": "2510.11162", "categories": ["cs.LG", "cs.NE", "nlin.AO", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.11162", "abs": "https://arxiv.org/abs/2510.11162", "authors": ["Roman A. Kononov", "Nikita A. Pospelov", "Konstantin V. Anokhin", "Vladimir V. Nekorkin", "Oleg V. Maslennikov"], "title": "Emergence of hybrid computational dynamics through reinforcement learning", "comment": "22 pages, 11 figures", "summary": "Understanding how learning algorithms shape the computational strategies that\nemerge in neural networks remains a fundamental challenge in machine\nintelligence. While network architectures receive extensive attention, the role\nof the learning paradigm itself in determining emergent dynamics remains\nlargely unexplored. Here we demonstrate that reinforcement learning (RL) and\nsupervised learning (SL) drive recurrent neural networks (RNNs) toward\nfundamentally different computational solutions when trained on identical\ndecision-making tasks. Through systematic dynamical systems analysis, we reveal\nthat RL spontaneously discovers hybrid attractor architectures, combining\nstable fixed-point attractors for decision maintenance with quasi-periodic\nattractors for flexible evidence integration. This contrasts sharply with SL,\nwhich converges almost exclusively to simpler fixed-point-only solutions. We\nfurther show that RL sculpts functionally balanced neural populations through a\npowerful form of implicit regularization -- a structural signature that\nenhances robustness and is conspicuously absent in the more heterogeneous\nsolutions found by SL-trained networks. The prevalence of these complex\ndynamics in RL is controllably modulated by weight initialization and\ncorrelates strongly with performance gains, particularly as task complexity\nincreases. Our results establish the learning algorithm as a primary\ndeterminant of emergent computation, revealing how reward-based optimization\nautonomously discovers sophisticated dynamical mechanisms that are less\naccessible to direct gradient-based optimization. These findings provide both\nmechanistic insights into neural computation and actionable principles for\ndesigning adaptive AI systems.", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u76d1\u7763\u5b66\u4e60\uff08SL\uff09\u5728\u76f8\u540c\u51b3\u7b56\u4efb\u52a1\u4e2d\u9a71\u52a8\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u4ea7\u751f\u6839\u672c\u4e0d\u540c\u7684\u8ba1\u7b97\u7b56\u7565\uff1aRL\u81ea\u53d1\u53d1\u73b0\u6df7\u5408\u5438\u5f15\u5b50\u67b6\u6784\uff0c\u7ed3\u5408\u7a33\u5b9a\u56fa\u5b9a\u70b9\u5438\u5f15\u5b50\u548c\u51c6\u5468\u671f\u5438\u5f15\u5b50\uff0c\u800cSL\u4e3b\u8981\u6536\u655b\u5230\u7b80\u5355\u7684\u56fa\u5b9a\u70b9\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u63a2\u7d22\u5b66\u4e60\u7b97\u6cd5\u672c\u8eab\u5982\u4f55\u5851\u9020\u795e\u7ecf\u7f51\u7edc\u4e2d\u6d8c\u73b0\u7684\u8ba1\u7b97\u7b56\u7565\uff0c\u7279\u522b\u662f\u6bd4\u8f83RL\u548cSL\u5728\u76f8\u540c\u4efb\u52a1\u4e2d\u5982\u4f55\u5f71\u54cdRNN\u7684\u52a8\u529b\u5b66\u7279\u6027\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u52a8\u529b\u5b66\u5206\u6790\uff0c\u5728\u76f8\u540c\u51b3\u7b56\u4efb\u52a1\u4e0a\u8bad\u7ec3RNN\uff0c\u6bd4\u8f83RL\u548cSL\u4ea7\u751f\u7684\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\u3002\u5206\u6790\u7f51\u7edc\u6743\u91cd\u521d\u59cb\u5316\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u5bf9\u52a8\u529b\u5b66\u7279\u6027\u7684\u5f71\u54cd\u3002", "result": "RL\u81ea\u53d1\u53d1\u73b0\u6df7\u5408\u5438\u5f15\u5b50\u67b6\u6784\uff0c\u5177\u6709\u529f\u80fd\u5e73\u8861\u7684\u795e\u7ecf\u7fa4\u4f53\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff1bSL\u5219\u4ea7\u751f\u66f4\u5f02\u8d28\u7684\u89e3\u51b3\u65b9\u6848\u3002RL\u7684\u590d\u6742\u52a8\u529b\u5b66\u4e0e\u6027\u80fd\u63d0\u5347\u5f3a\u76f8\u5173\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u5b66\u4e60\u7b97\u6cd5\u662f\u6d8c\u73b0\u8ba1\u7b97\u7684\u4e3b\u8981\u51b3\u5b9a\u56e0\u7d20\uff0c\u57fa\u4e8e\u5956\u52b1\u7684\u4f18\u5316\u80fd\u81ea\u4e3b\u53d1\u73b0\u590d\u6742\u7684\u52a8\u529b\u5b66\u673a\u5236\uff0c\u8fd9\u4e9b\u673a\u5236\u5728\u76f4\u63a5\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u4e2d\u8f83\u96be\u83b7\u5f97\u3002\u8fd9\u4e3a\u8bbe\u8ba1\u81ea\u9002\u5e94AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u673a\u5236\u6027\u89c1\u89e3\u548c\u53ef\u64cd\u4f5c\u539f\u5219\u3002"}}
{"id": "2510.11164", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11164", "abs": "https://arxiv.org/abs/2510.11164", "authors": ["Ilaria Vascotto", "Alex Rodriguez", "Alessandro Bonaita", "Luca Bortolussi"], "title": "Beyond single-model XAI: aggregating multi-model explanations for enhanced trustworthiness", "comment": "Accepted at the European Workshop on Trustworthy Artificial\n  Intelligence (TRUST-AI), co-located within ECAI 2025", "summary": "The use of Artificial Intelligence (AI) models in real-world and high-risk\napplications has intensified the discussion about their trustworthiness and\nethical usage, from both a technical and a legislative perspective. The field\nof eXplainable Artificial Intelligence (XAI) addresses this challenge by\nproposing explanations that bring to light the decision-making processes of\ncomplex black-box models. Despite being an essential property, the robustness\nof explanations is often an overlooked aspect during development: only robust\nexplanation methods can increase the trust in the system as a whole. This paper\ninvestigates the role of robustness through the usage of a feature importance\naggregation derived from multiple models ($k$-nearest neighbours, random forest\nand neural networks). Preliminary results showcase the potential in increasing\nthe trustworthiness of the application, while leveraging multiple model's\npredictive power.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76XAI\u4e2d\u89e3\u91ca\u9c81\u68d2\u6027\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u901a\u8fc7\u96c6\u6210k\u8fd1\u90bb\u3001\u968f\u673a\u68ee\u6797\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u7279\u5f81\u91cd\u8981\u6027\u6765\u589e\u5f3a\u89e3\u91ca\u7684\u9c81\u68d2\u6027\uff0c\u4ece\u800c\u63d0\u9ad8AI\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u968f\u7740AI\u5728\u73b0\u5b9e\u4e16\u754c\u548c\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u589e\u52a0\uff0c\u4ece\u6280\u672f\u548c\u7acb\u6cd5\u89d2\u5ea6\u90fd\u9700\u8981\u786e\u4fdd\u5176\u53ef\u4fe1\u5ea6\u548c\u9053\u5fb7\u4f7f\u7528\u3002XAI\u9886\u57df\u65e8\u5728\u89e3\u91ca\u590d\u6742\u9ed1\u76d2\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f46\u89e3\u91ca\u7684\u9c81\u68d2\u6027\u5f80\u5f80\u88ab\u5ffd\u89c6\uff0c\u800c\u53ea\u6709\u9c81\u68d2\u7684\u89e3\u91ca\u65b9\u6cd5\u624d\u80fd\u589e\u5f3a\u5bf9\u6574\u4e2a\u7cfb\u7edf\u7684\u4fe1\u4efb\u3002", "method": "\u4f7f\u7528\u7279\u5f81\u91cd\u8981\u6027\u805a\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u4e2a\u6a21\u578b\uff08k\u8fd1\u90bb\u3001\u968f\u673a\u68ee\u6797\u548c\u795e\u7ecf\u7f51\u7edc\uff09\u7684\u7279\u5f81\u91cd\u8981\u6027\u6765\u751f\u6210\u66f4\u9c81\u68d2\u7684\u89e3\u91ca\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u6709\u6f5c\u529b\u63d0\u9ad8\u5e94\u7528\u7684\u53ef\u4fe1\u5ea6\uff0c\u540c\u65f6\u5229\u7528\u4e86\u591a\u4e2a\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u6a21\u578b\u7684\u7279\u5f81\u91cd\u8981\u6027\u53ef\u4ee5\u589e\u5f3aXAI\u89e3\u91ca\u7684\u9c81\u68d2\u6027\uff0c\u8fd9\u5bf9\u4e8e\u63d0\u9ad8AI\u7cfb\u7edf\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u5ea6\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.11188", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.11188", "abs": "https://arxiv.org/abs/2510.11188", "authors": ["Xinhui Chen", "Zuchao Li", "Mengqi Gao", "Yufeng Zhang", "Chak Tou Leong", "Haoyang Li", "Jiaqi Chen"], "title": "Protein as a Second Language for LLMs", "comment": "Main paper: 9 pages, 6 figures. With references and appendix: 18\n  pages, 9 figures total. Submitted to ICLR 2026 (under review)", "summary": "Deciphering the function of unseen protein sequences is a fundamental\nchallenge with broad scientific impact, yet most existing methods depend on\ntask-specific adapters or large-scale supervised fine-tuning. We introduce the\n\"Protein-as-Second-Language\" framework, which reformulates amino-acid sequences\nas sentences in a novel symbolic language that large language models can\ninterpret through contextual exemplars. Our approach adaptively constructs\nsequence-question-answer triples that reveal functional cues in a zero-shot\nsetting, without any further training. To support this process, we curate a\nbilingual corpus of 79,926 protein-QA instances spanning attribute prediction,\ndescriptive understanding, and extended reasoning. Empirically, our method\ndelivers consistent gains across diverse open-source LLMs and GPT-4, achieving\nup to 17.2% ROUGE-L improvement (average +7%) and even surpassing fine-tuned\nprotein-specific language models. These results highlight that generic LLMs,\nwhen guided with protein-as-language cues, can outperform domain-specialized\nmodels, offering a scalable pathway for protein understanding in foundation\nmodels.", "AI": {"tldr": "\u63d0\u51fa\u201c\u86cb\u767d\u8d28\u4f5c\u4e3a\u7b2c\u4e8c\u8bed\u8a00\u201d\u6846\u67b6\uff0c\u5c06\u6c28\u57fa\u9178\u5e8f\u5217\u8f6c\u5316\u4e3a\u7b26\u53f7\u8bed\u8a00\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u4e0a\u4e0b\u6587\u793a\u4f8b\u8fdb\u884c\u96f6\u6837\u672c\u86cb\u767d\u8d28\u529f\u80fd\u9884\u6d4b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u86cb\u767d\u8d28\u529f\u80fd\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u9002\u914d\u5668\u6216\u5927\u89c4\u6a21\u76d1\u7763\u5fae\u8c03\uff0c\u9700\u8981\u66f4\u901a\u7528\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u6c28\u57fa\u9178\u5e8f\u5217\u91cd\u65b0\u8868\u8ff0\u4e3a\u7b26\u53f7\u8bed\u8a00\u53e5\u5b50\uff0c\u6784\u5efa\u5e8f\u5217-\u95ee\u9898-\u7b54\u6848\u4e09\u5143\u7ec4\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u901a\u8fc7\u4e0a\u4e0b\u6587\u793a\u4f8b\u63ed\u793a\u529f\u80fd\u7ebf\u7d22\u3002", "result": "\u5728\u591a\u6837\u5316\u5f00\u6e90LLM\u548cGPT-4\u4e0a\u53d6\u5f97\u4e00\u81f4\u63d0\u5347\uff0cROUGE-L\u6700\u9ad8\u63d0\u534717.2%\uff08\u5e73\u5747+7%\uff09\uff0c\u751a\u81f3\u8d85\u8d8a\u5fae\u8c03\u7684\u86cb\u767d\u8d28\u4e13\u7528\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u901a\u7528LLM\u5728\u86cb\u767d\u8d28\u4f5c\u4e3a\u8bed\u8a00\u63d0\u793a\u5f15\u5bfc\u4e0b\uff0c\u80fd\u591f\u8d85\u8d8a\u9886\u57df\u4e13\u7528\u6a21\u578b\uff0c\u4e3a\u57fa\u7840\u6a21\u578b\u7684\u86cb\u767d\u8d28\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\u3002"}}
{"id": "2510.11202", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.11202", "abs": "https://arxiv.org/abs/2510.11202", "authors": ["Marco Pintore", "Giorgio Piras", "Angelo Sotgiu", "Maura Pintor", "Battista Biggio"], "title": "Evaluating Line-level Localization Ability of Learning-based Code Vulnerability Detection Models", "comment": "Preprint", "summary": "To address the extremely concerning problem of software vulnerability, system\nsecurity is often entrusted to Machine Learning (ML) algorithms. Despite their\nnow established detection capabilities, such models are limited by design to\nflagging the entire input source code function as vulnerable, rather than\nprecisely localizing the concerned code lines. However, the detection\ngranularity is crucial to support human operators during software development,\nensuring that such predictions reflect the true code semantics to help debug,\nevaluate, and fix the detected vulnerabilities. To address this issue, recent\nwork made progress toward improving the detector's localization ability, thus\nnarrowing down the vulnerability detection \"window\" and providing more\nfine-grained predictions. Such approaches, however, implicitly disregard the\npresence of spurious correlations and biases in the data, which often\npredominantly influence the performance of ML algorithms. In this work, we\ninvestigate how detectors comply with this requirement by proposing an\nexplainability-based evaluation procedure. Our approach, defined as Detection\nAlignment (DA), quantifies the agreement between the input source code lines\nthat most influence the prediction and the actual localization of the\nvulnerability as per the ground truth. Through DA, which is model-agnostic and\nadaptable to different detection tasks, not limited to our use case, we analyze\nmultiple learning-based vulnerability detectors and datasets. As a result, we\nshow how the predictions of such models are consistently biased by\nnon-vulnerable lines, ultimately highlighting the high impact of biases and\nspurious correlations. The code is available at\nhttps://github.com/pralab/vuln-localization-eval.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91ca\u6027\u7684\u8bc4\u4f30\u65b9\u6cd5\u2014\u2014\u68c0\u6d4b\u5bf9\u9f50(DA)\uff0c\u7528\u4e8e\u91cf\u5316\u6f0f\u6d1e\u68c0\u6d4b\u6a21\u578b\u9884\u6d4b\u4e0e\u771f\u5b9e\u6f0f\u6d1e\u4f4d\u7f6e\u7684\u4e00\u81f4\u6027\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6f0f\u6d1e\u68c0\u6d4b\u6a21\u578b\u53ea\u80fd\u6807\u8bb0\u6574\u4e2a\u6e90\u4ee3\u7801\u51fd\u6570\u4e3a\u6613\u53d7\u653b\u51fb\uff0c\u65e0\u6cd5\u7cbe\u786e\u5b9a\u4f4d\u5177\u4f53\u6f0f\u6d1e\u4ee3\u7801\u884c\uff0c\u4e14\u5ffd\u89c6\u4e86\u6570\u636e\u4e2d\u7684\u865a\u5047\u76f8\u5173\u6027\u548c\u504f\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u68c0\u6d4b\u5bf9\u9f50(DA)\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u6a21\u578b\u9884\u6d4b\u4e2d\u6700\u5177\u5f71\u54cd\u529b\u7684\u8f93\u5165\u4ee3\u7801\u884c\u4e0e\u771f\u5b9e\u6f0f\u6d1e\u4f4d\u7f6e\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u6765\u8bc4\u4f30\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u4e0e\u6a21\u578b\u65e0\u5173\u4e14\u9002\u7528\u4e8e\u4e0d\u540c\u68c0\u6d4b\u4efb\u52a1\u3002", "result": "\u5206\u6790\u591a\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u6f0f\u6d1e\u68c0\u6d4b\u5668\u548c\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u7684\u9884\u6d4b\u59cb\u7ec8\u53d7\u5230\u975e\u6f0f\u6d1e\u4ee3\u7801\u884c\u7684\u504f\u5dee\u5f71\u54cd\uff0c\u51f8\u663e\u4e86\u504f\u5dee\u548c\u865a\u5047\u76f8\u5173\u6027\u7684\u9ad8\u5ea6\u5f71\u54cd\u3002", "conclusion": "\u73b0\u6709\u6f0f\u6d1e\u68c0\u6d4b\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u504f\u5dee\u95ee\u9898\uff0c\u68c0\u6d4b\u5bf9\u9f50\u65b9\u6cd5\u80fd\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u5b9a\u4f4d\u80fd\u529b\uff0c\u4e3a\u6539\u8fdb\u6f0f\u6d1e\u68c0\u6d4b\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2510.11209", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.11209", "abs": "https://arxiv.org/abs/2510.11209", "authors": ["Nicola Albor\u00e9", "Gabriele Di Antonio", "Fabrizio Coccetti", "Andrea Gabrielli"], "title": "Cross-Scale Reservoir Computing for large spatio-temporal forecasting and modeling", "comment": null, "summary": "We propose a new reservoir computing method for forecasting high-resolution\nspatiotemporal datasets. By combining multi-resolution inputs from coarser to\nfiner layers, our architecture better captures both local and global dynamics.\nApplied to Sea Surface Temperature data, it outperforms standard parallel\nreservoir models in long-term forecasting, demonstrating the effectiveness of\ncross-layers coupling in improving predictive accuracy. Finally, we show that\nthe optimal network dynamics in each layer become increasingly linear,\nrevealing the slow modes propagated to subsequent layers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u50a8\u5c42\u8ba1\u7b97\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4ece\u7c97\u5230\u7ec6\u7684\u591a\u5206\u8fa8\u7387\u8f93\u5165\u6765\u6539\u8fdb\u9ad8\u5206\u8fa8\u7387\u65f6\u7a7a\u6570\u636e\u96c6\u7684\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u7684\u5e76\u884c\u50a8\u5c42\u6a21\u578b\u5728\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u52a8\u6001\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u591a\u5c3a\u5ea6\u65f6\u7a7a\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u5206\u8fa8\u7387\u8f93\u5165\u67b6\u6784\uff0c\u4ece\u7c97\u5230\u7ec6\u7684\u5c42\u6b21\u7ed3\u5408\uff0c\u901a\u8fc7\u8de8\u5c42\u8026\u5408\u6765\u589e\u5f3a\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u5728\u6d77\u8868\u6e29\u5ea6\u6570\u636e\u4e0a\u7684\u5e94\u7528\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u957f\u671f\u9884\u6d4b\u65b9\u9762\u4f18\u4e8e\u6807\u51c6\u5e76\u884c\u50a8\u5c42\u6a21\u578b\uff0c\u5e76\u63ed\u793a\u4e86\u5404\u5c42\u7f51\u7edc\u52a8\u6001\u9010\u6e10\u7ebf\u6027\u5316\u7684\u8d8b\u52bf\u3002", "conclusion": "\u8de8\u5c42\u8026\u5408\u6709\u6548\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5404\u5c42\u6700\u4f18\u7f51\u7edc\u52a8\u6001\u7684\u7ebf\u6027\u5316\u63ed\u793a\u4e86\u6162\u901f\u6a21\u5f0f\u5411\u540e\u7eed\u5c42\u7684\u4f20\u64ad\u673a\u5236\u3002"}}
{"id": "2510.11227", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11227", "abs": "https://arxiv.org/abs/2510.11227", "authors": ["Ahmed Rashwan", "Keith Briggs", "Chris Budd", "Lisa Kreusser"], "title": "Enforcing convex constraints in Graph Neural Networks", "comment": null, "summary": "Many machine learning applications require outputs that satisfy complex,\ndynamic constraints. This task is particularly challenging in Graph Neural\nNetwork models due to the variable output sizes of graph-structured data. In\nthis paper, we introduce ProjNet, a Graph Neural Network framework which\nsatisfies input-dependant constraints. ProjNet combines a sparse vector\nclipping method with the Component-Averaged Dykstra (CAD) algorithm, an\niterative scheme for solving the best-approximation problem. We establish a\nconvergence result for CAD and develop a GPU-accelerated implementation capable\nof handling large-scale inputs efficiently. To enable end-to-end training, we\nintroduce a surrogate gradient for CAD that is both computationally efficient\nand better suited for optimization than the exact gradient. We validate ProjNet\non four classes of constrained optimisation problems: linear programming, two\nclasses of non-convex quadratic programs, and radio transmit power\noptimization, demonstrating its effectiveness across diverse problem settings.", "AI": {"tldr": "ProjNet\u662f\u4e00\u4e2a\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u6ee1\u8db3\u8f93\u5165\u4f9d\u8d56\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u7ed3\u5408\u7a00\u758f\u5411\u91cf\u88c1\u526a\u65b9\u6cd5\u548cCAD\u7b97\u6cd5\uff0c\u5728GPU\u4e0a\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u8f93\u5165\uff0c\u5e76\u5728\u591a\u79cd\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u56fe\u7ed3\u6784\u6570\u636e\u65f6\u9762\u4e34\u8f93\u51fa\u5c3a\u5bf8\u53ef\u53d8\u548c\u590d\u6742\u52a8\u6001\u7ea6\u675f\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6ee1\u8db3\u8f93\u5165\u4f9d\u8d56\u7ea6\u675f\u7684\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u7a00\u758f\u5411\u91cf\u88c1\u526a\u65b9\u6cd5\u548cComponent-Averaged Dykstra (CAD)\u7b97\u6cd5\uff0c\u5f00\u53d1GPU\u52a0\u901f\u5b9e\u73b0\uff0c\u5e76\u5f15\u5165\u8ba1\u7b97\u9ad8\u6548\u7684\u66ff\u4ee3\u68af\u5ea6\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5efa\u7acb\u4e86CAD\u7b97\u6cd5\u7684\u6536\u655b\u6027\u7ed3\u679c\uff0c\u5728\u56db\u7c7b\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff08\u7ebf\u6027\u89c4\u5212\u3001\u4e24\u7c7b\u975e\u51f8\u4e8c\u6b21\u89c4\u5212\u3001\u65e0\u7ebf\u7535\u53d1\u5c04\u529f\u7387\u4f18\u5316\uff09\u4e0a\u9a8c\u8bc1\u4e86ProjNet\u7684\u6709\u6548\u6027\u3002", "conclusion": "ProjNet\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u8f93\u5165\u4f9d\u8d56\u7ea6\u675f\u95ee\u9898\uff0c\u5728\u591a\u79cd\u7ea6\u675f\u4f18\u5316\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.11234", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11234", "abs": "https://arxiv.org/abs/2510.11234", "authors": ["Jegwang Ryu", "Minkyu Kim", "Seungjun Shin", "Hee Min Choi", "Dokwan Oh", "Jaeho Lee"], "title": "Neural Weight Compression for Language Models", "comment": null, "summary": "The efficient storage and transmission of language model weights is becoming\nincreasingly important, as their scale and adoption continue to grow. However,\nas our understanding of this new data modality is limited, designing a good\ncompression algorithm for language model weights heavily relies on manual,\ntrial-and-error approaches. In this paper, we propose a learned compression\nframework that trains neural codecs directly from pretrained language model\nweights. Unlike conventional data (e.g., images), language model weights pose\nunique challenges: the sizes and shapes of weight tensors vary significantly,\nand the reconstruction quality must be judged by downstream model predictions\nrather than na\\\"ive MSE loss. To address this, we introduce Neural Weight\nCompression (NWC), a novel autoencoder-based neural codec tailored to model\nweight compression. The proposed method inherits the advantages of\nautoencoder-based codecs while incorporating three technical components: (1)\ncolumn-wise tensor chunking and normalization; (2) an importance-aware training\nloss; (3) an inference-time error compensation mechanism guided by model\noutputs. Experiments on open-weight language models show that NWC achieves\ncompetitive or state-of-the-art accuracy-compression tradeoffs, with\nparticularly strong results at 4-6 bit precisions where accuracy remains nearly\non par with FP16 models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u795e\u7ecf\u6743\u91cd\u538b\u7f29\uff08NWC\uff09\u7684\u5b66\u4e60\u578b\u538b\u7f29\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9\u8bed\u8a00\u6a21\u578b\u6743\u91cd\u8fdb\u884c\u538b\u7f29\uff0c\u901a\u8fc7\u81ea\u52a8\u7f16\u7801\u5668\u67b6\u6784\u548c\u4e09\u9879\u5173\u952e\u6280\u672f\u5b9e\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u538b\u7f29\u6548\u679c\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u6743\u91cd\u7684\u9ad8\u6548\u5b58\u50a8\u548c\u4f20\u8f93\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u7531\u4e8e\u5bf9\u8be5\u6570\u636e\u6a21\u6001\u7406\u89e3\u6709\u9650\uff0c\u73b0\u6709\u538b\u7f29\u7b97\u6cd5\u4f9d\u8d56\u624b\u52a8\u8bd5\u9519\u65b9\u6cd5\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u538b\u7f29\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u795e\u7ecf\u7f16\u89e3\u7801\u5668\uff0c\u5305\u542b\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a\u5217\u5411\u5f20\u91cf\u5206\u5757\u548c\u5f52\u4e00\u5316\u3001\u91cd\u8981\u6027\u611f\u77e5\u8bad\u7ec3\u635f\u5931\u3001\u57fa\u4e8e\u6a21\u578b\u8f93\u51fa\u7684\u63a8\u7406\u65f6\u8bef\u5dee\u8865\u507f\u673a\u5236\u3002", "result": "\u5728\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNWC\u57284-6\u4f4d\u7cbe\u5ea6\u4e0b\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6216\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6-\u538b\u7f29\u6743\u8861\uff0c\u51c6\u786e\u7387\u51e0\u4e4e\u4e0eFP16\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "NWC\u6846\u67b6\u4e3a\u8bed\u8a00\u6a21\u578b\u6743\u91cd\u538b\u7f29\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5b66\u4e60\u578b\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u5b58\u50a8\u548c\u4f20\u8f93\u9700\u6c42\u3002"}}
{"id": "2510.11245", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11245", "abs": "https://arxiv.org/abs/2510.11245", "authors": ["Leonardo Di Nino", "Gabriele D'Acunto", "Sergio Barbarossa", "Paolo Di Lorenzo"], "title": "Learning the Structure of Connection Graphs", "comment": null, "summary": "Connection graphs (CGs) extend traditional graph models by coupling network\ntopology with orthogonal transformations, enabling the representation of global\ngeometric consistency. They play a key role in applications such as\nsynchronization, Riemannian signal processing, and neural sheaf diffusion. In\nthis work, we address the inverse problem of learning CGs directly from\nobserved signals. We propose a principled framework based on maximum\npseudo-likelihood under a consistency assumption, which enforces spectral\nproperties linking the connection Laplacian to the underlying combinatorial\nLaplacian. Based on this formulation, we introduce the Structured Connection\nGraph Learning (SCGL) algorithm, a block-optimization procedure over Riemannian\nmanifolds that jointly infers network topology, edge weights, and geometric\nstructure. Our experiments show that SCGL consistently outperforms existing\nbaselines in both topological recovery and geometric fidelity, while remaining\ncomputationally efficient.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5927\u4f2a\u4f3c\u7136\u7684\u7ed3\u6784\u5316\u8fde\u63a5\u56fe\u5b66\u4e60(SCGL)\u7b97\u6cd5\uff0c\u80fd\u591f\u4ece\u89c2\u6d4b\u4fe1\u53f7\u4e2d\u8054\u5408\u63a8\u65ad\u7f51\u7edc\u62d3\u6251\u3001\u8fb9\u6743\u91cd\u548c\u51e0\u4f55\u7ed3\u6784\u3002", "motivation": "\u8fde\u63a5\u56fe\u5c06\u7f51\u7edc\u62d3\u6251\u4e0e\u6b63\u4ea4\u53d8\u6362\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u8868\u793a\u5168\u5c40\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u5728\u540c\u6b65\u3001\u9ece\u66fc\u4fe1\u53f7\u5904\u7406\u548c\u795e\u7ecf\u675f\u6269\u6563\u7b49\u5e94\u7528\u4e2d\u53d1\u6325\u5173\u952e\u4f5c\u7528\u3002\u4f46\u5982\u4f55\u76f4\u63a5\u4ece\u89c2\u6d4b\u4fe1\u53f7\u4e2d\u5b66\u4e60\u8fde\u63a5\u56fe\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u9006\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u4e00\u81f4\u6027\u5047\u8bbe\u4e0b\u7684\u6700\u5927\u4f2a\u4f3c\u7136\u6846\u67b6\uff0c\u5f3a\u5236\u8fde\u63a5\u62c9\u666e\u62c9\u65af\u4e0e\u5e95\u5c42\u7ec4\u5408\u62c9\u666e\u62c9\u65af\u4e4b\u95f4\u7684\u8c31\u7279\u6027\u8054\u7cfb\u3002\u63d0\u51fa\u4e86SCGL\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u5728\u9ece\u66fc\u6d41\u5f62\u4e0a\u7684\u5757\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSCGL\u5728\u62d3\u6251\u6062\u590d\u548c\u51e0\u4f55\u4fdd\u771f\u5ea6\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4ece\u4fe1\u53f7\u6570\u636e\u4e2d\u5b66\u4e60\u8fde\u63a5\u56fe\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u62d3\u6251\u548c\u51e0\u4f55\u7ed3\u6784\u7684\u6062\u590d\u6027\u80fd\u3002"}}
{"id": "2510.11250", "categories": ["cs.LG", "I.5.2"], "pdf": "https://arxiv.org/pdf/2510.11250", "abs": "https://arxiv.org/abs/2510.11250", "authors": ["Sujan Chakraborty", "Rahul Bordoloi", "Anindya Sengupta", "Olaf Wolkenhauer", "Saptarshi Bej"], "title": "FUSE: Fast Semi-Supervised Node Embedding Learning via Structural and Label-Aware Optimization", "comment": null, "summary": "Graph-based learning is a cornerstone for analyzing structured data, with\nnode classification as a central task. However, in many real-world graphs,\nnodes lack informative feature vectors, leaving only neighborhood connectivity\nand class labels as available signals. In such cases, effective classification\nhinges on learning node embeddings that capture structural roles and\ntopological context. We introduce a fast semi-supervised embedding framework\nthat jointly optimizes three complementary objectives: (i) unsupervised\nstructure preservation via scalable modularity approximation, (ii) supervised\nregularization to minimize intra-class variance among labeled nodes, and (iii)\nsemi-supervised propagation that refines unlabeled nodes through\nrandom-walk-based label spreading with attention-weighted similarity. These\ncomponents are unified into a single iterative optimization scheme, yielding\nhigh-quality node embeddings. On standard benchmarks, our method consistently\nachieves classification accuracy at par with or superior to state-of-the-art\napproaches, while requiring significantly less computational cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u7684\u534a\u76d1\u7763\u56fe\u5d4c\u5165\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7ed3\u6784\u4fdd\u6301\u3001\u76d1\u7763\u6b63\u5219\u5316\u548c\u534a\u76d1\u7763\u4f20\u64ad\u4e09\u4e2a\u4e92\u8865\u76ee\u6807\uff0c\u5728\u7f3a\u4e4f\u8282\u70b9\u7279\u5f81\u4fe1\u606f\u7684\u56fe\u4e0a\u5b9e\u73b0\u9ad8\u6548\u8282\u70b9\u5206\u7c7b\u3002", "motivation": "\u8bb8\u591a\u73b0\u5b9e\u56fe\u6570\u636e\u4e2d\u8282\u70b9\u7f3a\u4e4f\u4fe1\u606f\u4e30\u5bcc\u7684\u7279\u5f81\u5411\u91cf\uff0c\u4ec5\u80fd\u5229\u7528\u90bb\u57df\u8fde\u63a5\u6027\u548c\u7c7b\u522b\u6807\u7b7e\u4f5c\u4e3a\u53ef\u7528\u4fe1\u53f7\uff0c\u9700\u8981\u5b66\u4e60\u80fd\u591f\u6355\u6349\u7ed3\u6784\u89d2\u8272\u548c\u62d3\u6251\u4e0a\u4e0b\u6587\u7684\u8282\u70b9\u5d4c\u5165\u3002", "method": "\u8054\u5408\u4f18\u5316\u4e09\u4e2a\u76ee\u6807\uff1a\u901a\u8fc7\u53ef\u6269\u5c55\u6a21\u5757\u5ea6\u8fd1\u4f3c\u5b9e\u73b0\u65e0\u76d1\u7763\u7ed3\u6784\u4fdd\u6301\uff1b\u76d1\u7763\u6b63\u5219\u5316\u6700\u5c0f\u5316\u6709\u6807\u7b7e\u8282\u70b9\u7684\u7c7b\u5185\u65b9\u5dee\uff1b\u57fa\u4e8e\u968f\u673a\u6e38\u8d70\u7684\u6ce8\u610f\u529b\u52a0\u6743\u76f8\u4f3c\u6027\u6807\u7b7e\u4f20\u64ad\u6765\u4f18\u5316\u65e0\u6807\u7b7e\u8282\u70b9\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5206\u7c7b\u51c6\u786e\u7387\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u8282\u70b9\u5d4c\u5165\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u5206\u7c7b\u6027\u80fd\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u7f3a\u4e4f\u8282\u70b9\u7279\u5f81\u4fe1\u606f\u7684\u56fe\u6570\u636e\u5206\u7c7b\u4efb\u52a1\u3002"}}
{"id": "2510.11257", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.11257", "abs": "https://arxiv.org/abs/2510.11257", "authors": ["Davide Borghini", "Davide Marchi", "Angelo Nardone", "Giordano Scerra", "Silvia Giulia Galfr\u00e8", "Alessandro Pingitore", "Giuseppe Prencipe", "Corrado Priami", "Alina S\u00eerbu"], "title": "MIEO: encoding clinical data to enhance cardiovascular event prediction", "comment": "Presented in the Poster Session of Computational Intelligence methods\n  for Bioinformatics and Biostatistics (CIBB) 2025", "summary": "As clinical data are becoming increasingly available, machine learning\nmethods have been employed to extract knowledge from them and predict clinical\nevents. While promising, approaches suffer from at least two main issues: low\navailability of labelled data and data heterogeneity leading to missing values.\nThis work proposes the use of self-supervised auto-encoders to efficiently\naddress these challenges. We apply our methodology to a clinical dataset from\npatients with ischaemic heart disease. Patient data is embedded in a latent\nspace, built using unlabelled data, which is then used to train a neural\nnetwork classifier to predict cardiovascular death. Results show improved\nbalanced accuracy compared to applying the classifier directly to the raw data,\ndemonstrating that this solution is promising, especially in conditions where\navailability of unlabelled data could increase.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u81ea\u76d1\u7763\u81ea\u52a8\u7f16\u7801\u5668\u89e3\u51b3\u4e34\u5e8a\u6570\u636e\u6807\u7b7e\u7a00\u7f3a\u548c\u5f02\u6784\u6027\u95ee\u9898\uff0c\u5728\u7f3a\u8840\u6027\u5fc3\u810f\u75c5\u60a3\u8005\u6570\u636e\u96c6\u4e0a\u9884\u6d4b\u5fc3\u8840\u7ba1\u6b7b\u4ea1\uff0c\u76f8\u6bd4\u76f4\u63a5\u5728\u539f\u59cb\u6570\u636e\u4e0a\u8bad\u7ec3\u5206\u7c7b\u5668\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\u51c6\u786e\u7387\u3002", "motivation": "\u4e34\u5e8a\u6570\u636e\u6807\u7b7e\u7a00\u7f3a\u4e14\u5b58\u5728\u5f02\u6784\u6027\u5bfc\u81f4\u7f3a\u5931\u503c\u95ee\u9898\uff0c\u9700\u8981\u6709\u6548\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u4f7f\u7528\u81ea\u76d1\u7763\u81ea\u52a8\u7f16\u7801\u5668\u6784\u5efa\u6f5c\u5728\u7a7a\u95f4\u5d4c\u5165\u60a3\u8005\u6570\u636e\uff0c\u7136\u540e\u5728\u8be5\u6f5c\u5728\u7a7a\u95f4\u4e0a\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\u9884\u6d4b\u5fc3\u8840\u7ba1\u6b7b\u4ea1\u3002", "result": "\u76f8\u6bd4\u76f4\u63a5\u5728\u539f\u59cb\u6570\u636e\u4e0a\u5e94\u7528\u5206\u7c7b\u5668\uff0c\u8be5\u65b9\u6cd5\u83b7\u5f97\u4e86\u6539\u8fdb\u7684\u5e73\u8861\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u672a\u6807\u8bb0\u6570\u636e\u53ef\u7528\u6027\u53ef\u80fd\u589e\u52a0\u7684\u60c5\u51b5\u4e0b\u7279\u522b\u6709\u524d\u666f\u3002"}}
{"id": "2510.11274", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11274", "abs": "https://arxiv.org/abs/2510.11274", "authors": ["Jianzhe Zhao", "Hailin Zhu", "Yu Zhang", "Ziqi Chen", "Guibing Guo"], "title": "FedLoRA-Optimizer: Federated LoRA Fine-Tuning with Global and Local Optimization in Heterogeneous Data Scenarios", "comment": null, "summary": "Federated efficient fine-tuning has emerged as an approach that leverages\ndistributed data and computational resources across nodes to address the\nchallenges of large-scale fine-tuning and privacy preservation. The Low-Rank\nAdaptation (LoRA) enables efficient fine-tuning of large-scale pre-trained\nmodels by introducing trainable low-rank matrices into weight updates.However,\nin heterogeneous data scenarios, client drift weakens the generalization of the\nglobal model, and local models often fail to meet the personalized needs of\nindividual clients.Moreover, existing federated LoRA efficient fine-tuning\ntechniques overlook fine-grained analysis of the tuning matrices. To address\nthis, we conducted preliminary experiments and found that different LoRA\nmatrices exhibit different sensitivity to changes in the direction and\nmagnitude of their vectors.We thus propose a fine-grained federated LoRA tuning\nmethod. By fine-tuning the more sensitive directional vectors in the A matrix,\nwhich encode shared knowledge, our method learns shared features more\neffectively across clients and enhances global generalization. Simultaneously,\nby fine-tuning the more sensitive magnitude vectors in the B matrix, which\nencode personalized knowledge, our method better captures personalized\nknowledge, enabling detailed adaptation to local data. The method uses a\npipeline combining global and local optimizers. Global optimization further\nimproves local models, achieving collaborative optimization between global and\nlocal levels. This improves both the generalization ability of the global model\nand the personalized adaptation of local models under heterogeneous data\nscenarios. Experiments on Databricks-Dolly-15k and Natural Instructions with\nLLaMA2-7B and Deepseek-7B confirm that our method improves global performance\nby 0.39% and local performance by 0.59%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u8054\u90a6LoRA\u8c03\u4f18\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u522b\u4f18\u5316A\u77e9\u9635\u7684\u65b9\u5411\u5411\u91cf\u548cB\u77e9\u9635\u7684\u5e45\u5ea6\u5411\u91cf\uff0c\u5728\u5f02\u6784\u6570\u636e\u573a\u666f\u4e0b\u540c\u65f6\u63d0\u5347\u5168\u5c40\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u672c\u5730\u6a21\u578b\u7684\u4e2a\u6027\u5316\u9002\u914d\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u9ad8\u6548\u5fae\u8c03\u4e2d\u5ba2\u6237\u7aef\u6f02\u79fb\u5bfc\u81f4\u5168\u5c40\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\uff0c\u4ee5\u53ca\u672c\u5730\u6a21\u578b\u65e0\u6cd5\u6ee1\u8db3\u4e2a\u6027\u5316\u9700\u6c42\u7684\u95ee\u9898\u3002\u73b0\u6709\u8054\u90a6LoRA\u6280\u672f\u7f3a\u4e4f\u5bf9\u8c03\u4f18\u77e9\u9635\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u4e0d\u540cLoRA\u77e9\u9635\u5bf9\u5411\u91cf\u65b9\u5411\u548c\u5e45\u5ea6\u53d8\u5316\u7684\u654f\u611f\u6027\u4e0d\u540c\u3002\u63d0\u51fa\u7ec6\u7c92\u5ea6\u8054\u90a6LoRA\u8c03\u4f18\u65b9\u6cd5\uff1a\u4f18\u5316A\u77e9\u9635\u4e2d\u66f4\u654f\u611f\u7684\u65b9\u5411\u5411\u91cf\u5b66\u4e60\u5171\u4eab\u7279\u5f81\uff0c\u4f18\u5316B\u77e9\u9635\u4e2d\u66f4\u654f\u611f\u7684\u5e45\u5ea6\u5411\u91cf\u6355\u83b7\u4e2a\u6027\u5316\u77e5\u8bc6\u3002\u91c7\u7528\u5168\u5c40\u548c\u5c40\u90e8\u4f18\u5316\u5668\u7ed3\u5408\u7684\u6d41\u6c34\u7ebf\u3002", "result": "\u5728Databricks-Dolly-15k\u548cNatural Instructions\u6570\u636e\u96c6\u4e0a\u4f7f\u7528LLaMA2-7B\u548cDeepseek-7B\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5168\u5c40\u6027\u80fd\u63d0\u53470.39%\uff0c\u672c\u5730\u6027\u80fd\u63d0\u53470.59%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5f02\u6784\u6570\u636e\u573a\u666f\u4e0b\u6709\u6548\u63d0\u5347\u4e86\u5168\u5c40\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u672c\u5730\u6a21\u578b\u7684\u4e2a\u6027\u5316\u9002\u914d\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u5168\u5c40\u548c\u5c40\u90e8\u5c42\u9762\u7684\u534f\u540c\u4f18\u5316\u3002"}}
{"id": "2510.11282", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11282", "abs": "https://arxiv.org/abs/2510.11282", "authors": ["Ning Yang", "Hengyu Zhong", "Haijun Zhang", "Randall Berry"], "title": "Vision-LLMs for Spatiotemporal Traffic Forecasting", "comment": null, "summary": "Accurate spatiotemporal traffic forecasting is a critical prerequisite for\nproactive resource management in dense urban mobile networks. While Large\nLanguage Models (LLMs) have shown promise in time series analysis, they\ninherently struggle to model the complex spatial dependencies of grid-based\ntraffic data. Effectively extending LLMs to this domain is challenging, as\nrepresenting the vast amount of information from dense geographical grids can\nbe inefficient and overwhelm the model's context. To address these challenges,\nwe propose ST-Vision-LLM, a novel framework that reframes spatiotemporal\nforecasting as a vision-language fusion problem. Our approach leverages a\nVision-LLM visual encoder to process historical global traffic matrices as\nimage sequences, providing the model with a comprehensive global view to inform\ncell-level predictions. To overcome the inefficiency of LLMs in handling\nnumerical data, we introduce an efficient encoding scheme that represents\nfloating-point values as single tokens via a specialized vocabulary, coupled\nwith a two-stage numerical alignment fine-tuning process. The model is first\ntrained with Supervised Fine-Tuning (SFT) and then further optimized for\npredictive accuracy using Group Relative Policy Optimization (GRPO), a\nmemory-efficient reinforcement learning method. Evaluations on real-world\nmobile traffic datasets demonstrate that ST-Vision-LLM outperforms existing\nmethods by 15.6% in long-term prediction accuracy and exceeds the second-best\nbaseline by over 30.04% in cross-domain few-shot scenarios. Our extensive\nexperiments validate the model's strong generalization capabilities across\nvarious data-scarce environments.", "AI": {"tldr": "ST-Vision-LLM\uff1a\u4e00\u79cd\u5c06\u65f6\u7a7a\u4ea4\u901a\u9884\u6d4b\u91cd\u6784\u4e3a\u89c6\u89c9-\u8bed\u8a00\u878d\u5408\u95ee\u9898\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7Vision-LLM\u89c6\u89c9\u7f16\u7801\u5668\u5904\u7406\u5386\u53f2\u4ea4\u901a\u77e9\u9635\u56fe\u50cf\u5e8f\u5217\uff0c\u7ed3\u5408\u9ad8\u6548\u6570\u503c\u7f16\u7801\u65b9\u6848\u548c\u4e24\u9636\u6bb5\u5fae\u8c03\uff0c\u5728\u771f\u5b9e\u79fb\u52a8\u6d41\u91cf\u6570\u636e\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u5efa\u6a21\u57fa\u4e8e\u7f51\u683c\u7684\u4ea4\u901a\u6570\u636e\u590d\u6742\u7a7a\u95f4\u4f9d\u8d56\u6027\u65b9\u9762\u7684\u56fa\u6709\u56f0\u96be\uff0c\u4ee5\u53ca\u8868\u793a\u5bc6\u96c6\u5730\u7406\u7f51\u683c\u4fe1\u606f\u65f6\u6548\u7387\u4f4e\u4e0b\u548c\u4e0a\u4e0b\u6587\u8fc7\u8f7d\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528Vision-LLM\u89c6\u89c9\u7f16\u7801\u5668\u5904\u7406\u5386\u53f2\u5168\u5c40\u4ea4\u901a\u77e9\u9635\u4f5c\u4e3a\u56fe\u50cf\u5e8f\u5217\uff1b\u5f15\u5165\u9ad8\u6548\u7f16\u7801\u65b9\u6848\u5c06\u6d6e\u70b9\u503c\u8868\u793a\u4e3a\u5355\u4e2atoken\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\uff1a\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u57fa\u4e8eGRPO\u7684\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u3002", "result": "\u5728\u771f\u5b9e\u79fb\u52a8\u6d41\u91cf\u6570\u636e\u96c6\u4e0a\uff0c\u957f\u671f\u9884\u6d4b\u51c6\u786e\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad815.6%\uff1b\u5728\u8de8\u57df\u5c11\u6837\u672c\u573a\u666f\u4e2d\u8d85\u8fc7\u7b2c\u4e8c\u4f73\u57fa\u7ebf30.04%\uff1b\u5728\u5404\u79cd\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ST-Vision-LLM\u6210\u529f\u5c06LLM\u6269\u5c55\u5230\u65f6\u7a7a\u9884\u6d4b\u9886\u57df\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u878d\u5408\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7a7a\u95f4\u4f9d\u8d56\u5efa\u6a21\u548c\u6570\u503c\u5904\u7406\u6548\u7387\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u573a\u666f\u4e0b\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.11283", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11283", "abs": "https://arxiv.org/abs/2510.11283", "authors": ["Antoine Mouchamps", "Arthur Malherbe", "Adrien Bolland", "Damien Ernst"], "title": "Gym-TORAX: Open-source software for integrating RL with plasma control simulators", "comment": null, "summary": "This paper presents Gym-TORAX, a Python package enabling the implementation\nof Reinforcement Learning (RL) environments for simulating plasma dynamics and\ncontrol in tokamaks. Users define succinctly a set of control actions and\nobservations, and a control objective from which Gym-TORAX creates a Gymnasium\nenvironment that wraps TORAX for simulating the plasma dynamics. The objective\nis formulated through rewards depending on the simulated state of the plasma\nand control action to optimize specific characteristics of the plasma, such as\nperformance and stability. The resulting environment instance is then\ncompatible with a wide range of RL algorithms and libraries and will facilitate\nRL research in plasma control. In its current version, one environment is\nreadily available, based on a ramp-up scenario of the International\nThermonuclear Experimental Reactor (ITER).", "AI": {"tldr": "Gym-TORAX\u662f\u4e00\u4e2aPython\u5305\uff0c\u7528\u4e8e\u521b\u5efa\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u6765\u6a21\u62df\u6258\u5361\u9a6c\u514b\u4e2d\u7684\u7b49\u79bb\u5b50\u4f53\u52a8\u529b\u5b66\u548c\u63a7\u5236\u3002", "motivation": "\u4fc3\u8fdb\u7b49\u79bb\u5b50\u4f53\u63a7\u5236\u9886\u57df\u7684\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\uff0c\u901a\u8fc7\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u73af\u5883\u6765\u7b80\u5316RL\u7b97\u6cd5\u7684\u5e94\u7528\u3002", "method": "\u7528\u6237\u5b9a\u4e49\u63a7\u5236\u52a8\u4f5c\u3001\u89c2\u6d4b\u548c\u63a7\u5236\u76ee\u6807\uff0cGym-TORAX\u57fa\u4e8e\u8fd9\u4e9b\u521b\u5efa\u5305\u88c5TORAX\u6a21\u62df\u5668\u7684Gymnasium\u73af\u5883\uff0c\u901a\u8fc7\u5956\u52b1\u51fd\u6570\u4f18\u5316\u7b49\u79bb\u5b50\u4f53\u7279\u6027\u3002", "result": "\u5f00\u53d1\u4e86\u57fa\u4e8eITER\u542f\u52a8\u573a\u666f\u7684\u73af\u5883\uff0c\u517c\u5bb9\u591a\u79cdRL\u7b97\u6cd5\u548c\u5e93\u3002", "conclusion": "Gym-TORAX\u4e3a\u7b49\u79bb\u5b50\u4f53\u63a7\u5236\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u5e73\u53f0\uff0c\u5c06\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u7b97\u6cd5\u5f00\u53d1\u548c\u5e94\u7528\u3002"}}
{"id": "2510.11292", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11292", "abs": "https://arxiv.org/abs/2510.11292", "authors": ["Wenbo Wu", "Qingyi Si", "Xiurui Pan", "Ye Wang", "Jie Zhang"], "title": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences", "comment": null, "summary": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios.", "AI": {"tldr": "LouisKV\u662f\u4e00\u4e2a\u9ad8\u6548\u7684KV\u7f13\u5b58\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u68c0\u7d22\u7b56\u7565\u548c\u7ec6\u7c92\u5ea6\u7ba1\u7406\u65b9\u6848\uff0c\u5728\u957f\u5e8f\u5217\u573a\u666f\u4e0b\u5b9e\u73b04.7\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u65e0\u635f\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684KV\u7f13\u5b58\u65b9\u6cd5\u5728\u957f\u5e8f\u5217\u573a\u666f\u4e2d\u5b58\u5728\u663e\u8457\u7684\u5185\u5b58\u5f00\u9500\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u957f\u8f93\u51fa\u63a8\u7406\u573a\u666f\u4e2d\uff0c\u7531\u4e8e\u9010token\u68c0\u7d22\u548c\u7c97\u7c92\u5ea6\u7684\u9875\u9762\u7ea7KV\u7ba1\u7406\uff0c\u5bfc\u81f4\u6548\u7387\u548c\u51c6\u786e\u6027\u74f6\u9888\u3002", "method": "\u57fa\u4e8e\u5173\u952eKV\u5177\u6709\u5f3a\u65f6\u95f4\u5c40\u90e8\u6027\u548c\u5728\u8f93\u5165\u63d0\u793a\u4e0e\u751f\u6210\u8f93\u51fa\u4e2d\u5448\u73b0\u4e0d\u540c\u5206\u5e03\u6a21\u5f0f\u7684\u89c2\u5bdf\uff0cLouisKV\u5f15\u5165\u4e86\u8bed\u4e49\u611f\u77e5\u68c0\u7d22\u7b56\u7565\uff08\u4ec5\u5728\u8bed\u4e49\u8fb9\u754c\u89e6\u53d1\u68c0\u7d22\uff09\u548c\u89e3\u8026\u7684\u7ec6\u7c92\u5ea6\u7ba1\u7406\u65b9\u6848\uff08\u4e3a\u8f93\u5165\u548c\u8f93\u51fa\u5e8f\u5217\u5b9a\u5236\u5dee\u5f02\u5316\u7b56\u7565\uff09\uff0c\u5e76\u5305\u542bTriton\u548cCUDA\u5185\u6838\u4f18\u5316\u6765\u52a0\u901fKV\u805a\u7c7b\u548c\u68c0\u7d22\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0cLouisKV\u5728\u591a\u6837\u5316\u957f\u5e8f\u5217\u4efb\u52a1\uff08\u5305\u62ec\u957f\u8f93\u5165\u77ed\u8f93\u51fa\u3001\u77ed\u8f93\u5165\u957f\u8f93\u51fa\u548c\u957f\u8f93\u5165\u957f\u8f93\u51fa\u573a\u666f\uff09\u4e2d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684KV\u68c0\u7d22\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe4.7\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u65e0\u635f\u7684\u51c6\u786e\u6027\u3002", "conclusion": "LouisKV\u901a\u8fc7\u5229\u7528KV\u7684\u65f6\u95f4\u5c40\u90e8\u6027\u548c\u5206\u5e03\u7279\u6027\uff0c\u7ed3\u5408\u8bed\u4e49\u611f\u77e5\u68c0\u7d22\u548c\u7ec6\u7c92\u5ea6\u7ba1\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u573a\u666f\u4e2dKV\u7f13\u5b58\u7684\u5185\u5b58\u548c\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.11335", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11335", "abs": "https://arxiv.org/abs/2510.11335", "authors": ["Mayank Nagda", "Phil Ostheimer", "Justus Arweiler", "Indra Jungjohann", "Jennifer Werner", "Dennis Wagner", "Aparna Muraleedharan", "Pouya Jafari", "Jochen Schmid", "Fabian Jirasek", "Jakob Burger", "Michael Bortz", "Hans Hasse", "Stephan Mandt", "Marius Kloft", "Sophie Fellenz"], "title": "DiffStyleTS: Diffusion Model for Style Transfer in Time Series", "comment": null, "summary": "Style transfer combines the content of one signal with the style of another.\nIt supports applications such as data augmentation and scenario simulation,\nhelping machine learning models generalize in data-scarce domains. While well\ndeveloped in vision and language, style transfer methods for time series data\nremain limited. We introduce DiffTSST, a diffusion-based framework that\ndisentangles a time series into content and style representations via\nconvolutional encoders and recombines them through a self-supervised\nattention-based diffusion process. At inference, encoders extract content and\nstyle from two distinct series, enabling conditional generation of novel\nsamples to achieve style transfer. We demonstrate both qualitatively and\nquantitatively that DiffTSST achieves effective style transfer. We further\nvalidate its real-world utility by showing that data augmentation with DiffTSST\nimproves anomaly detection in data-scarce regimes.", "AI": {"tldr": "DiffTSST\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65f6\u5e8f\u6570\u636e\u98ce\u683c\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u5377\u79ef\u7f16\u7801\u5668\u5c06\u65f6\u5e8f\u6570\u636e\u89e3\u8026\u4e3a\u5185\u5bb9\u548c\u98ce\u683c\u8868\u793a\uff0c\u5e76\u4f7f\u7528\u81ea\u76d1\u7763\u6ce8\u610f\u529b\u6269\u6563\u8fc7\u7a0b\u91cd\u65b0\u7ec4\u5408\u5b83\u4eec\uff0c\u5b9e\u73b0\u6709\u6548\u7684\u98ce\u683c\u8fc1\u79fb\u548c\u6570\u636e\u589e\u5f3a\u3002", "motivation": "\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u9886\u57df\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u5df2\u7ecf\u5f88\u6210\u719f\uff0c\u4f46\u65f6\u5e8f\u6570\u636e\u7684\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u4ecd\u7136\u6709\u9650\u3002\u65f6\u5e8f\u98ce\u683c\u8fc1\u79fb\u5728\u6570\u636e\u589e\u5f3a\u548c\u573a\u666f\u6a21\u62df\u7b49\u5e94\u7528\u4e2d\u5f88\u91cd\u8981\uff0c\u80fd\u5e2e\u52a9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u9886\u57df\u6cdb\u5316\u3002", "method": "\u4f7f\u7528\u5377\u79ef\u7f16\u7801\u5668\u5c06\u65f6\u5e8f\u6570\u636e\u89e3\u8026\u4e3a\u5185\u5bb9\u548c\u98ce\u683c\u8868\u793a\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u6ce8\u610f\u529b\u6269\u6563\u8fc7\u7a0b\u91cd\u65b0\u7ec4\u5408\u8fd9\u4e9b\u8868\u793a\u3002\u5728\u63a8\u7406\u65f6\uff0c\u4ece\u4e24\u4e2a\u4e0d\u540c\u7684\u5e8f\u5217\u4e2d\u63d0\u53d6\u5185\u5bb9\u548c\u98ce\u683c\uff0c\u5b9e\u73b0\u6761\u4ef6\u751f\u6210\u4ee5\u5b9e\u73b0\u98ce\u683c\u8fc1\u79fb\u3002", "result": "\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u8868\u660eDiffTSST\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u98ce\u683c\u8fc1\u79fb\u3002\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528DiffTSST\u8fdb\u884c\u6570\u636e\u589e\u5f3a\u53ef\u4ee5\u6539\u5584\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "DiffTSST\u662f\u4e00\u4e2a\u6709\u6548\u7684\u65f6\u5e8f\u6570\u636e\u98ce\u683c\u8fc1\u79fb\u6846\u67b6\uff0c\u5728\u6570\u636e\u589e\u5f3a\u5e94\u7528\u4e2d\u5177\u6709\u5b9e\u9645\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u80fd\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u7b49\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2510.11339", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11339", "abs": "https://arxiv.org/abs/2510.11339", "authors": ["Xingtong Yu", "Ruijuan Liang", "Xinming Zhang", "Yuan Fang"], "title": "Event-Aware Prompt Learning for Dynamic Graphs", "comment": "Under review", "summary": "Real-world graph typically evolve via a series of events, modeling dynamic\ninteractions between objects across various domains. For dynamic graph\nlearning, dynamic graph neural networks (DGNNs) have emerged as popular\nsolutions. Recently, prompt learning methods have been explored on dynamic\ngraphs. However, existing methods generally focus on capturing the relationship\nbetween nodes and time, while overlooking the impact of historical events. In\nthis paper, we propose EVP, an event-aware dynamic graph prompt learning\nframework that can serve as a plug-in to existing methods, enhancing their\nability to leverage historical events knowledge. First, we extract a series of\nhistorical events for each node and introduce an event adaptation mechanism to\nalign the fine-grained characteristics of these events with downstream tasks.\nSecond, we propose an event aggregation mechanism to effectively integrate\nhistorical knowledge into node representations. Finally, we conduct extensive\nexperiments on four public datasets to evaluate and analyze EVP.", "AI": {"tldr": "\u63d0\u51fa\u4e86EVP\u6846\u67b6\uff0c\u4e00\u79cd\u4e8b\u4ef6\u611f\u77e5\u7684\u52a8\u6001\u56fe\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u4f5c\u4e3a\u73b0\u6709\u65b9\u6cd5\u7684\u63d2\u4ef6\uff0c\u589e\u5f3a\u5176\u5229\u7528\u5386\u53f2\u4e8b\u4ef6\u77e5\u8bc6\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u52a8\u6001\u56fe\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8282\u70b9\u4e0e\u65f6\u95f4\u7684\u5173\u7cfb\uff0c\u4f46\u5ffd\u89c6\u4e86\u5386\u53f2\u4e8b\u4ef6\u7684\u5f71\u54cd\u3002", "method": "1) \u4e3a\u6bcf\u4e2a\u8282\u70b9\u63d0\u53d6\u5386\u53f2\u4e8b\u4ef6\u5e8f\u5217\uff0c\u5f15\u5165\u4e8b\u4ef6\u9002\u5e94\u673a\u5236\u5bf9\u9f50\u4e8b\u4ef6\u7279\u5f81\u4e0e\u4e0b\u6e38\u4efb\u52a1\uff1b2) \u63d0\u51fa\u4e8b\u4ef6\u805a\u5408\u673a\u5236\u5c06\u5386\u53f2\u77e5\u8bc6\u6574\u5408\u5230\u8282\u70b9\u8868\u793a\u4e2d\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u6765\u8bc4\u4f30\u548c\u5206\u6790EVP\u3002", "conclusion": "EVP\u6846\u67b6\u80fd\u6709\u6548\u589e\u5f3a\u73b0\u6709\u52a8\u6001\u56fe\u5b66\u4e60\u65b9\u6cd5\u5bf9\u5386\u53f2\u4e8b\u4ef6\u77e5\u8bc6\u7684\u5229\u7528\u80fd\u529b\u3002"}}
{"id": "2510.11345", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11345", "abs": "https://arxiv.org/abs/2510.11345", "authors": ["Han Lu", "Zichen Liu", "Shaopan Xiong", "Yancheng He", "Wei Gao", "Yanan Wu", "Weixun Wang", "Jiashun Liu", "Yang Li", "Haizhou Zhao", "Ju Huang", "Siran Yang", "Xiaoyang Li", "Yijia Luo", "Zihe Liu", "Ling Pan", "Junchi Yan", "Wei Wang", "Wenbo Su", "Jiamang Wang", "Lin Qu", "Bo Zheng"], "title": "Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with Asynchrony", "comment": null, "summary": "Synchronous Reinforcement Learning (RL) post-training has emerged as a\ncrucial step for enhancing Large Language Models (LLMs) with diverse\ncapabilities. However, many systems designed to accelerate RL post-training\nstill suffer from low resource utilization and limited scalability. We present\nROLL Flash, a system that extends ROLL with native support for asynchronous RL\npost-training. ROLL Flash is built upon two core design principles:\nfine-grained parallelism and rollout-train decoupling. Guided by these\nprinciples, ROLL Flash provides flexible programming interfaces that enable a\nfully asynchronous training architecture and support efficient rollout\nmechanisms, including queue scheduling and environment-level asynchronous\nexecution. Through comprehensive theoretical analysis and extensive\nexperiments, we demonstrate that ROLL Flash significantly improves resource\nutilization and scalability over synchronous RL post-training. ROLL Flash\nachieves up to 2.24x speedup on RLVR tasks and 2.72x on agentic tasks, using\nthe same GPU budget as synchronous baselines. Furthermore, we implement several\npopular off-policy algorithms and verify that asynchronous training can achieve\nperformance on par with synchronous training.", "AI": {"tldr": "ROLL Flash\u662f\u4e00\u4e2a\u652f\u6301\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5e76\u884c\u5316\u548crollout-train\u89e3\u8026\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8d44\u6e90\u5229\u7528\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u540c\u6b65\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7cfb\u7edf\u5b58\u5728\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u548c\u53ef\u6269\u5c55\u6027\u6709\u9650\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u67b6\u6784\u3002", "method": "\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u5e76\u884c\u5316\u548crollout-train\u89e3\u8026\u4e24\u5927\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\uff0c\u63d0\u4f9b\u7075\u6d3b\u7684\u7f16\u7a0b\u63a5\u53e3\uff0c\u652f\u6301\u5b8c\u5168\u5f02\u6b65\u8bad\u7ec3\u67b6\u6784\u548c\u9ad8\u6548\u7684rollout\u673a\u5236\u3002", "result": "\u5728\u76f8\u540cGPU\u9884\u7b97\u4e0b\uff0cROLL Flash\u5728RLVR\u4efb\u52a1\u4e0a\u5b9e\u73b02.24\u500d\u52a0\u901f\uff0c\u5728\u667a\u80fd\u4f53\u4efb\u52a1\u4e0a\u5b9e\u73b02.72\u500d\u52a0\u901f\uff0c\u5f02\u6b65\u8bad\u7ec3\u6027\u80fd\u4e0e\u540c\u6b65\u8bad\u7ec3\u76f8\u5f53\u3002", "conclusion": "ROLL Flash\u901a\u8fc7\u5f02\u6b65\u8bad\u7ec3\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u540c\u6b65\u8bad\u7ec3\u76f8\u5f53\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.11347", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11347", "abs": "https://arxiv.org/abs/2510.11347", "authors": ["Etzion Harari", "Moshe Unger"], "title": "Multi-View Graph Feature Propagation for Privacy Preservation and Feature Sparsity", "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable success in node\nclassification tasks over relational data, yet their effectiveness often\ndepends on the availability of complete node features. In many real-world\nscenarios, however, feature matrices are highly sparse or contain sensitive\ninformation, leading to degraded performance and increased privacy risks.\nFurthermore, direct exposure of information can result in unintended data\nleakage, enabling adversaries to infer sensitive information. To address these\nchallenges, we propose a novel Multi-view Feature Propagation (MFP) framework\nthat enhances node classification under feature sparsity while promoting\nprivacy preservation. MFP extends traditional Feature Propagation (FP) by\ndividing the available features into multiple Gaussian-noised views, each\npropagating information independently through the graph topology. The\naggregated representations yield expressive and robust node embeddings. This\nframework is novel in two respects: it introduces a mechanism that improves\nrobustness under extreme sparsity, and it provides a principled way to balance\nutility with privacy. Extensive experiments conducted on graph datasets\ndemonstrate that MFP outperforms state-of-the-art baselines in node\nclassification while substantially reducing privacy leakage. Moreover, our\nanalysis demonstrates that propagated outputs serve as alternative imputations\nrather than reconstructions of the original features, preserving utility\nwithout compromising privacy. A comprehensive sensitivity analysis further\nconfirms the stability and practical applicability of MFP across diverse\nscenarios. Overall, MFP provides an effective and privacy-aware framework for\ngraph learning in domains characterized by missing or sensitive features.", "AI": {"tldr": "\u63d0\u51fa\u591a\u89c6\u56fe\u7279\u5f81\u4f20\u64ad(MFP)\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7279\u5f81\u5212\u5206\u4e3a\u591a\u4e2a\u9ad8\u65af\u566a\u58f0\u89c6\u56fe\u8fdb\u884c\u72ec\u7acb\u4f20\u64ad\uff0c\u5728\u7279\u5f81\u7a00\u758f\u60c5\u51b5\u4e0b\u63d0\u5347\u8282\u70b9\u5206\u7c7b\u6027\u80fd\u5e76\u4fdd\u62a4\u9690\u79c1\u3002", "motivation": "\u89e3\u51b3\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u8282\u70b9\u5206\u7c7b\u4e2d\u9762\u4e34\u7684\u7279\u5f81\u7a00\u758f\u548c\u9690\u79c1\u6cc4\u9732\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u7279\u5f81\u4e0d\u5b8c\u6574\u6216\u654f\u611f\u65f6\u6027\u80fd\u4e0b\u964d\u4e14\u5b58\u5728\u6570\u636e\u6cc4\u9732\u98ce\u9669\u3002", "method": "\u6269\u5c55\u4f20\u7edf\u7279\u5f81\u4f20\u64ad\u65b9\u6cd5\uff0c\u5c06\u53ef\u7528\u7279\u5f81\u5212\u5206\u4e3a\u591a\u4e2a\u6dfb\u52a0\u9ad8\u65af\u566a\u58f0\u7684\u89c6\u56fe\uff0c\u6bcf\u4e2a\u89c6\u56fe\u901a\u8fc7\u56fe\u62d3\u6251\u72ec\u7acb\u4f20\u64ad\u4fe1\u606f\uff0c\u6700\u540e\u805a\u5408\u751f\u6210\u9c81\u68d2\u7684\u8282\u70b9\u5d4c\u5165\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMFP\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u9690\u79c1\u6cc4\u9732\uff0c\u4f20\u64ad\u8f93\u51fa\u4f5c\u4e3a\u66ff\u4ee3\u63d2\u8865\u800c\u975e\u539f\u59cb\u7279\u5f81\u91cd\u5efa\u3002", "conclusion": "MFP\u4e3a\u5177\u6709\u7f3a\u5931\u6216\u654f\u611f\u7279\u5f81\u7684\u56fe\u5b66\u4e60\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u9690\u79c1\u611f\u77e5\u7684\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u5b9e\u7528\u6027\u7684\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u3002"}}
{"id": "2510.11390", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11390", "abs": "https://arxiv.org/abs/2510.11390", "authors": ["Razvan Marinescu", "Victoria-Elisabeth Gruber", "Diego Fajardo"], "title": "Medical Interpretability and Knowledge Maps of Large Language Models", "comment": "29 pages, 34 figures, 5 tables", "summary": "We present a systematic study of medical-domain interpretability in Large\nLanguage Models (LLMs). We study how the LLMs both represent and process\nmedical knowledge through four different interpretability techniques: (1) UMAP\nprojections of intermediate activations, (2) gradient-based saliency with\nrespect to the model weights, (3) layer lesioning/removal and (4) activation\npatching. We present knowledge maps of five LLMs which show, at a\ncoarse-resolution, where knowledge about patient's ages, medical symptoms,\ndiseases and drugs is stored in the models. In particular for Llama3.3-70B, we\nfind that most medical knowledge is processed in the first half of the model's\nlayers. In addition, we find several interesting phenomena: (i) age is often\nencoded in a non-linear and sometimes discontinuous manner at intermediate\nlayers in the models, (ii) the disease progression representation is\nnon-monotonic and circular at certain layers of the model, (iii) in\nLlama3.3-70B, drugs cluster better by medical specialty rather than mechanism\nof action, especially for Llama3.3-70B and (iv) Gemma3-27B and MedGemma-27B\nhave activations that collapse at intermediate layers but recover by the final\nlayers. These results can guide future research on fine-tuning, un-learning or\nde-biasing LLMs for medical tasks by suggesting at which layers in the model\nthese techniques should be applied.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u56db\u79cd\u6280\u672f\u63a2\u7a76\u6a21\u578b\u5982\u4f55\u8868\u793a\u548c\u5904\u7406\u533b\u5b66\u77e5\u8bc6\uff0c\u53d1\u73b0\u4e86\u533b\u5b66\u77e5\u8bc6\u4e3b\u8981\u5206\u5e03\u5728\u6a21\u578b\u524d\u534a\u90e8\u5206\u5c42\u3001\u5e74\u9f84\u7f16\u7801\u7684\u975e\u7ebf\u6027\u7279\u5f81\u3001\u75be\u75c5\u8fdb\u5c55\u8868\u793a\u7684\u975e\u5355\u8c03\u6027\u7b49\u6709\u8da3\u73b0\u8c61\u3002", "motivation": "\u7406\u89e3LLMs\u5728\u533b\u5b66\u9886\u57df\u5982\u4f55\u8868\u793a\u548c\u5904\u7406\u77e5\u8bc6\uff0c\u4e3a\u533b\u5b66\u4efb\u52a1\u7684\u5fae\u8c03\u3001\u9057\u5fd8\u6216\u53bb\u504f\u63d0\u4f9b\u6307\u5bfc\uff0c\u786e\u5b9a\u5728\u54ea\u4e9b\u6a21\u578b\u5c42\u5e94\u7528\u8fd9\u4e9b\u6280\u672f\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff1a(1) UMAP\u6295\u5f71\u4e2d\u95f4\u6fc0\u6d3b\uff0c(2) \u57fa\u4e8e\u68af\u5ea6\u7684\u6743\u91cd\u663e\u8457\u6027\u5206\u6790\uff0c(3) \u5c42\u5207\u9664/\u79fb\u9664\uff0c(4) \u6fc0\u6d3b\u4fee\u8865\u3002\u7814\u7a76\u4e86\u4e94\u4e2aLLMs\u7684\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u53d1\u73b0\u533b\u5b66\u77e5\u8bc6\u4e3b\u8981\u5728\u524d\u534a\u90e8\u5206\u5c42\u5904\u7406\uff1b\u5e74\u9f84\u7f16\u7801\u5728\u4e2d\u95f4\u5c42\u5448\u73b0\u975e\u7ebf\u6027\uff1b\u75be\u75c5\u8fdb\u5c55\u8868\u793a\u5728\u67d0\u4e9b\u5c42\u5448\u975e\u5355\u8c03\u548c\u5faa\u73af\uff1b\u836f\u7269\u6309\u533b\u5b66\u4e13\u79d1\u800c\u975e\u4f5c\u7528\u673a\u5236\u805a\u7c7b\uff1b\u67d0\u4e9b\u6a21\u578b\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u5d29\u6e83\u4f46\u6700\u7ec8\u5c42\u6062\u590d\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u53ef\u4e3a\u533b\u5b66\u4efb\u52a1\u7684LLMs\u5fae\u8c03\u3001\u9057\u5fd8\u548c\u53bb\u504f\u63d0\u4f9b\u6307\u5bfc\uff0c\u5efa\u8bae\u5728\u7279\u5b9a\u6a21\u578b\u5c42\u5e94\u7528\u8fd9\u4e9b\u6280\u672f\u3002"}}
{"id": "2510.11400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11400", "abs": "https://arxiv.org/abs/2510.11400", "authors": ["Kahou Tam", "Chunlin Tian", "Li Li", "Haikai Zhao", "ChengZhong Xu"], "title": "FedHybrid: Breaking the Memory Wall of Federated Learning via Hybrid Tensor Management", "comment": "Sensys 2024", "summary": "Federated Learning (FL) emerges as a new learning paradigm that enables\nmultiple devices to collaboratively train a shared model while preserving data\nprivacy. However, one fundamental and prevailing challenge that hinders the\ndeployment of FL on mobile devices is the memory limitation. This paper\nproposes \\textit{FedHybrid}, a novel framework that effectively reduces the\nmemory footprint during the training process while guaranteeing the model\naccuracy and the overall training progress. Specifically, \\textit{FedHybrid}\nfirst selects the participating devices for each training round by jointly\nevaluating their memory budget, computing capability, and data diversity. After\nthat, it judiciously analyzes the computational graph and generates an\nexecution plan for each selected client in order to meet the corresponding\nmemory budget while minimizing the training delay through employing a hybrid of\nrecomputation and compression techniques according to the characteristic of\neach tensor. During the local training process, \\textit{FedHybrid} carries out\nthe execution plan with a well-designed activation compression technique to\neffectively achieve memory reduction with minimum accuracy loss. We conduct\nextensive experiments to evaluate \\textit{FedHybrid} on both simulation and\noff-the-shelf mobile devices. The experiment results demonstrate that\n\\textit{FedHybrid} achieves up to a 39.1\\% increase in model accuracy and a\n15.5$\\times$ reduction in wall clock time under various memory budgets compared\nwith the baselines.", "AI": {"tldr": "FedHybrid\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4f7f\u7528\u91cd\u8ba1\u7b97\u548c\u538b\u7f29\u6280\u672f\u6765\u51cf\u5c11\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u4fdd\u8bc1\u6a21\u578b\u7cbe\u5ea6\u548c\u8bad\u7ec3\u8fdb\u5ea6\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u9762\u4e34\u7684\u5185\u5b58\u9650\u5236\u95ee\u9898\uff0c\u8fd9\u662f\u963b\u788dFL\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5e94\u7528\u7684\u4e3b\u8981\u6311\u6218\u3002", "method": "1) \u57fa\u4e8e\u5185\u5b58\u9884\u7b97\u3001\u8ba1\u7b97\u80fd\u529b\u548c\u6570\u636e\u591a\u6837\u6027\u9009\u62e9\u53c2\u4e0e\u8bbe\u5907\uff1b2) \u5206\u6790\u8ba1\u7b97\u56fe\u5e76\u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u751f\u6210\u6267\u884c\u8ba1\u5212\uff1b3) \u4f7f\u7528\u91cd\u8ba1\u7b97\u548c\u538b\u7f29\u6280\u672f\u7684\u6df7\u5408\u7b56\u7565\uff1b4) \u91c7\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6fc0\u6d3b\u538b\u7f29\u6280\u672f\u3002", "result": "\u5728\u5404\u79cd\u5185\u5b58\u9884\u7b97\u4e0b\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0cFedHybrid\u5b9e\u73b0\u4e86\u6a21\u578b\u7cbe\u5ea6\u6700\u9ad8\u63d0\u534739.1%\uff0c\u5899\u949f\u65f6\u95f4\u51cf\u5c1115.5\u500d\u3002", "conclusion": "FedHybrid\u6846\u67b6\u80fd\u6709\u6548\u51cf\u5c11\u8054\u90a6\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u4fdd\u8bc1\u6a21\u578b\u7cbe\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u53ef\u884c\u6027\u3002"}}
{"id": "2510.11409", "categories": ["cs.LG", "cs.DL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.11409", "abs": "https://arxiv.org/abs/2510.11409", "authors": ["Lucas Joos", "Daniel A. Keim", "Maximilian T. Fischer"], "title": "Leveraging LLMs for Semi-Automatic Corpus Filtration in Systematic Literature Reviews", "comment": null, "summary": "The creation of systematic literature reviews (SLR) is critical for analyzing\nthe landscape of a research field and guiding future research directions.\nHowever, retrieving and filtering the literature corpus for an SLR is highly\ntime-consuming and requires extensive manual effort, as keyword-based searches\nin digital libraries often return numerous irrelevant publications. In this\nwork, we propose a pipeline leveraging multiple large language models (LLMs),\nclassifying papers based on descriptive prompts and deciding jointly using a\nconsensus scheme. The entire process is human-supervised and interactively\ncontrolled via our open-source visual analytics web interface, LLMSurver, which\nenables real-time inspection and modification of model outputs. We evaluate our\napproach using ground-truth data from a recent SLR comprising over 8,000\ncandidate papers, benchmarking both open and commercial state-of-the-art LLMs\nfrom mid-2024 and fall 2025. Results demonstrate that our pipeline\nsignificantly reduces manual effort while achieving lower error rates than\nsingle human annotators. Furthermore, modern open-source models prove\nsufficient for this task, making the method accessible and cost-effective.\nOverall, our work demonstrates how responsible human-AI collaboration can\naccelerate and enhance systematic literature reviews within academic workflows.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u591aLLM\u7684\u7ba1\u9053\uff0c\u901a\u8fc7\u63cf\u8ff0\u6027\u63d0\u793a\u5206\u7c7b\u8bba\u6587\u5e76\u4f7f\u7528\u5171\u8bc6\u673a\u5236\u8054\u5408\u51b3\u7b56\uff0c\u663e\u8457\u51cf\u5c11\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7684\u624b\u52a8\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u5b9e\u73b0\u6bd4\u5355\u4e2a\u4eba\u7c7b\u6807\u6ce8\u8005\u66f4\u4f4e\u7684\u9519\u8bef\u7387\u3002", "motivation": "\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7684\u6587\u732e\u68c0\u7d22\u548c\u7b5b\u9009\u8fc7\u7a0b\u8017\u65f6\u4e14\u9700\u8981\u5927\u91cf\u4eba\u5de5\u52aa\u529b\uff0c\u6570\u5b57\u56fe\u4e66\u9986\u4e2d\u57fa\u4e8e\u5173\u952e\u8bcd\u7684\u641c\u7d22\u7ecf\u5e38\u8fd4\u56de\u5927\u91cf\u4e0d\u76f8\u5173\u6587\u732e\u3002", "method": "\u5229\u7528\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u7ba1\u9053\uff0c\u57fa\u4e8e\u63cf\u8ff0\u6027\u63d0\u793a\u5bf9\u8bba\u6587\u8fdb\u884c\u5206\u7c7b\uff0c\u91c7\u7528\u5171\u8bc6\u65b9\u6848\u8fdb\u884c\u8054\u5408\u51b3\u7b56\uff0c\u6574\u4e2a\u8fc7\u7a0b\u901a\u8fc7\u5f00\u6e90\u53ef\u89c6\u5316\u5206\u6790\u754c\u9762LLMSurver\u8fdb\u884c\u4eba\u5de5\u76d1\u7763\u548c\u4ea4\u4e92\u63a7\u5236\u3002", "result": "\u5728\u5305\u542b8000\u591a\u7bc7\u5019\u9009\u8bba\u6587\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u624b\u52a8\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u5b9e\u73b0\u6bd4\u5355\u4e2a\u4eba\u7c7b\u6807\u6ce8\u8005\u66f4\u4f4e\u7684\u9519\u8bef\u7387\uff0c\u4e14\u73b0\u4ee3\u5f00\u6e90\u6a21\u578b\u8db3\u4ee5\u80dc\u4efb\u6b64\u4efb\u52a1\u3002", "conclusion": "\u8d1f\u8d23\u4efb\u7684\u4eba\u673a\u534f\u4f5c\u53ef\u4ee5\u52a0\u901f\u548c\u589e\u5f3a\u5b66\u672f\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u4f7f\u8be5\u65b9\u6cd5\u65e2\u6613\u4e8e\u83b7\u53d6\u53c8\u6210\u672c\u6548\u76ca\u9ad8\u3002"}}
{"id": "2510.11442", "categories": ["cs.LG", "cs.AI", "68T05", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.11442", "abs": "https://arxiv.org/abs/2510.11442", "authors": ["Xinyan Guan", "Yongfan Lai", "Jiarui Jin", "Jun Li", "Haoyu Wang", "Qinghao Zhao", "Deyun Zhang", "Shijia Geng", "Shenda Hong"], "title": "Reconstructing 12-Lead ECG from 3-Lead ECG using Variational Autoencoder to Improve Cardiac Disease Detection of Wearable ECG Devices", "comment": "24 pages, 5 figures, submitted to Nature Communications", "summary": "Twelve-lead electrocardiograms (ECGs) are the clinical gold standard for\ncardiac diagnosis, providing comprehensive spatial coverage of the heart\nnecessary to detect conditions such as myocardial infarction (MI). However,\ntheir lack of portability limits continuous and large-scale use. Three-lead ECG\nsystems are widely used in wearable devices due to their simplicity and\nmobility, but they often fail to capture pathologies in unmeasured regions. To\naddress this, we propose WearECG, a Variational Autoencoder (VAE) method that\nreconstructs twelve-lead ECGs from three leads: II, V1, and V5. Our model\nincludes architectural improvements to better capture temporal and spatial\ndependencies in ECG signals. We evaluate generation quality using MSE, MAE, and\nFrechet Inception Distance (FID), and assess clinical validity via a Turing\ntest with expert cardiologists. To further validate diagnostic utility, we\nfine-tune ECGFounder, a large-scale pretrained ECG model, on a multi-label\nclassification task involving over 40 cardiac conditions, including six\ndifferent myocardial infarction locations, using both real and generated\nsignals. Experiments on the MIMIC dataset show that our method produces\nphysiologically realistic and diagnostically informative signals, with robust\nperformance in downstream tasks. This work demonstrates the potential of\ngenerative modeling for ECG reconstruction and its implications for scalable,\nlow-cost cardiac screening.", "AI": {"tldr": "\u63d0\u51faWearECG\u65b9\u6cd5\uff0c\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4ece\u4e09\u4e2a\u5bfc\u8054\uff08II\u3001V1\u3001V5\uff09\u91cd\u5efa\u5341\u4e8c\u5bfc\u8054\u5fc3\u7535\u56fe\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u67b6\u6784\u6355\u6349\u65f6\u7a7a\u4f9d\u8d56\u6027\uff0c\u5728\u4e34\u5e8a\u8bca\u65ad\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u5341\u4e8c\u5bfc\u8054\u5fc3\u7535\u56fe\u662f\u5fc3\u810f\u8bca\u65ad\u7684\u91d1\u6807\u51c6\u4f46\u4fbf\u643a\u6027\u5dee\uff0c\u4e09\u5bfc\u8054\u7cfb\u7edf\u867d\u4fbf\u643a\u4f46\u65e0\u6cd5\u68c0\u6d4b\u672a\u6d4b\u91cf\u533a\u57df\u7684\u75c5\u7406\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u4ece\u4fbf\u643a\u7684\u4e09\u5bfc\u8054\u7cfb\u7edf\u91cd\u5efa\u5b8c\u6574\u7684\u5341\u4e8c\u5bfc\u8054\u5fc3\u7535\u56fe\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u67b6\u6784\uff0c\u6539\u8fdb\u6a21\u578b\u4ee5\u66f4\u597d\u5730\u6355\u6349ECG\u4fe1\u53f7\u7684\u65f6\u7a7a\u4f9d\u8d56\u6027\uff0c\u4ece\u4e09\u4e2a\u5bfc\u8054\uff08II\u3001V1\u3001V5\uff09\u91cd\u5efa\u5341\u4e8c\u5bfc\u8054ECG\u3002", "result": "\u5728MIMIC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u751f\u7406\u4e0a\u771f\u5b9e\u4e14\u5177\u6709\u8bca\u65ad\u4fe1\u606f\u7684\u4fe1\u53f7\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u901a\u8fc7\u4e13\u5bb6\u56fe\u7075\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u4e34\u5e8a\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u751f\u6210\u6a21\u578b\u5728ECG\u91cd\u5efa\u4e2d\u7684\u6f5c\u529b\uff0c\u5bf9\u53ef\u6269\u5c55\u3001\u4f4e\u6210\u672c\u7684\u5fc3\u810f\u7b5b\u67e5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.11471", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11471", "abs": "https://arxiv.org/abs/2510.11471", "authors": ["Sarthak Mittal", "Divyat Mahajan", "Guillaume Lajoie", "Mohammad Pezeshki"], "title": "Iterative Amortized Inference: Unifying In-Context Learning and Learned Optimizers", "comment": null, "summary": "Modern learning systems increasingly rely on amortized learning - the idea of\nreusing computation or inductive biases shared across tasks to enable rapid\ngeneralization to novel problems. This principle spans a range of approaches,\nincluding meta-learning, in-context learning, prompt tuning, learned optimizers\nand more. While motivated by similar goals, these approaches differ in how they\nencode and leverage task-specific information, often provided as in-context\nexamples. In this work, we propose a unified framework which describes how such\nmethods differ primarily in the aspects of learning they amortize - such as\ninitializations, learned updates, or predictive mappings - and how they\nincorporate task data at inference. We introduce a taxonomy that categorizes\namortized models into parametric, implicit, and explicit regimes, based on\nwhether task adaptation is externalized, internalized, or jointly modeled.\nBuilding on this view, we identify a key limitation in current approaches: most\nmethods struggle to scale to large datasets because their capacity to process\ntask data at inference (e.g., context length) is often limited. To address\nthis, we propose iterative amortized inference, a class of models that refine\nsolutions step-by-step over mini-batches, drawing inspiration from stochastic\noptimization. Our formulation bridges optimization-based meta-learning with\nforward-pass amortization in models like LLMs, offering a scalable and\nextensible foundation for general-purpose task adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u5206\u7c7b\u5404\u79cd\u644a\u9500\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bc6\u522b\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u8fed\u4ee3\u644a\u9500\u63a8\u7406\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u4ee3\u5b66\u4e60\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56\u644a\u9500\u5b66\u4e60\uff0c\u4f46\u5404\u79cd\u65b9\u6cd5\u5728\u7f16\u7801\u548c\u5229\u7528\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u5dee\u5f02\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u6846\u67b6\uff0c\u5c06\u644a\u9500\u6a21\u578b\u5206\u4e3a\u53c2\u6570\u5316\u3001\u9690\u5f0f\u548c\u663e\u5f0f\u4e09\u79cd\u673a\u5236\uff0c\u5e76\u5f15\u5165\u4e86\u8fed\u4ee3\u644a\u9500\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c0f\u6279\u91cf\u9010\u6b65\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u8be5\u6846\u67b6\u7edf\u4e00\u4e86\u4f18\u5316\u57fa\u7840\u7684\u5143\u5b66\u4e60\u548c\u524d\u5411\u4f20\u9012\u644a\u9500\u65b9\u6cd5\uff0c\u4e3a\u89e3\u51b3\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u4efb\u52a1\u9002\u5e94\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002", "conclusion": "\u8fed\u4ee3\u644a\u9500\u63a8\u7406\u65b9\u6cd5\u7ed3\u5408\u4e86\u968f\u673a\u4f18\u5316\u7684\u601d\u60f3\uff0c\u4e3a\u901a\u7528\u4efb\u52a1\u9002\u5e94\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.11472", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11472", "abs": "https://arxiv.org/abs/2510.11472", "authors": ["Yanjie Zhu", "Zhen Zhang", "Yunli Wang", "Zhiqiang Wang", "Yu Li", "Rufan Zhou", "Shiyang Wen", "Peng Jiang", "Chenhao Lin", "Jian Yang"], "title": "Differentiable Fast Top-K Selection for Large-Scale Recommendation", "comment": "12 pages, 5 figures", "summary": "Cascade ranking is a widely adopted paradigm in large-scale information\nretrieval systems for Top-K item selection. However, the Top-K operator is\nnon-differentiable, hindering end-to-end training. Existing methods include\nLearning-to-Rank approaches (e.g., LambdaLoss), which optimize ranking metrics\nlike NDCG and suffer from objective misalignment, and differentiable\nsorting-based methods (e.g., ARF, LCRON), which relax permutation matrices for\ndirect Top-K optimization but introduce gradient conflicts through matrix\naggregation. A promising alternative is to directly construct a differentiable\napproximation of the Top-K selection operator, bypassing the use of soft\npermutation matrices. However, even state-of-the-art differentiable Top-K\noperator (e.g., LapSum) require $O(n \\log n)$ complexity due to their\ndependence on sorting for solving the threshold. Thus, we propose DFTopK, a\nnovel differentiable Top-K operator achieving optimal $O(n)$ time complexity.\nBy relaxing normalization constraints, DFTopK admits a closed-form solution and\navoids sorting. DFTopK also avoids the gradient conflicts inherent in\ndifferentiable sorting-based methods. We evaluate DFTopK on both the public\nbenchmark RecFLow and an industrial system. Experimental results show that\nDFTopK significantly improves training efficiency while achieving superior\nperformance, which enables us to scale up training samples more efficiently. In\nthe online A/B test, DFTopK yielded a +1.77\\% revenue lift with the same\ncomputational budget compared to the baseline. To the best of our knowledge,\nthis work is the first to introduce differentiable Top-K operators into\nrecommendation systems and the first to achieve theoretically optimal\nlinear-time complexity for Top-K selection. We have open-sourced our\nimplementation to facilitate future research in both academia and industry.", "AI": {"tldr": "\u63d0\u51fa\u4e86DFTopK\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u5fae\u5206Top-K\u7b97\u5b50\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u7684O(n)\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u5e76\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u7ea7\u8054\u6392\u5e8f\u5728\u5927\u89c4\u6a21\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u4e2d\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46Top-K\u7b97\u5b50\u4e0d\u53ef\u5fae\u5206\uff0c\u963b\u788d\u4e86\u7aef\u5230\u7aef\u8bad\u7ec3\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u76ee\u6807\u4e0d\u4e00\u81f4\u6216\u68af\u5ea6\u51b2\u7a81\u7b49\u95ee\u9898\uff0c\u4e14\u6700\u5148\u8fdb\u7684\u53ef\u5fae\u5206Top-K\u7b97\u5b50\u9700\u8981O(n log n)\u590d\u6742\u5ea6\u3002", "method": "\u901a\u8fc7\u653e\u677e\u5f52\u4e00\u5316\u7ea6\u675f\uff0cDFTopK\u83b7\u5f97\u4e86\u95ed\u5f0f\u89e3\u5e76\u907f\u514d\u4e86\u6392\u5e8f\u64cd\u4f5c\uff0c\u5b9e\u73b0\u4e86\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002\u8be5\u65b9\u6cd5\u7ed5\u8fc7\u4e86\u4f7f\u7528\u8f6f\u7f6e\u6362\u77e9\u9635\uff0c\u76f4\u63a5\u6784\u5efa\u4e86Top-K\u9009\u62e9\u7b97\u5b50\u7684\u53ef\u5fae\u5206\u8fd1\u4f3c\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6RecFLow\u548c\u5de5\u4e1a\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDFTopK\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u5e76\u5b9e\u73b0\u4e86\u66f4\u4f18\u6027\u80fd\u3002\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\uff0c\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u5b9e\u73b0\u4e86+1.77%\u7684\u6536\u5165\u63d0\u5347\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5c06\u53ef\u5fae\u5206Top-K\u7b97\u5b50\u5f15\u5165\u63a8\u8350\u7cfb\u7edf\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4e86\u7406\u8bba\u6700\u4f18\u7684\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6Top-K\u9009\u62e9\u3002\u8be5\u65b9\u6cd5\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2510.11484", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.11484", "abs": "https://arxiv.org/abs/2510.11484", "authors": ["Lion Mueller", "Alberto Garcia-Ortiz", "Ardalan Najafi", "Adam Fuks", "Lennart Bamberg"], "title": "Rescaling-Aware Training for Efficient Deployment of Deep Learning Models on Full-Integer Hardware", "comment": "Submitted to IEEE Embedded Systems Letters", "summary": "Integer AI inference significantly reduces computational complexity in\nembedded systems. Quantization-aware training (QAT) helps mitigate accuracy\ndegradation associated with post-training quantization but still overlooks the\nimpact of integer rescaling during inference, which is a hardware costly\noperation in integer-only AI inference. This work shows that rescaling cost can\nbe dramatically reduced post-training, by applying a stronger quantization to\nthe rescale multiplicands at no model-quality loss. Furthermore, we introduce\nRescale-Aware Training, a fine tuning method for ultra-low bit-width rescaling\nmultiplicands. Experiments show that even with 8x reduced rescaler widths, the\nfull accuracy is preserved through minimal incremental retraining. This enables\nmore energy-efficient and cost-efficient AI inference for resource-constrained\nembedded systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51cf\u5c11\u6574\u6570AI\u63a8\u7406\u4e2d\u91cd\u65b0\u7f29\u653e\u64cd\u4f5c\u6210\u672c\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u540e\u8bad\u7ec3\u5f3a\u91cf\u5316\u91cd\u65b0\u7f29\u653e\u4e58\u6570\uff0c\u5e76\u5f15\u5165\u91cd\u65b0\u7f29\u653e\u611f\u77e5\u8bad\u7ec3\u6765\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u6574\u6570AI\u63a8\u7406\u663e\u8457\u964d\u4f4e\u4e86\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4f46\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u4ecd\u5ffd\u7565\u4e86\u6574\u6570\u91cd\u65b0\u7f29\u653e\u64cd\u4f5c\u7684\u5f71\u54cd\uff0c\u8fd9\u662f\u6574\u6570AI\u63a8\u7406\u4e2d\u786c\u4ef6\u6210\u672c\u8f83\u9ad8\u7684\u64cd\u4f5c\u3002", "method": "\u540e\u8bad\u7ec3\u5e94\u7528\u66f4\u5f3a\u7684\u91cf\u5316\u5230\u91cd\u65b0\u7f29\u653e\u4e58\u6570\uff0c\u5e76\u5f15\u5165\u91cd\u65b0\u7f29\u653e\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\u8fdb\u884c\u5fae\u8c03\uff0c\u652f\u6301\u8d85\u4f4e\u6bd4\u7279\u5bbd\u5ea6\u7684\u91cd\u65b0\u7f29\u653e\u4e58\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u91cd\u65b0\u7f29\u653e\u5668\u5bbd\u5ea6\u51cf\u5c118\u500d\uff0c\u901a\u8fc7\u6700\u5c0f\u589e\u91cf\u91cd\u65b0\u8bad\u7ec3\u4ecd\u80fd\u4fdd\u6301\u5b8c\u6574\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f7f\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u7cfb\u7edf\u80fd\u591f\u5b9e\u73b0\u66f4\u8282\u80fd\u548c\u6210\u672c\u6548\u76ca\u66f4\u9ad8\u7684AI\u63a8\u7406\u3002"}}
{"id": "2510.11499", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11499", "abs": "https://arxiv.org/abs/2510.11499", "authors": ["Xinsong Feng", "Leshu Tang", "Chenan Wang", "Haipeng Chen"], "title": "Offline Reinforcement Learning with Generative Trajectory Policies", "comment": "Preprint. Under review at ICLR 2026", "summary": "Generative models have emerged as a powerful class of policies for offline\nreinforcement learning (RL) due to their ability to capture complex,\nmulti-modal behaviors. However, existing methods face a stark trade-off: slow,\niterative models like diffusion policies are computationally expensive, while\nfast, single-step models like consistency policies often suffer from degraded\nperformance. In this paper, we demonstrate that it is possible to bridge this\ngap. The key to moving beyond the limitations of individual methods, we argue,\nlies in a unifying perspective that views modern generative models, including\ndiffusion, flow matching, and consistency models, as specific instances of\nlearning a continuous-time generative trajectory governed by an Ordinary\nDifferential Equation (ODE). This principled foundation provides a clearer\ndesign space for generative policies in RL and allows us to propose Generative\nTrajectory Policies (GTPs), a new and more general policy paradigm that learns\nthe entire solution map of the underlying ODE. To make this paradigm practical\nfor offline RL, we further introduce two key theoretically principled\nadaptations. Empirical results demonstrate that GTP achieves state-of-the-art\nperformance on D4RL benchmarks - it significantly outperforms prior generative\npolicies, achieving perfect scores on several notoriously hard AntMaze tasks.", "AI": {"tldr": "\u63d0\u51faGenerative Trajectory Policies (GTPs)\uff0c\u4e00\u79cd\u57fa\u4e8eODE\u7684\u7edf\u4e00\u751f\u6210\u6a21\u578b\u7b56\u7565\uff0c\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u79bb\u7ebfRL\u4e2d\u5b58\u5728\u6027\u80fd\u4e0e\u6548\u7387\u7684\u6743\u8861\uff1a\u6269\u6563\u7b56\u7565\u8ba1\u7b97\u6602\u8d35\u4f46\u6027\u80fd\u597d\uff0c\u4e00\u81f4\u6027\u7b56\u7565\u5feb\u901f\u4f46\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u517c\u987e\u4e24\u8005\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u6269\u6563\u3001\u6d41\u5339\u914d\u548c\u4e00\u81f4\u6027\u6a21\u578b\u7edf\u4e00\u89c6\u4e3a\u5b66\u4e60ODE\u63a7\u5236\u7684\u8fde\u7eed\u65f6\u95f4\u751f\u6210\u8f68\u8ff9\uff0c\u63d0\u51faGTPs\u5b66\u4e60\u5e95\u5c42ODE\u7684\u5b8c\u6574\u89e3\u6620\u5c04\uff0c\u5e76\u5f15\u5165\u4e24\u4e2a\u7406\u8bba\u9a71\u52a8\u7684\u9002\u914d\u65b9\u6cd5\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u5148\u524d\u751f\u6210\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u56f0\u96be\u7684AntMaze\u4efb\u52a1\u4e2d\u83b7\u5f97\u5b8c\u7f8e\u5206\u6570\u3002", "conclusion": "GTPs\u6210\u529f\u5f25\u5408\u4e86\u751f\u6210\u6a21\u578b\u5728\u79bb\u7ebfRL\u4e2d\u7684\u6027\u80fd-\u6548\u7387\u5dee\u8ddd\uff0c\u4e3a\u751f\u6210\u7b56\u7565\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u7684\u8bbe\u8ba1\u7a7a\u95f4\u548c\u7edf\u4e00\u6846\u67b6\u3002"}}
{"id": "2510.11501", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11501", "abs": "https://arxiv.org/abs/2510.11501", "authors": ["Emran Yasser Moustafa", "Ivana Dusparic"], "title": "Context-Aware Model-Based Reinforcement Learning for Autonomous Racing", "comment": "Accepted to IEEE ICAR 2025", "summary": "Autonomous vehicles have shown promising potential to be a groundbreaking\ntechnology for improving the safety of road users. For these vehicles, as well\nas many other safety-critical robotic technologies, to be deployed in\nreal-world applications, we require algorithms that can generalize well to\nunseen scenarios and data. Model-based reinforcement learning algorithms (MBRL)\nhave demonstrated state-of-the-art performance and data efficiency across a\ndiverse set of domains. However, these algorithms have also shown\nsusceptibility to changes in the environment and its transition dynamics.\n  In this work, we explore the performance and generalization capabilities of\nMBRL algorithms for autonomous driving, specifically in the simulated\nautonomous racing environment, Roboracer (formerly F1Tenth). We frame the\nhead-to-head racing task as a learning problem using contextual Markov decision\nprocesses and parameterize the driving behavior of the adversaries using the\ncontext of the episode, thereby also parameterizing the transition and reward\ndynamics. We benchmark the behavior of MBRL algorithms in this environment and\npropose a novel context-aware extension of the existing literature, cMask. We\ndemonstrate that context-aware MBRL algorithms generalize better to\nout-of-distribution adversary behaviors relative to context-free approaches. We\nalso demonstrate that cMask displays strong generalization capabilities, as\nwell as further performance improvement relative to other context-aware MBRL\napproaches when racing against adversaries with in-distribution behaviors.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u73af\u5883\u4e2d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7684cMask\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4e0a\u4e0b\u6587\u65e0\u5173\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u6cdb\u5316\u5230\u5206\u5e03\u5916\u5bf9\u624b\u884c\u4e3a\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u573a\u666f\u7684\u7b97\u6cd5\u3002\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u867d\u7136\u6027\u80fd\u4f18\u5f02\uff0c\u4f46\u5bf9\u73af\u5883\u53d8\u5316\u654f\u611f\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5c06\u5934\u5bf9\u5934\u8d5b\u8f66\u4efb\u52a1\u6784\u5efa\u4e3a\u4e0a\u4e0b\u6587\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u7528\u4e0a\u4e0b\u6587\u53c2\u6570\u5316\u5bf9\u624b\u7684\u9a7e\u9a76\u884c\u4e3a\uff0c\u63d0\u51fa\u4e0a\u4e0b\u6587\u611f\u77e5\u7684cMask\u6269\u5c55\u65b9\u6cd5\u3002", "result": "\u4e0a\u4e0b\u6587\u611f\u77e5\u7684MBRL\u7b97\u6cd5\u76f8\u6bd4\u4e0a\u4e0b\u6587\u65e0\u5173\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u6cdb\u5316\u5230\u5206\u5e03\u5916\u5bf9\u624b\u884c\u4e3a\uff0ccMask\u5728\u5206\u5e03\u5185\u5bf9\u624b\u884c\u4e3a\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u7684MBRL\u65b9\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u4efb\u52a1\u4e2d\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0ccMask\u65b9\u6cd5\u5728\u5e94\u5bf9\u4e0d\u540c\u5bf9\u624b\u884c\u4e3a\u65f6\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.11502", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11502", "abs": "https://arxiv.org/abs/2510.11502", "authors": ["Alexis Ross", "Jacob Andreas"], "title": "Learning to Make MISTAKEs: Modeling Incorrect Student Thinking And Key Errors", "comment": null, "summary": "Research on reasoning in language models (LMs) predominantly focuses on\nimproving the correctness of their outputs. But some important applications\nrequire modeling reasoning patterns that are incorrect. For example, automated\nsystems that can reason about and simulate student errors are useful for\nproviding real-time feedback in the classroom or offline practice for\neducators-in-training. This paper presents a new method, MISTAKE, that (1)\nconstructs high-quality synthetic examples of reasoning errors by leveraging\ncycle consistency between incorrect answers and latent misconceptions; and (2)\nuses the generated data to learn models for student simulation, misconception\nclassification, and answer generation. We evaluate MISTAKE on three educational\ntasks and find that it results in (1) higher accuracy when simulating incorrect\nstudent answers based on specific misconceptions, (2) increased performance\ninferring latent misconceptions from observed incorrect answers, and (3) higher\nalignment with expert-written distractor answers when generating incorrect\nanswers (e.g., for multiple-choice tests).", "AI": {"tldr": "MISTAKE\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u9519\u8bef\u7b54\u6848\u4e0e\u6f5c\u5728\u8bef\u89e3\u4e4b\u95f4\u7684\u5faa\u73af\u4e00\u81f4\u6027\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u9519\u8bef\u63a8\u7406\u793a\u4f8b\uff0c\u7528\u4e8e\u5b66\u751f\u6a21\u62df\u3001\u8bef\u89e3\u5206\u7c7b\u548c\u7b54\u6848\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u63d0\u9ad8\u8f93\u51fa\u6b63\u786e\u6027\uff0c\u4f46\u67d0\u4e9b\u5e94\u7528\u9700\u8981\u5efa\u6a21\u9519\u8bef\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u5982\u6a21\u62df\u5b66\u751f\u9519\u8bef\u4ee5\u63d0\u4f9b\u5b9e\u65f6\u8bfe\u5802\u53cd\u9988\u6216\u6559\u5e08\u57f9\u8bad\u3002", "method": "MISTAKE\u65b9\u6cd5\uff1a(1) \u5229\u7528\u9519\u8bef\u7b54\u6848\u4e0e\u6f5c\u5728\u8bef\u89e3\u4e4b\u95f4\u7684\u5faa\u73af\u4e00\u81f4\u6027\u6784\u5efa\u9ad8\u8d28\u91cf\u5408\u6210\u9519\u8bef\u793a\u4f8b\uff1b(2) \u4f7f\u7528\u751f\u6210\u6570\u636e\u5b66\u4e60\u5b66\u751f\u6a21\u62df\u3001\u8bef\u89e3\u5206\u7c7b\u548c\u7b54\u6848\u751f\u6210\u6a21\u578b\u3002", "result": "\u5728\u4e09\u4e2a\u6559\u80b2\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff1a\u5b66\u751f\u6a21\u62df\u9519\u8bef\u7b54\u6848\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u4ece\u89c2\u5bdf\u5230\u7684\u9519\u8bef\u7b54\u6848\u63a8\u65ad\u6f5c\u5728\u8bef\u89e3\u6027\u80fd\u63d0\u5347\uff0c\u751f\u6210\u9519\u8bef\u7b54\u6848\u4e0e\u4e13\u5bb6\u7f16\u5199\u7684\u5e72\u6270\u9879\u5bf9\u9f50\u5ea6\u66f4\u9ad8\u3002", "conclusion": "MISTAKE\u65b9\u6cd5\u80fd\u6709\u6548\u5efa\u6a21\u9519\u8bef\u63a8\u7406\u6a21\u5f0f\uff0c\u5728\u6559\u80b2\u5e94\u7528\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u5b66\u751f\u6a21\u62df\u3001\u8bef\u89e3\u8bc6\u522b\u548c\u5e72\u6270\u9879\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.11505", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11505", "abs": "https://arxiv.org/abs/2510.11505", "authors": ["Aleksei Rozanov", "Samikshya Subedi", "Vasudha Sharma", "Bryan C. Runck"], "title": "Knowledge-Guided Machine Learning Models to Upscale Evapotranspiration in the U.S. Midwest", "comment": null, "summary": "Evapotranspiration (ET) plays a critical role in the land-atmosphere\ninteractions, yet its accurate quantification across various spatiotemporal\nscales remains a challenge. In situ measurement approaches, like eddy\ncovariance (EC) or weather station-based ET estimation, allow for measuring ET\nat a single location. Agricultural uses of ET require estimates for each field\nover broad areas, making it infeasible to deploy sensing systems at each\nlocation. This study integrates tree-based and knowledge-guided machine\nlearning (ML) techniques with multispectral remote sensing data, griddled\nmeteorology and EC data to upscale ET across the Midwest United States. We\ncompare four tree-based models - Random Forest, CatBoost, XGBoost, LightGBM -\nand a simple feed-forward artificial neural network in combination with\nfeatures engineered using knowledge-guided ML principles. Models were trained\nand tested on EC towers located in the Midwest of the United States using\nk-fold cross validation with k=5 and site-year, biome stratified train-test\nsplit to avoid data leakage. Results show that LightGBM with knowledge-guided\nfeatures outperformed other methods with an R2=0.86, MSE=14.99 W m^-2 and MAE =\n8.82 W m^-2 according to grouped k-fold validation (k=5). Feature importance\nanalysis shows that knowledge-guided features were most important for\npredicting evapotranspiration. Using the best performing model, we provide a\ndata product at 500 m spatial and one-day temporal resolution for gridded ET\nfor the period of 2019-2024. Intercomparison between the new gridded product\nand state-level weather station-based ET estimates show best-in-class\ncorrespondence.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7ed3\u5408\u57fa\u4e8e\u6811\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u548c\u77e5\u8bc6\u5f15\u5bfc\u7279\u5f81\uff0c\u5229\u7528\u591a\u5149\u8c31\u9065\u611f\u6570\u636e\u3001\u683c\u70b9\u6c14\u8c61\u6570\u636e\u548c\u6da1\u5ea6\u534f\u65b9\u5dee\u6570\u636e\uff0c\u5728\u7f8e\u56fd\u4e2d\u897f\u90e8\u5730\u533a\u5b9e\u73b0\u4e86\u84b8\u6563\u53d1\u7684\u51c6\u786e\u5347\u5c3a\u5ea6\u4f30\u7b97\u3002", "motivation": "\u84b8\u6563\u53d1\u5728\u9646\u6c14\u76f8\u4e92\u4f5c\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u5355\u70b9\u6d4b\u91cf\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u519c\u4e1a\u5e94\u7528\u4e2d\u5bf9\u5927\u8303\u56f4\u533a\u57df\u84b8\u6563\u53d1\u7684\u4f30\u7b97\u9700\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u5347\u5c3a\u5ea6\u65b9\u6cd5\u3002", "method": "\u6bd4\u8f83\u4e86\u56db\u79cd\u57fa\u4e8e\u6811\u7684\u6a21\u578b\uff08\u968f\u673a\u68ee\u6797\u3001CatBoost\u3001XGBoost\u3001LightGBM\uff09\u548c\u7b80\u5355\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u77e5\u8bc6\u5f15\u5bfc\u7684\u673a\u5668\u5b66\u4e60\u7279\u5f81\u5de5\u7a0b\uff0c\u91c7\u7528k\u6298\u4ea4\u53c9\u9a8c\u8bc1\u548c\u7ad9\u70b9-\u5e74\u4efd\u5206\u5c42\u5212\u5206\u907f\u514d\u6570\u636e\u6cc4\u9732\u3002", "result": "LightGBM\u6a21\u578b\u7ed3\u5408\u77e5\u8bc6\u5f15\u5bfc\u7279\u5f81\u8868\u73b0\u6700\u4f73\uff0cR\u00b2=0.86\uff0cMSE=14.99 W m\u207b\u00b2\uff0cMAE=8.82 W m\u207b\u00b2\u3002\u77e5\u8bc6\u5f15\u5bfc\u7279\u5f81\u5bf9\u9884\u6d4b\u84b8\u6563\u53d1\u6700\u4e3a\u91cd\u8981\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86500\u7c73\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u4e00\u5929\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u84b8\u6563\u53d1\u6570\u636e\u4ea7\u54c1\uff0c\u4e0e\u5dde\u7ea7\u6c14\u8c61\u7ad9\u4f30\u7b97\u7ed3\u679c\u76f8\u6bd4\u5177\u6709\u6700\u4f73\u4e00\u81f4\u6027\uff0c\u4e3a\u533a\u57df\u84b8\u6563\u53d1\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2510.11541", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11541", "abs": "https://arxiv.org/abs/2510.11541", "authors": ["Yuchen Yan", "Zhihua Liu", "Hao Wang", "Weiming Li", "Xiaoshuai Hao"], "title": "Query-Specific GNN: A Comprehensive Graph Representation Learning Method for Retrieval Augmented Generation", "comment": null, "summary": "Retrieval-augmented generation (RAG) has demonstrated its ability to enhance\nLarge Language Models (LLMs) by integrating external knowledge sources.\nHowever, multi-hop questions, which require the identification of multiple\nknowledge targets to form a synthesized answer, raise new challenges for RAG\nsystems. Under the multi-hop settings, existing methods often struggle to fully\nunderstand the questions with complex semantic structures and are susceptible\nto irrelevant noise during the retrieval of multiple information targets. To\naddress these limitations, we propose a novel graph representation learning\nframework for multi-hop question retrieval. We first introduce a\nMulti-information Level Knowledge Graph (Multi-L KG) to model various\ninformation levels for a more comprehensive understanding of multi-hop\nquestions. Based on this, we design a Query-Specific Graph Neural Network\n(QSGNN) for representation learning on the Multi-L KG. QSGNN employs\nintra/inter-level message passing mechanisms, and in each message passing the\ninformation aggregation is guided by the query, which not only facilitates\nmulti-granular information aggregation but also significantly reduces the\nimpact of noise. To enhance its ability to learn robust representations, we\nfurther propose two synthesized data generation strategies for pre-training the\nQSGNN. Extensive experimental results demonstrate the effectiveness of our\nframework in multi-hop scenarios, especially in high-hop questions the\nimprovement can reach 33.8\\%. The code is available at:\nhttps://github.com/Jerry2398/QSGNN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u8868\u793a\u5b66\u4e60\u7684\u591a\u8df3\u95ee\u9898\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4fe1\u606f\u7ea7\u522b\u77e5\u8bc6\u56fe\u8c31\u548c\u67e5\u8be2\u7279\u5b9a\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6765\u89e3\u51b3\u591a\u8df3\u95ee\u9898\u4e2d\u7684\u8bed\u4e49\u7406\u89e3\u548c\u566a\u58f0\u5e72\u6270\u95ee\u9898\u3002", "motivation": "\u591a\u8df3\u95ee\u9898\u9700\u8981\u8bc6\u522b\u591a\u4e2a\u77e5\u8bc6\u76ee\u6807\u6765\u5408\u6210\u7b54\u6848\uff0c\u8fd9\u5bf9\u73b0\u6709\u7684RAG\u7cfb\u7edf\u63d0\u51fa\u4e86\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u7406\u89e3\u590d\u6742\u8bed\u4e49\u7ed3\u6784\u7684\u95ee\u9898\uff0c\u4e14\u5728\u68c0\u7d22\u591a\u4e2a\u4fe1\u606f\u76ee\u6807\u65f6\u5bb9\u6613\u53d7\u5230\u65e0\u5173\u566a\u58f0\u7684\u5f71\u54cd\u3002", "method": "\u9996\u5148\u6784\u5efa\u591a\u4fe1\u606f\u7ea7\u522b\u77e5\u8bc6\u56fe\u8c31(Multi-L KG)\u6765\u5efa\u6a21\u4e0d\u540c\u4fe1\u606f\u7ea7\u522b\uff0c\u7136\u540e\u8bbe\u8ba1\u67e5\u8be2\u7279\u5b9a\u7684\u56fe\u795e\u7ecf\u7f51\u7edc(QSGNN)\u8fdb\u884c\u8868\u793a\u5b66\u4e60\uff0c\u91c7\u7528\u5c42\u5185/\u5c42\u95f4\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u4e24\u79cd\u5408\u6210\u6570\u636e\u751f\u6210\u7b56\u7565\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u5728\u591a\u8df3\u573a\u666f\u4e0b\u8868\u73b0\u6709\u6548\uff0c\u7279\u522b\u662f\u5728\u9ad8\u8df3\u95ee\u9898\u4e2d\u6539\u8fdb\u53ef\u8fbe33.8%\u3002", "conclusion": "\u63d0\u51fa\u7684\u56fe\u8868\u793a\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u591a\u8df3\u95ee\u9898\u68c0\u7d22\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u8bed\u4e49\u7ed3\u6784\u548c\u51cf\u5c11\u566a\u58f0\u5f71\u54cd\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.11561", "categories": ["cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2510.11561", "abs": "https://arxiv.org/abs/2510.11561", "authors": ["Caglar Demir", "Alkid Baci", "N'Dah Jean Kouagou", "Leonie Nora Sieger", "Stefan Heindorf", "Simon Bin", "Lukas Bl\u00fcbaum", "Alexander Bigerl", "Axel-Cyrille Ngonga Ngomo"], "title": "Ontolearn-A Framework for Large-scale OWL Class Expression Learning in Python", "comment": null, "summary": "In this paper, we present Ontolearn-a framework for learning OWL class\nexpressions over large knowledge graphs. Ontolearn contains efficient\nimplementations of recent stateof-the-art symbolic and neuro-symbolic class\nexpression learners including EvoLearner and DRILL. A learned OWL class\nexpression can be used to classify instances in the knowledge graph.\nFurthermore, Ontolearn integrates a verbalization module based on an LLM to\ntranslate complex OWL class expressions into natural language sentences. By\nmapping OWL class expressions into respective SPARQL queries, Ontolearn can be\neasily used to operate over a remote triplestore. The source code of Ontolearn\nis available at https://github.com/dice-group/Ontolearn.", "AI": {"tldr": "Ontolearn\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u5927\u578b\u77e5\u8bc6\u56fe\u8c31\u4e0a\u5b66\u4e60OWL\u7c7b\u8868\u8fbe\u5f0f\u7684\u6846\u67b6\uff0c\u5305\u542b\u6700\u5148\u8fdb\u7684\u7b26\u53f7\u548c\u795e\u7ecf\u7b26\u53f7\u7c7b\u8868\u8fbe\u5f0f\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u96c6\u6210\u4e86\u57fa\u4e8eLLM\u7684\u6587\u672c\u5316\u6a21\u5757\u3002", "motivation": "\u4e3a\u4e86\u5728\u5927\u89c4\u6a21\u77e5\u8bc6\u56fe\u8c31\u4e0a\u9ad8\u6548\u5b66\u4e60OWL\u7c7b\u8868\u8fbe\u5f0f\uff0c\u5e76\u63d0\u4f9b\u7528\u6237\u53cb\u597d\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u529f\u80fd\u3002", "method": "\u5b9e\u73b0EvoLearner\u548cDRILL\u7b49\u5148\u8fdb\u7b97\u6cd5\uff0c\u96c6\u6210LLM\u6587\u672c\u5316\u6a21\u5757\uff0c\u901a\u8fc7SPARQL\u67e5\u8be2\u4e0e\u8fdc\u7a0b\u4e09\u5143\u7ec4\u5b58\u50a8\u4ea4\u4e92\u3002", "result": "\u5f00\u53d1\u4e86\u5b8c\u6574\u7684Ontolearn\u6846\u67b6\uff0c\u80fd\u591f\u5b66\u4e60OWL\u7c7b\u8868\u8fbe\u5f0f\u5e76\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u652f\u6301\u8fdc\u7a0b\u77e5\u8bc6\u56fe\u8c31\u64cd\u4f5c\u3002", "conclusion": "Ontolearn\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u7528\u6237\u53cb\u597d\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u77e5\u8bc6\u56fe\u8c31\u4e0a\u7684\u7c7b\u8868\u8fbe\u5f0f\u5b66\u4e60\u548c\u89e3\u91ca\u3002"}}
{"id": "2510.11653", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11653", "abs": "https://arxiv.org/abs/2510.11653", "authors": ["Prasanna Mayilvahanan", "Ricardo Dominguez-Olmedo", "Thadd\u00e4us Wiedemer", "Wieland Brendel"], "title": "MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model", "comment": null, "summary": "With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL)\nmethods has emerged that seem to unlock stronger mathematical reasoning.\nHowever, a closer look at the open-source ecosystem reveals a critical\nlimitation: with sufficiently many draws (e.g., $\\texttt{pass@1024}$), many\nexisting base models already solve nearly all questions on widely used math\nbenchmarks such as MATH-500 and AIME 2024. This suggests that the RL\nfine-tuning methods prevalent in the LLM reasoning literature largely sharpen\nexisting solution modes rather than discovering entirely new ones. Such\nsharpening stands in contrast to the broader promise of RL: to foster\nexploration and to acquire new skills. To move beyond this plateau, we\nintroduce MATH-Beyond (MATH-B), a benchmark deliberately constructed to defeat\ncommon open-source models of up to 8B parameters even under large sampling\nbudgets. Improving performance on our benchmark via RL requires methods that\nlearn to reason in ways that go beyond base model capabilities in repeated\nsampling. Since the problems are drawn from subsets of DAPO-Math-17K and\nDeepScaleR datasets, they remain topically equivalent to standard high-school\nmath. Validating our premise, RL fine-tuned models such as\nNemotron-Research-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform\npoorly on MATH-B at $\\texttt{pass@1024}$, showing how existing approaches fall\nshort on tackling harder instances. We hope MATH-B will catalyze\nexploration-driven RL approaches that elicit deeper reasoning capabilities. We\nrelease MATH-B at https://huggingface.co/datasets/brendel-group/MATH-Beyond.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u4e3b\u8981\u662f\u5728\u4f18\u5316\u5df2\u6709\u89e3\u9898\u6a21\u5f0f\u800c\u975e\u53d1\u73b0\u65b0\u65b9\u6cd5\uff0c\u4e3a\u6b64\u63d0\u51fa\u4e86MATH-B\u57fa\u51c6\u6765\u6311\u6218\u73b0\u6709\u6a21\u578b\uff0c\u63a8\u52a8\u63a2\u7d22\u6027\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53ea\u662f\u4f18\u5316\u5df2\u6709\u89e3\u9898\u6a21\u5f0f\uff0c\u672a\u80fd\u5b9e\u73b0\u771f\u6b63\u7684\u63a2\u7d22\u548c\u65b0\u6280\u80fd\u83b7\u53d6\uff0c\u9700\u8981\u65b0\u7684\u57fa\u51c6\u6765\u63a8\u52a8\u7a81\u7834\u3002", "method": "\u6784\u5efaMATH-B\u57fa\u51c6\uff0c\u4eceDAPO-Math-17K\u548cDeepScaleR\u6570\u636e\u96c6\u4e2d\u9009\u53d6\u80fd\u51fb\u8d25\u5e38\u89c1\u5f00\u6e90\u6a21\u578b\u7684\u95ee\u9898\uff0c\u5373\u4f7f\u5728\u5927\u91c7\u6837\u9884\u7b97\u4e0b\u4e5f\u96be\u4ee5\u89e3\u51b3\u3002", "result": "\u73b0\u6709RL\u5fae\u8c03\u6a21\u578b\u5982Nemotron-Research-Reasoning-Qwen-1.5B\u548cDeepScaleR-1.5B-Preview\u5728MATH-B\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9a8c\u8bc1\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "MATH-B\u57fa\u51c6\u5c06\u63a8\u52a8\u63a2\u7d22\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u53d1\u5c55\uff0c\u6fc0\u53d1\u66f4\u6df1\u5c42\u6b21\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.11686", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11686", "abs": "https://arxiv.org/abs/2510.11686", "authors": ["Jens Tuyls", "Dylan J. Foster", "Akshay Krishnamurthy", "Jordan T. Ash"], "title": "Representation-Based Exploration for Language Models: From Test-Time to Post-Training", "comment": "Website and code: https://rep-exp.github.io", "summary": "Reinforcement learning (RL) promises to expand the capabilities of language\nmodels, but it is unclear if current RL techniques promote the discovery of\nnovel behaviors, or simply sharpen those already present in the base model. In\nthis paper, we investigate the value of deliberate exploration -- explicitly\nincentivizing the model to discover novel and diverse behaviors -- and aim to\nunderstand how the knowledge in pre-trained models can guide this search. Our\nmain finding is that exploration with a simple, principled,\nrepresentation-based bonus derived from the pre-trained language model's hidden\nstates significantly improves diversity and pass@k rates -- both for\npost-training, and in a novel inference-time scaling setting we introduce. For\ninference-time, exploration with representation-based diversity improves\nefficiency, consistently improving pass@k rates across a variety of models and\nreasoning tasks. For example, for Qwen-2.5-14b-Instruct we obtain over 50%\nimprovement in verifier efficiency on almost all tasks. For post-training, we\nshow that integrating this exploration strategy into an RL pipeline improves\nreasoning performance over that of the initial model and over standard RL\npost-training. For example, on AIME 2024, our post-trained\nQwen-2.5-7b-Instruct's pass@80 matches the pass@256 of GRPO on the same model,\ndemonstrating a 3x improvement in test-time sample efficiency. Overall, our\nfindings suggest that deliberate exploration -- with the right notion of\ndiversity -- is a practical path toward discovery of new behaviors beyond\nsharpening.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u5f15\u5165\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u9690\u85cf\u72b6\u6001\u7684\u8868\u793a\u591a\u6837\u6027\u5956\u52b1\uff0c\u901a\u8fc7\u523b\u610f\u63a2\u7d22\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u4efb\u52a1\u7684\u591a\u6837\u6027\u548cpass@k\u7387\u3002", "motivation": "\u63a2\u7d22\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u662f\u5426\u771f\u6b63\u4fc3\u8fdb\u65b0\u884c\u4e3a\u7684\u53d1\u73b0\uff0c\u8fd8\u662f\u4ec5\u4ec5\u5f3a\u5316\u57fa\u7840\u6a21\u578b\u4e2d\u5df2\u6709\u7684\u884c\u4e3a\uff0c\u7814\u7a76\u523b\u610f\u63a2\u7d22\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4ef7\u503c\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u9690\u85cf\u72b6\u6001\u7684\u7b80\u5355\u3001\u539f\u5219\u6027\u8868\u793a\u591a\u6837\u6027\u5956\u52b1\uff0c\u5728\u8bad\u7ec3\u540e\u548c\u63a8\u7406\u65f6\u4e24\u79cd\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u63a2\u7d22\u3002", "result": "\u8868\u793a\u591a\u6837\u6027\u63a2\u7d22\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6837\u6027\u548cpass@k\u7387\uff0c\u5728\u63a8\u7406\u65f6\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u5728\u8bad\u7ec3\u540e\u6539\u8fdb\u4e86\u63a8\u7406\u6027\u80fd\u3002\u4f8b\u5982Qwen-2.5-14b-Instruct\u5728\u51e0\u4e4e\u6240\u6709\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u5668\u6548\u7387\u63d0\u5347\u8d85\u8fc750%\u3002", "conclusion": "\u523b\u610f\u63a2\u7d22\u7ed3\u5408\u6b63\u786e\u7684\u591a\u6837\u6027\u6982\u5ff5\u662f\u8d85\u8d8a\u7b80\u5355\u5f3a\u5316\u7684\u5b9e\u7528\u8def\u5f84\uff0c\u80fd\u591f\u53d1\u73b0\u65b0\u884c\u4e3a\u3002"}}
{"id": "2510.11709", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11709", "abs": "https://arxiv.org/abs/2510.11709", "authors": ["Edward Stevinson", "Lucas Prieto", "Melih Barsbey", "Tolga Birdal"], "title": "Adversarial Attacks Leverage Interference Between Features in Superposition", "comment": null, "summary": "Fundamental questions remain about when and why adversarial examples arise in\nneural networks, with competing views characterising them either as artifacts\nof the irregularities in the decision landscape or as products of sensitivity\nto non-robust input features. In this paper, we instead argue that adversarial\nvulnerability can stem from efficient information encoding in neural networks.\nSpecifically, we show how superposition - where networks represent more\nfeatures than they have dimensions - creates arrangements of latent\nrepresentations that adversaries can exploit. We demonstrate that adversarial\nperturbations leverage interference between superposed features, making attack\npatterns predictable from feature arrangements. Our framework provides a\nmechanistic explanation for two known phenomena: adversarial attack\ntransferability between models with similar training regimes and class-specific\nvulnerability patterns. In synthetic settings with precisely controlled\nsuperposition, we establish that superposition suffices to create adversarial\nvulnerability. We then demonstrate that these findings persist in a ViT trained\non CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct\nof networks' representational compression, rather than flaws in the learning\nprocess or non-robust inputs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5bf9\u6297\u6027\u6f0f\u6d1e\u6e90\u4e8e\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u9ad8\u6548\u4fe1\u606f\u7f16\u7801\u673a\u5236\uff0c\u7279\u522b\u662f\u53e0\u52a0\u73b0\u8c61\uff08\u7f51\u7edc\u7528\u66f4\u5c11\u7ef4\u5ea6\u8868\u793a\u66f4\u591a\u7279\u5f81\uff09\uff0c\u653b\u51fb\u8005\u5229\u7528\u53e0\u52a0\u7279\u5f81\u95f4\u7684\u5e72\u6270\u6765\u751f\u6210\u53ef\u9884\u6d4b\u7684\u653b\u51fb\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u5bf9\u6297\u6837\u672c\u4ea7\u751f\u539f\u56e0\u5b58\u5728\u4e89\u8bae\uff0c\u672c\u6587\u65e8\u5728\u4ece\u4fe1\u606f\u7f16\u7801\u89d2\u5ea6\u89e3\u91ca\u5bf9\u6297\u6027\u6f0f\u6d1e\uff0c\u8ba4\u4e3a\u8fd9\u662f\u7f51\u7edc\u8868\u793a\u538b\u7f29\u7684\u526f\u4ea7\u54c1\u800c\u975e\u5b66\u4e60\u7f3a\u9677\u3002", "method": "\u5728\u7cbe\u786e\u63a7\u5236\u53e0\u52a0\u7684\u5408\u6210\u73af\u5883\u4e2d\u9a8c\u8bc1\u53e0\u52a0\u8db3\u4ee5\u4ea7\u751f\u5bf9\u6297\u6027\u6f0f\u6d1e\uff0c\u5e76\u5728CIFAR-10\u4e0a\u8bad\u7ec3\u7684ViT\u6a21\u578b\u4e2d\u9a8c\u8bc1\u8fd9\u4e9b\u53d1\u73b0\u3002", "result": "\u8bc1\u660e\u4e86\u5bf9\u6297\u6027\u6270\u52a8\u5229\u7528\u53e0\u52a0\u7279\u5f81\u95f4\u7684\u5e72\u6270\uff0c\u653b\u51fb\u6a21\u5f0f\u53ef\u4ece\u7279\u5f81\u6392\u5217\u4e2d\u9884\u6d4b\uff0c\u5e76\u89e3\u91ca\u4e86\u6a21\u578b\u95f4\u653b\u51fb\u53ef\u8fc1\u79fb\u6027\u548c\u7c7b\u7279\u5b9a\u6f0f\u6d1e\u6a21\u5f0f\u3002", "conclusion": "\u5bf9\u6297\u6027\u6f0f\u6d1e\u662f\u7f51\u7edc\u8868\u793a\u538b\u7f29\u7684\u526f\u4ea7\u54c1\uff0c\u800c\u975e\u5b66\u4e60\u8fc7\u7a0b\u7f3a\u9677\u6216\u975e\u9c81\u68d2\u8f93\u5165\u7279\u5f81\u7684\u95ee\u9898\u3002"}}
{"id": "2509.16444", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16444", "abs": "https://arxiv.org/abs/2509.16444", "authors": ["Chenhan Lyu", "Yutong Song", "Pengfei Zhang", "Amir M. Rahmani"], "title": "Domain-Specific Constitutional AI: Enhancing Safety in LLM-Powered Mental Health Chatbots", "comment": null, "summary": "Mental health applications have emerged as a critical area in computational\nhealth, driven by rising global rates of mental illness, the integration of AI\nin psychological care, and the need for scalable solutions in underserved\ncommunities. These include therapy chatbots, crisis detection, and wellness\nplatforms handling sensitive data, requiring specialized AI safety beyond\ngeneral safeguards due to emotional vulnerability, risks like misdiagnosis or\nsymptom exacerbation, and precise management of vulnerable states to avoid\nsevere outcomes such as self-harm or loss of trust. Despite AI safety advances,\ngeneral safeguards inadequately address mental health-specific challenges,\nincluding crisis intervention accuracy to avert escalations, therapeutic\nguideline adherence to prevent misinformation, scale limitations in\nresource-constrained settings, and adaptation to nuanced dialogues where\ngenerics may introduce biases or miss distress signals. We introduce an\napproach to apply Constitutional AI training with domain-specific mental health\nprinciples for safe, domain-adapted CAI systems in computational mental health\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u9886\u57df\u7279\u5b9a\u7684\u5fc3\u7406\u5065\u5eb7\u539f\u5219\u8fdb\u884c\u5baa\u6cd5AI\u8bad\u7ec3\uff0c\u4ee5\u89e3\u51b3\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u4e2dAI\u5b89\u5168\u6027\u7684\u7279\u6b8a\u6311\u6218\u3002", "motivation": "\u5168\u7403\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0cAI\u5728\u5fc3\u7406\u62a4\u7406\u4e2d\u7684\u5e94\u7528\u9700\u8981\u4e13\u95e8\u7684\u5b89\u5168\u4fdd\u969c\uff0c\u56e0\u4e3a\u901a\u7528AI\u5b89\u5168\u63aa\u65bd\u65e0\u6cd5\u5145\u5206\u5e94\u5bf9\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u7279\u6b8a\u6311\u6218\uff0c\u5982\u60c5\u611f\u8106\u5f31\u6027\u3001\u8bef\u8bca\u98ce\u9669\u3001\u5371\u673a\u5e72\u9884\u51c6\u786e\u6027\u7b49\u3002", "method": "\u91c7\u7528\u5baa\u6cd5AI\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u7684\u5fc3\u7406\u5065\u5eb7\u539f\u5219\uff0c\u5f00\u53d1\u9002\u5e94\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684CAI\u7cfb\u7edf\u3002", "result": "\u8be5\u65b9\u6cd5\u65e8\u5728\u521b\u5efa\u66f4\u5b89\u5168\u3001\u66f4\u9002\u5408\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u9886\u57df\u7684AI\u7cfb\u7edf\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u654f\u611f\u6570\u636e\u3001\u5371\u673a\u5e72\u9884\u548c\u590d\u6742\u5bf9\u8bdd\u3002", "conclusion": "\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u7684\u5baa\u6cd5AI\u8bad\u7ec3\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u4e2dAI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\uff0c\u6ee1\u8db3\u8fd9\u4e00\u654f\u611f\u9886\u57df\u7684\u7279\u6b8a\u9700\u6c42\u3002"}}
