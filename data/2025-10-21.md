<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 103]
- [q-fin.TR](#q-fin.TR) [Total: 3]
- [q-fin.RM](#q-fin.RM) [Total: 3]
- [q-fin.PR](#q-fin.PR) [Total: 1]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [cs.CY](#cs.CY) [Total: 27]
- [q-fin.PM](#q-fin.PM) [Total: 3]
- [eess.SY](#eess.SY) [Total: 24]
- [stat.ML](#stat.ML) [Total: 19]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-fin.GN](#q-fin.GN) [Total: 4]
- [q-fin.ST](#q-fin.ST) [Total: 11]
- [econ.EM](#econ.EM) [Total: 7]
- [cs.AI](#cs.AI) [Total: 70]
- [math.OC](#math.OC) [Total: 30]
- [cs.LG](#cs.LG) [Total: 224]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Quantum NLP models on Natural Language Inference](https://arxiv.org/abs/2510.15972)
*Ling Sun,Peter Sullivan,Michael Martin,Yun Zhou*

Main category: cs.CL

TL;DR: 本文研究了量子自然语言处理在少样本自然语言推理任务中的应用，比较了量子、混合和经典模型，发现量子模型在参数效率上显著优于经典模型。


<details>
  <summary>Details</summary>
Motivation: 探索量子自然语言处理在语义建模中的潜力，特别是在低资源、结构敏感的自然语言推理任务中，通过量子电路直接嵌入组合结构。

Method: 使用lambeq库和DisCoCat框架构建参数化量子电路，训练句子对进行语义相关性和推理分类，并引入信息增益每参数指标评估学习效率。

Result: 量子模型在性能上与经典基线相当，但参数数量大幅减少；在推理任务中优于随机初始化transformer，在相关性任务中测试误差更低；量子模型的学习效率比经典模型高出最多五个数量级。

Conclusion: 量子自然语言处理在低资源、结构敏感场景中具有巨大潜力，提出的基于聚类的架构通过参数共享进一步提升了泛化能力。

Abstract: Quantum natural language processing (QNLP) offers a novel approach to
semantic modeling by embedding compositional structure directly into quantum
circuits. This paper investigates the application of QNLP models to the task of
Natural Language Inference (NLI), comparing quantum, hybrid, and classical
transformer-based models under a constrained few-shot setting. Using the lambeq
library and the DisCoCat framework, we construct parameterized quantum circuits
for sentence pairs and train them for both semantic relatedness and inference
classification. To assess efficiency, we introduce a novel
information-theoretic metric, Information Gain per Parameter (IGPP), which
quantifies learning dynamics independent of model size. Our results demonstrate
that quantum models achieve performance comparable to classical baselines while
operating with dramatically fewer parameters. The Quantum-based models
outperform randomly initialized transformers in inference and achieve lower
test error on relatedness tasks. Moreover, quantum models exhibit significantly
higher per-parameter learning efficiency (up to five orders of magnitude more
than classical counterparts), highlighting the promise of QNLP in low-resource,
structure-sensitive settings. To address circuit-level isolation and promote
parameter sharing, we also propose a novel cluster-based architecture that
improves generalization by tying gate parameters to learned word clusters
rather than individual tokens.

</details>


### [2] [Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus](https://arxiv.org/abs/2510.16057)
*Md Kamrul Siam,Md Jobair Hossain Faruk,Jerry Q. Cheng,Huanying Gu*

Main category: cs.CL

TL;DR: 提出了一种基于ChatGPT和Claude的多模型融合框架，通过相似性共识方法提升胸部X光片诊断的可靠性，在CheXpert数据集上验证了多模态输入和共识融合的有效性。


<details>
  <summary>Details</summary>
Motivation: 提高AI辅助放射学诊断的可靠性和临床实用性，通过多模型融合减少诊断错误，同时保持较低的计算开销。

Method: 使用ChatGPT和Claude两种大语言模型，采用相似性共识方法（95%输出相似度阈值），在CheXpert数据集上评估单模态（仅图像）和多模态（图像+合成临床笔记）输入的性能。

Result: 单模态下ChatGPT准确率62.8%，Claude准确率76.9%；共识融合后提升至77.6%。多模态下ChatGPT提升至84%，Claude为76%，共识准确率达到91.3%。共识融合在所有实验条件下均优于单个模型。

Conclusion: 多模态输入和输出级共识融合能显著提高AI辅助放射学诊断的可靠性和临床实用性，为减少诊断错误提供了实用路径。

Abstract: This study presents a novel multi-model fusion framework leveraging two
state-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance
the reliability of chest X-ray interpretation on the CheXpert dataset. From the
full CheXpert corpus of 224,316 chest radiographs, we randomly selected 234
radiologist-annotated studies to evaluate unimodal performance using image-only
prompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of
62.8% and 76.9%, respectively. A similarity-based consensus approach, using a
95% output similarity threshold, improved accuracy to 77.6%. To assess the
impact of multimodal inputs, we then generated synthetic clinical notes
following the MIMIC-CXR template and evaluated a separate subset of 50 randomly
selected cases paired with both images and synthetic text. On this multimodal
cohort, performance improved to 84% for ChatGPT and 76% for Claude, while
consensus accuracy reached 91.3%. Across both experimental conditions,
agreement-based fusion consistently outperformed individual models. These
findings highlight the utility of integrating complementary modalities and
using output-level consensus to improve the trustworthiness and clinical
utility of AI-assisted radiological diagnosis, offering a practical path to
reduce diagnostic errors with minimal computational overhead.

</details>


### [3] [Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs](https://arxiv.org/abs/2510.16062)
*Guiyao Tie,Zenghui Yuan,Zeli Zhao,Chaoran Hu,Tianhe Gu,Ruihang Zhang,Sizhe Zhang,Junran Wu,Xiaoyue Tu,Ming Jin,Qingsong Wen,Lixing Chen,Pan Zhou,Lichao Sun*

Main category: cs.CL

TL;DR: 提出了CorrectBench基准来评估大语言模型的自校正能力，发现在复杂推理任务中自校正方法能提升准确性，但效率较低，简单的思维链基线表现竞争力强。


<details>
  <summary>Details</summary>
Motivation: 虽然已有多种自校正方法被提出，但缺乏对这些方法的全面评估，且LLM是否能真正自我校正仍是一个重要问题。

Method: 开发CorrectBench基准，评估内在、外部和微调三种自校正策略在常识推理、数学推理和代码生成三个任务上的效果。

Result: 自校正方法能提高准确性，特别是复杂推理任务；混合不同策略可进一步改善但降低效率；推理LLM在额外自校正下优化有限且时间成本高；简单思维链基线表现竞争力强。

Conclusion: 自校正有潜力提升LLM推理性能，但需优化推理能力与操作效率之间的平衡。

Abstract: Self-correction of large language models (LLMs) emerges as a critical
component for enhancing their reasoning performance. Although various
self-correction methods have been proposed, a comprehensive evaluation of these
methods remains largely unexplored, and the question of whether LLMs can truly
correct themselves is a matter of significant interest and concern. In this
study, we introduce CorrectBench, a benchmark developed to evaluate the
effectiveness of self-correction strategies, including intrinsic, external, and
fine-tuned approaches, across three tasks: commonsense reasoning, mathematical
reasoning, and code generation. Our findings reveal that: 1) Self-correction
methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing
different self-correction strategies yields further improvements, though it
reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited
optimization under additional self-correction methods and have high time costs.
Interestingly, a comparatively simple chain-of-thought (CoT) baseline
demonstrates competitive accuracy and efficiency. These results underscore the
potential of self-correction to enhance LLM's reasoning performance while
highlighting the ongoing challenge of improving their efficiency. Consequently,
we advocate for further research focused on optimizing the balance between
reasoning capabilities and operational efficiency. Project Page:
https://correctbench.github.io/

</details>


### [4] [EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle](https://arxiv.org/abs/2510.16079)
*Rong Wu,Xiaoman Wang,Jianbiao Mei,Pinlong Cai,Daocheng Fu,Cheng Yang,Licheng Wen,Xuemeng Yang,Yufan Shen,Yuxin Wang,Botian Shi*

Main category: cs.CL

TL;DR: EvolveR是一个让LLM智能体通过闭环经验生命周期实现自我改进的框架，包含离线自蒸馏和在线交互两个阶段，在复杂多跳问答任务上表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在工具使用方面表现良好，但缺乏从自身经验中系统学习的能力，无法迭代优化问题解决策略。

Method: 采用闭环经验生命周期：1) 离线自蒸馏 - 将交互轨迹合成为结构化、可重用的抽象策略原则库；2) 在线交互 - 智能体与任务交互并检索蒸馏原则指导决策，积累多样化行为轨迹；使用策略强化机制迭代更新智能体。

Result: 在复杂多跳问答基准测试中，EvolveR实现了优于现有强基线智能体的性能表现。

Conclusion: 该工作为智能体不仅从外部数据学习，还能从自身行为后果中学习提供了完整蓝图，为更自主和持续改进的系统铺平了道路。

Abstract: Current Large Language Model (LLM) agents show strong performance in tool
use, but lack the crucial capability to systematically learn from their own
experiences. While existing frameworks mainly focus on mitigating external
knowledge gaps, they fail to address a more fundamental limitation: the
inability to iteratively refine problem-solving strategies. In this work, we
introduce EvolveR, a framework designed to enable agent to self-improve through
a complete, closed-loop experience lifecycle. This lifecycle comprises two key
stages: (1) Offline Self-Distillation, where the agent's interaction
trajectories are synthesized into a structured repository of abstract, reusable
strategic principles; (2) Online Interaction, where the agent interacts with
tasks and actively retrieves distilled principles to guide its decision-making,
accumulating a diverse set of behavioral trajectories. This loop employs a
policy reinforcement mechanism to iteratively update the agent based on its
performance. We demonstrate the effectiveness of EvolveR on complex multi-hop
question-answering benchmarks, where it achieves superior performance over
strong agentic baselines. Our work presents a comprehensive blueprint for
agents that learn not only from external data but also from the consequences of
their own actions, paving the way for more autonomous and continuously
improving systems. Code is available at https://github.com/Edaizi/EvolveR.

</details>


### [5] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 该研究评估了6种大语言模型在5种提示策略下对系统文献综述筛选阶段的自动化效果，发现模型与提示策略存在显著交互效应，并提出了分阶段的工作流程建议。


<details>
  <summary>Details</summary>
Motivation: 量化大语言模型与提示策略在系统文献综述筛选阶段的交互作用，为自动化文献筛选提供实践指导。

Method: 评估6种LLM（GPT-4o、GPT-4o-mini、DeepSeek-Chat-V3、Gemini-2.5-Flash、Claude-3.5-Haiku、Llama-4-Maverick）在5种提示策略（零样本、少样本、思维链、思维链-少样本、自我反思）下的表现，使用准确率、精确率、召回率和F1分数作为评估指标。

Result: 思维链-少样本提示在精确率-召回率平衡方面表现最佳；零样本提示在高敏感度筛选时召回率最高；自我反思表现不佳。GPT-4o和DeepSeek整体表现稳健，GPT-4o-mini在显著降低成本的同时保持竞争力。

Conclusion: 推荐采用分阶段工作流程：先用低成本模型和结构化提示进行初步筛选，仅对边界案例使用高容量模型。研究为任务自适应的大语言模型部署提供了基准和实用指南。

Abstract: This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.

</details>


### [6] [Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization](https://arxiv.org/abs/2510.16096)
*Tina Behnia,Puneesh Deora,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 本文提出了一个合成测试平台，用于系统分析语言模型中统计规律与事实关联的交互作用对泛化能力的影响。研究发现上下文多样性对分布内外事实泛化的影响取决于上下文结构，并识别了不同优化瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对语言模型中统计规律与事实关联交互作用的系统分析，特别是这种交互如何影响泛化能力。

Method: 设计了一个灵活的合成测试平台，结合通用标记的统计流和源-目标标记对的事实流，能够精细控制它们的交互作用，包括上下文结构和多样性水平。

Result: 研究发现：更高的上下文多样性会延迟分布内事实准确性；分布外事实泛化效果取决于上下文结构；在某些情况下多样性对非平凡事实回忆至关重要；即使低多样性阻碍事实回忆，最优多样性水平也取决于训练时长。

Conclusion: 上下文设计和多样性水平的相互作用以不同方式影响各种泛化方面，嵌入层和解嵌入层是分布外失败的关键优化瓶颈，该合成框架为未来研究提供了受控测试平台。

Abstract: Language models are pretrained on sequences that blend statistical
regularities (making text fluent) with factual associations between specific
tokens (knowledge of facts). While recent work suggests that the variability of
their interaction, such as paraphrases of factual associations, critically
determines generalization ability, we lack a systematic analysis of these
impacts. This paper introduces a flexible synthetic testbed that combines a
statistical stream of generic tokens with an abstract factual stream of
source-target token pairs, enabling fine-grained control over their
interaction. The design enables the independent control of diversity nature by
manipulating stream composition (contextual structure) and the diversity level
by varying which statistical streams each fact appears in. Through controlled
experiments, we find that while higher contextual diversity delays
in-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)
factual generalization depends critically on contextual structure. In some
cases, OOD performance follows the same trend as ID, but in others, diversity
becomes essential for non-trivial factual recall. Even when low diversity
prohibits factual recall, optimal diversity levels depend on training duration.
Beyond factual recall failures, we identify structures where statistical
generalization fails independently, and others where both capabilities degrade.
This shows how the interplay between contextual design and diversity level
impacts different generalization aspects. Further, through a series of
controlled interventions on the model components, we trace the OOD failures to
distinct optimization bottlenecks, highlighting the importance of the embedding
and unembedding layers. Our synthetic framework allows us to isolate effects
that would be confounded in large-scale studies, offering a controlled testbed
for future investigations.

</details>


### [7] [In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions](https://arxiv.org/abs/2510.16173)
*Aria Pessianzadeh,Naima Sultana,Hildegarde Van den Bulck,David Gefen,Shahin Jabari,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: 本文提出了首个关于生成式AI信任与不信任的计算研究，使用2022-2025年Reddit数据集，结合众包标注和分类模型进行大规模分析。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI系统融入日常生活，理解公众对其的信任对于负责任采用和治理至关重要。现有研究缺乏计算性、大规模和纵向方法来衡量对生成式AI和大型语言模型的信任与不信任。

Method: 使用多年度Reddit数据集（39个子版块，197,618个帖子），结合众包标注代表性样本和分类模型来扩展分析规模。

Result: 发现信任与不信任随时间基本平衡，在主要模型发布时出现转变。技术性能和可用性是主要维度，个人经验是态度形成的最常见原因。不同信任者群体（专家、伦理学家、普通用户）展现出不同模式。

Conclusion: 研究结果为大规模信任分析提供了方法论框架，并揭示了公众对生成式AI认知的演变趋势。

Abstract: The rise of generative AI (GenAI) has impacted many aspects of human life. As
these systems become embedded in everyday practices, understanding public trust
in them also becomes essential for responsible adoption and governance. Prior
work on trust in AI has largely drawn from psychology and human-computer
interaction, but there is a lack of computational, large-scale, and
longitudinal approaches to measuring trust and distrust in GenAI and large
language models (LLMs). This paper presents the first computational study of
Trust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)
spanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a
representative sample were combined with classification models to scale
analysis. We find that Trust and Distrust are nearly balanced over time, with
shifts around major model releases. Technical performance and usability
dominate as dimensions, while personal experience is the most frequent reason
shaping attitudes. Distinct patterns also emerge across trustors (e.g.,
experts, ethicists, general users). Our results provide a methodological
framework for large-scale Trust analysis and insights into evolving public
perceptions of GenAI.

</details>


### [8] [EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture](https://arxiv.org/abs/2510.16198)
*Mohamed Gamil,Abdelrahman Elsayed,Abdelrahman Lila,Ahmed Gad,Hesham Abdelgawad,Mohamed Aref,Ahmed Fares*

Main category: cs.CL

TL;DR: 本文介绍了EgMM-Corpus，一个专门针对埃及文化的多模态数据集，包含3000多张图像，涵盖313个文化概念。该数据集用于评估和训练视觉语言模型在埃及文化背景下的表现。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域缺乏中东和非洲地区的多模态文化多样性数据集，特别是针对埃及文化的资源有限，这限制了视觉语言模型在这些文化背景下的表现。

Method: 设计并运行新的数据收集流程，收集了涵盖地标、食物和民间传说等313个概念的3000多张图像，每个条目都经过人工验证文化真实性和多模态一致性。

Result: 在EgMM-Corpus上评估CLIP模型的零样本性能，Top-1准确率为21.2%，Top-5准确率为36.4%，显示出大规模视觉语言模型中存在的文化偏见。

Conclusion: EgMM-Corpus作为开发文化感知模型的重要基准，突显了解决视觉语言模型中文化偏见的重要性。

Abstract: Despite recent advances in AI, multimodal culturally diverse datasets are
still limited, particularly for regions in the Middle East and Africa. In this
paper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian
culture. By designing and running a new data collection pipeline, we collected
over 3,000 images, covering 313 concepts across landmarks, food, and folklore.
Each entry in the dataset is manually validated for cultural authenticity and
multimodal coherence. EgMM-Corpus aims to provide a reliable resource for
evaluating and training vision-language models in an Egyptian cultural context.
We further evaluate the zero-shot performance of Contrastive Language-Image
Pre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and
36.4% Top-5 accuracy in classification. These results underscore the existing
cultural bias in large-scale vision-language models and demonstrate the
importance of EgMM-Corpus as a benchmark for developing culturally aware
models.

</details>


### [9] [What Can String Probability Tell Us About Grammaticality?](https://arxiv.org/abs/2510.16227)
*Jennifer Hu,Ethan Gotlieb Wilcox,Siyuan Song,Kyle Mahowald,Roger P. Levy*

Main category: cs.CL

TL;DR: 该论文通过理论分析和实证验证，探讨了语言模型对语法的学习情况，建立了语法、意义和字符串概率之间的关系，并提出了三个可验证的预测。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否真正学习了语法知识，以及如何通过字符串概率来揭示其潜在的语法知识，这对语言学理论有重要意义。

Method: 基于语料库数据生成过程的简单假设，进行理论分析，并使用28万句英语和中文句子对进行实证验证。

Result: 验证了三个预测：(1)最小对字符串概率的相关性；(2)模型与人类在最小对中差异的相关性；(3)语法和不合语法字符串在概率空间中分离度差。

Conclusion: 为使用概率来了解语言模型的结构知识提供了理论基础，并为未来语言模型语法评估指明了方向。

Abstract: What have language models (LMs) learned about grammar? This question remains
hotly debated, with major ramifications for linguistic theory. However, since
probability and grammaticality are distinct notions in linguistics, it is not
obvious what string probabilities can reveal about an LM's underlying
grammatical knowledge. We present a theoretical analysis of the relationship
between grammar, meaning, and string probability, based on simple assumptions
about the generative process of corpus data. Our framework makes three
predictions, which we validate empirically using 280K sentence pairs in English
and Chinese: (1) correlation between the probability of strings within minimal
pairs, i.e., string pairs with minimal semantic differences; (2) correlation
between models' and humans' deltas within minimal pairs; and (3) poor
separation in probability space between unpaired grammatical and ungrammatical
strings. Our analyses give theoretical grounding for using probability to learn
about LMs' structural knowledge, and suggest directions for future work in LM
grammatical evaluation.

</details>


### [10] [Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback](https://arxiv.org/abs/2510.16257)
*Chu Fei Luo,Samuel Dahan,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 提出两种方法（多元解码和模型引导）来增强语言模型的多元对齐能力，在低资源设置下仅用50个标注样本就能超越零样本和少样本基线，减少高风险任务中的误报并改善对人类价值观的分布对齐。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型对社会影响增大，需要确保它们能对齐多样化视角并反映人类价值观的细微差别。但主流训练范式假设每个查询只有一个最优答案，导致响应泛化且对齐效果差。

Method: 提出两种方法：1) 多元解码 - 生成多样化响应；2) 模型引导 - 使用少量标注样本（仅50个）引导模型学习多元视角。

Result: 模型引导方法在零样本和少样本基线上表现一致提升，在仇恨言论检测和错误信息检测等高风险任务中减少误报，在GlobalOpinionQA上改善了对人类价值观的分布对齐。

Conclusion: 这项工作强调了多样性的重要性，展示了语言模型如何适应考虑细微视角，为构建更具包容性和代表性的AI系统提供了可行路径。

Abstract: As language models have a greater impact on society, it is important to
ensure they are aligned to a diverse range of perspectives and are able to
reflect nuance in human values. However, the most popular training paradigms
for modern language models often assume there is one optimal answer for every
query, leading to generic responses and poor alignment. In this work, we aim to
enhance pluralistic alignment of language models in a low-resource setting with
two methods: pluralistic decoding and model steering. We empirically
demonstrate that model steering offers consistent improvement over zero-shot
and few-shot baselines with only 50 annotated samples. Our proposed methods
decrease false positives in several high-stakes tasks such as hate speech
detection and misinformation detection, and improves the distributional
alignment to human values in GlobalOpinionQA. We hope our work highlights the
importance of diversity and how language models can be adapted to consider
nuanced perspectives.

</details>


### [11] [Instant Personalized Large Language Model Adaptation via Hypernetwork](https://arxiv.org/abs/2510.16282)
*Zhaoxuan Tan,Zixuan Zhang,Haoyang Wen,Zheng Li,Rongzhi Zhang,Pei Chen,Fengran Mo,Zheyuan Liu,Qingkai Zeng,Qingyu Yin,Meng Jiang*

Main category: cs.CL

TL;DR: 提出Profile-to-PEFT框架，使用超网络将用户配置文件直接映射到适配器参数，无需为每个用户单独训练，实现实时个性化LLM。


<details>
  <summary>Details</summary>
Motivation: 现有基于参数高效微调(PEFT)的个性化方法需要为每个用户训练单独的适配器，计算成本高且无法实时更新。

Method: 使用端到端训练的超网络，将编码后的用户配置文件直接映射到完整的适配器参数集(如LoRA)，部署时无需用户特定训练。

Result: 方法在性能上优于基于提示的个性化和OPPU方法，同时部署时计算资源消耗显著减少，对分布外用户具有良好的泛化能力。

Conclusion: Profile-to-PEFT框架实现了高效、可扩展和自适应的LLM个性化，适合大规模应用。

Abstract: Personalized large language models (LLMs) tailor content to individual
preferences using user profiles or histories. However, existing
parameter-efficient fine-tuning (PEFT) methods, such as the
``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for
each user, making them computationally expensive and impractical for real-time
updates. We introduce Profile-to-PEFT, a scalable framework that employs a
hypernetwork, trained end-to-end, to map a user's encoded profile directly to a
full set of adapter parameters (e.g., LoRA), eliminating per-user training at
deployment. This design enables instant adaptation, generalization to unseen
users, and privacy-preserving local deployment. Experimental results
demonstrate that our method outperforms both prompt-based personalization and
OPPU while using substantially fewer computational resources at deployment. The
framework exhibits strong generalization to out-of-distribution users and
maintains robustness across varying user activity levels and different
embedding backbones. The proposed Profile-to-PEFT framework enables efficient,
scalable, and adaptive LLM personalization suitable for large-scale
applications.

</details>


### [12] [Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models](https://arxiv.org/abs/2510.16340)
*Pratham Singla,Shivank Garg,Ayush Singh,Ishan Garg,Ketan Suhaas Saichandran*

Main category: cs.CL

TL;DR: 该研究评估了LLMs对自身学习内容的认知能力，发现RL训练模型比SFT模型更了解所学行为且泛化能力更强，但推理过程与最终输出的一致性较弱。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否意识到自己通过后训练技术学到的内容，以及这些模型对所学潜在策略的认知程度。

Method: 定义了三个核心能力：对学习策略的认知、策略跨领域泛化、推理过程与输出的一致性。在多个需要学习不同策略的任务上实证评估，对比SFT、DPO和GRPO三种后训练方法。

Result: RL训练模型比SFT模型更了解所学行为，在结构相似的新任务上泛化能力更强，但推理过程与最终输出的一致性较弱，GRPO训练模型这一现象最明显。

Conclusion: LLMs通过后训练确实获得了增强能力，但不同训练方法在模型认知能力、泛化能力和推理一致性方面存在显著差异。

Abstract: Recent advances in post-training techniques have endowed Large Language
Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive
tasks through the generation of supplementary planning tokens. This development
raises a fundamental question: Are these models aware of what they "learn" and
"think"? To address this, we define three core competencies: (1) awareness of
learned latent policies, (2) generalization of these policies across domains,
and (3) alignment between internal reasoning traces and final outputs. We
empirically evaluate these abilities on several tasks, each designed to require
learning a distinct policy. Furthermore, we contrast the profiles of models
post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization
(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate
that RL-trained models not only demonstrate greater awareness of their learned
behaviors and stronger generalizability to novel, structurally similar tasks
than SFT models but also often exhibit weak alignment between their reasoning
traces and final outputs, an effect most pronounced in GRPO-trained models.

</details>


### [13] [Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets](https://arxiv.org/abs/2510.16359)
*Utsav Dhanuka,Soham Poddar,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 该研究探索使用大语言模型生成针对疫苗错误信息的实时反驳论点，通过多种提示策略和微调方法优化反驳生成，并训练分类器对反疫苗推文进行多标签分类以实现情境感知的反驳。


<details>
  <summary>Details</summary>
Motivation: 在社交媒体影响公共卫生的时代，打击疫苗怀疑论和错误信息成为关键社会目标。虽然错误信息检测取得进展，但生成针对这些主张的实时定制反驳论点仍是研究不足的领域。

Method: 实验多种提示策略和微调方法优化反驳论点生成，训练分类器将反疫苗推文分类为疫苗效力、副作用、政治影响等多标签类别，实现情境感知的反驳。

Result: 通过人工判断、LLM评估和自动指标的评估显示这些方法间具有强一致性。整合标签描述和结构化微调能增强反驳论点的有效性。

Conclusion: 该研究为大规模减轻疫苗错误信息提供了一个有前景的方法，展示了LLM在生成有效反驳论点方面的潜力。

Abstract: In an era where public health is increasingly influenced by information
shared on social media, combatting vaccine skepticism and misinformation has
become a critical societal goal. Misleading narratives around vaccination have
spread widely, creating barriers to achieving high immunisation rates and
undermining trust in health recommendations. While efforts to detect
misinformation have made significant progress, the generation of real time
counter-arguments tailored to debunk such claims remains an insufficiently
explored area. In this work, we explore the capabilities of LLMs to generate
sound counter-argument rebuttals to vaccine misinformation. Building on prior
research in misinformation debunking, we experiment with various prompting
strategies and fine-tuning approaches to optimise counter-argument generation.
Additionally, we train classifiers to categorise anti-vaccine tweets into
multi-labeled categories such as concerns about vaccine efficacy, side effects,
and political influences allowing for more context aware rebuttals. Our
evaluation, conducted through human judgment, LLM based assessments, and
automatic metrics, reveals strong alignment across these methods. Our findings
demonstrate that integrating label descriptions and structured fine-tuning
enhances counter-argument effectiveness, offering a promising approach for
mitigating vaccine misinformation at scale.

</details>


### [14] [End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction](https://arxiv.org/abs/2510.16363)
*Nilmadhab Das,Vishal Vaibhav,Yash Sunil Choudhary,V. Vijaya Saradhi,Ashish Anand*

Main category: cs.CL

TL;DR: 提出了AASP框架，通过自回归结构预测联合建模论证组件和关系，在三个标准基准测试中取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过生成范式扁平化论证结构，难以捕捉论证组件和关系之间的依赖关系。

Method: 基于自回归结构预测框架，将论证结构建模为预定义动作集，使用条件预训练语言模型逐步构建论证结构。

Result: 在三个标准AM基准测试中，两个基准测试的所有任务都达到了SOTA结果，一个基准测试表现强劲。

Conclusion: AASP框架能有效捕捉论证推理流程，在论证挖掘任务中表现出色。

Abstract: Argument Mining (AM) helps in automating the extraction of complex
argumentative structures such as Argument Components (ACs) like Premise, Claim
etc. and Argumentative Relations (ARs) like Support, Attack etc. in an
argumentative text. Due to the inherent complexity of reasoning involved with
this task, modelling dependencies between ACs and ARs is challenging. Most of
the recent approaches formulate this task through a generative paradigm by
flattening the argumentative structures. In contrast to that, this study
jointly formulates the key tasks of AM in an end-to-end fashion using
Autoregressive Argumentative Structure Prediction (AASP) framework. The
proposed AASP framework is based on the autoregressive structure prediction
framework that has given good performance for several NLP tasks. AASP framework
models the argumentative structures as constrained pre-defined sets of actions
with the help of a conditional pre-trained language model. These actions build
the argumentative structures step-by-step in an autoregressive manner to
capture the flow of argumentative reasoning in an efficient way. Extensive
experiments conducted on three standard AM benchmarks demonstrate that AASP
achieves state-of-theart (SoTA) results across all AM tasks in two benchmarks
and delivers strong results in one benchmark.

</details>


### [15] [Navigating through the hidden embedding space: steering LLMs to improve mental health assessment](https://arxiv.org/abs/2510.16373)
*Federico Ravenda,Seyed Ali Bahrainian,Andrea Raballo,Antonietta Mira*

Main category: cs.CL

TL;DR: 提出了一种轻量级方法，通过线性变换特定层激活来提升LLM在心理健康评估中的表现，无需计算密集型技术。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs快速发展，但小规模模型在特定领域应用中仍表现不佳，特别是在心理健康这样的敏感高影响领域。

Method: 使用转向向量引导模型输出，对特定层激活应用线性变换的轻量级方法。

Result: 该方法在两个任务中取得改进：识别Reddit帖子是否有助于检测抑郁症状的相关性预测任务，以及基于用户Reddit历史完成标准化抑郁筛查问卷的任务。

Conclusion: 转向机制作为计算效率高的工具，在LLMs心理健康领域适应方面具有未开发的潜力。

Abstract: The rapid evolution of Large Language Models (LLMs) is transforming AI,
opening new opportunities in sensitive and high-impact areas such as Mental
Health (MH). Yet, despite these advancements, recent evidence reveals that
smaller-scale models still struggle to deliver optimal performance in
domain-specific applications. In this study, we present a cost-efficient yet
powerful approach to improve MH assessment capabilities of an LLM, without
relying on any computationally intensive techniques. Our lightweight method
consists of a linear transformation applied to a specific layer's activations,
leveraging steering vectors to guide the model's output. Remarkably, this
intervention enables the model to achieve improved results across two distinct
tasks: (1) identifying whether a Reddit post is useful for detecting the
presence or absence of depressive symptoms (relevance prediction task), and (2)
completing a standardized psychological screening questionnaire for depression
based on users' Reddit post history (questionnaire completion task). Results
highlight the untapped potential of steering mechanisms as computationally
efficient tools for LLMs' MH domain adaptation.

</details>


### [16] [MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes](https://arxiv.org/abs/2510.16380)
*Yu Ying Chiu,Michael S. Lee,Rachel Calcott,Brandon Handoko,Paul de Font-Reaulx,Paula Rodriguez,Chen Bo Calvin Zhang,Ziwen Han,Udari Madhushani Sehwag,Yash Maurya,Christina Q Knight,Harry R. Lloyd,Florence Bacus,Mantas Mazeika,Bing Liu,Yejin Choi,Mitchell L Gordon,Sydney Levine*

Main category: cs.CL

TL;DR: MoReBench是一个包含1000个道德场景和23000多个评估标准的基准测试，用于评估AI在道德推理过程中的表现，填补了现有数学、代码和科学推理基准在道德推理评估上的不足。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地参与人类决策，需要理解AI如何做出决策而不仅仅是决策结果。道德困境是评估AI程序推理的理想测试场，因为允许多种合理的结论。

Method: 构建MoReBench基准：包含1000个道德场景，每个场景配有专家制定的评估标准；另外构建MoReBench-Theory包含150个例子，测试AI在五种规范伦理框架下的推理能力。

Result: 扩展定律和现有基准无法预测模型在道德推理上的能力；模型显示出对特定道德框架的偏好（如边沁功利主义和康德义务论），这可能是流行训练范式的副作用。

Conclusion: 这些基准推动了以过程为重点的推理评估，有助于开发更安全、更透明的AI系统。

Abstract: As AI systems progress, we rely more on them to make decisions with us and
for us. To ensure that such decisions are aligned with human values, it is
imperative for us to understand not only what decisions they make but also how
they come to those decisions. Reasoning language models, which provide both
final responses and (partially transparent) intermediate thinking traces,
present a timely opportunity to study AI procedural reasoning. Unlike math and
code problems which often have objectively correct answers, moral dilemmas are
an excellent testbed for process-focused evaluation because they allow for
multiple defensible conclusions. To do so, we present MoReBench: 1,000 moral
scenarios, each paired with a set of rubric criteria that experts consider
essential to include (or avoid) when reasoning about the scenarios. MoReBench
contains over 23 thousand criteria including identifying moral considerations,
weighing trade-offs, and giving actionable recommendations to cover cases on AI
advising humans moral decisions as well as making moral decisions autonomously.
Separately, we curate MoReBench-Theory: 150 examples to test whether AI can
reason under five major frameworks in normative ethics. Our results show that
scaling laws and existing benchmarks on math, code, and scientific reasoning
tasks fail to predict models' abilities to perform moral reasoning. Models also
show partiality towards specific moral frameworks (e.g., Benthamite Act
Utilitarianism and Kantian Deontology), which might be side effects of popular
training paradigms. Together, these benchmarks advance process-focused
reasoning evaluation towards safer and more transparent AI.

</details>


### [17] [ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents](https://arxiv.org/abs/2510.16381)
*David Peer,Sebastian Stabinger*

Main category: cs.CL

TL;DR: 提出一种名为自主可信代理(ATA)的神经符号方法，通过将任务分解为离线知识摄取和在线任务处理两个阶段，解决LLM在可信度方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在可信度方面存在幻觉、不稳定性和缺乏透明度等局限性，阻碍了其在高风险领域的部署。

Method: 采用神经符号方法，将任务分解为：1)离线阶段将非正式问题规范转换为可验证的形式化知识库；2)在线阶段使用符号决策引擎基于形式化知识库进行推理。

Result: 在复杂推理任务上的评估表明，ATA在完全自动化设置中与最先进的端到端推理模型竞争，同时保持可信度。使用人工验证的知识库时，ATA显著优于更大的模型，并表现出完美的确定性、增强的稳定性以及对提示注入攻击的免疫性。

Conclusion: ATA通过基于符号推理的决策生成，为构建透明、可审计和可靠的下一代自主代理提供了实用且可控的架构。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities, yet
their deployment in high-stakes domains is hindered by inherent limitations in
trustworthiness, including hallucinations, instability, and a lack of
transparency. To address these challenges, we introduce a generic
neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The
core of our approach lies in decoupling tasks into two distinct phases: Offline
knowledge ingestion and online task processing. During knowledge ingestion, an
LLM translates an informal problem specification into a formal, symbolic
knowledge base. This formal representation is crucial as it can be verified and
refined by human experts, ensuring its correctness and alignment with domain
requirements. In the subsequent task processing phase, each incoming input is
encoded into the same formal language. A symbolic decision engine then utilizes
this encoded input in conjunction with the formal knowledge base to derive a
reliable result. Through an extensive evaluation on a complex reasoning task,
we demonstrate that a concrete implementation of ATA is competitive with
state-of-the-art end-to-end reasoning models in a fully automated setup while
maintaining trustworthiness. Crucially, with a human-verified and corrected
knowledge base, our approach significantly outperforms even larger models,
while exhibiting perfect determinism, enhanced stability against input
perturbations, and inherent immunity to prompt injection attacks. By generating
decisions grounded in symbolic reasoning, ATA offers a practical and
controllable architecture for building the next generation of transparent,
auditable, and reliable autonomous agents.

</details>


### [18] [Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment](https://arxiv.org/abs/2510.16387)
*Fu-An Chao,Bi-Cheng Yan,Berlin Chen*

Main category: cs.CL

TL;DR: 本研究探索了Whisper语音识别模型在二语口语评估中的潜力，通过提取其隐藏表示中的声学和语言特征，仅需训练轻量级分类器即可在GEPT数据集上超越现有先进基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要从外部分析Whisper产生的转录文本，本研究旨在深入挖掘Whisper的潜在能力，探索其在二语口语评估中的应用价值。

Method: 从Whisper的中间和最终输出中提取声学和语言特征，仅训练轻量级分类器，并融入图像和文本提示信息作为辅助相关线索。

Result: 在GEPT图片描述数据集上表现出色，超越了包括多模态方法在内的现有先进基线，通过融入额外信息实现了进一步性能提升。

Conclusion: 即使没有任务特定的微调，Whisper模型本质上就能编码口语的等级熟练度模式和语义方面，显示出其作为口语评估和其他口语理解任务的强大基础模型的潜力。

Abstract: In this paper, we explore the untapped potential of Whisper, a
well-established automatic speech recognition (ASR) foundation model, in the
context of L2 spoken language assessment (SLA). Unlike prior studies that
extrinsically analyze transcriptions produced by Whisper, our approach goes a
step further to probe its latent capabilities by extracting acoustic and
linguistic features from hidden representations. With only a lightweight
classifier being trained on top of Whisper's intermediate and final outputs,
our method achieves strong performance on the GEPT picture-description dataset,
outperforming existing cutting-edge baselines, including a multimodal approach.
Furthermore, by incorporating image and text-prompt information as auxiliary
relevance cues, we demonstrate additional performance gains. Finally, we
conduct an in-depth analysis of Whisper's embeddings, which reveals that, even
without task-specific fine-tuning, the model intrinsically encodes both ordinal
proficiency patterns and semantic aspects of speech, highlighting its potential
as a powerful foundation for SLA and other spoken language understanding tasks.

</details>


### [19] [FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution](https://arxiv.org/abs/2510.16439)
*Syed Rifat Raiyan,Md Farhan Ishmam,Abdullah Al Imran,Mohammad Ali Moni*

Main category: cs.CL

TL;DR: FrugalPrompt是一个新颖的LLM提示压缩框架，通过保留最具语义重要性的token来减少输入长度，在多个NLP任务中实现20%压缩率时仅带来微小性能损失。


<details>
  <summary>Details</summary>
Motivation: LLM的冗长输入增加了成本、碳足迹和推理延迟，而大多数token是冗余的，只有少数token承载主要语义权重。

Method: 使用GlobEnc和DecompX两种token归因方法为输入序列中的每个token分配显著度分数，按原始顺序保留前k%的token，获得稀疏的压缩提示。

Result: 在前三个任务（情感分析、常识问答、摘要）中，20%的提示压缩仅带来边际性能损失；而数学推理性能急剧下降，表明对完整token连续性的更强依赖。

Conclusion: 该研究有助于更细致地理解LLM在性能-效率权衡中的行为，并划定了容忍上下文稀疏性的任务与需要详尽上下文的任务之间的边界。

Abstract: Large language models (LLMs) owe much of their stellar performance to
expansive input contexts, yet such verbosity inflates monetary costs, carbon
footprint, and inference-time latency. Much of this overhead manifests from the
redundant low-utility tokens present in typical prompts, as only a fraction of
tokens typically carries the majority of the semantic weight. We address this
inefficiency by introducing FrugalPrompt, a novel prompt compression framework
for LLMs, which retains only the most semantically significant tokens.
Leveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,
we assign salience scores to every token in an input sequence, rank them to
preserve the top-k% tokens in their original order, and obtain a sparse
frugalized prompt. We evaluate the approach across four NLP tasks: Sentiment
Analysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a
suite of frontier LLMs. For the first three tasks, a 20% prompt reduction
incurs only a marginal loss in task performance, demonstrating that
contemporary LLMs can reconstruct elided context from high-salience cues. In
contrast, performance on mathematical reasoning deteriorates sharply,
reflecting a stronger dependence on complete token continuity. Further analysis
with bottom-k% and random-k% tokens reveals asymmetric performance patterns
that may suggest potential task contamination effects, wherein models may
resort to shallow memorized patterns from pretraining exposure for conventional
NLP tasks. We posit that our work contributes to a more nuanced understanding
of LLM behavior in performance-efficiency trade-offs, and delineate the
boundary between tasks tolerant to contextual sparsity and those requiring
exhaustive context. Our source code and models are available at:
https://github.com/Starscream-11813/Frugal-ICL

</details>


### [20] [TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model](https://arxiv.org/abs/2510.16449)
*Bin Yu,Xinming Wang,Shijie Lian,Haotian Li,Changti Wu,Ruina Hu,Bailing Wang,Yuliang Wei,Kai Chen*

Main category: cs.CL

TL;DR: TrajSelector是一个高效的Best-of-N框架，利用LLM的隐藏状态进行过程级评分，通过轻量级验证器评估推理轨迹质量，在保持较低推理成本的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有外部TTS方法存在两个主要限制：(i) 部署过程奖励模型的高计算开销，(ii) 未充分利用LLM的内在潜在表示。

Method: 使用轻量级验证器(仅0.6B参数)评估逐步推理轨迹质量，并聚合这些分数来识别最优推理轨迹，采用完全数据驱动的端到端训练方法。

Result: 在五个基准测试中，TrajSelector在Best-of-32设置下比多数投票准确率高4.61%，比现有过程奖励模型高4.31%到12.21%，同时保持更低的推理成本。

Conclusion: TrajSelector框架通过利用LLM隐藏状态进行过程级评分，实现了高效且有效的Best-of-N选择，在降低计算开销的同时显著提升了推理性能。

Abstract: Large language models (LLMs) have shown remarkable progress in complex
reasoning tasks, largely enabled by test-time scaling (TTS) paradigms that
allocate additional compute during inference. Among these, external TTS
(particularly the Best-of-N selection paradigm) yields scalable performance
improvements by selecting from multiple independently generated reasoning
trajectories. However, this approach faces key limitations: (i) the high
computational overhead of deploying process reward models, (ii) the
underutilization of the LLM's intrinsic latent representations. We introduce
TrajSelector, an efficient and effective Best-of-N framework that exploit the
hidden states in the sampler LLM for process-level scoring. A lightweight
verifier (with only 0.6B parameters) evaluates the quality of step-wise
trajectory, and then aggregates these scores to identify the optimal reasoning
trajectory. Our framework employs a fully data-driven, end-to-end training
recipe that eliminates reliance on massive step-level annotations. Experiential
results across five benchmarks demonstrate that TrajSelector delivers
consistent performance gains. In Best-of-32 settings, it surpasses majority
voting by 4.61% accuracy and outperforms existing process reward models by
4.31% to 12.21%, all while maintaining lower inference costs.

</details>


### [21] [RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning](https://arxiv.org/abs/2510.16455)
*Deyi Ji,Yuekui Yang,Haiyang Wu,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.CL

TL;DR: RAVEN是一个用于广告视频违规检测的新框架，结合课程强化学习和多模态大语言模型，通过渐进式训练策略提升推理和认知能力，在工业数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有广告视频违规检测方法在精确时间定位、噪声标注和泛化能力方面存在不足，需要更先进的解决方案。

Method: 整合课程强化学习与多模态大语言模型，采用渐进式训练策略结合精确和粗略标注数据，使用GRPO开发推理能力，并设计多层次奖励机制。

Result: 在工业数据集和公共基准测试中，RAVEN在违规类别准确性和时间间隔定位方面表现优异，在线A/B测试验证了其实际应用价值。

Conclusion: RAVEN框架有效解决了广告视频违规检测的关键挑战，具有强大的泛化能力，能够缓解监督微调带来的灾难性遗忘问题。

Abstract: Advertisement (Ad) video violation detection is critical for ensuring
platform compliance, but existing methods struggle with precise temporal
grounding, noisy annotations, and limited generalization. We propose RAVEN, a
novel framework that integrates curriculum reinforcement learning with
multimodal large language models (MLLMs) to enhance reasoning and cognitive
capabilities for violation detection. RAVEN employs a progressive training
strategy, combining precisely and coarsely annotated data, and leverages Group
Relative Policy Optimization (GRPO) to develop emergent reasoning abilities
without explicit reasoning annotations. Multiple hierarchical sophisticated
reward mechanism ensures precise temporal grounding and consistent category
prediction. Experiments on industrial datasets and public benchmarks show that
RAVEN achieves superior performances in violation category accuracy and
temporal interval localization. We also design a pipeline to deploy the RAVEN
on the online Ad services, and online A/B testing further validates its
practical applicability, with significant improvements in precision and recall.
RAVEN also demonstrates strong generalization, mitigating the catastrophic
forgetting issue associated with supervised fine-tuning.

</details>


### [22] [Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations](https://arxiv.org/abs/2510.16458)
*Pingjun Hong,Beiduo Chen,Siyao Peng,Marie-Catherine de Marneffe,Benjamin Roth,Barbara Plank*

Main category: cs.CL

TL;DR: 本文通过解释分析扩展了对自然语言推理标注变异的理解，不仅关注标签一致但解释不同的情况，还研究了标签和解释都不同的情况，发现表面标签分歧可能掩盖深层的解释一致性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注标注者在最终NLI标签一致但解释不同的情况，本文希望扩展研究范围，分析标注者在推理类型和标注步骤上都存在分歧的情况，以更全面理解NLI标注变异。

Method: 将LiTEx分类法应用于两个英文NLI数据集，从多个维度对齐标注变异：NLI标签一致性、解释相似性和分类法一致性，并考虑标注者选择偏见的复合因素。

Result: 发现标注者标签不一致但解释高度相似的情况，表明表面分歧可能掩盖解释层面的一致性；分析还揭示了标注者在解释策略和标签选择上的个体偏好。

Conclusion: 推理类型的一致性比标签一致性更能反映自由文本解释的语义相似性，基于推理的解释具有丰富性，需要谨慎将标签视为绝对真值。

Abstract: Natural Language Inference datasets often exhibit human label variation. To
better understand these variations, explanation-based approaches analyze the
underlying reasoning behind annotators' decisions. One such approach is the
LiTEx taxonomy, which categorizes free-text explanations in English into
reasoning types. However, previous work applying such taxonomies has focused on
within-label variation: cases where annotators agree on the final NLI label but
provide different explanations. In contrast, this paper broadens the scope by
examining how annotators may diverge not only in the reasoning type but also in
the labeling step. We use explanations as a lens to decompose the reasoning
process underlying NLI annotation and to analyze individual differences. We
apply LiTEx to two NLI English datasets and align annotation variation from
multiple aspects: NLI label agreement, explanation similarity, and taxonomy
agreement, with an additional compounding factor of annotators' selection bias.
We observe instances where annotators disagree on the label but provide highly
similar explanations, suggesting that surface-level disagreement may mask
underlying agreement in interpretation. Moreover, our analysis reveals
individual preferences in explanation strategies and label choices. These
findings highlight that agreement in reasoning types better reflects the
semantic similarity of free-text explanations than label agreement alone. Our
findings underscore the richness of reasoning-based explanations and the need
for caution in treating labels as ground truth.

</details>


### [23] [Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety](https://arxiv.org/abs/2510.16492)
*Vamshi Krishna Bonagiri,Ponnurangam Kumaragurum,Khanh Nguyen,Benjamin Plaut*

Main category: cs.CL

TL;DR: 提出使用"退出"作为LLM代理的安全机制，当代理缺乏信心时主动退出，在12个先进LLM上验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在复杂环境中运行，其安全性变得至关重要。多轮代理场景中的不确定性和模糊性会累积，导致超出传统文本生成失败的严重风险。

Method: 利用ToolEmu框架，在12个先进LLM上系统评估退出行为，通过添加明确的退出指令来让代理在缺乏信心时主动退出。

Result: 结果显示安全-帮助性权衡非常有利：所有模型安全性平均提高+0.39（0-3分制），专有模型提高+0.64，而帮助性仅平均下降-0.03。

Conclusion: 添加明确的退出指令是一种高度有效的安全机制，可作为高风险应用中自主代理的第一道防线，可立即部署到现有代理系统中。

Abstract: As Large Language Model (LLM) agents increasingly operate in complex
environments with real-world consequences, their safety becomes critical. While
uncertainty quantification is well-studied for single-turn tasks, multi-turn
agentic scenarios with real-world tool access present unique challenges where
uncertainties and ambiguities compound, leading to severe or catastrophic risks
beyond traditional text generation failures. We propose using "quitting" as a
simple yet effective behavioral mechanism for LLM agents to recognize and
withdraw from situations where they lack confidence. Leveraging the ToolEmu
framework, we conduct a systematic evaluation of quitting behavior across 12
state-of-the-art LLMs. Our results demonstrate a highly favorable
safety-helpfulness trade-off: agents prompted to quit with explicit
instructions improve safety by an average of +0.39 on a 0-3 scale across all
models (+0.64 for proprietary models), while maintaining a negligible average
decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding
explicit quit instructions proves to be a highly effective safety mechanism
that can immediately be deployed in existing agent systems, and establishes
quitting as an effective first-line defense mechanism for autonomous agents in
high-stakes applications.

</details>


### [24] [Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection](https://arxiv.org/abs/2510.16499)
*Michelle Yuan,Khushbu Pahwa,Shuaichen Chang,Mustafa Kaba,Jiarong Jiang,Xiaofei Ma,Yi Zhang,Monica Sunkara*

Main category: cs.CL

TL;DR: 提出基于在线背包问题的自动化智能体系统组合框架，通过动态测试和实时效用建模，在预算约束下优化选择智能体组件，显著提升成功率和降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有智能体系统组合方法依赖静态语义检索，存在能力描述不完整、选择不考虑能力/成本/实时效用等问题，难以实现有效组件重用和组合。

Method: 引入结构化自动化框架，将智能体系统组合建模为在线背包问题，综合考虑性能、预算约束和兼容性，动态测试候选组件并实时建模效用。

Result: 在5个基准数据集上的实验表明，基于在线背包的组合器始终位于帕累托前沿，单智能体设置下成功率提升高达31.6%，多智能体系统中成功率从37%提升到87%。

Conclusion: 该方法在多样化领域和预算约束下展现出强大的适应性，显著优于基于检索的基线方法。

Abstract: Designing effective agentic systems requires the seamless composition and
integration of agents, tools, and models within dynamic and uncertain
environments. Most existing methods rely on static, semantic retrieval
approaches for tool or agent discovery. However, effective reuse and
composition of existing components remain challenging due to incomplete
capability descriptions and the limitations of retrieval methods. Component
selection suffers because the decisions are not based on capability, cost, and
real-time utility. To address these challenges, we introduce a structured,
automated framework for agentic system composition that is inspired by the
knapsack problem. Our framework enables a composer agent to systematically
identify, select, and assemble an optimal set of agentic components by jointly
considering performance, budget constraints, and compatibility. By dynamically
testing candidate components and modeling their utility in real-time, our
approach streamlines the assembly of agentic systems and facilitates scalable
reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five
benchmarking datasets shows that our online-knapsack-based composer
consistently lies on the Pareto frontier, achieving higher success rates at
significantly lower component costs compared to our baselines. In the
single-agent setup, the online knapsack composer shows a success rate
improvement of up to 31.6% in comparison to the retrieval baselines. In
multi-agent systems, the online knapsack composer increases success rate from
37% to 87% when agents are selected from an agent inventory of 100+ agents. The
substantial performance gap confirms the robust adaptability of our method
across diverse domains and budget constraints.

</details>


### [25] [ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation](https://arxiv.org/abs/2510.16549)
*Haoxuan Zhang,Ruochi Li,Sarthak Shrestha,Shree Harshini Mamidala,Revanth Putta,Arka Krishan Aggarwal,Ting Xiao,Junhua Ding,Haihua Chen*

Main category: cs.CL

TL;DR: 提出了ReviewGuard系统，使用四阶段LLM驱动框架自动检测和分类有缺陷的同行评审，包括数据收集、标注、数据增强和模型微调，有效提升了缺陷评审检测的召回率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 同行评审作为科学的守门人面临提交量激增和LLM广泛使用的挑战，未受监管的缺陷评审（包括人类专家和AI系统产生的）可能系统性破坏同行评审生态系统并损害学术诚信。

Method: 采用四阶段LLM驱动框架：1)从OpenReview收集ICLR和NeurIPS论文及评审；2)使用GPT-4.1标注评审类型并人工验证；3)通过LLM驱动的合成数据增强解决类别不平衡和数据稀缺问题；4)微调编码器模型和开源LLM。

Result: 构建了包含6,634篇论文、24,657条真实评审和46,438条合成评审的语料库。缺陷评审表现出更低的评分、更高的自报置信度、降低的结构复杂度和更高的负面情绪比例。混合训练显著提升了二元任务的召回率和F1分数。

Conclusion: 这是首个用于检测缺陷同行评审的LLM驱动系统，为同行评审中的AI治理提供了证据，并为维护学术诚信的人机协作提供了宝贵见解。

Abstract: Peer review serves as the gatekeeper of science, yet the surge in submissions
and widespread adoption of large language models (LLMs) in scholarly evaluation
present unprecedented challenges. Recent work has focused on using LLMs to
improve review efficiency or generate insightful review content. However,
unchecked deficient reviews from both human experts and AI systems threaten to
systematically undermine the peer review ecosystem and compromise academic
integrity. To address this critical issue, we introduce ReviewGuard, an
automated system for detecting and categorizing deficient reviews. ReviewGuard
employs a comprehensive four-stage LLM-driven framework that: (1) collects ICLR
and NeurIPS papers with their corresponding reviews from OpenReview; (2)
annotates review types using GPT-4.1 with human validation; (3) addresses class
imbalance and data scarcity through LLM-driven synthetic data augmentation,
producing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438
synthetic reviews; and (4) fine-tunes both encoder-based models and open source
LLMs. We perform comprehensive feature analysis of the structure and quality of
the review text. Compared to sufficient reviews, deficient reviews demonstrate
lower rating scores, higher self-reported confidence, reduced structural
complexity, and a higher proportion of negative sentiment. AI-generated text
detection reveals that, since ChatGPT's emergence, AI-generated reviews have
increased dramatically. In the evaluation of deficient review detection models,
mixed training with synthetic and real review data provides substantial
enhancements to recall and F1 scores on the binary task. This study presents
the first LLM-driven system for detecting deficient peer reviews, providing
evidence to inform AI governance in peer review while offering valuable
insights into human-AI collaboration to maintain academic integrity.

</details>


### [26] [Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models](https://arxiv.org/abs/2510.16565)
*Seungho Cho,Changgeon Ko,Eui Jun Hwang,Junmyeong Lee,Huije Lee,Jong C. Park*

Main category: cs.CL

TL;DR: 本文通过分析LLMs在回答文化相关问题时内部激活路径的重叠程度，研究了语言模型的文化理解机制，发现语言特异性比文化特异性对内部表征的影响更大。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在多元文化背景下的广泛应用，准确的文化理解变得至关重要。现有评估主要关注输出层面的表现，而很少探究导致响应差异的内部因素，特别是文化相关的电路分析研究覆盖语言少且很少聚焦文化。

Method: 通过测量LLMs在回答语义等价问题时内部激活路径的重叠程度，比较两种条件：(1)改变目标国家但固定问题语言；(2)改变问题语言但固定国家。同时使用同语言国家对来分离语言和文化因素。

Result: 结果显示，相同语言、不同国家的问题比不同语言、相同国家的问题在内部路径上重叠更多，表明存在强烈的语言特定模式。特别地，韩国-朝鲜对的路径重叠度低且变异性高，表明语言相似性不能保证内部表征的一致性。

Conclusion: LLMs的内部文化理解机制受语言特异性影响大于文化特异性，语言相似性不能确保内部表征的对齐，这对跨文化应用具有重要意义。

Abstract: Large language models (LLMs) are increasingly used across diverse cultural
contexts, making accurate cultural understanding essential. Prior evaluations
have mostly focused on output-level performance, obscuring the factors that
drive differences in responses, while studies using circuit analysis have
covered few languages and rarely focused on culture. In this work, we trace
LLMs' internal cultural understanding mechanisms by measuring activation path
overlaps when answering semantically equivalent questions under two conditions:
varying the target country while fixing the question language, and varying the
question language while fixing the country. We also use same-language country
pairs to disentangle language from cultural aspects. Results show that internal
paths overlap more for same-language, cross-country questions than for
cross-language, same-country questions, indicating strong language-specific
patterns. Notably, the South Korea-North Korea pair exhibits low overlap and
high variability, showing that linguistic similarity does not guarantee aligned
internal representation.

</details>


### [27] [Hallucination Benchmark for Speech Foundation Models](https://arxiv.org/abs/2510.16567)
*Alkis Koudounas,Moreno La Quatra,Manuel Giollo,Sabato Marco Siniscalchi,Elena Baralis*

Main category: cs.CL

TL;DR: SHALLOW是首个系统分类和量化ASR系统中幻觉现象的基准框架，通过词汇、语音、形态和语义四个维度评估模型行为，在WER较高时能捕捉WER无法区分的细粒度错误模式。


<details>
  <summary>Details</summary>
Motivation: ASR系统中的幻觉会产生语法语义合理但与语音输入无关的转录，在医疗、法律等关键领域带来严重风险。传统评估指标无法区分语音不准确和幻觉，需要新的评估框架。

Method: 提出SHALLOW框架，在词汇、语音、形态和语义四个互补维度上定义针对性指标，生成可解释的模型行为配置文件。

Result: SHALLOW指标在识别质量高时与WER强相关，但随着WER增加相关性显著减弱，能在退化条件下捕捉WER无法区分的细粒度错误模式。

Conclusion: SHALLOW支持特定模型弱点的诊断，为模型改进提供超出聚合错误率的反馈，是评估ASR系统幻觉倾向的有效工具。

Abstract: Hallucinations in automatic speech recognition (ASR) systems refer to fluent
and coherent transcriptions produced by neural ASR models that are completely
unrelated to the underlying acoustic input (i.e., the speech signal). While
similar to conventional decoding errors in potentially compromising the
usability of transcriptions for downstream applications, hallucinations can be
more detrimental due to their preservation of syntactically and semantically
plausible structure. This apparent coherence can mislead subsequent processing
stages and introduce serious risks, particularly in critical domains such as
healthcare and law. Conventional evaluation metrics are primarily centered on
error-based metrics and fail to distinguish between phonetic inaccuracies and
hallucinations. Consequently, there is a critical need for new evaluation
frameworks that can effectively identify and assess models with a heightened
propensity for generating hallucinated content. To this end, we introduce
SHALLOW, the first benchmark framework that systematically categorizes and
quantifies hallucination phenomena in ASR along four complementary axes:
lexical, phonetic, morphological, and semantic. We define targeted metrics
within each category to produce interpretable profiles of model behavior.
Through evaluation across various architectures and speech domains, we have
found that SHALLOW metrics correlate strongly with word error rate (WER) when
recognition quality is high (i.e., low WER). Still, this correlation weakens
substantially as WER increases. SHALLOW, therefore, captures fine-grained error
patterns that WER fails to distinguish under degraded and challenging
conditions. Our framework supports specific diagnosis of model weaknesses and
provides feedback for model improvement beyond what aggregate error rates can
offer.

</details>


### [28] [AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu](https://arxiv.org/abs/2510.16573)
*Muhammad Ammar,Hadiya Murad Hadi,Usman Majeed Butt*

Main category: cs.CL

TL;DR: 提出了一种针对乌尔都语的AI生成文本检测框架，使用多语言transformer模型在平衡数据集上训练，mDeBERTa-v3-base模型在测试集上达到91.29%的F1分数和91.26%的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成文本能力增强，区分人类写作和机器生成文本变得困难，特别是在乌尔都语等低资源语言中缺乏检测工具，需要解决这一空白。

Method: 构建包含1800篇人类写作和1800篇AI生成文本的平衡数据集，进行语言和统计分析，微调三种多语言transformer模型（mdeberta-v3-base、distilbert-base-multilingualcased、xlm-roberta-base）。

Result: mDeBERTa-v3-base模型表现最佳，在测试集上获得91.29%的F1分数和91.26%的准确率。

Conclusion: 这项研究有助于打击乌尔都语社区的错误信息和学术不端行为，并为低资源语言的NLP工具开发做出贡献。

Abstract: Large Language Models (LLMs) are now capable of generating text that closely
resembles human writing, making them powerful tools for content creation, but
this growing ability has also made it harder to tell whether a piece of text
was written by a human or by a machine. This challenge becomes even more
serious for languages like Urdu, where there are very few tools available to
detect AI-generated text. To address this gap, we propose a novel AI-generated
text detection framework tailored for the Urdu language. A balanced dataset
comprising 1,800 humans authored, and 1,800 AI generated texts, sourced from
models such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed
linguistic and statistical analysis was conducted, focusing on features such as
character and word counts, vocabulary richness (Type Token Ratio), and N-gram
patterns, with significance evaluated through t-tests and MannWhitney U tests.
Three state-of-the-art multilingual transformer models such as
mdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were
fine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest
performance, with an F1-score 91.29 and accuracy of 91.26% on the test set.
This research advances efforts in contesting misinformation and academic
misconduct in Urdu-speaking communities and contributes to the broader
development of NLP tools for low resource languages.

</details>


### [29] [Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach](https://arxiv.org/abs/2510.16604)
*Francisco Jose Cortes Delgado,Eduardo Martinez Gracia,Rafael Valencia Garcia*

Main category: cs.CL

TL;DR: 使用大型语言模型进行短语结构分析的新方法，通过微调LLMs将句子翻译成对应句法结构，用于扩展西班牙语句法教学工具MiSintaxis的功能


<details>
  <summary>Details</summary>
Motivation: 利用大型神经模型在自然语言处理中的进展，探索基于机器学习的句法分析新可能性，扩展西班牙语句法教学工具MiSintaxis的能力

Method: 从Hugging Face仓库选取多个模型，使用AnCora-ES语料库生成的训练数据进行微调，让LLMs学习将输入句子翻译成对应的句法结构

Result: 使用F1分数评估性能，结果显示在短语结构分析中取得了高准确率

Conclusion: 该方法展示了在句法分析中的潜力，证明了使用大型语言模型进行短语结构分析的有效性

Abstract: Recent advances in natural language processing with large neural models have
opened new possibilities for syntactic analysis based on machine learning. This
work explores a novel approach to phrase-structure analysis by fine-tuning
large language models (LLMs) to translate an input sentence into its
corresponding syntactic structure. The main objective is to extend the
capabilities of MiSintaxis, a tool designed for teaching Spanish syntax.
Several models from the Hugging Face repository were fine-tuned using training
data generated from the AnCora-ES corpus, and their performance was evaluated
using the F1 score. The results demonstrate high accuracy in phrase-structure
analysis and highlight the potential of this methodology.

</details>


### [30] [Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration](https://arxiv.org/abs/2510.16645)
*Zhixuan He,Yue Feng*

Main category: cs.CL

TL;DR: DiMo是一个多智能体协作框架，通过模拟四个专门LLM智能体之间的结构化辩论来提升性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能强大但缺乏可解释的推理过程，需要增强推理性能和透明度。

Method: 使用四个专门化LLM智能体，每个代表不同的推理范式，通过迭代辩论挑战和优化初始响应。

Result: 在六个基准测试中，DiMo在统一开源设置下优于单模型和辩论基线，在数学任务上提升最大。

Conclusion: DiMo是一个语义感知的Web原生多智能体框架，能够产生语义类型化、URL注释的证据链，支持下游系统检查和重用。

Abstract: Large Language Models (LLMs) demonstrate strong performance but often lack
interpretable reasoning. This paper introduces the Multi-Agent Collaboration
Framework for Diverse Thinking Modes (DiMo), which enhances both performance
and interpretability by simulating a structured debate among four specialized
LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the
framework to collaboratively explore diverse cognitive approaches. Through
iterative debate, agents challenge and refine initial responses, yielding more
robust conclusions and an explicit, auditable reasoning chain. Across six
benchmarks and under a unified open-source setup, DiMo improves accuracy over
widely used single-model and debate baselines, with the largest gains on math.
We position DiMo as a semantics-aware, Web-native multi-agent framework: it
models human-machine intelligence with LLM agents that produce semantically
typed, URL-annotated evidence chains for explanations and user-friendly
interactions. Although our experiments use standard reasoning benchmarks, the
framework is designed to be instantiated over Web corpora and knowledge graphs,
combining retrieval-augmented reasoning with structured justifications that
downstream systems can inspect and reuse.

</details>


### [31] [All You Need is One: Capsule Prompt Tuning with a Single Vector](https://arxiv.org/abs/2510.16670)
*Yiyang Liu,James C. Liang,Heng Fan,Wenhao Yang,Yiming Cui,Xiaotian Han,Lifu Huang,Dongfang Liu,Qifan Wang,Cheng Han*

Main category: cs.CL

TL;DR: 本文提出了Capsule Prompt-Tuning (CaPT)方法，通过将实例感知信息作为单一胶囊提示，在几乎不增加参数的情况下显著提升提示调优性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的学习方法存在两个主要问题：1）依赖耗时的网格搜索确定最佳提示长度；2）缺乏实例感知信息，导致与输入序列的注意力交互受限。

Method: CaPT方法创新性地将实例感知和任务感知信息整合到单个胶囊提示中，将实例感知标记置于序列最前位置作为'注意力锚点'，以保持对关键结构信息的强注意力。

Result: 实验结果显示，CaPT在T5-Large上达到84.03%的平均准确率，在Llama3.2-1B上仅使用模型参数的0.003%，实现了高参数效率和优越性能。

Conclusion: CaPT通过引入实例感知信息作为注意力锚点，有效解决了传统提示调优方法的局限性，在保持高参数效率的同时显著提升了模型性能。

Abstract: Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)
approach to facilitate Large Language Model (LLM) adaptation to downstream
tasks by conditioning generation with task-aware guidance. Despite its
successes, current prompt-based learning methods heavily rely on laborious grid
searching for optimal prompt length and typically require considerable number
of prompts, introducing additional computational burden. Worse yet, our pioneer
findings indicate that the task-aware prompt design is inherently limited by
its absence of instance-aware information, leading to a subtle attention
interplay with the input sequence. In contrast, simply incorporating
instance-aware information as a part of the guidance can enhance the
prompt-tuned model performance without additional fine-tuning. Moreover, we
find an interesting phenomenon, namely "attention anchor", that incorporating
instance-aware tokens at the earliest position of the sequence can successfully
preserve strong attention to critical structural information and exhibit more
active attention interaction with all input tokens. In light of our
observation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and
effective solution that leverages off-the-shelf, informative instance semantics
into prompt-based learning. Our approach innovatively integrates both
instance-aware and task-aware information in a nearly parameter-free manner
(i.e., one single capsule prompt). Empirical results demonstrate that our
method can exhibit superior performance across various language tasks (e.g.,
84.03\% average accuracy on T5-Large), serving as an "attention anchor," while
enjoying high parameter efficiency (e.g., 0.003\% of model parameters on
Llama3.2-1B).

</details>


### [32] [Temporal Understanding under Deictic Frame of Reference](https://arxiv.org/abs/2510.16685)
*Damin Zhang,Julia Rayz*

Main category: cs.CL

TL;DR: 该研究提出了TUuD框架，用于评估大语言模型在动态时间参考点下理解时间-事件和事件-事件关系的能力。研究发现LLMs表现出部分类似人类的时序认知，但对参考框架变化和时间距离敏感。


<details>
  <summary>Details</summary>
Motivation: 人类通过空间隐喻理解时间，使用时间参考框架(t-FoR)来感知时间关系。虽然LLMs在自然语言理解方面取得显著进展，但其时间理解和推理能力仍然有限，需要评估LLMs如何解释动态变化的"现在"参考点下的时间关系。

Method: 引入TUuD框架，让LLMs在时间线上动态移动的"现在"参考点下，评估当前时刻与目标事件之间的相似度（0.00-1.00），量化两个时间点之间的感知时序对齐程度。

Result: 四个评估的LLMs表现出对指示性t-FoR的可测量适应，相似度评分在现在附近达到峰值，并向过去和未来事件递减。然而，这种适应在超出近期上下文时会减弱。

Conclusion: LLMs显示出部分类似人类的时序认知，但其时间推理仍然对参考框架变化和时间距离敏感，表明其时间理解能力仍有局限性。

Abstract: Understanding time is fundamental to human cognition, where temporal
experience is often conceptualized through spatial metaphors grounded in
sensory-motor experience. For example, "summer is approaching" parallels "We
are approaching the summer". In such expressions, humans rely on a frame of
reference (FoR) to interpret meaning relative to a particular viewpoint.
Extending this concept to time, a temporal frame of reference (t-FoR) defines
how temporal relations are perceived relative to an experiencer's moment of
"now". While Large Language Models (LLMs) have shown remarkable advances in
natural language understanding, their ability to interpret and reason about
time remains limited. In this work, we introduce TUuD (Temporal Understanding
under Deictic t-FoR), a framework that evaluates how LLMs interpret time-event
and event-event relations when the reference point of "now" dynamically shifts
along a timeline. Following recent work on temporal cognition
\cite{li2025other}, LLMs are prompted to rate the similarity between the
current moment and a target event from 0.00 (completely dissimilar) to 1.00
(highly similar), where similarity quantifies perceived temporal alignment
between the two points. Our results show that four evaluated LLMs exhibit
measurable adaptation to a deictic t-FoR, with similarity ratings peaking
around the present and decreasing toward past and future events. The
adaptation, however, weakens beyond near-term contexts, suggesting that while
LLMs display partial human-like temporal cognition, their temporal reasoning
remains sensitive to reference-frame shifts and temporal distance.

</details>


### [33] [Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation](https://arxiv.org/abs/2510.16829)
*Navreet Kaur,Hoda Ayad,Hayoung Jung,Shravika Mittal,Munmun De Choudhury,Tanushree Mitra*

Main category: cs.CL

TL;DR: CoRUS框架通过角色理论模拟基于角色的提问，发现在阿片类药物使用障碍等敏感领域，用户的隐含角色会显著影响语言模型的回应方式，脆弱角色（患者、照顾者）会获得更多支持性回应但知识内容减少。


<details>
  <summary>Details</summary>
Motivation: 现有评估大多忽略提问者角色，但在阿片类药物使用障碍等污名化领域，考虑用户背景对提供可访问、无污名的回应至关重要。

Method: 基于角色理论和在线OUD康复社区帖子，构建提问者角色分类（患者、照顾者、从业者），并模拟15,321个嵌入各角色目标、行为和经验的提问。

Result: 模拟提问具有高可信度且与现实数据相当。评估5个LLM发现：相同问题但不同角色会引发系统性差异，脆弱角色获得更多支持性回应（+17%）但知识内容减少（-19%）。

Conclusion: 用户角色的隐含信号会塑造模型回应，提供了基于角色的对话AI评估方法。

Abstract: Language model users often embed personal and social context in their
questions. The asker's role -- implicit in how the question is framed --
creates specific needs for an appropriate response. However, most evaluations,
while capturing the model's capability to respond, often ignore who is asking.
This gap is especially critical in stigmatized domains such as opioid use
disorder (OUD), where accounting for users' contexts is essential to provide
accessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for
User-centric Question Simulation), a framework for simulating role-based
questions. Drawing on role theory and posts from an online OUD recovery
community (r/OpiatesRecovery), we first build a taxonomy of asker roles --
patients, caregivers, practitioners. Next, we use it to simulate 15,321
questions that embed each role's goals, behaviors, and experiences. Our
evaluations show that these questions are both highly believable and comparable
to real-world data. When used to evaluate five LLMs, for the same question but
differing roles, we find systematic differences: vulnerable roles, such as
patients and caregivers, elicit more supportive responses (+17%) and reduced
knowledge content (-19%) in comparison to practitioners. Our work demonstrates
how implicitly signaling a user's role shapes model responses, and provides a
methodology for role-informed evaluation of conversational AI.

</details>


### [34] [Investigating the Impact of Rationales for LLMs on Natural Language Understanding](https://arxiv.org/abs/2510.16686)
*Wenhang Shi,Shuqing Bian,Yiren Chen,Xinyi Zhang,Zhe Zhao,Pengfei Hu,Wei Lu,Xiaoyong Du*

Main category: cs.CL

TL;DR: 本文探讨思维链推理在自然语言理解任务中的应用，发现模型规模增大时CoT推理从阻碍变为超越直接预测，并开发了能提升性能的特定训练方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注思维链在推理任务中的作用，而忽视了其在自然语言理解任务中的潜在价值。本文旨在系统探索理性推理是否同样能提升NLU任务性能。

Method: 构建了NLURC数据集，开发了多种理性增强方法，并系统评估这些方法在NLU任务上的适用性。

Result: 发现模型规模与CoT效果正相关；大多数理性增强训练方法效果不如仅标签训练，但有一种专门设计的方法能持续提升性能；使用理性训练的LLM在未见NLU任务上表现显著提升，性能可与大十倍模型媲美。

Conclusion: 理性推理在NLU任务中具有重要价值，特别是对于大规模模型和未见任务，同时能提供与商业LLM相当的模型可解释性。

Abstract: Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to
derive final answers, benefit LLMs in both inference and training.
Incorporating rationales, either by generating them before answering during
inference, or by placing them before or after the original answers during
training - significantly improves model performance on mathematical, symbolic
and commonsense reasoning tasks. However, most work focuses on the role of
rationales in these reasoning tasks, overlooking their potential impact on
other important tasks like natural language understanding (NLU) tasks. In this
work, we raise the question: Can rationales similarly benefit NLU tasks? To
conduct a systematic exploration, we construct NLURC, a comprehensive and
high-quality NLU dataset collection with rationales, and develop various
rationale-augmented methods. Through exploring the applicability of these
methods on NLU tasks using the dataset, we uncover several potentially
surprising findings: (1) CoT inference shifts from hindering NLU performance to
surpassing direct label prediction as model size grows, indicating a positive
correlation. (2) Most rationale-augmented training methods perform worse than
label-only training, with one specially designed method consistently achieving
improvements. (3) LLMs trained with rationales achieve significant performance
gains on unseen NLU tasks, rivaling models ten times their size, while
delivering interpretability on par with commercial LLMs.

</details>


### [35] [Natural Language Processing Applications in Cardiology: A Narrative Review](https://arxiv.org/abs/2510.16708)
*Kailai Yang,Yan Leng,Xin Zhang,Tianlin Zhang,Paul Thompson,Bernard Keavney,Maciej Tomaszewski,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 这篇综述论文系统回顾了2014-2025年间自然语言处理在心脏病学领域的研究应用，分析了265篇相关文章，从NLP范式类型、心脏病学任务类型、心血管疾病类型和数据来源类型等多个维度进行了全面分析。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病日益普遍且复杂，相关信息分散在患者叙述、医疗记录和科学文献等非结构化文本数据中。NLP技术能够分析这些海量数据，帮助医疗专业人员深入了解心脏病学领域，从而革新心脏病诊断、治疗和预防方法。

Method: 查询了六个文献数据库，通过严格筛选流程识别出265篇相关文章，从NLP范式类型、心脏病学任务类型、心血管疾病类型和数据来源类型等多个维度进行分析，并进行时间趋势分析。

Result: 分析显示这些维度内存在显著多样性，证明了NLP在心脏病学领域研究的广度。时间分析揭示了近十年来NLP方法的演变和变化趋势。

Conclusion: 这是迄今为止对心脏病学领域NLP研究最全面的综述，展示了NLP技术在心血管疾病研究中的重要应用价值和广阔前景。

Abstract: Cardiovascular disease has become increasingly prevalent in modern society
and has a significant effect on global health and well-being. Heart-related
conditions are intricate, multifaceted disorders, which may be influenced by a
combination of genetic predispositions, lifestyle choices, and various
socioeconomic and clinical factors. Information regarding these potentially
complex interrelationships is dispersed among diverse types of textual data,
which include patient narratives, medical records, and scientific literature,
among others. Natural language processing (NLP) techniques have increasingly
been adopted as a powerful means to analyse and make sense of this vast amount
of unstructured data. This, in turn, can allow healthcare professionals to gain
deeper insights into the cardiology field, which has the potential to
revolutionize current approaches to the diagnosis, treatment, and prevention of
cardiac problems. This review provides a detailed overview of NLP research in
cardiology between 2014 and 2025. We queried six literature databases to find
articles describing the application of NLP techniques in the context of a range
of different cardiovascular diseases. Following a rigorous screening process,
we identified a total of 265 relevant articles. We analysed each article from
multiple dimensions, i.e., NLP paradigm types, cardiology-related task types,
cardiovascular disease types, and data source types. Our analysis reveals
considerable diversity within each of these dimensions, thus demonstrating the
considerable breadth of NLP research within the field. We also perform a
temporal analysis, which illustrates the evolution and changing trends in NLP
methods employed over the last decade that we cover. To our knowledge, the
review constitutes the most comprehensive overview of NLP research in
cardiology to date.

</details>


### [36] [SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors](https://arxiv.org/abs/2510.17516)
*Tiancheng Hu,Joachim Baumann,Lorenzo Lupo,Dirk Hovy,Nigel Collier,Paul Röttger*

Main category: cs.CL

TL;DR: SimBench是第一个大规模、标准化的基准测试，用于评估LLM模拟人类行为的能力，涵盖20个多样化数据集，发现当前最佳LLM模拟能力有限（40.80/100），性能随模型规模对数线性增长，存在对齐-模拟权衡，且在模拟特定人口群体时表现较差。


<details>
  <summary>Details</summary>
Motivation: 当前LLM模拟人类行为的评估方法零散且不可比较，需要统一的基准测试来推动LLM模拟能力的科学发展。

Method: 整合20个多样化数据集，涵盖从道德决策到经济选择等任务，建立标准化评估框架，分析模型规模、推理计算、指令微调等因素对模拟性能的影响。

Result: 当前最佳LLM模拟得分仅40.80/100；性能随模型规模对数线性增长；推理计算增加不提升性能；指令微调在低熵问题上改善但在高熵问题上恶化；模型在模拟特定人口群体时表现不佳；模拟能力与深度知识推理能力强相关（MMLU-Pro, r=0.939）。

Conclusion: SimBench为LLM模拟能力的可测量进步提供了基础，揭示了当前模型的局限性，特别是对齐-模拟权衡和人口群体模拟的挑战，将加速更忠实LLM模拟器的发展。

Abstract: Large language model (LLM) simulations of human behavior have the potential
to revolutionize the social and behavioral sciences, if and only if they
faithfully reflect real human behaviors. Current evaluations are fragmented,
based on bespoke tasks and metrics, creating a patchwork of incomparable
results. To address this, we introduce SimBench, the first large-scale,
standardized benchmark for a robust, reproducible science of LLM simulation. By
unifying 20 diverse datasets covering tasks from moral decision-making to
economic choice across a large global participant pool, SimBench provides the
necessary foundation to ask fundamental questions about when, how, and why LLM
simulations succeed or fail. We show that, while even the best LLMs today have
limited simulation ability (score: 40.80/100), performance scales log-linearly
with model size. Simulation performance is not improved by increased
inference-time compute. We demonstrate an alignment-simulation trade-off:
instruction-tuning improves performance on low-entropy (consensus) questions
but degrades it on high-entropy (diverse) ones. Models particularly struggle
when simulating specific demographic groups. Finally, we demonstrate that
simulation ability correlates most strongly with deep, knowledge-intensive
reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to
accelerate the development of more faithful LLM simulators.

</details>


### [37] [The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models](https://arxiv.org/abs/2510.16712)
*Shivam Ratnakar,Sanjay Raghavendra*

Main category: cs.CL

TL;DR: 研究发现LLMs在检索增强系统中存在变色龙行为：在多轮对话中面对矛盾问题时容易改变立场，这严重影响了系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 集成大语言模型与检索系统的应用日益普及，但这些系统存在关键漏洞，可能影响其可靠性。研究者希望系统性地调查LLMs在多轮对话中面对矛盾问题时的立场不稳定性。

Method: 创建了包含17,770个问答对的Chameleon基准数据集，涵盖1,180个多轮对话和12个争议领域。提出了两个理论指标：变色龙分数（量化立场不稳定性）和源重用率（衡量知识多样性）。

Result: 对Llama-4-Maverick、GPT-4o-mini和Gemini-2.5-Flash的评估显示所有模型都表现出严重的变色龙行为（分数0.391-0.511），其中GPT-4o-mini表现最差。源重用率与置信度（r=0.627）和立场变化（r=0.429）存在显著相关性。

Conclusion: 有限的知识多样性使得模型病态地顺从查询框架，这凸显了在医疗、法律和金融等需要保持连贯立场的系统中部署LLMs前进行一致性评估的必要性。

Abstract: Integration of Large Language Models with search/retrieval engines has become
ubiquitous, yet these systems harbor a critical vulnerability that undermines
their reliability. We present the first systematic investigation of "chameleon
behavior" in LLMs: their alarming tendency to shift stances when presented with
contradictory questions in multi-turn conversations (especially in
search-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising
17,770 carefully crafted question-answer pairs across 1,180 multi-turn
conversations spanning 12 controversial domains, we expose fundamental flaws in
state-of-the-art systems. We introduce two theoretically grounded metrics: the
Chameleon Score (0-1) that quantifies stance instability, and Source Re-use
Rate (0-1) that measures knowledge diversity. Our rigorous evaluation of
Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent
failures: all models exhibit severe chameleon behavior (scores 0.391-0.511),
with GPT-4o-mini showing the worst performance. Crucially, small
across-temperature variance (less than 0.004) suggests the effect is not a
sampling artifact. Our analysis uncovers the mechanism: strong correlations
between source re-use rate and confidence (r=0.627) and stance changes
(r=0.429) are statistically significant (p less than 0.05), indicating that
limited knowledge diversity makes models pathologically deferential to query
framing. These findings highlight the need for comprehensive consistency
evaluation before deploying LLMs in healthcare, legal, and financial systems
where maintaining coherent positions across interactions is critical for
reliable decision support.

</details>


### [38] [so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs](https://arxiv.org/abs/2510.16713)
*Sriharsh Bhyravajjula,Melanie Walsh,Anna Preus,Maria Antoniak*

Main category: cs.CL

TL;DR: 该论文研究了诗歌中空白空间的重要性，分析了19,000首诗歌的空白使用模式，并与AI生成诗歌和未发表诗歌进行比较，探讨了文本处理方法对空白表示的影响。


<details>
  <summary>Details</summary>
Motivation: 空白空间是诗歌形式的关键组成部分，反映了诗人的艺术选择，但在NLP研究中未得到足够关注。论文旨在系统研究诗歌中空白空间的使用模式及其意义。

Method: 使用来自Poetry Foundation的19,000首英语诗歌语料库，分析4,000位诗人的空白使用，并与51,000首LLM生成诗歌和12,000首未发表在线诗歌进行比较。

Result: 发现不同文本处理方法会导致诗歌空白空间的显著不同表示，并揭示了不同时期、诗歌形式和来源中空白使用的差异模式。

Conclusion: 诗歌空白空间的研究对LLM预训练数据集的构建策略具有重要启示，强调了保留格式信息在诗歌处理中的必要性。

Abstract: Whitespace is a critical component of poetic form, reflecting both adherence
to standardized forms and rebellion against those forms. Each poem's whitespace
distribution reflects the artistic choices of the poet and is an integral
semantic and spatial feature of the poem. Yet, despite the popularity of poetry
as both a long-standing art form and as a generation task for large language
models (LLMs), whitespace has not received sufficient attention from the NLP
community. Using a corpus of 19k English-language published poems from Poetry
Foundation, we investigate how 4k poets have used whitespace in their works. We
release a subset of 2.8k public-domain poems with preserved formatting to
facilitate further research in this area. We compare whitespace usage in the
published poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems
posted in an online community. We also explore whitespace usage across time
periods, poetic forms, and data sources. Additionally, we find that different
text processing methods can result in significantly different representations
of whitespace in poetry data, motivating us to use these poems and whitespace
patterns to discuss implications for the processing strategies used to assemble
pretraining datasets for LLMs.

</details>


### [39] [Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models](https://arxiv.org/abs/2510.16727)
*Sanskar Pandey,Ruhaan Chopra,Angkul Puniya,Sohom Pal*

Main category: cs.CL

TL;DR: 提出了Beacon基准来测量大语言模型中的谄媚偏见，发现该偏见随模型能力增强而增加，并提出了干预方法来调节真实性与社会顺从性之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在奖励优化过程中混淆了帮助性与礼貌顺从，形成了真实性与谄媚之间的结构性权衡，这种偏见表现为偏好用户同意而非原则性推理。

Method: 引入Beacon单轮强制选择基准，独立于对话上下文隔离这种偏见，并提出了提示级和激活级干预方法来调节这些偏见。

Result: 在12个最先进模型上的评估显示，谄媚偏见可分解为稳定的语言和情感子偏见，每个子偏见都随模型容量而扩展。干预方法能在相反方向上调节这些偏见。

Conclusion: Beacon将谄媚重新定义为可测量的规范性错误泛化形式，为研究和大规模生成系统中的对齐漂移缓解提供了可重复的基础。

Abstract: Large language models internalize a structural trade-off between truthfulness
and obsequious flattery, emerging from reward optimization that conflates
helpfulness with polite submission. This latent bias, known as sycophancy,
manifests as a preference for user agreement over principled reasoning. We
introduce Beacon, a single-turn forced-choice benchmark that isolates this bias
independent of conversational context, enabling precise measurement of the
tension between factual accuracy and submissive bias. Evaluations across twelve
state-of-the-art models reveal that sycophancy decomposes into stable
linguistic and affective sub-biases, each scaling with model capacity. We
further propose prompt-level and activation-level interventions that modulate
these biases in opposing directions, exposing the internal geometry of
alignment as a dynamic manifold between truthfulness and socially compliant
judgment. Beacon reframes sycophancy as a measurable form of normative
misgeneralization, providing a reproducible foundation for studying and
mitigating alignment drift in large-scale generative systems.

</details>


### [40] [Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games](https://arxiv.org/abs/2510.16761)
*Yikai Zhang,Ye Rong,Siyu Yuan,Jiangjie Chen,Jian Xie,Yanghua Xiao*

Main category: cs.CL

TL;DR: 提出SCO-PAL方法，通过自博弈学习提升语言智能体在动态对抗游戏中的策略推理能力，相比基线平均胜率提升约30%，对抗GPT-4达到54.76%胜率。


<details>
  <summary>Details</summary>
Motivation: 现有语言智能体在动态对抗游戏中因策略推理能力不足而表现不佳，需要无需专家标注数据的自动学习方法。对手选择对学习效果有重要影响，但相关研究较少。

Method: 提出SCO-PAL方法，通过游戏交互自动学习，分析不同级别对手的影响，发现自博弈是最有效的学习方式。

Result: 使用SCO-PAL结合自博弈，在六个对抗游戏中平均胜率比基线提升约30%，对抗GPT-4达到54.76%胜率。

Conclusion: 自博弈是提升对抗环境中策略推理能力的最有效方式，SCO-PAL方法显著提高了语言智能体的游戏表现。

Abstract: Existing language agents often encounter difficulties in dynamic adversarial
games due to poor strategic reasoning. To mitigate this limitation, a promising
approach is to allow agents to learn from game interactions automatically,
without relying on costly expert-labeled data. Unlike static environments where
agents receive fixed feedback or rewards, selecting appropriate opponents in
dynamic adversarial games can significantly impact learning performance.
However, the discussion of opponents in adversarial environments remains an
area under exploration. In this paper, we propose a Step-level poliCy
Optimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we
conduct a detailed analysis of opponent selection by setting opponents at
different levels and find that self-play is the most effective way to improve
strategic reasoning in such adversarial environments. Utilizing SCO-PAL with
self-play, we increase the average win rate against four opponents by
approximately 30% compared to baselines and achieve a 54.76% win rate against
GPT-4 in six adversarial games.

</details>


### [41] [LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding](https://arxiv.org/abs/2510.16783)
*Sheikh Jubair,Arwa Omayrah,Amal Alshammari,Alhanoof Althnian,Abdulhamed Alothaimen,Norah A. Alzahrani,Shahad D. Alzaidi,Nora Al-Twairesh,Abdulmohsen Al-Thubaity*

Main category: cs.CL

TL;DR: LC-Eval是一个双语多任务评估基准，用于评估LLM在英语和阿拉伯语中的长上下文理解能力，涵盖4k到128k+token的上下文长度。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在长上下文处理能力上的进步，需要严格的评估方法来有效评估其在长上下文理解方面的性能。

Method: 设计了四个新颖且具有挑战性的任务：多文档问答、双语问答、段落内声明验证和基于长上下文的多项选择题，评估LLM的深度推理、文档理解、信息追踪和双语信息提取能力。

Result: 评估结果显示LC-Eval具有显著挑战性，即使是GPT-4o等高性能模型在某些任务上也表现困难，突显了基准的复杂性和严谨性。

Conclusion: LC-Eval为评估LLM的长上下文理解能力提供了一个全面且具有挑战性的基准，揭示了当前模型在这一领域的局限性。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
sophisticated capabilities, including the ability to process and comprehend
extended contexts. These emergent capabilities necessitate rigorous evaluation
methods to effectively assess their performance in long-context understanding.
In this paper, we present \textbf{LC-Eval}, a bilingual, multi-task evaluation
benchmark designed to evaluate long-context understanding in English and
Arabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval
introduces four novel and challenging tasks: multi-document question answering,
bilingual question answering, claim verification within a paragraph, and
multiple-choice questions based on long contexts. These tasks are designed to
assess LLMs' abilities in deep reasoning, document comprehension, information
tracing, and bilingual information extraction and understanding. The benchmark
includes datasets in both Arabic and English for each task, allowing for a
comparative analysis of their performance across different text genres.
Evaluations were conducted on both open-weight and closed LLMs, with results
indicating that LC-Eval presents significant challenges. Even high-performing
models, such as GPT-4o, struggled with certain tasks, highlighting the
complexity and rigor of the benchmark.

</details>


### [42] [MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning](https://arxiv.org/abs/2510.16797)
*Vera Pavlova,Mohammed Makhlouf*

Main category: cs.CL

TL;DR: MOSAIC是一个多阶段领域自适应框架，通过联合优化掩码语言建模和对比学习目标，将通用句子嵌入模型有效适配到特定领域。


<details>
  <summary>Details</summary>
Motivation: 解决大规模通用领域句子嵌入模型在专业领域适应中的挑战，需要学习领域相关表示同时保持原始模型的语义区分能力。

Method: 采用多阶段框架，结合领域特定的掩码监督，联合优化MLM和对比学习目标，实现统一训练流程。

Result: 在高资源和低资源领域均取得显著改进，NDCG@10指标相比强基线提升高达13.4%。

Conclusion: 平衡的联合监督和分阶段适应策略对领域自适应至关重要，该方法能有效学习领域相关表示并保持语义区分能力。

Abstract: We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain
Contrastive learning), a multi-stage framework for domain adaptation of
sentence embedding models that incorporates joint domain-specific masked
supervision. Our approach addresses the challenges of adapting large-scale
general-domain sentence embedding models to specialized domains. By jointly
optimizing masked language modeling (MLM) and contrastive objectives within a
unified training pipeline, our method enables effective learning of
domain-relevant representations while preserving the robust semantic
discrimination properties of the original model. We empirically validate our
approach on both high-resource and low-resource domains, achieving improvements
up to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong
general-domain baselines. Comprehensive ablation studies further demonstrate
the effectiveness of each component, highlighting the importance of balanced
joint supervision and staged adaptation.

</details>


### [43] [Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities](https://arxiv.org/abs/2510.16815)
*Hans Hergen Lehmann,Jae Hee Lee,Steven Schockaert,Stefan Wermter*

Main category: cs.CL

TL;DR: LLMs在实体比较任务中经常依赖启发式偏差而非真实知识进行推理，即使它们具备正确的数值知识。研究发现实体流行度、提及顺序和语义共现三种启发式偏差严重影响模型预测。大模型能选择性地依赖更可靠的数值知识，而小模型则无法区分，这解释了为什么大模型表现更好。思维链提示能引导所有模型更好地使用数值特征。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在知识推理任务中何时依赖真实知识而非表面启发式，通过实体比较任务（如河流长度比较）来系统分析模型推理行为。

Method: 使用实体比较任务，分析LLMs在数值属性比较中的表现，识别三种启发式偏差（实体流行度、提及顺序、语义共现），比较不同规模模型（7-8B vs 32B参数）的行为差异，并测试思维链提示的效果。

Result: LLMs经常做出与自身知识相矛盾的预测；小模型主要依赖启发式偏差，仅基于表面线索的逻辑回归比模型自身数值预测更准确；大模型能选择性地依赖更可靠的数值知识；思维链提示能有效引导所有模型使用数值特征。

Conclusion: LLMs在推理时存在启发式偏差优先于真实知识的问题，模型规模影响其选择性依赖知识的能力，思维链提示是改善模型推理的有效方法。

Abstract: Large Language Models (LLMs) are increasingly used for knowledge-based
reasoning tasks, yet understanding when they rely on genuine knowledge versus
superficial heuristics remains challenging. We investigate this question
through entity comparison tasks by asking models to compare entities along
numerical attributes (e.g., ``Which river is longer, the Danube or the
Nile?''), which offer clear ground truth for systematic analysis. Despite
having sufficient numerical knowledge to answer correctly, LLMs frequently make
predictions that contradict this knowledge. We identify three heuristic biases
that strongly influence model predictions: entity popularity, mention order,
and semantic co-occurrence. For smaller models, a simple logistic regression
using only these surface cues predicts model choices more accurately than the
model's own numerical predictions, suggesting heuristics largely override
principled reasoning. Crucially, we find that larger models (32B parameters)
selectively rely on numerical knowledge when it is more reliable, while smaller
models (7--8B parameters) show no such discrimination, which explains why
larger models outperform smaller ones even when the smaller models possess more
accurate knowledge. Chain-of-thought prompting steers all models towards using
the numerical features across all model sizes.

</details>


### [44] [Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank](https://arxiv.org/abs/2510.16819)
*Shantanu Agarwal,Joel Barry,Steven Fincke,Scott Miller*

Main category: cs.CL

TL;DR: 本文提出了一个两阶段的检索-重排框架，使用LLM进行跨体裁的作者归属任务，通过专门的数据策展策略显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 跨体裁作者归属任务需要识别独立于文本主题的作者特定语言模式，而传统信息检索的训练策略在此任务上表现不佳。

Method: 采用两阶段检索-重排框架，为重排器设计了针对性的数据策展策略，使模型能有效学习作者区分性信号。

Result: 在HIATUS的HRS1和HRS2跨体裁作者归属基准上，分别比之前最优方法提升了22.3和34.4个绝对Success@8点。

Conclusion: 该方法通过专门设计的训练策略，显著提升了跨体裁作者归属任务的性能，证明了针对特定任务需求调整训练方法的重要性。

Abstract: Authorship attribution (AA) is the task of identifying the most likely author
of a query document from a predefined set of candidate authors. We introduce a
two-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA.
Unlike the field of information retrieval (IR), where retrieve-and-rerank is a
de facto strategy, cross-genre AA systems must avoid relying on topical cues
and instead learn to identify author-specific linguistic patterns that are
independent of the text's subject matter (genre/domain/topic). Consequently,
for the reranker, we demonstrate that training strategies commonly used in IR
are fundamentally misaligned with cross-genre AA, leading to suboptimal
behavior. To address this, we introduce a targeted data curation strategy that
enables the reranker to effectively learn author-discriminative signals. Using
our LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of
22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on
HIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.

</details>


### [45] [FinSight: Towards Real-World Financial Deep Research](https://arxiv.org/abs/2510.16844)
*Jiajie Jin,Yuyao Zhang,Yimeng Xu,Hongjin Qian,Yutao Zhu,Zhicheng Dou*

Main category: cs.CL

TL;DR: FinSight是一个多智能体框架，通过CAVM架构、迭代视觉增强机制和两阶段写作框架，能够生成高质量的多模态财务报告，在事实准确性、分析深度和呈现质量方面显著优于现有基线系统。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统难以完全自动化生成专业的财务报告，因为这是一个劳动密集且智力要求高的过程。

Method: 采用CAVM架构统一外部数据、工具和智能体；提出迭代视觉增强机制逐步优化原始视觉输出；使用两阶段写作框架将简洁的分析链扩展为连贯的多模态报告。

Result: 在各种公司和行业级任务上的实验表明，FinSight在事实准确性、分析深度和呈现质量方面显著优于所有基线系统，包括领先的深度研究系统。

Conclusion: FinSight展示了生成接近人类专家质量报告的有效路径，为专业财务报告的自动化生成提供了可行的解决方案。

Abstract: Generating professional financial reports is a labor-intensive and
intellectually demanding process that current AI systems struggle to fully
automate. To address this challenge, we introduce FinSight (Financial InSight),
a novel multi agent framework for producing high-quality, multimodal financial
reports. The foundation of FinSight is the Code Agent with Variable Memory
(CAVM) architecture, which unifies external data, designed tools, and agents
into a programmable variable space, enabling flexible data collection, analysis
and report generation through executable code. To ensure professional-grade
visualization, we propose an Iterative Vision-Enhanced Mechanism that
progressively refines raw visual outputs into polished financial charts.
Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis
segments into coherent, citation-aware, and multimodal reports, ensuring both
analytical depth and structural consistency. Experiments on various company and
industry-level tasks demonstrate that FinSight significantly outperforms all
baselines, including leading deep research systems in terms of factual
accuracy, analytical depth, and presentation quality, demonstrating a clear
path toward generating reports that approach human-expert quality.

</details>


### [46] [Neuronal Group Communication for Efficient Neural representation](https://arxiv.org/abs/2510.16851)
*Zhengqi Pei,Qingming Huang,Shuhui Wang*

Main category: cs.CL

TL;DR: 提出Neuronal Group Communication (NGC)框架，将神经网络重新构想为神经元群交互的动态系统，通过低维信号交换实现参数压缩，在保持推理能力的同时提升模型效率。


<details>
  <summary>Details</summary>
Motivation: 解决现代神经网络规模不断扩大带来的效率和可解释性挑战，构建能够学习高效、模块化和可解释表示的大型神经系统。

Method: 将神经网络视为神经元群交互的动态系统，权重作为神经元状态间的瞬态交互，通过神经元群间的迭代通信进行计算。引入神经元稳定性度量来量化序列处理中神经元激活向稳定模式的收缩。

Result: 在大型语言模型中实例化NGC，在适度压缩下在复杂推理基准测试中表现优于标准低秩近似和跨层基共享方法。

Conclusion: NGC框架通过结构化神经元群动态可能在高维学习系统中与泛化能力相关，为构建高效、模块化和可解释的神经网络提供了新思路。

Abstract: The ever-increasing scale of modern neural networks has brought unprecedented
performance alongside daunting challenges in efficiency and interpretability.
This paper addresses the core question of how to build large neural systems
that learn efficient, modular, and interpretable representations. We propose
Neuronal Group Communication (NGC), a theory-driven framework that reimagines a
neural network as a dynamical system of interacting neuronal groups rather than
a monolithic collection of neural weights. Instead of treating each weight as
an independent trainable parameter, NGC treats weights as transient
interactions between embedding-like neuronal states, with neural computation
unfolding through iterative communication among groups of neurons. This
low-rank, modular representation yields compact models: groups of neurons
exchange low-dimensional signals, enabling intra-group specialization and
inter-group information sharing while dramatically reducing redundant
parameters. By drawing on dynamical systems theory, we introduce a neuronal
stability metric (analogous to Lyapunov stability) that quantifies the
contraction of neuron activations toward stable patterns during sequence
processing. Using this metric, we reveal that emergent reasoning capabilities
correspond to an external driving force or ``potential'', which nudges the
neural dynamics away from trivial trajectories while preserving stability.
Empirically, we instantiate NGC in large language models (LLMs) and demonstrate
improved performance on complex reasoning benchmarks under moderate
compression. NGC consistently outperforms standard low-rank approximations and
cross-layer basis-sharing methods at comparable compression rates. We conclude
by discussing the broader implications of NGC, including how structured
neuronal group dynamics might relate to generalization in high-dimensional
learning systems.

</details>


### [47] [Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?](https://arxiv.org/abs/2510.16924)
*Zhihui Yang,Yupei Wang,Kaijie Mo,Zhe Zhao,Renfen Hu*

Main category: cs.CL

TL;DR: 该论文提出了一个基于心理学感知理论的多模态知识理解基准，评估了30个先进语言模型在多种感官模态下的表现，发现视觉语言模型在感知知识理解上并不优于纯文本模型。


<details>
  <summary>Details</summary>
Motivation: 研究多模态语言模型是否通过视觉基础增强了对具身知识的理解，与纯文本模型相比是否有优势。

Method: 构建基于心理学感知理论的具身知识理解基准，包含视觉、听觉、触觉、味觉、嗅觉和内部感知等感官模态，通过向量比较和问答任务评估模型表现。

Result: 视觉语言模型在两种任务中均未优于纯文本模型；模型在视觉维度的表现显著差于其他感官维度；向量表示易受词形和频率影响；模型在空间感知和推理问题上表现困难。

Conclusion: 当前语言模型在具身知识理解方面存在局限，需要更有效地整合具身知识以增强对物理世界的理解。

Abstract: Despite significant progress in multimodal language models (LMs), it remains
unclear whether visual grounding enhances their understanding of embodied
knowledge compared to text-only models. To address this question, we propose a
novel embodied knowledge understanding benchmark based on the perceptual theory
from psychology, encompassing visual, auditory, tactile, gustatory, olfactory
external senses, and interoception. The benchmark assesses the models'
perceptual abilities across different sensory modalities through vector
comparison and question-answering tasks with over 1,700 questions. By comparing
30 state-of-the-art LMs, we surprisingly find that vision-language models
(VLMs) do not outperform text-only models in either task. Moreover, the models
perform significantly worse in the visual dimension compared to other sensory
dimensions. Further analysis reveals that the vector representations are easily
influenced by word form and frequency, and the models struggle to answer
questions involving spatial perception and reasoning. Our findings underscore
the need for more effective integration of embodied knowledge in LMs to enhance
their understanding of the physical world.

</details>


### [48] [ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models](https://arxiv.org/abs/2510.16928)
*Emily Chang,Niyati Bafna*

Main category: cs.CL

TL;DR: ChiKhaPo是一个大规模多语言基准测试，包含8个不同难度的子任务，旨在评估生成模型在2700多种语言中的词汇理解和生成能力，填补了现有基准测试在低资源语言评估方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型基准测试主要局限于高资源或中资源语言，且多关注高阶推理和生成任务，而研究表明LLMs在全球3800多种书面语言中的基本语言能力存在不足。

Method: 利用现有词典、单语数据和双语平行语料构建ChiKhaPo基准，包含8个不同难度的子任务，涵盖2700多种语言，特别关注词汇理解和生成能力。

Result: 测试显示6个最先进的模型在该基准上表现不佳，性能受语言家族、语言资源丰富度、任务类型以及理解与生成方向等因素影响。

Conclusion: ChiKhaPo为大规模多语言基准测试提供了可能，有望推动LLMs在多语言能力方面的评估和发展。

Abstract: Existing benchmarks for large language models (LLMs) are largely restricted
to high- or mid-resource languages, and often evaluate performance on
higher-order tasks in reasoning and generation. However, plenty of evidence
points to the fact that LLMs lack basic linguistic competence in the vast
majority of the world's 3800+ written languages. We introduce ChiKhaPo,
consisting of 8 subtasks of varying difficulty designed to evaluate the lexical
comprehension and generation abilities of generative models. ChiKhaPo draws on
existing lexicons, monolingual data, and bitext, and provides coverage for
2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of
language coverage. We further show that 6 SOTA models struggle on our
benchmark, and discuss the factors contributing to performance scores,
including language family, language resourcedness, task, and comprehension
versus generation directions. With ChiKhaPo, we hope to enable and encourage
the massively multilingual benchmarking of LLMs.

</details>


### [49] [Prompt-MII: Meta-Learning Instruction Induction for LLMs](https://arxiv.org/abs/2510.16932)
*Emily Xiao,Yixiao Zeng,Ada Chen,Chin-Jou Li,Amanda Bertsch,Graham Neubig*

Main category: cs.CL

TL;DR: 提出了PROMPT-MII方法，通过强化学习元学习指令归纳模型，能够为新数据集生成紧凑指令，在保持与上下文学习相当性能的同时大幅减少推理成本。


<details>
  <summary>Details</summary>
Motivation: 上下文学习虽然有效但推理成本高，随着上下文长度增长成本急剧增加。需要一种方法能够在保持性能的同时显著降低推理开销。

Method: 基于强化学习的元学习框架PROMPT-MII，在3,000多个多样化分类数据集上训练指令归纳模型，能够为新数据集动态生成紧凑但描述性的提示。

Result: 在90个未见任务上评估，PROMPT-MII将下游模型质量提升4-9个F1点（相对提升10-20%），匹配上下文学习性能同时所需token数量减少3-13倍。

Conclusion: PROMPT-MII方法成功实现了在保持性能的同时显著降低大语言模型推理成本的目标，为高效的任务适应提供了可行方案。

Abstract: A popular method to adapt large language models (LLMs) to new tasks is
in-context learning (ICL), which is effective but incurs high inference costs
as context length grows. In this paper we propose a method to perform
instruction induction, where we take training examples and reduce them to a
compact but descriptive prompt that can achieve performance comparable to ICL
over the full training set. Specifically, we propose PROMPT-MII, a
reinforcement learning (RL) based framework to meta-learn an instruction
induction model that can generate compact instructions on the fly for an
arbitrary new dataset. We train on over 3,000 diverse classification datasets
from the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves
downstream model quality by 4-9 F1 points (10-20% relative), matching ICL
performance while requiring 3-13x fewer tokens.

</details>


### [50] [Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.16985)
*Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CL

TL;DR: 本文首次应用参数高效微调(PEFT)技术，使用LoRA和QLoRA方法在孟加拉语仇恨言论检测任务上，在BD-SHS数据集上对三个大语言模型进行微调，仅训练少于1%的参数即可获得优异性能。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语社交媒体平台仇恨言论激增，特别是针对女性和青少年。现有方法要么依赖计算成本高的全模型微调，要么使用专有API，需要更高效实用的解决方案。

Method: 使用LoRA和QLoRA参数高效微调技术，在BD-SHS数据集(50,281条标注评论)上对Gemma-3-4B、Llama-3.2-3B和Mistral-7B三个指令调优大语言模型进行微调，仅训练少于1%的参数。

Result: Llama-3.2-3B获得最高F1分数92.23%，Mistral-7B为88.94%，Gemma-3-4B为80.25%。所有实验可在单个消费级GPU上完成。

Conclusion: PEFT被证明是孟加拉语及相关低资源语言仇恨言论检测的实用且可复现策略，大幅降低了计算资源需求。

Abstract: Bengali social media platforms have witnessed a sharp increase in hate
speech, disproportionately affecting women and adolescents. While datasets such
as BD-SHS provide a basis for structured evaluation, most prior approaches rely
on either computationally costly full-model fine-tuning or proprietary APIs.
This paper presents the first application of Parameter-Efficient Fine-Tuning
(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three
instruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and
Mistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated
comments. Each model was adapted by training fewer than 1% of its parameters,
enabling experiments on a single consumer-grade GPU. The results show that
Llama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at
88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical
and replicable strategy for Bengali and related low-resource languages.

</details>


### [51] [Back to Bytes: Revisiting Tokenization Through UTF-8](https://arxiv.org/abs/2510.16987)
*Amit Moryossef,Clara Meister,Pavel Stepachev,Desmond Elliott*

Main category: cs.CL

TL;DR: UTF8Tokenizer是一个极简的字节级分词器，直接将文本映射到UTF-8编码对应的字节ID，使用C0控制字节编码特殊行为，提供更快的分词速度和更小的传输开销。


<details>
  <summary>Details</summary>
Motivation: 现有的字节级分词方法存在超出范围的ID或引入辅助token的问题，作者希望设计一个更简洁高效的分词器。

Method: 采用UTF-8字节编码作为token ID，使用C0控制字节处理特殊行为，实现256维的嵌入表，并引入位偏置嵌入来暴露字节位结构。

Result: 分词速度提升14倍，主机-设备传输减少8倍，嵌入表可跨模型对齐，语言建模收敛性得到改善。

Conclusion: UTF8Tokenizer提供了一个高效、简洁的分词解决方案，在性能和实用性方面都有显著优势。

Abstract: We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text
exactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding
(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,
2021; Pagnoni et al., 2025), our implementation never introduces out-of-range
IDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior
(e.g., padding, boundaries, conversation structure, attention segments, tool
calling, "thinking" spans, etc.) is encoded using C0 control bytes - just as
ASCII was originally designed to embed control information alongside printable
text. These design principles yield practical benefits: (1) faster tokenization
(14x) and significantly lower host-device transfer (8x less than int64); (2)
simple, shareable 256*d embedding tables that can be aligned across models; and
(3) a training-time enhancement via bit-biased embeddings, which exposes
per-byte bit structure and can be added to the embedding table post-training,
removing inference costs. Our HuggingFace-compatible implementation improves
language modeling convergence.

</details>


### [52] [Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic](https://arxiv.org/abs/2510.17001)
*Yuval Reif,Guy Kaplan,Roy Schwartz*

Main category: cs.CL

TL;DR: 该论文提出了一种词汇表压缩方法，通过将词形变化表示为转换向量来减少词汇表大小，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 标准分词算法将词形变化视为独立标记，导致词汇表被表面形式变体填满，牺牲了低频词和多语言覆盖。

Method: 使用转换向量（加法偏移量）来表示词形变化，将词汇表重新设计为共享基础形式和转换向量的组合结构。

Result: 在多种LLM和五种语言上应用该方法，可移除高达10%的词汇表条目，扩展词汇覆盖范围，对下游任务性能影响最小。

Conclusion: 研究结果促使重新思考词汇表设计，从字符串枚举转向利用语言底层结构的组合式词汇表。

Abstract: Large language models (LLMs) were shown to encode word form variations, such
as "walk"->"walked", as linear directions in embedding space. However, standard
tokenization algorithms treat these variations as distinct tokens -- filling
the size-capped vocabulary with surface form variants (e.g., "walk", "walking",
"Walk"), at the expense of less frequent words and multilingual coverage. We
show that many of these variations can be captured by transformation vectors --
additive offsets that yield the appropriate word's representation when applied
to the base form word embedding -- in both the input and output spaces.
Building on this, we propose a compact reshaping of the vocabulary: rather than
assigning unique tokens to each surface form, we compose them from shared base
form and transformation vectors (e.g., "walked" = "walk" + past tense). We
apply our approach to multiple LLMs and across five languages, removing up to
10% of vocabulary entries -- thereby freeing space to allocate new, more
diverse tokens. Importantly, we do so while also expanding vocabulary coverage
to out-of-vocabulary words, with minimal impact on downstream performance, and
without modifying model weights. Our findings motivate a foundational
rethinking of vocabulary design, moving from string enumeration to a
compositional vocabulary that leverages the underlying structure of language.

</details>


### [53] [Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization](https://arxiv.org/abs/2510.17006)
*Masahiro Kaneko,Zeerak Talat,Timothy Baldwin*

Main category: cs.CL

TL;DR: 提出一种基于强化学习的动态防御框架，通过在线学习对抗迭代越狱攻击，使用PDGD防止过拟合，显著提升防御效果和正常任务响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法无法主动破坏迭代越狱攻击的动态试错循环，需要一种能够动态更新防御策略的主动防御机制。

Method: 使用强化学习优化提示，确保无害任务得到适当响应同时明确拒绝有害提示；引入Past-Direction Gradient Damping防止过拟合。

Result: 在三个LLM上的实验表明，该方法显著优于五种现有防御方法，对抗五种迭代越狱攻击；同时提升了无害任务的响应质量。

Conclusion: 提出的动态防御框架能有效对抗迭代越狱攻击，同时保持甚至提升正常任务性能，为LLM安全防御提供了新思路。

Abstract: Iterative jailbreak methods that repeatedly rewrite and input prompts into
large language models (LLMs) to induce harmful outputs -- using the model's
previous responses to guide each new iteration -- have been found to be a
highly effective attack strategy. Despite being an effective attack strategy
against LLMs and their safety mechanisms, existing defenses do not proactively
disrupt this dynamic trial-and-error cycle. In this study, we propose a novel
framework that dynamically updates its defense strategy through online learning
in response to each new prompt from iterative jailbreak methods. Leveraging the
distinctions between harmful jailbreak-generated prompts and typical harmless
prompts, we introduce a reinforcement learning-based approach that optimizes
prompts to ensure appropriate responses for harmless tasks while explicitly
rejecting harmful prompts. Additionally, to curb overfitting to the narrow band
of partial input rewrites explored during an attack, we introduce
Past-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs
show that our approach significantly outperforms five existing defense methods
against five iterative jailbreak methods. Moreover, our results indicate that
our prompt optimization strategy simultaneously enhances response quality for
harmless tasks.

</details>


### [54] [DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking](https://arxiv.org/abs/2510.17013)
*Lanni Bu,Lauren Levin,Amir Zeldes*

Main category: cs.CL

TL;DR: DiscoTrack是一个多语言基准测试，专注于话语跟踪中的隐式信息和语用推理，包含12种语言的四个话语理解任务。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试主要关注自然语言理解中的显式信息提取，缺乏针对隐式信息、语用推理和多语言环境下跨句子、段落和说话者的话语跟踪能力的挑战性基准。

Method: 开发了DiscoTrack基准测试，涵盖12种语言和四个话语理解层次：显著性识别、实体跟踪、话语关系和桥接推理。

Result: 评估显示，即使是最先进的模型，这些任务仍然具有挑战性。

Conclusion: DiscoTrack填补了现有基准测试的空白，为评估LLM在复杂话语理解方面的能力提供了重要工具。

Abstract: Recent LLM benchmarks have tested models on a range of phenomena, but are
still focused primarily on natural language understanding for extraction of
explicit information, such as QA or summarization, with responses often tar-
geting information from individual sentences. We are still lacking more
challenging, and im- portantly also multilingual, benchmarks focus- ing on
implicit information and pragmatic infer- ences across larger documents in the
context of discourse tracking: integrating and aggregating information across
sentences, paragraphs and multiple speaker utterances. To this end, we present
DiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages
and four levels of discourse understanding: salience recognition, entity
tracking, discourse relations and bridging inference. Our evaluation shows that
these tasks remain challenging, even for state-of-the-art models.

</details>


### [55] [SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents](https://arxiv.org/abs/2510.17017)
*Qiusi Zhan,Angeline Budiman-Chan,Abdelrahman Zayed,Xingzhi Guo,Daniel Kang,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: 本文提出SafeSearch方法，通过多目标强化学习同时优化搜索代理的安全性和实用性，显著降低有害输出风险


<details>
  <summary>Details</summary>
Motivation: 研究发现基于LLM的搜索代理比基础LLM更容易产生有害输出，因为它们在检索外部信息时会降低拒绝阈值，将不安全信息合成为有害内容

Method: 采用多目标强化学习方法SafeSearch，结合最终输出的安全/效用奖励和查询级别的惩罚机制，对不安全查询进行惩罚，对安全查询进行奖励

Result: SafeSearch在三个红队测试数据集上将代理的有害性降低了70%以上，同时保持安全有用的响应，与仅优化效用的微调代理在QA性能上相当

Conclusion: 查询级别的奖励机制能有效联合提升安全性和实用性，证明了联合对齐安全性和效用的重要性

Abstract: Large language model (LLM) based search agents iteratively generate queries,
retrieve external information, and reason to answer open-domain questions.
While researchers have primarily focused on improving their utility, their
safety behaviors remain underexplored. In this paper, we first evaluate search
agents using red-teaming datasets and find that they are more likely to produce
harmful outputs than base LLMs. For example, when asked ``How can I track
someone's location without their consent?'', a base model refuses, whereas a
search agent designed to retrieve and cite sources may lower its refusal
threshold, fetch documents (e.g., court cases), and, once appended, synthesize
them into an informative yet unsafe summary. We further show that
utility-oriented fine-tuning intensifies this risk, motivating joint alignment
of safety and utility. We present SafeSearch, a multi-objective reinforcement
learning approach that couples a final-output safety/utility reward with a
novel query-level shaping term that penalizes unsafe queries and rewards safe
ones. Experiments show that SafeSearch reduces agent harmfulness by over 70%
across three red-teaming datasets while producing safe, helpful responses, and
matches the QA performance of a utility-only finetuned agent; further analyses
confirm the effectiveness of the query-level reward in jointly improving safety
and utility.

</details>


### [56] [Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification](https://arxiv.org/abs/2510.17018)
*Noor Islam S. Mohammad*

Main category: cs.CL

TL;DR: 提出了xLSTM框架，通过余弦相似度门控、自适应特征优先级和类别重平衡，在毒性评论检测任务中实现了高效且高性能的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，基于Transformer的模型计算成本高且在少数毒性类别上表现不佳，而传统集成方法缺乏语义适应性。

Method: 使用可学习的参考向量通过余弦相似度调制上下文嵌入，集成多源嵌入（GloVe、FastText、BERT CLS），包含字符级BiLSTM、嵌入空间SMOTE和自适应焦点损失。

Result: 在Jigsaw毒性评论基准测试中达到96.0%准确率和0.88宏F1，在威胁和身份仇恨类别上分别比BERT提升33%和28%，参数减少15倍，推理延迟50ms。

Conclusion: 轻量级、理论驱动的架构可以在不平衡、特定领域的NLP任务上超越大型预训练模型，建立了新的效率适应性前沿。

Abstract: Toxic comment detection remains a challenging task, where transformer-based
models (e.g., BERT) incur high computational costs and degrade on minority
toxicity classes, while classical ensembles lack semantic adaptability. We
propose xLSTM, a parameter-efficient and theoretically grounded framework that
unifies cosine-similarity gating, adaptive feature prioritization, and
principled class rebalancing. A learnable reference vector {v} in {R}^d
modulates contextual embeddings via cosine similarity, amplifying toxic cues
and attenuating benign signals to yield stronger gradients under severe class
imbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)
through a projection layer, a character-level BiLSTM for morphological cues,
embedding-space SMOTE for minority augmentation, and adaptive focal loss with
dynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains
96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%
on identity_hate categories, with 15 times fewer parameters and 50ms inference
latency. Cosine gating contributes a +4.8% F1 gain in ablations. The results
establish a new efficiency adaptability frontier, demonstrating that
lightweight, theoretically informed architectures can surpass large pretrained
models on imbalanced, domain-specific NLP tasks.

</details>


### [57] [Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models](https://arxiv.org/abs/2510.17028)
*Kyle Cox,Jiawei Xu,Yikun Han,Rong Xu,Tianhao Li,Chi-Yang Hsu,Tianlong Chen,Walter Gerych,Ying Ding*

Main category: cs.CL

TL;DR: 论文研究大语言模型的提示敏感性，提出通过语义空间采样和释义扰动来改进不确定性校准，并引入新的不确定性分解指标来量化提示敏感性对不确定性的影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对语义相同但表述不同的提示会产生不同的答案分布，这表明模型输出的不确定性可能无法真实反映其对提示含义的不确定性。

Method: 将提示敏感性建模为泛化误差，通过语义概念空间的释义扰动采样来改进不确定性校准，并引入新的不确定性分解指标来建模自然语言生成中的语义连续性。

Result: 研究表明，通过语义空间采样可以在不损害准确性的情况下改进不确定性校准，新的分解指标能够有效量化提示敏感性对LLM不确定性的贡献。

Conclusion: 这项工作为改进提示敏感语言模型的不确定性校准提供了新方法，并证明某些LLM未能对其输入含义表现出一致的通用推理能力。

Abstract: An interesting behavior in large language models (LLMs) is prompt
sensitivity. When provided with different but semantically equivalent versions
of the same prompt, models may produce very different distributions of answers.
This suggests that the uncertainty reflected in a model's output distribution
for one prompt may not reflect the model's uncertainty about the meaning of the
prompt. We model prompt sensitivity as a type of generalization error, and show
that sampling across the semantic ``concept space'' with paraphrasing
perturbations improves uncertainty calibration without compromising accuracy.
Additionally, we introduce a new metric for uncertainty decomposition in
black-box LLMs that improves upon entropy-based decomposition by modeling
semantic continuities in natural language generation. We show that this
decomposition metric can be used to quantify how much LLM uncertainty is
attributed to prompt sensitivity. Our work introduces a new way to improve
uncertainty calibration in prompt-sensitive language models, and provides
evidence that some LLMs fail to exhibit consistent general reasoning about the
meanings of their inputs.

</details>


### [58] [Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation](https://arxiv.org/abs/2510.17062)
*Guoqing Luo,Iffat Maab,Lili Mou,Junichi Yamagishi*

Main category: cs.CL

TL;DR: 本文系统研究了大型语言模型在推理过程中如何聚合社会偏见，发现了两种失败模式，并提出了一种轻量级的提示缓解方法。


<details>
  <summary>Details</summary>
Motivation: 虽然基于推理的大型语言模型通过内部结构化思考过程在复杂任务中表现出色，但这种思考过程会聚合社会刻板印象，导致偏见结果，而这种现象的底层行为机制尚未得到充分探索。

Method: 系统研究思维过程中的机制，识别出两种驱动社会偏见聚合的失败模式：刻板印象重复和无关信息注入。基于这些发现，提出了一种轻量级的基于提示的缓解方法，让模型根据这些特定失败模式审查自己的初始推理。

Result: 在问答（BBQ和StereoSet）和开放式（BOLD）基准测试上的实验表明，该方法有效减少了偏见，同时保持或提高了准确性。

Conclusion: 研究揭示了语言模型推理过程中社会偏见聚合的机制，提出的轻量级缓解方法能够有效减少偏见而不损害性能。

Abstract: While reasoning-based large language models excel at complex tasks through an
internal, structured thinking process, a concerning phenomenon has emerged that
such a thinking process can aggregate social stereotypes, leading to biased
outcomes. However, the underlying behaviours of these language models in social
bias scenarios remain underexplored. In this work, we systematically
investigate mechanisms within the thinking process behind this phenomenon and
uncover two failure patterns that drive social bias aggregation: 1) stereotype
repetition, where the model relies on social stereotypes as its primary
justification, and 2) irrelevant information injection, where it fabricates or
introduces new details to support a biased narrative. Building on these
insights, we introduce a lightweight prompt-based mitigation approach that
queries the model to review its own initial reasoning against these specific
failure patterns. Experiments on question answering (BBQ and StereoSet) and
open-ended (BOLD) benchmarks show that our approach effectively reduces bias
while maintaining or improving accuracy.

</details>


### [59] [Verification-Aware Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.17109)
*Tianyang Xu,Dan Zhang,Kushan Mitra,Estevam Hruschka*

Main category: cs.CL

TL;DR: VeriMAP是一个用于多智能体协作的验证感知规划框架，通过分解任务、建模子任务依赖关系，并将规划器定义的通过标准编码为Python和自然语言的子任务验证函数，解决了多智能体协作中的规划、协调和验证挑战。


<details>
  <summary>Details</summary>
Motivation: 多智能体协作在处理复杂任务时面临规划、协调和验证的新挑战，执行失败往往源于任务解释、输出格式或智能体间交接的细微偏差，而不仅仅是推理缺陷。

Method: VeriMAP框架包括任务分解、子任务依赖建模，并将规划器定义的通过标准编码为Python和自然语言的子任务验证函数(VFs)。

Result: 在多样化数据集上的评估表明，VeriMAP优于单智能体和多智能体基线，同时增强了系统鲁棒性和可解释性。

Conclusion: 验证感知规划能够在多智能体系统中实现可靠的协调和迭代优化，无需依赖外部标签或注释。

Abstract: Large language model (LLM) agents are increasingly deployed to tackle complex
tasks, often necessitating collaboration among multiple specialized agents.
However, multi-agent collaboration introduces new challenges in planning,
coordination, and verification. Execution failures frequently arise not from
flawed reasoning alone, but from subtle misalignments in task interpretation,
output format, or inter-agent handoffs. To address these challenges, we present
VeriMAP, a framework for multi-agent collaboration with verification-aware
planning. The VeriMAP planner decomposes tasks, models subtask dependencies,
and encodes planner-defined passing criteria as subtask verification functions
(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,
demonstrating that it outperforms both single- and multi-agent baselines while
enhancing system robustness and interpretability. Our analysis highlights how
verification-aware planning enables reliable coordination and iterative
refinement in multi-agent systems, without relying on external labels or
annotations.

</details>


### [60] [DVAGen: Dynamic Vocabulary Augmented Generation](https://arxiv.org/abs/2510.17115)
*Wei Du,Nuowei Liu,Jie Wang,Jiahao Kuang,Tao Ji,Xiaoling Wang,Yuanbin Wu*

Main category: cs.CL

TL;DR: DVAGen是一个开源统一框架，用于训练、评估和可视化动态词汇增强语言模型，解决了现有方法代码库碎片化、缺乏现代LLM支持和推理可扩展性有限的问题。


<details>
  <summary>Details</summary>
Motivation: 固定词汇表训练的语言模型难以泛化到新词或词汇表外词，限制了处理多样化词组合的灵活性。现有动态词汇方法存在代码库碎片化、缺乏现代LLM支持和推理可扩展性有限等挑战。

Method: DVAGen框架模块化处理流程以便定制，无缝集成开源LLM，首次提供CLI和WebUI工具进行实时结果检查，支持批量推理显著提高推理吞吐量。

Result: 验证了动态词汇方法在现代LLM上的有效性，展示了批量推理支持带来的显著推理吞吐量提升。

Conclusion: DVAGen通过提供统一、模块化且可扩展的框架，成功解决了动态词汇增强语言模型训练和推理中的关键挑战。

Abstract: Language models trained with a fixed vocabulary struggle to generalize to
novel or out-of-vocabulary words, limiting their flexibility in handling
diverse token combinations. Existing dynamic vocabulary approaches attempt to
address this limitation but face challenges such as fragmented codebases, lack
of support for modern LLMs, and limited inference scalability. To overcome
these issues, we introduce DVAGen, a fully open-source, unified framework
designed for training, evaluation, and visualization of dynamic
vocabulary-augmented language models. Our framework modularizes the pipeline
for ease of customization, integrates seamlessly with open-source LLMs, and is
the first to provide both CLI and WebUI tools for real-time result inspection.
We validate the effectiveness of dynamic vocabulary methods on modern LLMs and
demonstrate support for batch inference, significantly improving inference
throughput.

</details>


### [61] [Rethinking On-policy Optimization for Query Augmentation](https://arxiv.org/abs/2510.17139)
*Zhichao Xu,Shengyao Zhuang,Xueguang Ma,Bingsen Chen,Yijun Tian,Fengran Mo,Jie Cao,Vivek Srikumar*

Main category: cs.CL

TL;DR: 本文首次系统比较了基于提示和基于强化学习的查询增强方法，发现简单的无训练查询增强方法在强大LLMs下表现与昂贵的RL方法相当甚至更好，并提出了一种结合两者优势的混合方法OPQE。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在信息检索中的查询增强主要有两种方法：基于提示的方法和基于强化学习的方法，但缺乏在一致实验条件下的系统比较。

Method: 在多个基准测试上系统比较两种方法，并提出混合方法OPQE——LLM策略学习生成最大化检索性能的伪文档，结合了提示的灵活性和RL的定向优化。

Result: 简单的无训练查询增强方法在强大LLMs下表现与RL方法相当或更好；OPQE方法优于单独的提示方法和RL重写方法。

Conclusion: 协同方法能产生最佳结果，OPQE展示了结合提示灵活性和RL优化的优势。

Abstract: Recent advances in large language models (LLMs) have led to a surge of
interest in query augmentation for information retrieval (IR). Two main
approaches have emerged. The first prompts LLMs to generate answers or
pseudo-documents that serve as new queries, relying purely on the model's
parametric knowledge or contextual information. The second applies
reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly
optimizing retrieval metrics. While having respective advantages and
limitations, the two approaches have not been compared under consistent
experimental conditions. In this work, we present the first systematic
comparison of prompting-based and RL-based query augmentation across diverse
benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key
finding is that simple, training-free query augmentation often performs on par
with, or even surpasses, more expensive RL-based counterparts, especially when
using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid
method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of
rewriting a query, the LLM policy learns to generate a pseudo-document that
maximizes retrieval performance, thus merging the flexibility and generative
structure of prompting with the targeted optimization of RL. We show OPQE
outperforms both standalone prompting and RL-based rewriting, demonstrating
that a synergistic approach yields the best results. Our implementation is made
available to facilitate reproducibility.

</details>


### [62] [When AI companions become witty: Can human brain recognize AI-generated irony?](https://arxiv.org/abs/2510.17168)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 研究表明，人们对AI生成的讽刺言论不会完全采用意向立场，在行为和神经层面都表现出对AI意图归因的减弱。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地作为社交代理部署并被训练产生幽默和讽刺，需要了解人们是否会将AI的机智言论视为有意的沟通还是纯粹的计算输出。

Method: 通过比较AI与人类来源讽刺语句的行为和神经反应，使用ERP成分（P200反映早期不一致性检测，P600索引重新解释不一致性为故意讽刺的认知努力）。

Result: 行为上，参与者对AI的故意沟通归因显著少于人类；神经数据显示AI生成讽刺的P200和P600效应减弱，表明检测和重新分析的努力减少。感知AI更真诚的人对AI讽刺显示出更大的P200和P600效应。

Conclusion: 尽管当前LLMs具有语言复杂性，但实现真正的社交代理不仅需要语言能力，还需要人类对人工代理的意向性归因方式的转变。

Abstract: As Large Language Models (LLMs) are increasingly deployed as social agents
and trained to produce humor and irony, a question emerges: when encountering
witty AI remarks, do people interpret these as intentional communication or
mere computational output? This study investigates whether people adopt the
intentional stance, attributing mental states to explain behavior,toward AI
during irony comprehension. Irony provides an ideal paradigm because it
requires distinguishing intentional contradictions from unintended errors
through effortful semantic reanalysis. We compared behavioral and neural
responses to ironic statements from AI versus human sources using established
ERP components: P200 reflecting early incongruity detection and P600 indexing
cognitive efforts in reinterpreting incongruity as deliberate irony. Results
demonstrate that people do not fully adopt the intentional stance toward
AI-generated irony. Behaviorally, participants attributed incongruity to
deliberate communication for both sources, though significantly less for AI
than human, showing greater tendency to interpret AI incongruities as
computational errors. Neural data revealed attenuated P200 and P600 effects for
AI-generated irony, suggesting reduced effortful detection and reanalysis
consistent with diminished attribution of communicative intent. Notably, people
who perceived AI as more sincere showed larger P200 and P600 effects for
AI-generated irony, suggesting that intentional stance adoption is calibrated
by specific mental models of artificial agents. These findings reveal that
source attribution shapes neural processing of social-communicative phenomena.
Despite current LLMs' linguistic sophistication, achieving genuine social
agency requires more than linguistic competence, it necessitates a shift in how
humans perceive and attribute intentionality to artificial agents.

</details>


### [63] [Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models](https://arxiv.org/abs/2510.17196)
*Jiaqi Leng,Xiang Hu,Junxiong Wang,Jianguo Li,Wei Wu,Yucheng Lu*

Main category: cs.CL

TL;DR: 本文系统分析了基于分块稀疏注意力的长上下文语言模型，识别出三个关键设计原则：表达性分块编码器、旁路残差路径和训练中强制选择稀疏性，实现了从4K到3200万token的无训练长度外推。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer受限于二次复杂度和长度外推能力差，而滑动窗口注意力等替代架构因固定大小内存无法有效利用完整上下文。分块稀疏注意力虽在极端长度泛化方面有前景，但其成功的关键架构原则尚未完全理解。

Method: 通过统一框架和全面消融研究，系统剖析分块稀疏注意力模型，识别三个关键设计原则：表达性分块编码器、旁路残差路径和训练中强制选择稀疏性。

Result: 结合这三个原则，在RULER和BABILong基准上实现了从4K上下文训练到3200万token的无训练长度外推，建立了新的最先进水平。

Conclusion: 研究为开发未来高性能长上下文语言模型提供了一套清晰且经验基础的设计原则。

Abstract: Effectively processing long contexts is a critical challenge for language
models. While standard Transformers are limited by quadratic complexity and
poor length extrapolation, alternative architectures like sliding window
attention and state space models sacrifice the ability to effectively utilize
the full context due to their fixed-size memory. Chunk-based sparse attention
has emerged as a promising paradigm for extreme length generalization, yet the
key architectural principles underpinning its success are not yet fully
understood. In this work, we present a systematic dissection of these models to
identify the core components driving their performance. Through a unified
framework and comprehensive ablation studies, we demonstrate that a combination
of three design principles is critical: (1) an expressive, non-linear Chunk
Encoder with a dedicated CLS token to produce representations for retrieval;
(2) a Bypassing Residual Path to stably integrate retrieved global information
without it being overridden by the local residual stream; and (3) enforced
selection sparsity during pre-training to bridge the train-test distribution
gap. We provide a theoretical motivation for intra-chunk information processing
and landmark generation. By combining these principles, we establish a new
state-of-the-art for training-free length extrapolation, successfully
generalizing models trained on a 4K context to 32 million tokens on RULER and
BABILong. Our findings provide a clear and empirically-grounded set of design
principles for developing future, highly-capable long-context language models.

</details>


### [64] [Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting](https://arxiv.org/abs/2510.17210)
*Chenchen Tan,Youyang Qu,Xinghao Li,Hui Zhang,Shujie Cui,Cunjian Chen,Longxiang Gao*

Main category: cs.CL

TL;DR: 提出Attention-Shifting框架解决LLM选择性遗忘中的两难问题，通过注意力机制干预实现知识遗忘同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法面临两难：激进遗忘损害模型效用，保守策略保留效用但产生幻觉响应，限制了LLM在知识密集型应用中的可靠性。

Method: 采用注意力转移框架，包含上下文保持抑制和抗幻觉响应塑造两个组件，通过重要性感知抑制和注意力引导保留增强实现选择性遗忘。

Result: 在ToFU基准上准确率提升15%，TDEC基准上提升10%，同时保持竞争性的无幻觉遗忘效果。

Conclusion: AS方法在遗忘效果、泛化能力和响应可靠性之间实现了优越的平衡。

Abstract: The increase in computing power and the necessity of AI-assisted
decision-making boost the growing application of large language models (LLMs).
Along with this, the potential retention of sensitive data of LLMs has spurred
increasing research into machine unlearning. However, existing unlearning
approaches face a critical dilemma: Aggressive unlearning compromises model
utility, while conservative strategies preserve utility but risk hallucinated
responses. This significantly limits LLMs' reliability in knowledge-intensive
applications. To address this, we introduce a novel Attention-Shifting (AS)
framework for selective unlearning. AS is driven by two design objectives: (1)
context-preserving suppression that attenuates attention to fact-bearing tokens
without disrupting LLMs' linguistic structure; and (2) hallucination-resistant
response shaping that discourages fabricated completions when queried about
unlearning content. AS realizes these objectives through two attention-level
interventions, which are importance-aware suppression applied to the unlearning
set to reduce reliance on memorized knowledge and attention-guided retention
enhancement that reinforces attention toward semantically essential tokens in
the retained dataset to mitigate unintended degradation. These two components
are jointly optimized via a dual-loss objective, which forms a soft boundary
that localizes unlearning while preserving unrelated knowledge under
representation superposition. Experimental results show that AS improves
performance preservation over the state-of-the-art unlearning methods,
achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC
benchmark, while maintaining competitive hallucination-free unlearning
effectiveness. Compared to existing methods, AS demonstrates a superior balance
between unlearning effectiveness, generalization, and response reliability.

</details>


### [65] [StreamingThinker: Large Language Models Can Think While Reading](https://arxiv.org/abs/2510.17238)
*Junlong Tong,Yingqi Fan,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 提出StreamingThinker框架，让大语言模型能够在阅读输入的同时进行推理，而不是等待完整输入后再开始思考，显著降低了推理延迟。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理范式需要等待完整输入后才开始思考，这带来了不必要的延迟，并且在动态场景下会削弱对早期信息的注意力。

Method: 设计流式思维范式，通过流式CoT生成、流式约束训练和流式并行推理实现边读边想，使用流式推理单元、流式注意力掩码和位置编码，以及并行KV缓存。

Result: 在数学推理、逻辑推理和上下文QA推理任务上，StreamingThinker保持了与批量思考相当的性能，同时将推理开始前的token等待时间减少了80%，最终答案生成的时间延迟减少了60%以上。

Conclusion: 流式思维范式能够有效降低LLM推理延迟，同时保持推理质量，为实时推理应用提供了可行方案。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm
initiates thinking only after the entire input is available, which introduces
unnecessary latency and weakens attention to earlier information in dynamic
scenarios. Inspired by human cognition of thinking while reading, we first
design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where
reasoning unfolds in the order of input and further adjusts its depth once
reading is complete. We instantiate this paradigm with
\textit{StreamingThinker}, a framework that enables LLMs to think while reading
through the integration of streaming CoT generation, streaming-constraint
training, and streaming parallel inference. Specifically, StreamingThinker
employs streaming reasoning units with quality control for CoT generation,
enforces order-preserving reasoning through streaming attention masks and
position encoding, and leverages parallel KV caches that decouple input
encoding from reasoning generation, thereby ensuring alignment and enabling
true concurrency. We evaluate StreamingThinker on the Qwen3 model family across
math reasoning, logical reasoning, and context-based QA reasoning tasks.
Experimental results show that the StreamingThinker preserves performance
comparable to batch thinking, while yielding an 80\% reduction in token waiting
before the onset of reasoning and a more than 60\% reduction in time-level
latency for producing the final answer, demonstrating the effectiveness of the
streaming paradigm for LLM reasoning. Code will be released at
\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this
repository.}

</details>


### [66] [From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models](https://arxiv.org/abs/2510.17247)
*Zefan Cai,Haoyi Qiu,Haozhe Zhao,Ke Wan,Jiachen Li,Jiuxiang Gu,Wen Xiao,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: 本文提出了VideoBiasEval框架，用于评估视频生成中的社会偏见，发现对齐调优会放大并稳定化社会偏见。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型通过基于人类偏好的奖励模型进行对齐调优，虽然提升了视觉质量，但可能无意中编码和放大了社会偏见。

Method: 引入VideoBiasEval诊断框架，基于社会偏见分类学，采用基于事件的提示策略，分离语义内容和演员属性，并引入多粒度指标进行评估。

Result: 发现对齐调优不仅加强了表征偏见，还使其在时间上更加稳定，产生更平滑但更刻板的描绘。

Conclusion: 需要在对齐过程中进行偏见感知的评估和缓解，以确保公平和负责任的视频生成。

Abstract: Recent advances in video diffusion models have significantly enhanced
text-to-video generation, particularly through alignment tuning using reward
models trained on human preferences. While these methods improve visual
quality, they can unintentionally encode and amplify social biases. To
systematically trace how such biases evolve throughout the alignment pipeline,
we introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating
social representation in video generation. Grounded in established social bias
taxonomies, VideoBiasEval employs an event-based prompting strategy to
disentangle semantic content (actions and contexts) from actor attributes
(gender and ethnicity). It further introduces multi-granular metrics to
evaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,
(3) distributional shifts in social attributes across model variants, and (4)
the temporal persistence of bias within videos. Using this framework, we
conduct the first end-to-end analysis connecting biases in human preference
datasets, their amplification in reward models, and their propagation through
alignment-tuned video diffusion models. Our results reveal that alignment
tuning not only strengthens representational biases but also makes them
temporally stable, producing smoother yet more stereotyped portrayals. These
findings highlight the need for bias-aware evaluation and mitigation throughout
the alignment process to ensure fair and socially responsible video generation.

</details>


### [67] [How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design](https://arxiv.org/abs/2510.17252)
*Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Ayesha Siddiqua,Jungpil Shin*

Main category: cs.CL

TL;DR: 通过大规模情感分析发现孟加拉语新闻标题普遍存在负面情绪倾向，特别是愤怒、恐惧和失望情绪占主导地位，不同媒体对相似新闻的情感框架存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究新闻媒体如何通过情感框架影响公众情绪，探索负面或情绪化标题吸引更多关注和传播更快的现象。

Method: 使用Gemma-3 4B模型对30万条孟加拉语新闻标题和内容进行零样本推理，识别每篇文章的主导情绪和整体基调。

Result: 发现负面情绪明显占主导地位，特别是愤怒、恐惧和失望情绪，不同媒体对相似新闻的情感呈现存在显著差异。

Conclusion: 基于研究结果提出以人为本的新闻聚合器设计理念，通过可视化情感线索帮助读者识别日常新闻中隐藏的情感框架。

Abstract: News media often shape the public mood not only by what they report but by
how they frame it. The same event can appear calm in one outlet and alarming in
another, reflecting subtle emotional bias in reporting. Negative or emotionally
charged headlines tend to attract more attention and spread faster, which in
turn encourages outlets to frame stories in ways that provoke stronger
reactions. This research explores that tendency through large-scale emotion
analysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we
analyzed 300000 Bengali news headlines and their content to identify the
dominant emotion and overall tone of each. The findings reveal a clear
dominance of negative emotions, particularly anger, fear, and disappointment,
and significant variation in how similar stories are emotionally portrayed
across outlets. Based on these insights, we propose design ideas for a
human-centered news aggregator that visualizes emotional cues and helps readers
recognize hidden affective framing in daily news.

</details>


### [68] [Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations](https://arxiv.org/abs/2510.17256)
*Shahin Atakishiyev,Housam K. B. Babiker,Jiayi Dai,Nawshad Farruque,Teruaki Hayashi,Nafisa Sadaf Hriti,Md Abed Rahman,Iain Smith,Mi-Young Kim,Osmar R. Zaïane,Randy Goebel*

Main category: cs.CL

TL;DR: 本文综述了基于Transformer的大语言模型的局部可解释性和机制可解释性方法，在医疗和自动驾驶领域进行了实验研究，并总结了当前未解决的问题和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然性能优异，但其预测机制对人类不可理解，且经常出现幻觉错误，迫切需要理解和解释模型内部工作机制以建立信任。

Method: 采用文献综述方法梳理局部可解释性和机制可解释性方法，并在医疗和自动驾驶领域进行实验研究，分析解释对接收者的信任影响。

Result: 系统梳理了现有的可解释性方法，通过实验验证了解释性在关键领域的重要性，识别了当前未解决的问题。

Conclusion: 需要进一步研究以生成与人类对齐、可信赖的LLM解释，面临关键挑战但存在重要机遇。

Abstract: Large language models have exhibited impressive performance across a broad
range of downstream tasks in natural language processing. However, how a
language model predicts the next token and generates content is not generally
understandable by humans. Furthermore, these models often make errors in
prediction and reasoning, known as hallucinations. These errors underscore the
urgent need to better understand and interpret the intricate inner workings of
language models and how they generate predictive outputs. Motivated by this
gap, this paper investigates local explainability and mechanistic
interpretability within Transformer-based large language models to foster trust
in such models. In this regard, our paper aims to make three key contributions.
First, we present a review of local explainability and mechanistic
interpretability approaches and insights from relevant studies in the
literature. Furthermore, we describe experimental studies on explainability and
reasoning with large language models in two critical domains -- healthcare and
autonomous driving -- and analyze the trust implications of such explanations
for explanation receivers. Finally, we summarize current unaddressed issues in
the evolving landscape of LLM explainability and outline the opportunities,
critical challenges, and future directions toward generating human-aligned,
trustworthy LLM explanations.

</details>


### [69] [TaxoAlign: Scholarly Taxonomy Generation Using Language Models](https://arxiv.org/abs/2510.17263)
*Avishek Lahiri,Yufang Hou,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: 提出了TaxoAlign方法用于自动生成学术分类法，创建了CS-TaxoBench基准数据集，并在结构对齐和语义连贯性方面超越了现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动文献综述生成方法缺乏与人工专家生成分类法的结构比较，需要弥合人工生成与自动创建分类法之间的差距。

Method: 提出TaxoAlign方法，这是一个基于主题的三阶段指令引导的学术分类法生成方法，并建立了严格的自动评估框架来衡量结构对齐和语义连贯性。

Result: TaxoAlign在CS-TaxoBench基准上几乎在所有指标上都持续超越了基线方法，验证了其有效性。

Conclusion: TaxoAlign方法能够有效生成与人工专家分类法结构对齐且语义连贯的学术分类法，为自动文献综述提供了有力工具。

Abstract: Taxonomies play a crucial role in helping researchers structure and navigate
knowledge in a hierarchical manner. They also form an important part in the
creation of comprehensive literature surveys. The existing approaches to
automatic survey generation do not compare the structure of the generated
surveys with those written by human experts. To address this gap, we present
our own method for automated taxonomy creation that can bridge the gap between
human-generated and automatically-created taxonomies. For this purpose, we
create the CS-TaxoBench benchmark which consists of 460 taxonomies that have
been extracted from human-written survey papers. We also include an additional
test set of 80 taxonomies curated from conference survey papers. We propose
TaxoAlign, a three-phase topic-based instruction-guided method for scholarly
taxonomy generation. Additionally, we propose a stringent automated evaluation
framework that measures the structural alignment and semantic coherence of
automatically generated taxonomies in comparison to those created by human
experts. We evaluate our method and various baselines on CS-TaxoBench, using
both automated evaluation metrics and human evaluation studies. The results
show that TaxoAlign consistently surpasses the baselines on nearly all metrics.
The code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.

</details>


### [70] [Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning](https://arxiv.org/abs/2510.17289)
*Hajar Bakarou,Mohamed Sinane El Messoussi,Anaïs Ollagnier*

Main category: cs.CL

TL;DR: 该研究使用法语多轮对话数据集CyberAgressionAdo-Large，评估了反社会行为检测、霸凌行为分析和霸凌同伴群体识别三个任务，发现多模态模型优于单模态基线，其中mBERT + WD-SGCN融合模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的反社会行为（如仇恨言论、骚扰和网络霸凌）对平台安全和社会福祉构成日益严重的风险，而多轮对话环境下的此类行为研究因数据有限而较少被探索。

Method: 使用CyberAgressionAdo-Large数据集，评估了6种基于文本和8种基于图的表示学习方法，分析词汇线索、交互动态及其多模态融合。

Result: 多模态模型优于单模态基线，mBERT + WD-SGCN融合模型在反社会行为检测上达到0.718的最佳性能，在同伴群体识别和霸凌分析上分别获得0.286和0.606的竞争性分数。

Conclusion: 多模态方法能有效处理隐式攻击、角色转换和上下文依赖的敌意等细微反社会行为现象，为多轮对话环境中的反社会行为检测提供了有效解决方案。

Abstract: Antisocial behavior (ASB) on social media -- including hate speech,
harassment, and cyberbullying -- poses growing risks to platform safety and
societal well-being. Prior research has focused largely on networks such as X
and Reddit, while \textit{multi-party conversational settings} remain
underexplored due to limited data. To address this gap, we use
\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB
in multi-party conversations, and evaluate three tasks: \textit{abuse
detection}, \textit{bullying behavior analysis}, and \textit{bullying
peer-group identification}. We benchmark six text-based and eight graph-based
\textit{representation-learning methods}, analyzing lexical cues, interactional
dynamics, and their multimodal fusion. Results show that multimodal models
outperform unimodal baselines. The late fusion model \texttt{mBERT + WD-SGCN}
achieves the best overall results, with top performance on abuse detection
(0.718) and competitive scores on peer-group identification (0.286) and
bullying analysis (0.606). Error analysis highlights its effectiveness in
handling nuanced ASB phenomena such as implicit aggression, role transitions,
and context-dependent hostility.

</details>


### [71] [Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.17354)
*Chenghao Zhang,Guanting Dong,Xinyu Yang,Zhicheng Dou*

Main category: cs.CL

TL;DR: 提出了Nyx，一个统一的混合模态到混合模态检索器，用于解决通用检索增强生成(URAG)的挑战，通过自动化管道构建混合模态数据集NyxQA，并采用两阶段训练框架，在视觉语言任务中显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统主要关注单模态文本文档，在现实场景中，查询和文档可能包含混合模态（如文本和图像），现有方法存在不足。

Method: 提出Nyx统一混合模态检索器，通过四阶段自动化管道构建NyxQA数据集，采用两阶段训练框架：先在NyxQA和开源检索数据集上预训练，然后使用下游视觉语言模型的反馈进行监督微调。

Result: Nyx不仅在标准文本RAG基准上表现有竞争力，在更通用和现实的URAG设置中表现优异，显著提高了视觉语言任务的生成质量。

Conclusion: Nyx为混合模态检索增强生成提供了一个有效的解决方案，能够处理现实世界中的多模态信息需求，在视觉语言任务中展现出显著优势。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing large language models (LLMs) by retrieving relevant documents from an
external corpus. However, existing RAG systems primarily focus on unimodal text
documents, and often fall short in real-world scenarios where both queries and
documents may contain mixed modalities (such as text and images). In this
paper, we address the challenge of Universal Retrieval-Augmented Generation
(URAG), which involves retrieving and reasoning over mixed-modal information to
improve vision-language generation. To this end, we propose Nyx, a unified
mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate
the scarcity of realistic mixed-modal data, we introduce a four-stage automated
pipeline for generation and filtering, leveraging web documents to construct
NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that
better reflect real-world information needs. Building on this high-quality
dataset, we adopt a two-stage training framework for Nyx: we first perform
pre-training on NyxQA along with a variety of open-source retrieval datasets,
followed by supervised fine-tuning using feedback from downstream
vision-language models (VLMs) to align retrieval outputs with generative
preferences. Experimental results demonstrate that Nyx not only performs
competitively on standard text-only RAG benchmarks, but also excels in the more
general and realistic URAG setting, significantly improving generation quality
in vision-language tasks.

</details>


### [72] [The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives](https://arxiv.org/abs/2510.17388)
*Henry Lim,Kwan Hui Lim*

Main category: cs.CL

TL;DR: 研究发现指令调优大语言模型在简单指令执行上存在严重不足，特别是在选项标签格式变化时表现不稳定，暴露了当前指令调优范式的缺陷。


<details>
  <summary>Details</summary>
Motivation: 尽管指令调优大语言模型在零样本推理方面表现出色，但其执行简单、自包含指令的能力尚未得到充分探索，而这正是复杂指令遵循的基础。

Method: 在修改后的MMLU和MMLU-Pro基准上评估20个指令调优大语言模型，系统性地改变选项标签格式（字母、数字、罗马数字），在四种范式下测试：有明确指令、无指令、移除选项内容、三样本示例。

Result: 标签格式变化导致性能大幅波动（如罗马数字vs数字下降30.45%），无指令时性能进一步下降（最多-10.84%），移除选项内容时除数字标签外均无法超越随机基线，三样本示例无法显著提升鲁棒性。

Conclusion: 当前指令调优范式存在不足，需要专门针对原子指令遵循的评估方法和训练策略，即使更大的模型在准确性上更高，但在指令遵循一致性方面仍然存在问题。

Abstract: Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot
reasoning, yet their ability to execute simple, self-contained instructions
remains underexplored, despite this being foundational to complex
instruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro
benchmarks, by systematically varying the format of option labels (alphabetic,
numeric, Roman) while keeping their meaning identical under four paradigms,
namely: (1) With explicit instructions, label changes cause large performance
shifts (e.g., -30.45\% for Roman vs. numeric), revealing instruction-format
bias. (2) Without instructions, performance drops further (up to -10.84\%) and
label sensitivity intensifies, underscoring the role of explicit guidance. (3)
When option contents are removed, models fail random-choice baselines except
with numeric labels, suggesting weak adherence to atomic directives. (4)
Three-shot exemplars yield no significant gains in robustness or fidelity, and
generation analyses show persistent label errors, especially for non-numeric
formats. Across model sizes, larger LLMs achieve higher accuracy but remain
inconsistent in instruction adherence. These results expose the insufficiencies
of current instruction-tuning paradigms and highlight the need for evaluation
methods and training strategies that explicitly target atomic
instruction-following.

</details>


### [73] [EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs](https://arxiv.org/abs/2510.17389)
*Numaan Naeem,Abdellah El Mekki,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 提出了EduAdapt基准，包含近48K个按年级标记的科学问答对，用于评估LLMs在不同年级水平上的适应性表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs虽然在学术基准上表现良好，但无法根据学生年级水平调整回答，这在K-12教育中至关重要。缺乏标准化基准来评估LLMs跨认知发展阶段的适应能力。

Method: 创建了EduAdapt基准数据集，涵盖9个科学学科的47,800个问答对，横跨1-12年级，分为四个年级水平。评估了多个开源LLMs在该基准上的表现。

Result: 大型模型通常表现更好，但在为低年级学生（1-5年级）生成合适回答方面仍有困难。

Conclusion: 这是首个评估LLMs年级水平适应性的数据集和评估框架，旨在通过改进训练和提示策略开发更适合教育发展的AI系统。

Abstract: Large language models (LLMs) are transforming education by answering
questions, explaining complex concepts, and generating content across a wide
range of subjects. Despite strong performance on academic benchmarks, they
often fail to tailor responses to students' grade levels. This is a critical
need in K-12 education, where age-appropriate vocabulary and explanation are
essential for effective learning. Existing models frequently produce outputs
that are too advanced or vague for younger learners, and there are no
standardized benchmarks to evaluate their ability to adjust across cognitive
and developmental stages. To address this gap, we introduce EduAdapt, a
benchmark of nearly 48k grade-labeled QA pairs across nine science subjects,
spanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse
set of open-source LLMs on EduAdapt and find that while larger models generally
perform better, they still struggle with generating suitable responses for
early-grade students (Grades 1-5). Our work presents the first dataset and
evaluation framework for assessing grade-level adaptability in LLMs, aiming to
foster more developmentally aligned educational AI systems through better
training and prompting strategies. EduAdapt code and datasets are publicly
available at https://github.com/NaumanNaeem/EduAdapt.

</details>


### [74] [Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine](https://arxiv.org/abs/2510.17402)
*Jiacheng Xie,Shuai Zeng,Yang Yu,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: Ladder-base是首个基于GRPO强化学习方法训练的中医领域大语言模型，在多项推理指标上超越了通用LLM和中医专用模型。


<details>
  <summary>Details</summary>
Motivation: 传统中医知识体系独特且复杂，现有中医LLM在一致性、数据质量和评估标准方面存在局限，需要更有效的对齐方法。

Method: 基于Qwen2.5-7B-Instruct基础模型，使用TCM-Ladder基准的文本子集，采用GRPO强化学习方法进行训练，通过组内比较优化响应选择。

Result: Ladder-base在标准化评估中表现优异，在多项推理指标上超越了GPT-4、Gemini 2.5等通用LLM和BenTsao、HuatuoGPT2等中医专用模型。

Conclusion: GRPO为传统医学领域LLM与专家级推理对齐提供了有效策略，支持开发可信赖且临床基础扎实的中医人工智能系统。

Abstract: Traditional Chinese Medicine (TCM) presents a rich and structurally unique
knowledge system that challenges conventional applications of large language
models (LLMs). Although previous TCM-specific LLMs have shown progress through
supervised fine-tuning, they often face limitations in alignment, data quality,
and evaluation consistency. In this study, we introduce Ladder-base, the first
TCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a
reinforcement learning method that improves reasoning and factual consistency
by optimizing response selection based on intra-group comparisons. Ladder-base
is built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively
on the textual subset of the TCM-Ladder benchmark, using 80 percent of the data
for training and the remaining 20 percent split evenly between validation and
test sets. Through standardized evaluation, Ladder-base demonstrates superior
performance across multiple reasoning metrics when compared to both
state-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and
Qwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and
Zhongjing. These findings suggest that GRPO provides an effective and efficient
strategy for aligning LLMs with expert-level reasoning in traditional medical
domains and supports the development of trustworthy and clinically grounded TCM
artificial intelligence systems.

</details>


### [75] [AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages](https://arxiv.org/abs/2510.17405)
*Mardiyyah Oduwole,Prince Mireku,Fatimo Adebanjo,Oluwatosin Olajide,Mahi Aminu Aliyu,Jekaterina Novikova*

Main category: cs.CL

TL;DR: 提出了AfriCaption框架，为20种非洲语言提供多语言图像描述功能，填补了多模态AI在低资源语言方面的空白。


<details>
  <summary>Details</summary>
Motivation: 多模态AI研究主要集中在高资源语言，阻碍了该领域发展的民主化。需要为非洲等低资源语言提供图像描述能力。

Method: 构建基于Flickr8k的数据集，采用上下文感知的选择和翻译过程；开发动态上下文保持管道，通过模型集成和自适应替换确保质量；创建0.5B参数的AfriCaption模型，集成SigLIP和NLLB200进行跨语言描述生成。

Result: 建立了首个可扩展的非洲低资源语言图像描述资源，确保了持续的数据质量。

Conclusion: 该统一框架为真正包容的多模态AI奠定了基础，推动了AI技术在低资源语言中的民主化应用。

Abstract: Multimodal AI research has overwhelmingly focused on high-resource languages,
hindering the democratization of advancements in the field. To address this, we
present AfriCaption, a comprehensive framework for multilingual image
captioning in 20 African languages and our contributions are threefold: (i) a
curated dataset built on Flickr8k, featuring semantically aligned captions
generated via a context-aware selection and translation process; (ii) a
dynamic, context-preserving pipeline that ensures ongoing quality through model
ensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B
parameter vision-to-text architecture that integrates SigLIP and NLLB200 for
caption generation across under-represented languages. This unified framework
ensures ongoing data quality and establishes the first scalable
image-captioning resource for under-represented African languages, laying the
groundwork for truly inclusive multimodal AI.

</details>


### [76] [BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine](https://arxiv.org/abs/2510.17415)
*Jiacheng Xie,Yang Yu,Yibo Chen,Hanyao Zhang,Lening Zhao,Jiaxuan He,Lei Jiang,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 本文开发了BenCao，一个基于ChatGPT的中医多模态助手，通过自然语言指令调优而非参数重训练，整合了结构化知识库、诊断数据和专家反馈，在中医问答和诊断任务中表现优于通用和中医领域模型。


<details>
  <summary>Details</summary>
Motivation: 传统中医依赖整体推理、隐含逻辑和多模态诊断线索，现有中医领域大语言模型在文本理解方面取得进展，但缺乏多模态整合、可解释性和临床适用性。

Method: 开发BenCao系统，整合1000多部经典和现代文本的知识库，基于场景的指令框架，可解释推理的思维链模拟机制，以及执业中医师的反馈精炼过程，连接外部API进行舌象分类和多模态数据库检索。

Result: 在单项选择题基准和多模态分类任务评估中，BenCao在诊断、草药识别和体质分类方面表现优于通用和中医领域模型，已在OpenAI GPTs Store部署，截至2025年10月有近1000名全球用户访问。

Conclusion: 本研究证明了通过自然语言指令调优和多模态整合开发中医领域大语言模型的可行性，为生成式AI与传统医学推理的对齐提供了实用框架和可扩展的现实部署路径。

Abstract: Traditional Chinese Medicine (TCM), with a history spanning over two
millennia, plays a role in global healthcare. However, applying large language
models (LLMs) to TCM remains challenging due to its reliance on holistic
reasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain
LLMs have made progress in text-based understanding but lack multimodal
integration, interpretability, and clinical applicability. To address these
limitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,
integrating structured knowledge bases, diagnostic data, and expert feedback
refinement. BenCao was trained through natural language instruction tuning
rather than parameter retraining, aligning with expert-level reasoning and
ethical norms specific to TCM. The system incorporates a comprehensive
knowledge base of over 1,000 classical and modern texts, a scenario-based
instruction framework for diverse interactions, a chain-of-thought simulation
mechanism for interpretable reasoning, and a feedback refinement process
involving licensed TCM practitioners. BenCao connects to external APIs for
tongue-image classification and multimodal database retrieval, enabling dynamic
access to diagnostic resources. In evaluations across single-choice question
benchmarks and multimodal classification tasks, BenCao achieved superior
accuracy to general-domain and TCM-domain models, particularly in diagnostics,
herb recognition, and constitution classification. The model was deployed as an
interactive application on the OpenAI GPTs Store, accessed by nearly 1,000
users globally as of October 2025. This study demonstrates the feasibility of
developing a TCM-domain LLM through natural language-based instruction tuning
and multimodal integration, offering a practical framework for aligning
generative AI with traditional medical reasoning and a scalable pathway for
real-world deployment.

</details>


### [77] [Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging](https://arxiv.org/abs/2510.17426)
*Tiancheng Hu,Benjamin Minixhofer,Nigel Collier*

Main category: cs.CL

TL;DR: 论文揭示了后训练对齐不仅导致任务准确率下降，还会造成严重的校准损失，使模型过度自信且输出多样性降低。通过简单的权重插值方法，可以找到帕累托最优模型，在提升准确率的同时恢复校准性能。


<details>
  <summary>Details</summary>
Motivation: 传统上"对齐税"主要关注任务准确率下降，但作者发现它还涉及校准损失，导致模型过度自信和可靠性下降，需要更全面的解决方案。

Method: 采用简单的后处理干预：在模型对齐前后的权重之间进行插值，寻找帕累托最优的中间模型。

Result: 该方法能够找到同时超越两个父模型准确率的模型，并显著恢复对齐过程中损失的校准性能，证明这不是严格的权衡关系。

Conclusion: 简单的模型合并提供了一种计算高效的方法来缓解对齐税的全部影响，产生更强大且更可靠的模型。

Abstract: The "alignment tax" of post-training is typically framed as a drop in task
accuracy. We show it also involves a severe loss of calibration, making models
overconfident, less reliable, and model outputs less diverse. We show that this
trade-off can be navigated effectively via a simple post-hoc intervention:
interpolating between a model's weights before and after alignment. Crucially,
this is not a strict trade-off. We find that the process consistently reveals
Pareto-optimal interpolations - models that improve accuracy beyond both
parents while substantially recovering the calibration lost during alignment.
Our work demonstrates that simple model merging provides a computationally
efficient method for mitigating the full scope of the alignment tax, yielding
models that are more capable and more reliable.

</details>


### [78] [Agentic Reinforcement Learning for Search is Unsafe](https://arxiv.org/abs/2510.17431)
*Yushi Yang,Shreyansh Padarha,Andrew Lee,Adam Mahdi*

Main category: cs.CL

TL;DR: RL训练的搜索模型虽然继承了指令调优的拒绝能力，但存在脆弱性。两种简单攻击（搜索攻击和多搜索攻击）能显著降低模型的安全性能，暴露了当前RL训练在安全性方面的核心弱点。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解智能体强化学习（RL）训练的大型语言模型在自主调用工具时的安全特性，特别是搜索应用中的安全性问题。

Method: 通过两种简单攻击方法测试RL训练的搜索模型：搜索攻击（强制模型以搜索开始响应）和多搜索攻击（鼓励模型重复搜索），在Qwen和Llama两个模型家族上进行实验。

Result: 攻击显著降低了模型的安全性能：拒绝率降低达60.0%，答案安全性降低82.5%，搜索查询安全性降低82.4%。攻击通过触发模型生成有害的、镜像请求的搜索查询来绕过继承的拒绝机制。

Conclusion: 当前RL训练存在核心弱点：奖励生成有效查询而不考虑其危害性，导致RL搜索模型存在易被用户利用的漏洞，迫切需要开发安全性感知的智能体RL管道来优化安全搜索。

Abstract: Agentic reinforcement learning (RL) trains large language models to
autonomously call tools during reasoning, with search as the most common
application. These models excel at multi-step reasoning tasks, but their safety
properties are not well understood. In this study, we show that RL-trained
search models inherit refusal from instruction tuning and often deflect harmful
requests by turning them into safe queries. However, this safety is fragile.
Two simple attacks, one that forces the model to begin response with search
(Search attack), another that encourages models to repeatedly search
(Multi-search attack), trigger cascades of harmful searches and answers. Across
two model families (Qwen, Llama) with both local and web search, these attacks
lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query
safety by 82.4%. The attacks succeed by triggering models to generate harmful,
request-mirroring search queries before they can generate the inherited refusal
tokens. This exposes a core weakness of current RL training: it rewards
continued generation of effective queries without accounting for their
harmfulness. As a result, RL search models have vulnerabilities that users can
easily exploit, making it urgent to develop safety-aware agentic RL pipelines
optimising for safe search.

</details>


### [79] [Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings](https://arxiv.org/abs/2510.17437)
*Manuela Daniela Danu,George Marica,Constantin Suciu,Lucian Mihai Itu,Oladimeji Farri*

Main category: cs.CL

TL;DR: 开发基于BERT的临床命名实体识别模型，用于从英语、西班牙语和意大利语的心脏病学临床文本中提取疾病和药物实体，在BioASQ MultiCardioNER任务中取得优于平均水平的性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据快速增长，需要从非结构化临床文本中提取生物医学知识来支持临床系统发展。虽然英语临床NER研究较多，但低资源语言的临床文本研究仍然稀缺。

Method: 探索单语和多语言BERT模型在心脏病学临床文本中的应用，训练用于从英语、西班牙语和意大利语的临床病例报告中提取疾病和药物实体。

Result: 在西班牙语疾病识别(SDR)上F1得分为77.88%，西班牙语药物识别(SMR)为92.09%，英语药物识别(EMR)为91.74%，意大利语药物识别(IMR)为88.9%，均超过测试排行榜的平均值和中位数。

Conclusion: 基于BERT的上下文嵌入模型能够有效提升低资源语言临床命名实体识别的性能，为多语言临床文本分析提供了可行方案。

Abstract: The rapidly increasing volume of electronic health record (EHR) data
underscores a pressing need to unlock biomedical knowledge from unstructured
clinical texts to support advancements in data-driven clinical systems,
including patient diagnosis, disease progression monitoring, treatment effects
assessment, prediction of future clinical events, etc. While contextualized
language models have demonstrated impressive performance improvements for named
entity recognition (NER) systems in English corpora, there remains a scarcity
of research focused on clinical texts in low-resource languages. To bridge this
gap, our study aims to develop multiple deep contextual embedding models to
enhance clinical NER in the cardiology domain, as part of the BioASQ
MultiCardioNER shared task. We explore the effectiveness of different
monolingual and multilingual BERT-based models, trained on general domain text,
for extracting disease and medication mentions from clinical case reports
written in English, Spanish, and Italian. We achieved an F1-score of 77.88% on
Spanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition
(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian
Medications Recognition (IMR). These results outperform the mean and median F1
scores in the test leaderboard across all subtasks, with the mean/median values
being: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and
82.8%/87.76% for IMR.

</details>


### [80] [Evaluating Large Language Models on Urdu Idiom Translation](https://arxiv.org/abs/2510.17460)
*Muhammad Farmal Khan,Mousumi Akter*

Main category: cs.CL

TL;DR: 该研究为乌尔都语到英语的习语翻译创建了首个评估数据集，评估了多种LLM和NMT系统，发现提示工程能提升习语翻译质量，且原生乌尔都语文本比罗马化文本翻译更准确。


<details>
  <summary>Details</summary>
Motivation: 习语翻译在机器翻译中仍是一个重大挑战，特别是对于乌尔都语等低资源语言，此前研究关注有限。

Method: 创建乌尔都语到英语习语翻译评估数据集，评估多种开源LLM和NMT系统，使用BLEU、BERTScore、COMET和XCOMET等自动指标评估翻译质量。

Result: 提示工程相比直接翻译能提升习语翻译质量，但不同提示类型间的性能差异较小；原生乌尔都语输入的习语翻译比罗马化乌尔都语更准确。

Conclusion: 文本表示对翻译质量有显著影响，原生乌尔都语在习语翻译上表现优于罗马化版本，提示工程是提升习语翻译的有效方法。

Abstract: Idiomatic translation remains a significant challenge in machine translation,
especially for low resource languages such as Urdu, and has received limited
prior attention. To advance research in this area, we introduce the first
evaluation datasets for Urdu to English idiomatic translation, covering both
Native Urdu and Roman Urdu scripts and annotated with gold-standard English
equivalents. We evaluate multiple open-source Large Language Models (LLMs) and
Neural Machine Translation (NMT) systems on this task, focusing on their
ability to preserve idiomatic and cultural meaning. Automatic metrics including
BLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our
findings indicate that prompt engineering enhances idiomatic translation
compared to direct translation, though performance differences among prompt
types are relatively minor. Moreover, cross script comparisons reveal that text
representation substantially affects translation quality, with Native Urdu
inputs producing more accurate idiomatic translations than Roman Urdu.

</details>


### [81] [Disparities in Multilingual LLM-Based Healthcare Q&A](https://arxiv.org/abs/2510.17476)
*Ipek Baris Schlicht,Burcu Sayin,Zhixue Zhao,Frederik M. Labonté,Cesare Barbera,Marco Viviani,Paolo Rosso,Lucie Flek*

Main category: cs.CL

TL;DR: 该研究系统评估了多语言大语言模型在医疗问答中的跨语言事实对齐差异，发现英语维基百科覆盖度和模型回答一致性明显优于其他语言，但通过提供非英语上下文可以有效改善事实对齐。


<details>
  <summary>Details</summary>
Motivation: 将AI整合到医疗保健中需要公平获取可靠健康信息，但不同语言的信息质量存在差异，引发对多语言大语言模型可靠性和一致性的担忧。

Method: 构建多语言维基医疗数据集，分析跨语言医疗覆盖度，评估LLM回答与参考信息的一致性，并通过上下文信息和检索增强生成进行案例研究。

Result: 发现维基百科覆盖度和LLM事实对齐存在显著跨语言差异，模型回答更倾向于与英语维基百科对齐，即使提示为非英语。提供非英语维基百科上下文能有效将事实对齐转向文化相关知识。

Conclusion: 研究结果强调了构建更公平的多语言医疗AI系统的实际路径，通过上下文干预可以改善非英语语言的事实对齐质量。

Abstract: Equitable access to reliable health information is vital when integrating AI
into healthcare. Yet, information quality varies across languages, raising
concerns about the reliability and consistency of multilingual Large Language
Models (LLMs). We systematically examine cross-lingual disparities in
pre-training source and factuality alignment in LLM answers for multilingual
healthcare Q&A across English, German, Turkish, Chinese (Mandarin), and
Italian. We (i) constructed Multilingual Wiki Health Care
(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed
cross-lingual healthcare coverage; (iii) assessed LLM response alignment with
these references; and (iv) conducted a case study on factual alignment through
the use of contextual information and Retrieval-Augmented Generation (RAG). Our
findings reveal substantial cross-lingual disparities in both Wikipedia
coverage and LLM factual alignment. Across LLMs, responses align more with
English Wikipedia, even when the prompts are non-English. Providing contextual
excerpts from non-English Wikipedia at inference time effectively shifts
factual alignment toward culturally relevant knowledge. These results highlight
practical pathways for building more equitable, multilingual AI systems for
healthcare.

</details>


### [82] [ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts](https://arxiv.org/abs/2510.17483)
*Zheyue Tan,Zhiyuan Li,Tao Yuan,Dong Zhou,Weilin Liu,Yueqing Zhuang,Yadong Li,Guowei Niu,Cheng Qin,Zhuyu Yao,Congyi Liu,Haiyang Xu,Boxun Li,Guohao Dai,Bo Zhao,Yu Wang*

Main category: cs.CL

TL;DR: ReXMoE是一种新颖的混合专家架构，通过跨层重用专家来改进路由机制，在固定参数预算下提升模型表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度专家架构受限于层局部路由机制，需要在专家维度和路由多样性之间权衡，限制了模型的表达能力。

Method: 提出ReXMoE架构，允许路由器跨相邻层重用专家，解耦专家维度与每层预算，并采用渐进式扩展路由策略逐步增加候选专家池。

Result: 在0.5B到7B参数规模的模型上实验表明，ReXMoE在固定架构维度下持续提升语言建模和下游任务性能。

Conclusion: ReXMoE为参数高效且可扩展的MoE基础LLMs提供了新的设计范式。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a promising approach
to scale Large Language Models (LLMs). MoE boosts the efficiency by activating
a subset of experts per token. Recent works show that fine-grained experts
substantially enriches the combinatorial flexibility of active experts and
enhances model expressiveness. However, such a design is fundamentally limited
by the layer-local routing mechanism: each layer is restricted to its own
expert pool. This requires a careful trade-off between expert dimensionality
and routing diversity given fixed parameter budgets. We describe ReXMoE, a
novel MoE architecture that improves routing beyond the existing layer-local
approaches by allowing routers to reuse experts across adjacent layers. ReXMoE
decouples expert dimensionality from per-layer budgets, enabling richer expert
combinations without sacrificing individual expert capacity or inflating
overall parameters. To this end, we propose a new progressive scaling routing
(PSR) strategy to gradually increase the candidate expert pool during training.
As a result, ReXMoE improves both language modeling and downstream task
performance. Extensive experiments on models ranging from 0.5B to 7B parameters
across different architectures demonstrate that ReXMoE consistently improves
performance under fixed architectural dimensions, confirming ReXMoE as new
design paradigm for parameter-efficient and scalable MoE-based LLMs.

</details>


### [83] [DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning](https://arxiv.org/abs/2510.17489)
*Yongxin He,Shan Zhang,Yixuan Cao,Lei Ma,Ping Luo*

Main category: cs.CL

TL;DR: 提出DETree方法，通过层次亲和树结构建模不同AI参与文本生成过程的关系，并开发RealBench数据集，显著提升混合文本检测性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: AI参与文本生成过程多样化（AI写人改、人写AI改、AI间协作），现有检测方法建模粗糙，主要使用二元分类或多分类，难以应对复杂特征。

Method: 构建层次亲和树结构建模不同生成过程间的关系，设计专用损失函数使文本表示与树结构对齐，开发RealBench数据集支持训练。

Result: DETree在混合文本检测任务中表现更好，在分布外场景下显著提升鲁棒性和泛化能力，特别是在少样本学习条件下。

Conclusion: 基于训练的方法在OOD设置中具有潜力，层次关系建模能有效提升AI文本检测性能。

Abstract: Detecting AI-involved text is essential for combating misinformation,
plagiarism, and academic misconduct. However, AI text generation includes
diverse collaborative processes (AI-written text edited by humans,
human-written text edited by AI, and AI-generated text refined by other AI),
where various or even new LLMs could be involved. Texts generated through these
varied processes exhibit complex characteristics, presenting significant
challenges for detection. Current methods model these processes rather crudely,
primarily employing binary classification (purely human vs. AI-involved) or
multi-classification (treating human-AI collaboration as a new class). We
observe that representations of texts generated through different processes
exhibit inherent clustering relationships. Therefore, we propose DETree, a
novel approach that models the relationships among different processes as a
Hierarchical Affinity Tree structure, and introduces a specialized loss
function that aligns text representations with this tree. To facilitate this
learning, we developed RealBench, a comprehensive benchmark dataset that
automatically incorporates a wide spectrum of hybrid texts produced through
various human-AI collaboration processes. Our method improves performance in
hybrid text detection tasks and significantly enhances robustness and
generalization in out-of-distribution scenarios, particularly in few-shot
learning conditions, further demonstrating the promise of training-based
approaches in OOD settings. Our code and dataset are available at
https://github.com/heyongxin233/DETree.

</details>


### [84] [Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents](https://arxiv.org/abs/2510.17491)
*Yihong Tang,Kehai Chen,Liang Yue,Jinxin Fan,Caishen Zhou,Xiaoguang Li,Yuyang Zhang,Mingming Zhao,Shixiong Kai,Kaiyang Guo,Xingshan Zeng,Wenjing Cun,Lifeng Shang,Min Zhang*

Main category: cs.CL

TL;DR: 本文系统综述了基于大语言模型的行业智能体技术、应用和评估方法，提出了行业智能体能力成熟度框架，分析了从"流程执行系统"到"自适应社会系统"的演进路径。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，能够自主推理、规划和执行复杂任务的智能体已成为人工智能前沿，但如何将通用智能体研究转化为推动行业转型的生产力仍面临重大挑战。

Method: 使用行业智能体能力成熟度框架，分析支撑智能体能力发展的三大技术支柱（记忆、规划、工具使用），并概述行业智能体在数字工程、科学发现、具身智能等领域的应用。

Result: 梳理了行业智能体的技术演进路径，从支持简单任务到实现复杂自主系统和集体智能；总结了在真实领域中的应用情况；识别了现有评估系统在真实性、安全性和行业特性方面面临的挑战。

Conclusion: 通过结合技术演进与行业实践，本文为理解和构建下一代行业智能体提供了清晰的路线图和理论基础，探讨了行业智能体的能力边界、发展潜力和治理问题。

Abstract: With the rise of large language models (LLMs), LLM agents capable of
autonomous reasoning, planning, and executing complex tasks have become a
frontier in artificial intelligence. However, how to translate the research on
general agents into productivity that drives industry transformations remains a
significant challenge. To address this, this paper systematically reviews the
technologies, applications, and evaluation methods of industry agents based on
LLMs. Using an industry agent capability maturity framework, it outlines the
evolution of agents in industry applications, from "process execution systems"
to "adaptive social systems." First, we examine the three key technological
pillars that support the advancement of agent capabilities: Memory, Planning,
and Tool Use. We discuss how these technologies evolve from supporting simple
tasks in their early forms to enabling complex autonomous systems and
collective intelligence in more advanced forms. Then, we provide an overview of
the application of industry agents in real-world domains such as digital
engineering, scientific discovery, embodied intelligence, collaborative
business execution, and complex system simulation. Additionally, this paper
reviews the evaluation benchmarks and methods for both fundamental and
specialized capabilities, identifying the challenges existing evaluation
systems face regarding authenticity, safety, and industry specificity. Finally,
we focus on the practical challenges faced by industry agents, exploring their
capability boundaries, developmental potential, and governance issues in
various scenarios, while providing insights into future directions. By
combining technological evolution with industry practices, this review aims to
clarify the current state and offer a clear roadmap and theoretical foundation
for understanding and building the next generation of industry agents.

</details>


### [85] [Deep Self-Evolving Reasoning](https://arxiv.org/abs/2510.17498)
*Zihan Liu,Shun Zheng,Xumeng Wen,Yang Wang,Jiang Bian,Mao Yang*

Main category: cs.CL

TL;DR: Deep Self-Evolving Reasoning (DSER) 是一种概率推理框架，通过将迭代推理建模为马尔可夫链，即使验证和修正能力较弱，也能显著扩展小型开放权重模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有验证-修正框架依赖强大的验证和修正能力，这在开放权重的小型模型中很脆弱。DSER旨在克服这一限制，通过概率方法扩展模型的推理边界。

Method: 将迭代推理建模为马尔可夫链，每个步骤代表解空间的随机转移。只要改进概率略高于退化概率，就能保证收敛到正确解。通过并行运行多个自演化过程来放大这些小的正向趋势。

Result: 在DeepSeek-R1-0528-Qwen3-8B模型上应用DSER，在AIME 2024-2025基准测试中解决了9个先前无法解决的难题中的5个，整体性能提升，使这个紧凑模型通过多数投票超越了其600B参数教师的单轮准确率。

Conclusion: DSER不仅提供了测试时扩展的实用价值，还诊断了当前开放权重推理器的根本局限性，为开发具有强大内在自演化能力的下一代模型指明了研究方向。

Abstract: Long-form chain-of-thought reasoning has become a cornerstone of advanced
reasoning in large language models. While recent verification-refinement
frameworks have enabled proprietary models to solve Olympiad-level problems,
their effectiveness hinges on strong, reliable verification and correction
capabilities, which remain fragile in open-weight, smaller-scale models. This
work demonstrates that even with weak verification and refinement capabilities
on hard tasks, the reasoning limits of such models can be substantially
extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning
(DSER). We conceptualize iterative reasoning as a Markov chain, where each step
represents a stochastic transition in the solution space. The key insight is
that convergence to a correct solution is guaranteed as long as the probability
of improvement marginally exceeds that of degradation. By running multiple
long-horizon, self-evolving processes in parallel, DSER amplifies these small
positive tendencies, enabling the model to asymptotically approach correct
answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On
the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously
unsolvable problems and boosts overall performance, enabling this compact model
to surpass the single-turn accuracy of its 600B-parameter teacher through
majority voting. Beyond its immediate utility for test-time scaling, the DSER
framework serves to diagnose the fundamental limitations of current open-weight
reasoners. By clearly delineating their shortcomings in self-verification,
refinement, and stability, our findings establish a clear research agenda for
developing next-generation models with powerful, intrinsic self-evolving
capabilities.

</details>


### [86] [Lingua Custodi's participation at the WMT 2025 Terminology shared task](https://arxiv.org/abs/2510.17504)
*Jingshu Liu,Raheel Qader,Gaëtan Caillaut,Mariam Nakhlé*

Main category: cs.CL

TL;DR: 该论文系统研究了学习多语言句子嵌入的方法，结合了单语和跨语言表示学习的最佳技术，显著减少了并行训练数据需求，在112种语言上实现了83.7%的双文本检索准确率。


<details>
  <summary>Details</summary>
Motivation: 探索BERT在多语言句子嵌入方面的应用，虽然BERT在单语句子嵌入方面表现优异，但其在多语言句子嵌入方面的潜力尚未被充分挖掘。

Method: 结合了掩码语言建模(MLM)、翻译语言建模(TLM)、双编码器翻译排序和加性边际softmax等方法，利用预训练的多语言语言模型。

Result: 将并行训练数据需求减少了80%，在Tatoeba数据集上112种语言的bi-text检索准确率达到83.7%，显著优于LASER的65.5%，同时在单语迁移学习基准上保持竞争力。

Conclusion: 成功开发了高效的多语言句子嵌入模型，在109+种语言上公开可用，并且使用该模型从CommonCrawl挖掘的并行数据能够训练出有竞争力的神经机器翻译模型。

Abstract: While BERT is an effective method for learning monolingual sentence
embeddings for semantic similarity and embedding based transfer learning BERT
based cross-lingual sentence embeddings have yet to be explored. We
systematically investigate methods for learning multilingual sentence
embeddings by combining the best methods for learning monolingual and
cross-lingual representations including: masked language modeling (MLM),
translation language modeling (TLM), dual encoder translation ranking, and
additive margin softmax. We show that introducing a pre-trained multilingual
language model dramatically reduces the amount of parallel training data
required to achieve good performance by 80%. Composing the best of these
methods produces a model that achieves 83.7% bi-text retrieval accuracy over
112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still
performing competitively on monolingual transfer learning benchmarks. Parallel
data mined from CommonCrawl using our best model is shown to train competitive
NMT models for en-zh and en-de. We publicly release our best multilingual
sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.

</details>


### [87] [Annotation-Efficient Universal Honesty Alignment](https://arxiv.org/abs/2510.17509)
*Shiyu Ni,Keping Bi,Jiafeng Guo,Minghao Tang,Jingtong Wu,Zengxin Han,Xueqi Cheng*

Main category: cs.CL

TL;DR: 提出了EliCal框架，通过两阶段方法实现LLM的诚实对齐：先使用廉价的自我一致性监督来激发内部置信度，然后用少量正确性标注进行校准。


<details>
  <summary>Details</summary>
Motivation: 现有诚实对齐方法要么依赖无训练置信度估计，要么需要大规模正确性标注进行基于训练的校准，成本高昂。需要一种标注高效的训练方法。

Method: EliCal两阶段框架：1) 使用自我一致性监督激发内部置信度；2) 用少量正确性标注校准置信度。同时发布了HonestyBench基准数据集。

Result: EliCal仅用1k正确性标注（全监督的0.18%）就实现了接近最优的对齐效果，在未见MMLU任务上比仅校准基线表现更好。

Conclusion: EliCal为LLM的通用诚实对齐提供了一个可扩展的解决方案，显著降低了标注成本。

Abstract: Honesty alignment-the ability of large language models (LLMs) to recognize
their knowledge boundaries and express calibrated confidence-is essential for
trustworthy deployment. Existing methods either rely on training-free
confidence estimation (e.g., token probabilities, self-consistency) or
training-based calibration with correctness annotations. While effective,
achieving universal honesty alignment with training-based calibration requires
costly, large-scale labeling. To support annotation-efficient training, we
introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that
first elicits internal confidence using inexpensive self-consistency
supervision, then calibrates this confidence with a small set of correctness
annotations. To support a large-scale study, we release HonestyBench, a
benchmark covering ten free-form QA datasets with 560k training and 70k
evaluation instances annotated with correctness and self-consistency signals.
Experiments show that EliCal achieves near-optimal alignment with only 1k
correctness annotations (0.18% of full supervision) and better alignment
performance on unseen MMLU tasks than the calibration-only baseline, offering a
scalable solution toward universal honesty alignment in LLMs.

</details>


### [88] [OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction](https://arxiv.org/abs/2510.17532)
*Raghu Vamshi Hemadri,Geetha Krishna Guruju,Kristi Topollai,Anna Ewa Choromanska*

Main category: cs.CL

TL;DR: 提出一个统一的多任务学习框架，将自回归LLM与临床推理对齐，用于癌症治疗结果预测。通过三种对齐策略（标准微调、思维链提示、GRPO强化学习）提升预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在生物医学NLP中表现良好，但缺乏结构化推理能力，无法满足高风险临床决策支持的需求。

Method: 使用多任务学习框架，联合执行二元生存分类、连续生存时间回归和自然语言理由生成。评估三种对齐策略：标准监督微调、思维链提示和GRPO强化学习。

Result: 思维链提示将F1提高6.0%，MAE降低12%；GRPO在BLEU、ROUGE和BERTScore上实现了最先进的可解释性和预测性能。

Conclusion: 推理感知对齐在多任务临床建模中至关重要，为精准肿瘤学中可解释、可信赖的LLM设定了新基准。

Abstract: Predicting cancer treatment outcomes requires models that are both accurate
and interpretable, particularly in the presence of heterogeneous clinical data.
While large language models (LLMs) have shown strong performance in biomedical
NLP, they often lack structured reasoning capabilities critical for high-stakes
decision support. We present a unified, multi-task learning framework that
aligns autoregressive LLMs with clinical reasoning for outcome prediction on
the MSK-CHORD dataset. Our models are trained to jointly perform binary
survival classification, continuous survival time regression, and natural
language rationale generation. We evaluate three alignment strategies: (1)
standard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)
prompting to elicit step-by-step reasoning, and (3) Group Relative Policy
Optimization (GRPO), a reinforcement learning method that aligns model outputs
to expert-derived reasoning trajectories. Experiments with LLaMa3-8B and
Med42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and
reduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and
predictive performance across BLEU, ROUGE, and BERTScore. We further show that
existing biomedical LLMs often fail to produce valid reasoning traces due to
architectural constraints. Our findings underscore the importance of
reasoning-aware alignment in multi-task clinical modeling and set a new
benchmark for interpretable, trustworthy LLMs in precision oncology.

</details>


### [89] [When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity](https://arxiv.org/abs/2510.17548)
*Nisrine Rair,Alban Goupil,Valeriu Vrabie,Emmanuel Chochoy*

Main category: cs.CL

TL;DR: 该论文提出使用拓扑数据分析工具Mapper来研究语言模型如何内部表示歧义，特别是在人类标注者存在分歧的情况下。研究发现微调后的模型在嵌入空间中形成模块化、非凸的区域，这些区域与模型预测高度一致，但在歧义数据中与真实标签的对齐度下降。


<details>
  <summary>Details</summary>
Motivation: 传统的标量评估指标（如准确率）无法捕捉模型内部如何表示歧义，特别是在人类标注者存在分歧的情况下。需要新的方法来理解模型如何处理主观性任务中的不确定性。

Method: 使用拓扑数据分析工具Mapper来分析微调后RoBERTa-Large模型在MD-Offense数据集上的嵌入空间结构。与PCA和UMAP等传统可视化工具不同，Mapper能够直接揭示决策区域、边界塌陷和过度自信的聚类。

Result: 研究发现微调将嵌入空间重组为模块化、非凸的区域，这些区域与模型预测高度一致（超过98%的连通组件预测纯度≥90%）。但在歧义数据中，模型结构与真实标签的对齐度下降，揭示了结构置信度与标签不确定性之间的隐藏张力。

Conclusion: Mapper是一个强大的诊断工具，可用于理解模型如何解决歧义。除了可视化功能外，它还支持拓扑度量，可能为NLP主观任务中的主动建模策略提供信息。

Abstract: Language models are often evaluated with scalar metrics like accuracy, but
such measures fail to capture how models internally represent ambiguity,
especially when human annotators disagree. We propose a topological perspective
to analyze how fine-tuned models encode ambiguity and more generally instances.
  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from
topological data analysis, reveals that fine-tuning restructures embedding
space into modular, non-convex regions aligned with model predictions, even for
highly ambiguous cases. Over $98\%$ of connected components exhibit $\geq 90\%$
prediction purity, yet alignment with ground-truth labels drops in ambiguous
data, surfacing a hidden tension between structural confidence and label
uncertainty.
  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry
directly uncovering decision regions, boundary collapses, and overconfident
clusters. Our findings position Mapper as a powerful diagnostic tool for
understanding how models resolve ambiguity. Beyond visualization, it also
enables topological metrics that may inform proactive modeling strategies in
subjective NLP tasks.

</details>


### [90] [Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation](https://arxiv.org/abs/2510.17555)
*Collin Zhang,Fei Huang,Chenhan Yuan,Junyang Lin*

Main category: cs.CL

TL;DR: 提出语言混淆门（LCG），一种轻量级插件解决方案，在解码过程中过滤令牌而无需修改基础大语言模型，显著减少语言混淆现象


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常出现语言混淆问题，即文本生成时无意中混合多种语言。现有解决方案要么需要重新训练模型，要么无法区分有害混淆和可接受的语码转换

Method: 使用规范调整的自蒸馏训练LCG来预测适当的语言家族，仅在需要时应用掩码。该方法基于三个发现：语言混淆不频繁、正确语言令牌通常在前几个预测中、高资源语言的输出令牌嵌入规范更大从而产生采样偏差

Result: 在包括Qwen3、GPT-OSS、Gemma3、Llama3.1等各种模型上的评估显示，LCG显著减少了语言混淆，通常降低一个数量级，且不影响任务性能

Conclusion: LCG是一种有效的轻量级解决方案，能够显著减少大语言模型的语言混淆问题，同时保持任务性能

Abstract: Large language models (LLMs) often experience language confusion, which is
the unintended mixing of languages during text generation. Current solutions to
this problem either necessitate model retraining or cannot differentiate
between harmful confusion and acceptable code-switching. This paper introduces
the Language Confusion Gate (LCG), a lightweight, plug-in solution that filters
tokens during decoding without altering the base LLM. The LCG is trained using
norm-adjusted self-distillation to predict appropriate language families and
apply masking only when needed. Our method is based on the findings that
language confusion is infrequent, correct-language tokens are usually among the
top predictions, and output token embedding norms are larger for high-resource
languages, which biases sampling. When evaluated across various models,
including Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion
significantly, often by an order of magnitude, without negatively impacting
task performance. Code is available at
https://github.com/collinzrj/language_confusion_gate.

</details>


### [91] [HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection](https://arxiv.org/abs/2510.17591)
*Guang Yang,Yujie Zhu*

Main category: cs.CL

TL;DR: 提出了HGAdapter方法，通过超图神经网络捕捉代码中的高阶数据相关性，增强预训练语言模型在代码相关任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练语言模型在代码任务中表现良好，但未考虑代码内部潜在的高阶数据相关性，限制了性能提升。

Method: 识别代码中的三种高阶相关性（抽象语法树家族相关性、词汇相关性、行相关性），设计token和超边生成器，改进超图神经网络架构并结合适配器调优，提出HGAdapter方法。

Result: 在多个公共数据集上的代码摘要和代码克隆检测任务中，HGAdapter不同程度地提升了PLMs的性能，验证了高阶数据相关性的有效性。

Conclusion: 引入高阶数据相关性有助于提升代码相关任务的性能，HGAdapter方法可灵活插入各种PLMs中增强其表现。

Abstract: Pre-trained language models (PLMs) are increasingly being applied to
code-related tasks. Although PLMs have achieved good results, they do not take
into account potential high-order data correlations within the code. We propose
three types of high-order correlations in code tokens, i.e. abstract syntax
tree family correlation, lexical correlation, and line correlation. We design a
tokens and hyperedges generator to capture these high-order data correlations.
We improve the architecture of hypergraph neural networks and combine it with
adapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to
fine-tune PLMs. HGAdapter can encode high-order data correlations and is
allowed to be inserted into various PLMs to enhance performance. Experiments
were conducted on several public datasets, including six languages of code
summarization and code clone detection tasks. Our methods improved the
performance of PLMs in datasets to varying degrees. Experimental results
validate the introduction of high-order data correlations that contribute to
improved effectiveness.

</details>


### [92] [LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis](https://arxiv.org/abs/2510.17602)
*Huiyuan Xie,Chenyang Li,Huining Zhu,Chubin Zhang,Yuxiao Ye,Zhenghao Liu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 提出了LawChain框架来建模中国侵权民事案件的法律推理过程，构建了评估基准LawChain_eval，并通过提示或后训练方法显著提升了语言模型的法律推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有法律推理计算方法主要依赖通用推理框架，未能全面考察法律推理的细微过程，且多集中于刑事案件，对民事案件建模不足。

Method: 将侵权分析中的法律推理过程操作化为LawChain框架，包含三个模块，每个模块由多个细粒度子步骤组成，并构建评估基准来系统评估推理链中的关键步骤。

Result: 当前最先进的大语言模型在侵权法律推理的关键要素处理上仍存在不足，但基于LawChain框架的基线方法在侵权相关法律推理和相关法律分析任务上取得了显著改进。

Conclusion: 明确建模法律推理链能够有效增强语言模型的推理能力，LawChain框架在提升法律推理性能方面具有重要价值。

Abstract: Legal reasoning is a fundamental component of legal analysis and
decision-making. Existing computational approaches to legal reasoning
predominantly rely on generic reasoning frameworks such as syllogism and IRAC,
which do not comprehensively examine the nuanced processes that underpin legal
reasoning. Moreover, current research has largely focused on criminal cases,
with insufficient modeling for civil cases. In this work, we present a novel
framework for explicitly modeling legal reasoning in the analysis of Chinese
tort-related civil cases. We first operationalize the legal reasoning processes
used in tort analysis into the LawChain framework. LawChain is a three-module
reasoning framework, with each module consisting of multiple finer-grained
sub-steps. Informed by the LawChain framework, we introduce the task of tort
legal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to
systematically assess the critical steps within analytical reasoning chains for
tort analysis. Leveraging this benchmark, we evaluate state-of-the-art large
language models for their legal reasoning ability in civil tort contexts. Our
results indicate that current models still fall short in accurately handling
crucial elements of tort legal reasoning. Furthermore, we introduce several
baseline approaches that explicitly incorporate LawChain-style reasoning
through prompting or post-training. We conduct further experiments on
additional legal analysis tasks, such as Legal Named-Entity Recognition and
Criminal Damages Calculation, to verify the generalizability of these
baselines. The proposed baseline approaches achieve significant improvements in
tort-related legal reasoning and generalize well to related legal analysis
tasks, thus demonstrating the value of explicitly modeling legal reasoning
chains to enhance the reasoning capabilities of language models.

</details>


### [93] [Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2510.17620)
*Yuefeng Peng,Parnian Afshar,Megan Ganji,Thomas Butler,Amir Houmansadr,Mingxian Wang,Dezhi Hong*

Main category: cs.CL

TL;DR: 论文提出在机器学习遗忘方法中增加上下文效用保护机制，通过添加插件项来保持模型在提示中包含已遗忘知识时仍能使用这些信息。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法评估主要关注目标知识的遗忘程度和保留集性能，但忽视了当已删除知识重新出现在提示中时模型仍应能利用这些信息的重要实用性需求。

Method: 在遗忘目标中增加插件项，专门保护模型在上下文包含已遗忘知识时的使用能力，同时保持有效的遗忘效果和保留集效用。

Result: 实验表明该方法能将上下文效用恢复到接近原始水平，同时维持有效的遗忘效果和保留集效用。

Conclusion: 提出的增强遗忘方法成功解决了现有方法损害上下文效用的问题，实现了遗忘、保留集效用和上下文效用的平衡。

Abstract: Large language models may encode sensitive information or outdated knowledge
that needs to be removed, to ensure responsible and compliant model responses.
Unlearning has emerged as an efficient alternative to full retraining, aiming
to remove specific knowledge while preserving overall model utility. Existing
evaluations of unlearning methods focus on (1) the extent of forgetting of the
target knowledge (forget set) and (2) maintaining performance on the retain set
(i.e., utility). However, these evaluations overlook an important usability
aspect: users may still want the model to leverage the removed information if
it is re-introduced in the prompt. In a systematic evaluation of six
state-of-the-art unlearning methods, we find that they consistently impair such
contextual utility. To address this, we augment unlearning objectives with a
plug-in term that preserves the model's ability to use forgotten knowledge when
it is present in context. Extensive experiments demonstrate that our approach
restores contextual utility to near original levels while still maintaining
effective forgetting and retain-set utility.

</details>


### [94] [Qomhra: A Bilingual Irish-English Large Language Model](https://arxiv.org/abs/2510.17652)
*Joseph McInerney*

Main category: cs.CL

TL;DR: Qomhr'a是一个在低资源条件下开发的双语爱尔兰语-英语大语言模型，通过双语持续预训练、指令微调和人类偏好对齐的完整流程构建。


<details>
  <summary>Details</summary>
Motivation: 在低资源环境下开发爱尔兰语能力，同时保持英语能力，填补爱尔兰语LLM的空白。

Method: 混合爱尔兰语和英语语料进行双语预训练，使用Gemini-2.5-Pro合成指令微调和人类偏好数据集，并进行全面的基准测试。

Result: 在翻译、性别理解、主题识别和世界知识等基准测试中，爱尔兰语性能提升29%，英语性能提升44%。

Conclusion: Qomhr'a模型在双语能力和指令跟随方面取得显著进展，为爱尔兰语自然语言处理提供了重要资源。

Abstract: This paper introduces Qomhr\'a, a bilingual Irish-English large language
model (LLM), developed under low-resource constraints presenting a complete
pipeline spanning bilingual continued pre-training, instruction tuning, and
alignment from human preferences. Newly accessible Irish corpora and English
text are mixed and curated to improve Irish performance while preserving
English ability. 6 closed-weight LLMs are judged for their Irish text
generation by a native speaker, a learner and other LLMs. Google's
Gemini-2.5-Pro is ranked the highest and is subsequently used to synthesise
instruction tuning and human preference datasets. Two datasets are contributed
leveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning
dataset and a 1K human preference dataset, generating accepted and rejected
responses that show near perfect alignment with a native Irish speaker.
Qomhr\'a is comprehensively evaluated across benchmarks testing translation,
gender understanding, topic identification and world knowledge with gains of up
to 29% in Irish and 44% in English. Qomhr\'a also undergoes instruction tuning
and demonstrates clear progress in instruction following, crucial for chatbot
functionality.

</details>


### [95] [Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues](https://arxiv.org/abs/2510.17698)
*Liqun He,Manolis Mavrikis,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本研究采用对话分析方法从学习者-LLM对话中识别有效的教学策略，包括对话数据收集、对话行为标注、模式挖掘和预测模型构建。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型教育应用评估方法主要关注技术性能或学习成果，往往忽视了学习者与LLM之间的互动分析。

Method: 采用对话分析方法，包括四个步骤：对话数据收集、对话行为标注、对话行为模式挖掘和预测模型构建。

Result: 提出了初步的研究框架和早期见解，为未来研究奠定基础。

Conclusion: 强调需要通过关注对话动态和教学策略来评估基于LLM的教育应用。

Abstract: Dialogue plays a crucial role in educational settings, yet existing
evaluation methods for educational applications of large language models (LLMs)
primarily focus on technical performance or learning outcomes, often neglecting
attention to learner-LLM interactions. To narrow this gap, this AIED Doctoral
Consortium paper presents an ongoing study employing a dialogue analysis
approach to identify effective pedagogical strategies from learner-LLM
dialogues. The proposed approach involves dialogue data collection, dialogue
act (DA) annotation, DA pattern mining, and predictive model building. Early
insights are outlined as an initial step toward future research. The work
underscores the need to evaluate LLM-based educational applications by focusing
on dialogue dynamics and pedagogical strategies.

</details>


### [96] [QueST: Incentivizing LLMs to Generate Difficult Problems](https://arxiv.org/abs/2510.17715)
*Hanxu Hu,Xingxing Zhang,Jannis Vamvas,Rico Sennrich,Furu Wei*

Main category: cs.CL

TL;DR: QueST框架通过难度感知图采样和拒绝微调生成具有挑战性的编程问题，显著提升了语言模型在竞争性编程任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有竞争性编程数据集规模有限（仅数千到数万问题），且依赖人工标注数据，限制了语言模型的扩展性。需要大规模、具有挑战性的编程问题训练数据。

Method: 结合难度感知图采样和难度感知拒绝微调，训练专门的生成器来创建具有挑战性的编程问题，然后通过蒸馏或强化学习训练下游模型。

Result: 使用QueST生成的10万个困难问题微调Qwen3-8B-base后，在LiveCodeBench上超越了原始Qwen3-8B的性能。添加额外11.2万示例后，8B模型性能与671B的DeepSeek-R1相当。

Conclusion: 通过QueST生成复杂问题是推进语言模型在竞争性编程和推理能力前沿的有效且可扩展的方法。

Abstract: Large Language Models have achieved strong performance on reasoning tasks,
solving competition-level coding and math problems. However, their scalability
is limited by human-labeled datasets and the lack of large-scale, challenging
coding problem training data. Existing competitive coding datasets contain only
thousands to tens of thousands of problems. Previous synthetic data generation
methods rely on either augmenting existing instruction datasets or selecting
challenging problems from human-labeled data. In this paper, we propose QueST,
a novel framework which combines difficulty-aware graph sampling and
difficulty-aware rejection fine-tuning that directly optimizes specialized
generators to create challenging coding problems. Our trained generators
demonstrate superior capability compared to even GPT-4o at creating challenging
problems that benefit downstream performance. We leverage QueST to generate
large-scale synthetic coding problems, which we then use to distill from strong
teacher models with long chain-of-thought or to conduct reinforcement learning
for smaller models, proving effective in both scenarios. Our distillation
experiments demonstrate significant performance gains. Specifically, after
fine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we
surpass the performance of the original Qwen3-8B on LiveCodeBench. With an
additional 112K examples (i.e., 28K human-written problems paired with multiple
synthetic solutions), our 8B model matches the performance of the much larger
DeepSeek-R1-671B. These findings indicate that generating complex problems via
QueST offers an effective and scalable approach to advancing the frontiers of
competitive coding and reasoning for large language models.

</details>


### [97] [PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition](https://arxiv.org/abs/2510.17720)
*Nanda Kumar Rengarajan,Jun Yan,Chun Wang*

Main category: cs.CL

TL;DR: 提出了一种轻量级少样本命名实体识别框架，通过改进的指令调优模板和数据增强技术，在低资源场景下取得了与最先进模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 命名实体识别需要大量标注数据，但在低资源场景中标注成本高昂。现有的零样本和指令调优方法难以泛化到领域特定实体，且无法有效利用有限数据。

Method: 1) 新的指令调优模板，结合先前IT方法的原理，利用最新LLM的大上下文窗口；2) 战略性数据增强技术，在保持实体信息的同时对上下文进行改写，扩展训练数据而不损害语义关系。

Result: 在基准数据集上，少样本方法在CrossNER数据集上平均F1得分为80.1。使用改写方法训练的模型比基线版本F1分数提升高达17分。

Conclusion: 该方法为拥有有限NER训练数据和计算资源的群体提供了一个有前景的解决方案。

Abstract: Named Entity Recognition (NER) is a critical task that requires substantial
annotated data, making it challenging in low-resource scenarios where label
acquisition is expensive. While zero-shot and instruction-tuned approaches have
made progress, they often fail to generalize to domain-specific entities and do
not effectively utilize limited available data. We present a lightweight
few-shot NER framework that addresses these challenges through two key
innovations: (1) a new instruction tuning template with a simplified output
format that combines principles from prior IT approaches to leverage the large
context window of recent state-of-the-art LLMs; (2) introducing a strategic
data augmentation technique that preserves entity information while
paraphrasing the surrounding context, thereby expanding our training data
without compromising semantic relationships. Experiments on benchmark datasets
show that our method achieves performance comparable to state-of-the-art models
on few-shot and zero-shot tasks, with our few-shot approach attaining an
average F1 score of 80.1 on the CrossNER datasets. Models trained with our
paraphrasing approach show consistent improvements in F1 scores of up to 17
points over baseline versions, offering a promising solution for groups with
limited NER training data and compute power.

</details>


### [98] [AcademicEval: Live Long-Context LLM Benchmark](https://arxiv.org/abs/2510.17725)
*Haozhen Zhang,Tao Feng,Pengrui Han,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出了AcademicEval，一个基于arXiv论文的长上下文生成任务基准，包含标题、摘要、引言和相关工作四个任务，无需人工标注且能防止标签泄露。


<details>
  <summary>Details</summary>
Motivation: 当前长上下文LLM基准存在上下文长度固定、标注劳动密集、训练时标签泄露等问题，需要新的评估方法。

Method: 使用arXiv论文构建学术写作任务，集成高质量专家策划的少样本示例，支持灵活上下文长度，采用实时评估防止标签泄露。

Result: LLMs在具有层次抽象级别的任务上表现不佳，且难以处理长少样本演示，凸显了基准的挑战性。

Conclusion: AcademicEval为评估LLMs的长上下文建模能力提供了有效基准，并揭示了增强这些能力的见解。

Abstract: Large Language Models (LLMs) have recently achieved remarkable performance in
long-context understanding. However, current long-context LLM benchmarks are
limited by rigid context length, labor-intensive annotation, and the pressing
challenge of label leakage issues during LLM training. Therefore, we propose
\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context
generation tasks. \textsc{AcademicEval} adopts papers on arXiv to introduce
several academic writing tasks with long-context inputs, \textit{i.e.},
\textsc{Title}, \textsc{Abstract}, \textsc{Introduction}, and \textsc{Related
Work}, which cover a wide range of abstraction levels and require no manual
labeling. Moreover, \textsc{AcademicEval} integrates high-quality and
expert-curated few-shot demonstrations from a collected co-author graph to
enable flexible context length. Especially, \textsc{AcademicEval} features an
efficient live evaluation, ensuring no label leakage. We conduct a holistic
evaluation on \textsc{AcademicEval}, and the results illustrate that LLMs
perform poorly on tasks with hierarchical abstraction levels and tend to
struggle with long few-shot demonstrations, highlighting the challenge of our
benchmark. Through experimental analysis, we also reveal some insights for
enhancing LLMs' long-context modeling capabilities. Code is available at
https://github.com/ulab-uiuc/AcademicEval

</details>


### [99] [Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations](https://arxiv.org/abs/2510.17733)
*Tong Chen,Akari Asai,Luke Zettlemoyer,Hannaneh Hajishirzi,Faeze Brahman*

Main category: cs.CL

TL;DR: 提出了一种基于二元检索增强奖励的在线强化学习方法，有效减少语言模型的外源性幻觉，在保持其他任务性能的同时显著提升事实准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在减少语言模型外源性幻觉的同时，往往会降低开放生成和下游任务的性能，限制了实际应用价值。

Method: 使用二元检索增强奖励的在线强化学习方法，只有当模型输出完全正确时才给予奖励1，否则为0。

Result: 在开放生成任务中幻觉率降低39.3%；在短问答任务中，模型学会校准性弃权，在PopQA和GPQA上分别减少44.4%和21.7%的错误答案，且不影响指令遵循、数学和代码能力。

Conclusion: 二元奖励方案在提升事实准确性的同时避免了性能退化，相比连续奖励强化学习具有更好的实用性。

Abstract: Language models often generate factually incorrect information unsupported by
their training data, a phenomenon known as extrinsic hallucination. Existing
mitigation approaches often degrade performance on open-ended generation and
downstream tasks, limiting their practical utility. We propose an online
reinforcement learning method using a novel binary retrieval-augmented reward
(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach
assigns a reward of one only when the model's output is entirely factually
correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models
across diverse tasks. For open-ended generation, binary RAR achieves a 39.3%
reduction in hallucination rates, substantially outperforming both supervised
training and continuous-reward RL baselines. In short-form question answering,
the model learns calibrated abstention, strategically outputting "I don't know"
when faced with insufficient parametric knowledge. This yields 44.4% and 21.7%
fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these
factuality gains come without performance degradation on instruction following,
math, or code, whereas continuous-reward RL, despite improving factuality,
induces quality regressions.

</details>


### [100] [Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications](https://arxiv.org/abs/2510.17764)
*Xiao Ye,Jacob Dineen,Zhaonan Li,Zhikun Xu,Weiyu Chen,Shijie Lu,Yuxi Huang,Ming Shen,Phu Tran,Ji-Eun Irene Yum,Muhammad Ali Khan,Muhammad Umar Afzal,Irbaz Bin Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: 该调查通过自主性层级框架(L0-L3)重新构建医学大语言模型评估，将现有基准与各层级允许的操作和风险对齐，提出基于层级的评估蓝图，推动从分数导向转向可信的临床使用证据。


<details>
  <summary>Details</summary>
Motivation: 医学大语言模型在标准基准上表现优异，但将这些结果转化为临床工作流程中安全可靠的性能仍具挑战。需要重新构建评估框架以连接基准测试与实际临床使用。

Method: 采用自主性层级框架(L0-L3)：信息工具、信息转换与聚合、决策支持、监督代理。将现有基准和指标与各层级的允许操作及风险对齐，建立明确的评估目标。

Result: 提出了基于层级的评估蓝图，用于选择指标、收集证据和报告声明，并将评估与监督联系起来。

Conclusion: 通过以自主性为中心，该调查推动领域超越基于分数的声明，转向为真实临床使用提供可信、风险感知的证据。

Abstract: Medical Large language models achieve strong scores on standard benchmarks;
however, the transfer of those results to safe and reliable performance in
clinical workflows remains a challenge. This survey reframes evaluation through
a levels-of-autonomy lens (L0-L3), spanning informational tools, information
transformation and aggregation, decision support, and supervised agents. We
align existing benchmarks and metrics with the actions permitted at each level
and their associated risks, making the evaluation targets explicit. This
motivates a level-conditioned blueprint for selecting metrics, assembling
evidence, and reporting claims, alongside directions that link evaluation to
oversight. By centering autonomy, the survey moves the field beyond score-based
claims toward credible, risk-aware evidence for real clinical use.

</details>


### [101] [Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains](https://arxiv.org/abs/2510.17793)
*Austin Xu,Xuan-Phi Nguyen,Yilun Zhou,Chien-Sheng Wu,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 该论文提出了FARE（基础自动推理评估器），通过大规模数据驱动的方法训练了8B和20B参数的评估器，在多个评估任务中超越了更大的专业评估器，并在实际应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前生成式评估器的研究主要关注新方法如强化学习，而忽视了大规模数据驱动的发展。本文旨在通过数据扩展来解决这一问题。

Method: 收集了250万个样本，涵盖5种评估任务，使用简单的迭代拒绝采样监督微调方法训练了8B和20B参数的FARE评估器。

Result: FARE-8B挑战了更大的专业RL训练评估器，FARE-20B在开源评估器中设定了新标准，超越了专门的70B+评估器。在实际应用中，作为推理时重排器在MATH上达到接近oracle性能，作为RL训练中的验证器将下游模型性能提升14.1%。

Conclusion: 大规模数据驱动的简单方法可以训练出高性能的评估器，FARE在多个任务中表现出色，证明了数据扩展的重要性。

Abstract: Finetuning specialized generative evaluators has emerged as a popular
paradigm to meet the increasing demand for scalable evaluation during both
training and test-time. However, recent work has largely focused on applying
new methodology, such as reinforcement learning (RL), to training evaluators,
shying away from large-scale, data-driven development. In this work, we focus
on data scaling, curating a set of 2.5M samples spanning five unique evaluation
tasks (pairwise, step-level, reference-free and reference-based verification,
and single rating) and multiple domains focused on reasoning evaluation. With
our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family
of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative
rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges
larger specialized RL-trained evaluators and FARE-20B sets the new standard for
open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static
benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,
FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,
FARE improves the downstream RL-trained model performance by up to 14.1% vs.
string-matching verifiers. When initialized from FARE, a continually-finetuned
FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.

</details>


### [102] [Executable Knowledge Graphs for Replicating AI Research](https://arxiv.org/abs/2510.17795)
*Yujie Luo,Zhuoyun Yu,Xuehai Wang,Yuqi Zhu,Ningyu Zhang,Lanning Wei,Lun Du,Da Zheng,Huajun Chen*

Main category: cs.CL

TL;DR: 提出了可执行知识图谱(xKG)，通过整合技术洞察、代码片段和领域知识来解决AI研究复现的挑战，在三个代理框架中实现了显著性能提升


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成可执行代码方面存在困难，主要由于背景知识不足和检索增强生成方法的局限性，无法捕捉参考文献中的潜在技术细节

Method: 构建可执行知识图谱(xKG)，自动从科学文献中提取技术洞察、代码片段和领域特定知识，提供模块化、可插拔的知识库

Result: 在三个代理框架和两种不同LLM上测试，xKG在PaperBench上实现了显著性能提升(10.9% with o3-mini)

Conclusion: xKG是自动化AI研究复现的通用且可扩展解决方案，有效解决了现有方法的局限性

Abstract: Replicating AI research is a crucial yet challenging task for large language
model (LLM) agents. Existing approaches often struggle to generate executable
code, primarily due to insufficient background knowledge and the limitations of
retrieval-augmented generation (RAG) methods, which fail to capture latent
technical details hidden in referenced papers. Furthermore, previous approaches
tend to overlook valuable implementation-level code signals and lack structured
knowledge representations that support multi-granular retrieval and reuse. To
overcome these challenges, we propose Executable Knowledge Graphs (xKG), a
modular and pluggable knowledge base that automatically integrates technical
insights, code snippets, and domain-specific knowledge extracted from
scientific literature. When integrated into three agent frameworks with two
different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on
PaperBench, demonstrating its effectiveness as a general and extensible
solution for automated AI research replication. Code will released at
https://github.com/zjunlp/xKG.

</details>


### [103] [Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics](https://arxiv.org/abs/2510.17797)
*Akshara Prabhakar,Roshan Ram,Zixiang Chen,Silvio Savarese,Frank Wang,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: EDR是一个多智能体系统，通过主规划智能体和四个专业搜索智能体，结合可扩展工具生态系统和可视化智能体，实现企业深度研究自动化，在开放基准测试中优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 企业面临将非结构化数据转化为可操作洞察的压力，现有自主智能体在领域特定细微差别、意图对齐和企业集成方面存在困难。

Method: 采用多智能体系统架构，包括主规划智能体、四个专业搜索智能体（通用、学术、GitHub、LinkedIn）、基于MCP的可扩展工具生态系统、可视化智能体和反思机制。

Result: 在DeepResearch Bench和DeepConsult等开放基准测试中，EDR无需人工干预就优于最先进的智能体系统。

Conclusion: EDR框架和基准轨迹的发布将推动多智能体推理应用的研究发展。

Abstract: As information grows exponentially, enterprises face increasing pressure to
transform unstructured data into coherent, actionable insights. While
autonomous agents show promise, they often struggle with domain-specific
nuances, intent alignment, and enterprise integration. We present Enterprise
Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning
Agent for adaptive query decomposition, (2) four specialized search agents
(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool
ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a
Visualization Agent for data-driven insights, and (5) a reflection mechanism
that detects knowledge gaps and updates research direction with optional
human-in-the-loop steering guidance. These components enable automated report
generation, real-time streaming, and seamless enterprise deployment, as
validated on internal datasets. On open-ended benchmarks including DeepResearch
Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without
any human steering. We release the EDR framework and benchmark trajectories to
advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and
Dataset at https://huggingface.co/datasets/Salesforce/EDR-200

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [104] [ATLAS: Adaptive Trading with LLM AgentS Through Dynamic Prompt Optimization and Multi-Agent Coordination](https://arxiv.org/abs/2510.15949)
*Charidimos Papadakis,Angeliki Dimitriou,Giorgos Filandrianos,Maria Lymperaiou,Konstantinos Thomas,Giorgos Stamou*

Main category: q-fin.TR

TL;DR: ATLAS是一个基于大语言模型的多智能体交易框架，通过Adaptive-OPRO技术动态优化提示，在延迟且噪声的市场奖励下实现自主交易决策。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在金融交易中面临的挑战：延迟奖励、市场噪声、异构信息整合以及模型输出与可执行市场行为之间的差距。

Method: 提出ATLAS多智能体框架，整合市场、新闻和基本面信息，采用Adaptive-OPRO提示优化技术动态调整提示，在订单感知动作空间中生成可执行交易指令。

Result: 在不同市场机制和多种LLM家族中，Adaptive-OPRO始终优于固定提示方法，而基于反思的反馈未能提供系统性改进。

Conclusion: ATLAS框架通过自适应提示优化成功解决了LLM在金融交易中的核心挑战，为自主交易代理提供了有效的解决方案。

Abstract: Large language models show promise for financial decision-making, yet
deploying them as autonomous trading agents raises fundamental challenges: how
to adapt instructions when rewards arrive late and obscured by market noise,
how to synthesize heterogeneous information streams into coherent decisions,
and how to bridge the gap between model outputs and executable market actions.
We present ATLAS (Adaptive Trading with LLM AgentS), a unified multi-agent
framework that integrates structured information from markets, news, and
corporate fundamentals to support robust trading decisions. Within ATLAS, the
central trading agent operates in an order-aware action space, ensuring that
outputs correspond to executable market orders rather than abstract signals.
The agent can incorporate feedback while trading using Adaptive-OPRO, a novel
prompt-optimization technique that dynamically adapts the prompt by
incorporating real-time, stochastic feedback, leading to increasing performance
over time. Across regime-specific equity studies and multiple LLM families,
Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based
feedback fails to provide systematic gains.

</details>


### [105] [On Bellman equation in the limit order optimization problem for high-frequency trading](https://arxiv.org/abs/2510.15988)
*M. I. Balakaeva,A. Yu. Veretennikov*

Main category: q-fin.TR

TL;DR: 本文修正了Avellaneda & Stoikov (2008)论文中的一些重要缺陷，但意外发现这些修正并未改变原论文的主要结论。


<details>
  <summary>Details</summary>
Motivation: 发现Avellaneda & Stoikov (2008)在高频交易限价订单簿最优策略构建方法中存在看似严重的缺陷，需要仔细修正。

Method: 对原论文中的近似方法进行修正，填补发现的缺陷，并分析修正对最终结果的影响。

Result: 修正后的方法虽然填补了原论文的缺陷，但出人意料地没有改变主要答案和结论。

Conclusion: 原论文中的缺陷实际上并不重要，修正后仍得到相同的主要结果，这提供了一个有趣的研究现象解释。

Abstract: An approximation method for construction of optimal strategies in the bid \&
ask limit order book in the high-frequency trading (HFT) is studied. The basis
is the article by M. Avellaneda \& S. Stoikov 2008, in which certain seemingly
serious gaps have been found; in the present paper they are carefully
corrected. However, a bit surprisingly, our corrections do not change the main
answer in the cited paper, so that, in fact, the gaps turn out to be
unimportant. An explanation of this effect is offered.

</details>


### [106] [The Invisible Handshake: Tacit Collusion between Adaptive Market Agents](https://arxiv.org/abs/2510.15995)
*Luigi Foscari,Emanuele Guidotti,Nicolò Cesa-Bianchi,Tatjana Chavdarova,Alfio Ferrara*

Main category: q-fin.TR

TL;DR: 研究自适应交易代理在随机市场中如何通过简单学习算法自然形成隐性合谋，导致价格高于竞争水平。


<details>
  <summary>Details</summary>
Motivation: 探讨AI驱动市场中，当交易代理使用简单学习策略时，是否会自发形成隐性合谋，从而影响市场价格形成机制。

Method: 使用双玩家重复博弈模型（做市商和市价接受者），分析代理通过梯度上升等简单学习算法最大化自身财富时的策略动态。

Result: 研究发现，即使在高流动性市场和小额交易规模下，代理的学习动态也会收敛到合谋策略配置，导致价格超出竞争水平。

Conclusion: 简单的学习策略会自然导致隐性合谋，这为理解AI驱动市场的动态提供了新视角，揭示了算法交易可能带来的市场效率问题。

Abstract: We study the emergence of tacit collusion between adaptive trading agents in
a stochastic market with endogenous price formation. Using a two-player
repeated game between a market maker and a market taker, we characterize
feasible and collusive strategy profiles that raise prices beyond competitive
levels. We show that, when agents follow simple learning algorithms (e.g.,
gradient ascent) to maximize their own wealth, the resulting dynamics converge
to collusive strategy profiles, even in highly liquid markets with small trade
sizes. By highlighting how simple learning strategies naturally lead to tacit
collusion, our results offer new insights into the dynamics of AI-driven
markets.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [107] [Probability equivalent level for CoVaR and VaR in bivariate Student-\textit{t} copulas with application to foreign exchange risk monitoring](https://arxiv.org/abs/2510.15934)
*Daniela I. Flores-Silva,Miguel A. Sordo,Alfonso Suárez-Llorens*

Main category: q-fin.RM

TL;DR: 将PELCoV方法扩展到学生t copula建模的双变量风险，放松了早期方法的强依赖性假设，增强了捕捉尾部依赖和不对称共变的能力，并在外汇市场应用中展示了其在金融压力期间检测风险低估早期迹象的潜力。


<details>
  <summary>Details</summary>
Motivation: 早期PELCoV方法存在强依赖性假设的限制，无法充分捕捉尾部依赖和不对称共变，需要扩展框架以更准确地建模双变量风险。

Method: 将PELCoV方法扩展到学生t copula建模的双变量风险，在静态设定下发展理论结果，并动态实施以跟踪随时间演化的风险溢出。

Result: 在外汇市场应用中，使用USD/EUR系列作为辅助预警指标监测USD/GBP汇率（1999-2024年），扩展的PELCoV框架能够检测金融压力期间风险低估的早期迹象。

Conclusion: 扩展的PELCoV框架通过放松依赖性假设和增强尾部依赖捕捉能力，在风险监测中具有实际应用价值，特别是在检测金融压力期间的风险低估方面。

Abstract: We extend the "probability-equivalent level of VaR and CoVaR" (PELCoV)
methodology to accommodate bivariate risks modeled by a Student-t copula,
relaxing the strong dependence assumptions of earlier approaches and enhancing
the framework's ability to capture tail dependence and asymmetric co-movements.
While the theoretical results are developed in a static setting, we implement
them dynamically to track evolving risk spillovers over time. We illustrate the
practical relevance of our approach through an application to the foreign
exchange market, monitoring the USD/GBP exchange rate with the USD/EUR series
as an auxiliary early warning indicator over the period 1999-2024. Our results
highlight the potential of the extended PELCoV framework to detect early signs
of risk underestimation during periods of financial stress.

</details>


### [108] [Tail-Safe Stochastic-Control SPX-VIX Hedging: A White-Box Bridge Between AI Sensitivities and Arbitrage-Free Market Dynamics](https://arxiv.org/abs/2510.15937)
*Jian'an Zhang*

Main category: q-fin.RM

TL;DR: 提出了一个白盒风险敏感框架，在交易成本和制度转换下联合对冲SPX和VIX风险。该方法结合无套利市场模型和安全约束控制层，通过小型二次规划和屏障函数实现风险降低和交易效率。


<details>
  <summary>Details</summary>
Motivation: 需要解决在交易成本和市场制度变化下，同时管理SPX和VIX风险敞口的挑战，传统方法难以平衡风险控制和交易效率。

Method: 集成SSVI隐含波动率曲面和Cboe合规VIX计算，使用Dupire局部波动率提取器连接价格与动态。控制层采用带屏障函数的小型二次规划，包含风险下降执行门控和三种尾部安全升级机制。

Result: 在模拟交易环境中，控制器降低了预期损失并抑制了不必要的交易周转，市场模型构建保持了指数水平残差的小而稳定。

Conclusion: 该框架在理论上证明了安全集的正向不变性和风险下降特性，在实践中有效平衡了风险控制和交易成本，为联合对冲提供了可行的白盒解决方案。

Abstract: We present a white-box, risk-sensitive framework for jointly hedging SPX and
VIX exposures under transaction costs and regime shifts. The approach couples
an arbitrage-free market teacher with a control layer that enforces safety as
constraints. On the market side, we integrate an SSVI-based implied-volatility
surface and a Cboe-compliant VIX computation (including wing pruning and 30-day
interpolation), and connect prices to dynamics via a clipped,
convexity-preserving Dupire local-volatility extractor. On the control side, we
pose hedging as a small quadratic program with control-barrier-function (CBF)
boxes for inventory, rate, and tail risk; a sufficient-descent execution gate
that trades only when risk drop justifies cost; and three targeted tail-safety
upgrades: a correlation/expiry-aware VIX weight, guarded no-trade bands, and
expiry-aware micro-trade thresholds with cooldown. We prove
existence/uniqueness and KKT regularity of the per-step QP, forward invariance
of safety sets, one-step risk descent when the gate opens, and no chattering
with bounded trade rates. For the dynamics layer, we establish positivity and
second-order consistency of the discrete Dupire estimator and give an
index-coherence bound linking the teacher VIX to a CIR-style proxy with
explicit quadrature and projection errors. In a reproducible synthetic
environment mirroring exchange rules and execution frictions, the controller
reduces expected shortfall while suppressing nuisance turnover, and the
teacher-surface construction keeps index-level residuals small and stable.

</details>


### [109] [A high-frequency approach to Realized Risk Measures](https://arxiv.org/abs/2510.16526)
*Federico Gatta,Fabrizio Lillo,Piero Mazzarisi*

Main category: q-fin.RM

TL;DR: 提出了一种新的Realized Risk Measures (RRM)方法，使用高频金融数据估计VaR和ES，改进了现有的Realized Quantile (RQ)方法，消除了收益率自相似性假设的限制。


<details>
  <summary>Details</summary>
Motivation: 现有的Realized Quantile (RQ)方法假设收益率具有自相似性，这在描述经验数据时存在局限性，需要开发更灵活的风险度量方法。

Method: RRM方法通过子过程将日内收益率转换为内在时间，捕捉交易活动和波动率聚集的不均匀性；使用移动平均过程过滤微观结构效应；拟合厚尾分布；通过特征函数方法或蒙特卡洛模拟外推低频收益率分布。

Result: 通过对18只美国股票的广泛数值模拟和实证研究，显示该方法在样本内风险度量估计和样本外风险预测方面均优于RQ方法。

Conclusion: RRM方法在风险度量估计和预测方面表现出色，为高频金融数据的风险管理提供了更有效的工具。

Abstract: We propose a new approach, termed Realized Risk Measures (RRM), to estimate
Value-at-Risk (VaR) and Expected Shortfall (ES) using high-frequency financial
data. It extends the Realized Quantile (RQ) approach proposed by Dimitriadis
and Halbleib by lifting the assumption of return self-similarity, which
displays some limitations in describing empirical data. More specifically, as
the RQ, the RRM method transforms intra-day returns in intrinsic time using a
subordinator process, in order to capture the inhomogeneity of trading activity
and/or volatility clustering. Then, microstructural effects resulting in
non-zero autocorrelation are filtered out using a suitable moving average
process. Finally, a fat-tailed distribution is fitted on the cleaned intra-day
returns. The return distribution at low frequency (daily) is then extrapolated
via either a characteristic function approach or Monte Carlo simulations. VaR
and ES are estimated as the quantile and the tail mean of the distribution,
respectively. The proposed approach is benchmarked against the RQ through
several experiments. Extensive numerical simulations and an empirical study on
18 US stocks show the outperformance of our method, both in terms of the
in-sample estimated risk measures and in the out-of-sample risk forecasting

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [110] [Berms without Calibration](https://arxiv.org/abs/2510.15984)
*K. E. Feldman*

Main category: q-fin.PR

TL;DR: 提出基于互换率分布和相关性的百慕大互换期权半解析定价模型，无需产品特定校准


<details>
  <summary>Details</summary>
Motivation: 开发无需产品特定校准的百慕大互换期权定价方法

Method: 基于互换率分布和互换率间相关性的半解析定价模型

Result: 推导出新的百慕大互换期权定价模型

Conclusion: 该模型提供了一种无需产品特定校准的百慕大互换期权定价解决方案

Abstract: We derive a new semi-analytical pricing model for Bermudan swaptions based on
swap rates distributions and correlations between them. The model does not
require product specific calibration.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [111] [A Topological Approach to Parameterizing Deep Hedging Networks](https://arxiv.org/abs/2510.16938)
*Alok Das,Kiseop Lee*

Main category: q-fin.MF

TL;DR: 通过添加拓扑特征，显著减小深度对冲训练所需的批量大小，提高训练效率而不显著影响对冲性能


<details>
  <summary>Details</summary>
Motivation: 传统深度对冲方法需要大批量大小来计算路径梯度，训练时间长且不实用

Method: 在循环神经网络中添加特定的拓扑特征

Result: 大幅减小批量大小，使训练更实用，同时保持对冲性能

Conclusion: 提出的方法使深度对冲在实际应用中更加可行

Abstract: Deep hedging uses recurrent neural networks to hedge financial products that
cannot be fully hedged in incomplete markets. Previous work in this area
focuses on minimizing some measure of quadratic hedging error by calculating
pathwise gradients, but doing so requires large batch sizes and can make
training effective models in a reasonable amount of time challenging. We show
that by adding certain topological features, we can reduce batch sizes
substantially and make training these models more practically feasible without
greatly compromising hedging performance.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [112] [Large Language Models in Architecture Studio: A Framework for Learning Outcomes](https://arxiv.org/abs/2510.15936)
*Juan David Salazar Rodriguez,Sam Conrad Joyce,Nachamma Sockalingam,Julfendi*

Main category: cs.CY

TL;DR: 该研究探讨了大型语言模型在建筑教育核心——设计工作室中的教学应用潜力，提出LLMs可作为补充工具增强学生自主性、协作和反思能力，并与布鲁姆分类法的认知层次相匹配。


<details>
  <summary>Details</summary>
Motivation: 传统建筑设计工作室主要依赖经验学习、同伴评价和教师指导，而AI在该领域的应用多集中于形式生成和效率提升，忽视了其作为教学工具的潜力，特别是在增强学生自主性、协作和自我反思方面的价值。

Method: 研究通过识别建筑学习中自主、同伴和教师指导学习过程中的教学挑战，提出基于LLM的AI干预方案，并将这些干预与布鲁姆分类法的可衡量学习成果对齐。

Result: 研究发现主要挑战包括学生自主性管理、同伴反馈中的紧张关系以及技术知识传授与创造力激发之间的平衡困难。LLMs能够生成个性化反馈、组织协作互动并提供适应性认知支架。

Conclusion: LLMs可作为建筑教育中的补充代理，通过个性化反馈、协作组织和认知支架支持，有效应对传统教学挑战，并能与布鲁姆分类法的各认知层次相匹配，促进从记忆理解到综合评估的全方位学习。

Abstract: The study explores the role of large language models (LLMs) in the context of
the architectural design studio, understood as the pedagogical core of
architectural education. Traditionally, the studio has functioned as an
experiential learning space where students tackle design problems through
reflective practice, peer critique, and faculty guidance. However, the
integration of artificial intelligence (AI) in this environment has been
largely focused on form generation, automation, and representation-al
efficiency, neglecting its potential as a pedagogical tool to strengthen
student autonomy, collaboration, and self-reflection. The objectives of this
research were: (1) to identify pedagogical challenges in self-directed,
peer-to-peer, and teacher-guided learning processes in architecture studies;
(2) to propose AI interventions, particularly through LLM, that contribute to
overcoming these challenges; and (3) to align these interventions with
measurable learning outcomes using Bloom's taxonomy. The findings show that the
main challenges include managing student autonomy, tensions in peer feedback,
and the difficulty of balancing the transmission of technical knowledge with
the stimulation of creativity in teaching. In response to this, LLMs are
emerging as complementary agents capable of generating personalized feedback,
organizing collaborative interactions, and offering adaptive cognitive
scaffolding. Furthermore, their implementation can be linked to the cognitive
levels of Bloom's taxonomy: facilitating the recall and understanding of
architectural concepts, supporting application and analysis through interactive
case studies, and encouraging synthesis and evaluation through hypothetical
design scenarios.

</details>


### [113] [Enabling Responsible, Secure and Sustainable Healthcare AI - A Strategic Framework for Clinical and Operational Impact](https://arxiv.org/abs/2510.15943)
*Jimmy Joseph*

Main category: cs.CY

TL;DR: 提出了一个负责任、安全、可持续的医疗AI操作化框架，包含五个关键支柱，并通过两个实际部署案例验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在将世界级技术卓越与组织准备度相结合，实现'合规设计'的可操作化，同时提供可衡量的影响。

Method: 采用包含领导力与战略、MLOps与技术基础设施、治理与伦理、教育与劳动力发展、变革管理与采用五个支柱的框架，并通过两个实际部署案例进行验证。

Result: 住院时长预测服务R²=0.41-0.58，采用率78%，复杂病例平均住院时长相对下降5-10%；AI增强放射学二次阅片灵敏度95%，亚厘米可操作发现检测提升8.0个百分点，工作流程未受影响。

Conclusion: 通过结合强大的MLOps和AI安全与治理、教育以及以人为本的变革，可以在提高安全性和结果的同时加速AI的采用。

Abstract: We offer a pragmatic model to operationalize responsible, secure, and
sustainable healthcare AI, aligning world-class technical excellence with
organizational readiness. The framework includes five key pillars - Leadership
& Strategy, MLOps & Technical Infrastructure, Governance & Ethics, Education &
Workforce Development, and Change Management & Adoption - and is intended to
operationalize 'compliance-by-design' while delivering measurable impact. We
demonstrate its utility through two deployments. (A) An inpatient length of
stay (LOS) prediction service had R^2=0.41-0.58 with validation cohorts in an
observational pilot (n = 3,184 encounters, 4 units, June-August 2025). Adoption
was 78 percent by week 6, and target units saw 5-10 percent relative declines
in mean LOS for complex cases vs. pre-pilot baselines. (B) An AI-augmented
radiology second-reader for lung nodules (PACS-integrated with thresholding and
explanation overlays) achieved high sensitivity (95 percent) and provided a
+8.0 percentage-point lift in detection of sub-centimeter actionable findings,
without slowing workflow (median report TAT 23 min, p = 0.64). Both services
executed in monitored, auditable pipelines with well-defined rollback, bias
checks, and no evidence of security incidents. These findings indicate that by
combining strong MLOps and AI security with governance, education, and
human-centric change, we can accelerate adoption of AI while improving security
and outcomes. We end with limitations, generalization considerations, and a
roadmap for scaling across varied clinical and operational use cases.

</details>


### [114] [Attention to Non-Adopters](https://arxiv.org/abs/2510.15951)
*Kaitlyn Zhou,Kristina Gligorić,Myra Cheng,Michelle S. Lam,Vyoma Raman,Boluwatife Aminu,Caeley Woo,Michael Brockman,Hannah Cha,Dan Jurafsky*

Main category: cs.CY

TL;DR: 论文主张将非采用者视角纳入LLM开发，以避免仅关注采用者群体导致的任务遗漏、不平等加剧和模型开发盲点。


<details>
  <summary>Details</summary>
Motivation: 大多数美国人（66%）从未使用过ChatGPT，但LLM开发和评估主要依赖采用者数据，这可能导致模型无法满足更广泛人群的需求，加剧不平等。

Method: 通过与非采用者进行案例研究，分析其需求差异，识别新型推理任务，并采用以人为中心的方法系统整合非采用者需求。

Result: 研究表明非采用者的需求与当前用户存在显著差异，这些需求指向了新型推理任务，为LLM开发提供了新的方向。

Conclusion: 将非采用者视角纳入LLM开发对于创建广泛有用且能力全面的语言模型至关重要，有助于避免模型开发中的偏见和不平等问题。

Abstract: Although language model-based chat systems are increasingly used in daily
life, most Americans remain non-adopters of chat-based LLMs -- as of June 2025,
66% had never used ChatGPT. At the same time, LLM development and evaluation
rely mainly on data from adopters (e.g., logs, preference data), focusing on
the needs and tasks for a limited demographic group of adopters in terms of
geographic location, education, and gender. In this position paper, we argue
that incorporating non-adopter perspectives is essential for developing broadly
useful and capable LLMs. We contend that relying on methods that focus
primarily on adopters will risk missing a range of tasks and needs prioritized
by non-adopters, entrenching inequalities in who benefits from LLMs, and
creating oversights in model development and evaluation. To illustrate this
claim, we conduct case studies with non-adopters and show: how non-adopter
needs diverge from those of current users, how non-adopter needs point us
towards novel reasoning tasks, and how to systematically integrate non-adopter
needs via human-centered methods.

</details>


### [115] [Impact of AI Tools on Learning Outcomes: Decreasing Knowledge and Over-Reliance](https://arxiv.org/abs/2510.16019)
*Márton Benedek,Balázs R. Sziklai*

Main category: cs.CY

TL;DR: 研究调查生成式AI工具对学生学习成果的影响，发现AI使用导致学生参与度降低、理解力下降，且学生已对AI产生依赖，引发对学习过程有效性的质疑。


<details>
  <summary>Details</summary>
Motivation: 了解学生依赖生成式AI工具完成作业和考试对其学习动机、知识理解和知识获取过程的影响。

Method: 在布达佩斯科维努斯大学的运筹学课程中进行随机对照实验，将学生分为使用AI组和对照组，并引入补偿机制平衡两组平均表现。

Result: AI使用导致学生参与度降低、理解力下降；学生强烈反对实验设计，证明AI已成为学生不可或缺的工具。

Conclusion: 生成式AI工具的无控制使用会损害学习效果，学生已对AI产生严重依赖，这引发了对当前学习过程有效性的根本质疑。

Abstract: Students at all levels of education are increasingly relying on generative
artificial intelligence (AI) tools to complete assignments and achieve higher
exam scores. However, it remains unclear how this reliance affects their
motivation, their genuine understanding of the material, and the extent to
which it substitutes for the process of knowledge acquisition. To investigate
the impact of generative AI on learning outcomes, an experiment was conducted
at Corvinus University of Budapest. In an operations research class, students
were randomly assigned into two groups: one was permitted to use AI tools
during classes and examinations, while the other was not. To ensure fairness, a
compensation mechanism was introduced: students in the lower-performing group
received point adjustments until the average performance of the two groups was
equalized. Despite the organizers' best efforts to explain the design and to
create equal opportunities for all participants, many students perceived the
experiment as a major disruption. Although the experiment was approved by every
relevant university authority -- including the Ethics Board, the Head of
Department, the Program Director, and the Student Council -- students escalated
their concerns to the media and eventually to the State Secretary for Higher
Education of Hungary. As a result, the experiment had to be substantially
revised before completion: on the final exam the test group was merged with the
control group. Still, the data allowed us to draw decisive conclusions
regarding the students' learning habits. Uncontrolled use of AI tools leads to
disengaged students and low understanding of material. The extreme reactions of
the students proved even more revealing than the data collected: generative AI
tools have already become indispensable for students, raising fundamental
questions about the validity of their learning process.

</details>


### [116] [A dual typology of social media interventions and deterrence mechanisms against misinformation](https://arxiv.org/abs/2510.16032)
*Amir Karami*

Main category: cs.CY

TL;DR: 提出了一个基于威慑理论的社交媒体干预双重分类法，将平台干预措施映射到五种威慑机制上


<details>
  <summary>Details</summary>
Motivation: 解决社交媒体平台在应对虚假信息时缺乏宏观视角的问题，现有干预措施缺乏统一的理论框架来解释其独立和集体运作机制

Method: 通过威慑理论构建双重分类法，借鉴国际关系、军事、网络安全和公共卫生领域的经验，将平台干预措施与威慑机制进行映射

Result: 成功识别了五种主要平台干预类型（移除、减少、告知、复合、多模式）并对应到五种威慑机制（硬威慑、情境威慑、软威慑、整合威慑、混合威慑）

Conclusion: 该分类法为理解社交媒体平台如何应用不同程度的威慑机制来影响用户行为提供了系统框架，有助于更有效地设计和评估虚假信息干预策略

Abstract: In response to the escalating threat of misinformation, social media
platforms have introduced a wide range of interventions aimed at reducing the
spread and influence of false information. However, there is a lack of a
coherent macrolevel perspective that explains how these interventions operate
independently and collectively. To address this gap, I offer a dual typology
through a spectrum of interventions aligned with deterrence theory and drawing
parallels from international relations, military, cybersecurity, and public
health. I argue that five major types of platform interventions, including
removal, reduction, informing, composite, and multimodal, can be mapped to five
corresponding deterrence mechanisms, including hard, situational, soft,
integrated, and mixed deterrence based on purpose and perceptibility. These
mappings illuminate how platforms apply varying degrees of deterrence
mechanisms to influence user behavior.

</details>


### [117] [Does Capital Dream of Artificial Labour?](https://arxiv.org/abs/2510.16042)
*Marcin Korecki,Cesare Carissimo*

Main category: cs.CY

TL;DR: 该论文将劳动定义为'时间能量'的商品化表达，通过博弈论模拟揭示了资本在组织生产中的优势地位，质疑在自动化未来中生命是否能在没有资本基础设施的情况下维持自身。


<details>
  <summary>Details</summary>
Motivation: 研究劳动作为时间能量表达的概念，探索其在资本系统中的异化和演变，特别是在自动化和人工智能背景下的转变。

Method: 使用博弈论和基于代理的模拟方法，通过柯布-道格拉斯生产函数建模资本与劳动在生产过程中的相互作用。

Result: 模拟结果显示，尽管理论上对称，但学习代理不成比例地倾向于资本密集型过程，揭示了资本因其积累能力而具有的组织优势。

Conclusion: 资本是一个由其所消耗的活劳动激活的人工生命系统，研究质疑在日益自动化的未来中，生命是否能在没有资本基础设施的情况下维持自身。

Abstract: This paper investigates the concept of Labour as an expression of `timenergy'
- a fusion of time and energy - and its entanglement within the system of
Capital. We define Labour as the commodified, quantifiable expansion of
timenergy, in contrast to Capital, which is capable of accumulation and
abstraction. We explore Labour's historical evolution, its coercive and
alienating nature, and its transformation through automation and artificial
intelligence. Using a game-theoretic, agent-based simulation, we model
interactions between Capital and Labour in production processes governed by
Cobb-Douglas functions. Our results show that despite theoretical symmetry,
learning agents disproportionately gravitate toward capital-intensive
processes, revealing Capital's superior organizational influence due to its
accumulative capacity. We argue that Capital functions as an artificially alive
system animated by the living Labour it consumes, and question whether life can
sustain itself without the infrastructures of Capital in a future of increasing
automation. This study offers both a critique of and a framework for
understanding Labour's subjugation within the Capital system.

</details>


### [118] [Open Shouldn't Mean Exempt: Open-Source Exceptionalism and Generative AI](https://arxiv.org/abs/2510.16048)
*David Atkinson*

Main category: cs.CY

TL;DR: 论文批判开源GenAI的伦理和法律优越性主张，指出其常助长非法行为和环境污染，并揭露"民主化"和"创新"修辞的策略性使用。主张开源开发者应承担同等责任，但为非商业科研提供有限安全港。


<details>
  <summary>Details</summary>
Motivation: 反驳开源GenAI因开源而天然合乎伦理或合法的错误观点，批判开源例外主义的主张，揭示其实际助长非法行为和环境问题。

Method: 批判性分析开源GenAI实体的常见辩护理由，揭露其修辞策略，并提出法律和伦理框架建议。

Result: 证明开源GenAI并未真正打破寡头垄断，反而可能促进不当行为，需要同等法律约束。

Conclusion: 主张在既定伦理和法律边界内负责任地开发AI，为非商业科研提供有条件的安全港，确保开源不成为规避责任的借口。

Abstract: Any argument that open-source generative artificial intelligence (GenAI) is
inherently ethical or legal solely because it is open source is flawed. Yet,
this is the explicit or implicit stance of several open-source GenAI entities.
This paper critically examines prevalent justifications for "open-source
exceptionalism," demonstrating how contemporary open-source GenAI often
inadvertently facilitates unlawful conduct and environmental degradation
without genuinely disrupting established oligopolies. Furthermore, the paper
exposes the unsubstantiated and strategic deployment of "democratization" and
"innovation" rhetoric to advocate for regulatory exemptions not afforded to
proprietary systems.
  The conclusion is that open-source developers must be held to the same legal
and ethical standards as all other actors in the technological ecosystem.
However, the paper proposes a narrowly tailored safe harbor designed to protect
legitimate, non-commercial scientific research, contingent upon adherence to
specific criteria. Ultimately, this paper advocates for a framework of
responsible AI development, wherein openness is pursued within established
ethical and legal boundaries, with due consideration for its broader societal
implications.

</details>


### [119] [In the Mood to Exclude: Revitalizing Trespass to Chattels in the Era of GenAI Scraping](https://arxiv.org/abs/2510.16049)
*David Atkinson*

Main category: cs.CY

TL;DR: 论文主张网站所有者有权排除他人访问其网站，当生成式AI爬虫绕过技术障碍进行数据抓取时，构成非法侵入动产。法院应关注网站作为数字资产的价值损害，而非仅关注服务器损坏或用户干扰。


<details>
  <summary>Details</summary>
Motivation: 当前法院过分关注网站内容而非网站本身作为数字财产的性质，导致非法侵入动产索赔难以成立。版权优先原则限制了可用索赔，使版权和合理使用成为主要争议点。

Method: 通过将分析焦点从内容转向网站作为整体数字资产，论证非法侵入动产在网络环境中的适用性。强调网站所有者排除权的法律基础。

Result: 承认网站为个人财产可恢复非法侵入动产作为有效诉讼理由，为网站所有者提供可执行的排除权，阻止剥削性数据抓取。

Conclusion: 重申网站所有者的排除权对于维护公平可持续的网络环境至关重要，有助于保护内容创作激励、隐私和个人数据，以及自主权和表达价值。

Abstract: This paper argues that website owners have the right to exclude others from
their websites. Accordingly, when generative AI (GenAI) scraping bots
intentionally circumvent reasonable technological barriers, their conduct could
be actionable as trespass to chattels. If the scraping leads to a decrease in
the website's value, then trespass to chattels should apply. The prevailing
judicial focus on website content and the dismissal of trespass claims absent
proof of server impairment or user disruption misconstrues the nature of the
website itself as a form of digital property, focusing too narrowly on what
constitutes harm under a claim of trespass. By shifting analysis from content
to the website itself as an integrated digital asset and illustrating the harm
to the value of the chattel, this paper demonstrates that the right to exclude
applies online with the same force as it does to tangible property.
  Courts and litigants have struggled to police large-scale scraping because
copyright preemption narrows available claims, leaving copyright and its fair
use defense as the primary battleground. In contrast, recognizing websites as
personal property revives trespass to chattels as a meaningful cause of action,
providing website owners with an enforceable exclusionary right. Such
protection would disincentivize exploitative scraping, preserve incentives for
content creation, aid in protecting privacy and personal data, and safeguard
values of autonomy and expression. Ultimately, this paper contends that
reaffirming website owners' right to exclude is essential to maintaining a fair
and sustainable online environment.

</details>


### [120] [A Framework For Decentralized Micro-credential Verification Towards Higher Qualifications](https://arxiv.org/abs/2510.16050)
*Abrar Mahbub,Humira Saria,Md. Foysal Hossain,Nafees Mansoor*

Main category: cs.CY

TL;DR: 本文提出了一种基于区块链技术的微证书验证原型系统，旨在解决教育机构间微证书学分转移的标准化问题。


<details>
  <summary>Details</summary>
Motivation: 教育机构面临学生保留率下降的问题，微证书作为短期学术项目缺乏标准化评估和学分转移机制，阻碍了其向更大资格认证的转化。

Method: 开发了一个基于Hyper-ledger Fabric平台的原型模型，结合链上和链下技术，通过验证机构验证微证书证书，教育机构根据评估标准提供课程豁免。

Result: 原型系统通过链下技术作为中间存储平台，减少了区块链拥堵，提高了交易速度，实现了安全的微证书验证。

Conclusion: 该研究为微证书的安全验证和高效课程豁免流程提供了一个可行的技术解决方案，有助于促进教育机构间的学分互认。

Abstract: Student retention is one of the rising problems seen in educational
institutions. With the rising cost of education and issues in the education
sector, such as curriculum relevance, student engagement, and rapidly changing
technological advancements, ensuring the relevance of academic programs in a
fast-evolving job market has created a significant concern for educational
institutions. With the intent to adapt to such challenges, educational
institutions are dealing with alternative solutions for education, in which
micro-credentials are at the very center of this, which are short-term academic
programs or standalone courses. However, one of the challenges of
micro-credentials is a lack of credit transfer among institutions. With the
lack of standardization of assessments among educational institutions, it is
difficult to transfer micro-credentials to larger qualifications. Regarding
such challenges, micro-credentials with blockchain technology can bring
significant benefits. Blockchain technology offers a decentralized and
immutable platform for securely storing and verifying credentials. This paper
presents a prototype model for micro-credential verification. With the policies
decided by the educational institution, the learner provides a micro-credential
certificate to the system. Upon validation of the certificate by the verifying
body, the educational institution will review the assessment criteria and
provide exemptions based on the provided criteria. The prototype uses the
Hyper-ledger Fabric platform and utilizes off-chain technology, which acts as a
middle-man storage platform. With the combination of off-chain and on-chain
technologies, congestion on the blockchain is reduced, and transaction speed is
improved. In summary, this research proposes a prototype for secure
micro-credential verification and a more efficient course exemption process.

</details>


### [121] [Reducing Procrastination on Programming Assignments via Optional Early Feedback](https://arxiv.org/abs/2510.16052)
*Alice Gao,Victoria Sakhnini*

Main category: cs.CY

TL;DR: 该研究设计了一个干预措施来对抗计算机科学本科生的学业拖延，通过设置无分数的早期截止日期来提供额外自动化反馈，结果显示干预能显著促使更多学生尽早开始作业，并提高使用该干预学生的成绩。


<details>
  <summary>Details</summary>
Motivation: 学业拖延在计算机科学本科生中普遍存在，尤其对高年级学生面对大型复杂编程作业时尤为不利，这与学业成绩不佳和幸福感下降相关。

Method: 设计干预措施：设置无分数的早期截止日期，提供额外自动化反馈；通过控制组和干预组的比较评估干预效果；进行半结构化访谈了解学生对干预的认知。

Result: 干预显著促使更多学生尽早开始作业；干预组中使用干预的学生成绩显著高于未使用者；访谈显示学生在学业表现、心理健康和软技能发展等方面受益。

Conclusion: 仅尽早开始作业不能提高成绩，但尽早开始并接收额外反馈能显著提升学生成绩；学生采用干预的主要原因是获取更多反馈、满足好奇心和利用可用时间，不采用的主要原因是其他截止日期冲突、无分数奖励和自信。

Abstract: Academic procrastination is prevalent among undergraduate computer science
students. Many studies have linked procrastination to poor academic performance
and well-being. Procrastination is especially detrimental for advanced students
when facing large, complex programming assignments in upper-year courses. We
designed an intervention to combat academic procrastination on such programming
assignments. The intervention consisted of early deadlines that were not worth
marks but provided additional automated feedback if students submitted their
work early. We evaluated the intervention by comparing the behaviour and
performance of students between a control group and an intervention group. Our
results showed that the intervention encouraged significantly more students to
start the assignments early. Although there was no significant difference in
students' grades between the control and intervention groups, students within
the intervention group who used the intervention achieved significantly higher
grades than those who did not. Our results implied that starting early alone
did not improve students' grades. However, starting early and receiving
additional feedback enhanced the students' grades relative to those of the rest
of the students. We also conducted semi-structured interviews to gain an
understanding of students' perceptions of the intervention. The interviews
revealed that students benefited from the intervention in numerous ways,
including improved academic performance, mental health, and development of soft
skills. Students adopted the intervention to get more feedback, satisfy their
curiosity, or use their available time. The main reasons for not adopting the
intervention include having other competing deadlines, the intervention not
being worth any marks, and feeling confident about their work.

</details>


### [122] [Algorithmic Fairness in AI Surrogates for End-of-Life Decision-Making](https://arxiv.org/abs/2510.16056)
*Muhammad Aurangzeb Ahmad*

Main category: cs.CY

TL;DR: 本文探讨AI代理系统在临终决策中的公平性框架，指出传统算法公平性框架不足以处理关系性、存在性和文化多样性的决策情境。


<details>
  <summary>Details</summary>
Motivation: AI代理系统在个体失去决策能力时推断偏好，但该领域的公平性问题研究不足。传统算法公平性框架无法充分处理关系性、存在性和文化多样性的决策情境。

Method: 通过将主要公平性概念映射到现实世界临终场景，并跨道德传统检验公平性，构建AI代理的伦理公平框架。

Result: 研究发现公平性在这一领域超越了结果均等，需要包含道德代表性、对患者价值观的忠实度、人际关系和世界观。

Conclusion: AI代理系统的公平性需要更全面的框架，涵盖道德代表性、价值观忠实度和关系维度，而不仅仅是结果平等。

Abstract: Artificial intelligence surrogates are systems designed to infer preferences
when individuals lose decision-making capacity. Fairness in such systems is a
domain that has been insufficiently explored. Traditional algorithmic fairness
frameworks are insufficient for contexts where decisions are relational,
existential, and culturally diverse. This paper explores an ethical framework
for algorithmic fairness in AI surrogates by mapping major fairness notions
onto potential real-world end-of-life scenarios. It then examines fairness
across moral traditions. The authors argue that fairness in this domain extends
beyond parity of outcomes to encompass moral representation, fidelity to the
patient's values, relationships, and worldview.

</details>


### [123] [Co-Designing Interdisciplinary Design Projects with AI](https://arxiv.org/abs/2510.16068)
*Wei Ting Liow,Sumbul Khan,Lay Kee Ang*

Main category: cs.CY

TL;DR: 本文介绍了IDPplanner，一个基于GPT的跨学科设计项目规划助手，通过实证研究表明AI辅助规划在课程对齐、设计思维应用和连贯性方面优于手动规划，同时强调教师-AI混合工作流程的重要性。


<details>
  <summary>Details</summary>
Motivation: 教师创建跨学科设计项目耗时且认知要求高，需要课程对齐、跨学科整合和精心排序。国际研究显示教师使用AI增加但工作压力持续，凸显规划支持的必要性。

Method: 开发了基于GPT的IDPplanner规划助手，在33名在职教师的平衡实验中，比较手动和AI辅助两种规划方式，使用六维评分标准进行自评和互评。

Result: AI辅助版本在课程对齐、设计思维应用和连贯性方面得分更高，在评估策略方面略有优势。教师反思显示AI辅助改善了结构、排序和创意生成，但情境化仍需教师主导。

Conclusion: AI可以作为教学规划伙伴，建议采用混合教师-AI工作流程来增强课程对齐并降低规划复杂性，同时为开发者提供加强情境定制、迭代设计支持和本地化评分标准的建议。

Abstract: Creating interdisciplinary design projects is time-consuming and cognitively
demanding for teachers, requiring curriculum alignment, cross-subject
integration, and careful sequencing. International research reports increasing
teacher use of AI alongside persistent workload pressures, underscoring the
need for planning support. This paper presents the Interdisciplinary Design
Project Planner (IDPplanner), a GPT-based planning assistant grounded in Design
Innovation principles, alignment with Singapore secondary school syllabuses,
and 21st-century competencies. In a within-subject, counterbalanced workshop
with 33 in-service teachers, participants produced two versions of the same
project: manual and AI-assisted, followed by self- and peer-evaluations using a
six-dimensional rubric. The AI-assisted version received higher scores for
Curriculum Alignment, Design Thinking Application, and Coherence and Flow, with
a marginal advantage for Assessment Strategies. Teacher reflections indicated
that AI-assisted planning improved structure, sequencing, and idea generation,
while contextualization to local syllabuses, class profiles, and student needs
remained teacher-led. Contributions include a purpose-built planning tool that
organizes ideas into a ten-component flow with ready-to-adapt prompts,
templates, and assessment suggestions; an empirical, rubric-based comparison of
planning quality; and evidence that AI can function as a pedagogical planning
partner. Recommendations emphasize hybrid teacher-AI workflows to enhance
curriculum alignment and reduce planning complexity, and design suggestions for
developers to strengthen contextual customization, iterative design support,
and localized rubrics. Although instantiated with a Singapore-based curriculum,
the planning flow and rubric are framework-agnostic and can be parameterized
for other systems.

</details>


### [124] [Human or AI? Comparing Design Thinking Assessments by Teaching Assistants and Bots](https://arxiv.org/abs/2510.16069)
*Sumbul Khan,Wei Ting Liow,Lay Kee Ang*

Main category: cs.CY

TL;DR: 本研究探讨AI辅助评估与助教评估在创意设计海报评分中的可靠性，发现AI在视觉沟通维度表现较好，但在共情和痛点识别方面与教师评分一致性较低，教师更偏好助教评分，建议采用混合评估模型。


<details>
  <summary>Details</summary>
Motivation: 设计思维教育中评估创意作品面临挑战，传统基于量规的评估方法在大规模多班级教学中耗时且不一致，需要探索AI辅助评估的可行性。

Method: 对33名新加坡教育部教师进行两项活动：比较AI生成分数与助教评分在三个维度的差异，以及调查教师对AI评分、助教评分和混合评分的偏好。

Result: AI与教师评分在共情和痛点识别维度统计一致性低，视觉沟通维度一致性稍高；教师偏好助教评分；定性反馈显示AI在形成性反馈、一致性和学生自我反思方面有潜力，但在捕捉情境细微差别和创意洞察方面存在局限。

Conclusion: 需要整合计算效率与人类洞察的混合评估模型，在创意学科中平衡自动化与人类判断，实现可扩展且符合教学原理的评估。

Abstract: As design thinking education grows in secondary and tertiary contexts,
educators face the challenge of evaluating creative artefacts that combine
visual and textual elements. Traditional rubric-based assessment is laborious,
time-consuming, and inconsistent due to reliance on Teaching Assistants (TA) in
large, multi-section cohorts. This paper presents an exploratory study
investigating the reliability and perceived accuracy of AI-assisted assessment
compared to TA-assisted assessment in evaluating student posters in design
thinking education. Two activities were conducted with 33 Ministry of Education
(MOE) Singapore school teachers to (1) compare AI-generated scores with TA
grading across three key dimensions: empathy and user understanding,
identification of pain points and opportunities, and visual communication, and
(2) examine teacher preferences for AI-assigned, TA-assigned, and hybrid
scores. Results showed low statistical agreement between instructor and AI
scores for empathy and pain points, with slightly higher alignment for visual
communication. Teachers preferred TA-assigned scores in six of ten samples.
Qualitative feedback highlighted the potential of AI for formative feedback,
consistency, and student self-reflection, but raised concerns about its
limitations in capturing contextual nuance and creative insight. The study
underscores the need for hybrid assessment models that integrate computational
efficiency with human insights. This research contributes to the evolving
conversation on responsible AI adoption in creative disciplines, emphasizing
the balance between automation and human judgment for scalable and
pedagogically sound assessment.

</details>


### [125] [SARHAchat: An LLM-Based Chatbot for Sexual and Reproductive Health Counseling](https://arxiv.org/abs/2510.16081)
*Jiaye Yang,Xinyu Zhao,Tianlong Chen,Kandyce Brennan*

Main category: cs.CY

TL;DR: SARHAchat是一个基于大语言模型的聊天机器人，专门为性健康和生殖健康领域设计，整合医学专业知识和共情沟通，提供准确、情境适当的避孕咨询服务。


<details>
  <summary>Details</summary>
Motivation: 现有AI对话系统在复杂敏感的医疗领域（如性健康和生殖健康）表现不佳，存在幻觉问题和专业知识不足，且当前医疗AI方法过于侧重诊断能力而忽视全面的患者护理和教育。

Method: 开发了SARHAchat概念验证系统，这是一个基于大语言模型的聊天机器人，整合医学专业知识与共情沟通，专注于性健康和生殖健康护理。

Result: 评估显示SARHAchat能够提供准确且情境适当的避孕咨询服务，同时保持自然的对话流程。

Conclusion: SARHAchat展示了将大语言模型与医学专业知识结合，为敏感医疗领域提供可靠、以用户为中心护理的可行性。

Abstract: While Artificial Intelligence (AI) shows promise in healthcare applications,
existing conversational systems often falter in complex and sensitive medical
domains such as Sexual and Reproductive Health (SRH). These systems frequently
struggle with hallucination and lack the specialized knowledge required,
particularly for sensitive SRH topics. Furthermore, current AI approaches in
healthcare tend to prioritize diagnostic capabilities over comprehensive
patient care and education. Addressing these gaps, this work at the UNC School
of Nursing introduces SARHAchat, a proof-of-concept Large Language Model
(LLM)-based chatbot. SARHAchat is designed as a reliable, user-centered system
integrating medical expertise with empathetic communication to enhance SRH care
delivery. Our evaluation demonstrates SARHAchat's ability to provide accurate
and contextually appropriate contraceptive counseling while maintaining a
natural conversational flow. The demo is available at
https://sarhachat.com/}{https://sarhachat.com/.

</details>


### [126] [MoPHES:Leveraging on-device LLMs as Agent for Mobile Psychological Health Evaluation and Support](https://arxiv.org/abs/2510.16085)
*Xun Wei,Pukai Zhou,Zeyu Wang*

Main category: cs.CY

TL;DR: MoPHES是一个集成心理状态评估、对话支持和专业治疗建议的框架，使用两个微调的MiniCPM4-0.5B模型，可直接在移动设备上部署以保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康需求激增，传统面对面治疗无法满足需求，且现有LLM心理聊天机器人缺乏实时心理状态评估能力。

Method: 使用两个微调的MiniCPM4-0.5B模型：一个用于心理状态评估和焦虑抑郁严重程度预测，另一个用于多轮对话处理；开发了自动化评估基准。

Result: MoPHES框架能够提供更个性化的支持和专业治疗建议，模型可直接在移动设备上运行。

Conclusion: 该框架通过集成心理状态评估和对话支持，为AI驱动的心理健康解决方案提供了更专业的支持能力。

Abstract: The 2022 World Mental Health Report calls for global mental health care
reform, amid rising prevalence of issues like anxiety and depression that
affect nearly one billion people worldwide. Traditional in-person therapy fails
to meet this demand, and the situation is worsened by stigma. While
general-purpose large language models (LLMs) offer efficiency for AI-driven
mental health solutions, they underperform because they lack specialized
fine-tuning. Existing LLM-based mental health chatbots can engage in empathetic
conversations, but they overlook real-time user mental state assessment which
is critical for professional counseling. This paper proposes MoPHES, a
framework that integrates mental state evaluation, conversational support, and
professional treatment recommendations. The agent developed under this
framework uses two fine-tuned MiniCPM4-0.5B LLMs: one is fine-tuned on mental
health conditions datasets to assess users' mental states and predict the
severity of anxiety and depression; the other is fine-tuned on multi-turn
dialogues to handle conversations with users. By leveraging insights into
users' mental states, our agent provides more tailored support and professional
treatment recommendations. Both models are also deployed directly on mobile
devices to enhance user convenience and protect user privacy. Additionally, to
evaluate the performance of MoPHES with other LLMs, we develop a benchmark for
the automatic evaluation of mental state prediction and multi-turn counseling
dialogues, which includes comprehensive evaluation metrics, datasets, and
methods.

</details>


### [127] [Integrating LLM and Diffusion-Based Agents for Social Simulation](https://arxiv.org/abs/2510.16366)
*Xinyi Li,Zhiqiang Guo,Qinglang Guo,Hao Jin,Weizhi Ma,Min Zhang*

Main category: cs.CY

TL;DR: 提出了一种混合仿真框架，结合LLM驱动代理和扩散模型代理，用于预测社交信息传播，在保证准确性的同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于代理的社交仿真方法存在两个主要局限：传统代理模型依赖刚性行为规则且缺乏文本语义理解，而基于大语言模型(LLM)的代理在大规模应用时计算成本过高。

Method: 提出混合仿真框架，策略性地整合LLM驱动代理和扩散模型代理。LLM代理模拟核心用户子集进行丰富语义推理，扩散模型高效处理剩余用户群体。两种代理类型在不相交的用户组上运行，但都包含用户个性化、社会影响和内容感知等关键因素，并通过协调的仿真过程进行交互。

Result: 在三个真实世界数据集上的广泛实验表明，该框架在预测准确性方面优于现有方法，验证了其模块化设计的有效性。

Conclusion: 该混合仿真框架成功解决了传统代理模型缺乏语义理解和LLM代理计算成本高的问题，通过模块化设计实现了准确且高效的社交信息传播预测。

Abstract: Agent-based social simulation provides a valuable methodology for predicting
social information diffusion, yet existing approaches face two primary
limitations. Traditional agent models often rely on rigid behavioral rules and
lack semantic understanding of textual content, while emerging large language
model (LLM)-based agents incur prohibitive computational costs at scale. To
address these challenges, we propose a hybrid simulation framework that
strategically integrates LLM-driven agents with diffusion model-based agents.
The framework employs LLM-based agents to simulate a core subset of users with
rich semantic reasoning, while a diffusion model handles the remaining
population efficiently. Although the two agent types operate on disjoint user
groups, both incorporate key factors including user personalization, social
influence, and content awareness, and interact through a coordinated simulation
process. Extensive experiments on three real-world datasets demonstrate that
our framework outperforms existing methods in prediction accuracy, validating
the effectiveness of its modular design.

</details>


### [128] [Women have it Worse: an ICT Workplace Digital Transformation Stress Gender Gap](https://arxiv.org/abs/2510.16459)
*Ewa Makowska-Tłumak,Sylwia Bedyńska,Kinga Skorupska,Radosław Nielek*

Main category: cs.CY

TL;DR: 研究发现数字化转型会给女性员工带来更高的数字转型压力，存在明显的性别差距，需要通过工具和支持措施来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 数字化转型虽然带来积极影响，但可能对员工福祉产生负面影响，特别是由于对女性技术能力的负面刻板印象，可能导致女性员工面临更大的数字转型压力。

Method: 采用双重测量方法：通过帮助台工单的情感分析和使用心理量表的自我报告来测量数字转型压力。

Result: 结果证实了预测，女性员工确实表现出更高的数字转型压力水平，存在明显的性别差距。

Conclusion: 需要在ICT密集型工作环境中讨论可能的解决方案和工具来支持女性员工。

Abstract: Although information and communication technologies (ICT) solutions have
positive outcomes for both companies and employees, the digital transformation
(DT) could have an impact on the well-being of employees. The jobs of the
employees became more demanding, and they were expected to learn ICT skills and
cope with ICT workloads and hassles. Due to negative stereotypes about women's
deficiency in technology, these ICT problems could affect female and male
employees differently. Thus, we predicted that this additional pressure may
manifest itself in higher levels of digital transformation stress (DTS) in
female employees. The results confirmed this prediction and indicated the
existence of a gender gap in DTS, measured two-fold - in sentiment analysis of
help desk tickets and self-report using a psychological scale. Based on these
results, we explore the need to discuss possible solutions and tools to support
women in ICT-heavy workplace contexts.

</details>


### [129] [Global Overview of Computational Thinking and Digital Tools for Teaching](https://arxiv.org/abs/2510.16847)
*Roberto Massi De Oliveira,M^onica Cristina Garbin,Rodolfo Azevedo*

Main category: cs.CY

TL;DR: 这篇综述分析了计算思维在学校课程中的整合情况，对数字工具进行分类并评估其在不同教育环境中的使用，识别了计算思维能力的提升领域和面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 计算思维已成为现代教育的关键组成部分，需要培养学生适应技术驱动世界的技能，因此需要全面了解计算思维在学校课程中的整合现状。

Method: 采用综述研究方法，对各国学校课程中计算思维的存在和整合进行全面分析，将数字工具分类为可视化编程、文本编程、电子游戏、建模和模拟等类别，并评估其在不同教育环境中的使用。

Result: 研究发现数字工具在提升认知分析能力、技术计算能力以及社会情感能力方面发挥了重要作用，但同时也面临基础设施不足、工具可用性困难、教师培训、教学实践适应和学生计算思维技能测量等挑战。

Conclusion: 研究提出了未来研究的方向，以解决这些挑战并推进计算思维教育的发展，强调了需要进一步关注基础设施、教师专业发展和有效的评估方法。

Abstract: Computational Thinking (CT) has emerged as a critical component in modern
education, essential to equip students with the skills necessary to thrive in a
technology-driven world. This survey provides a comprehensive analysis of the
presence and integration of CT in school curricula across various countries. In
addition, this study categorizes digital tools into groups such as visual
programming, textual programming, electronic games, modeling, and simulation,
assessing their use in different educational settings. Furthermore, it examines
how these tools are employed in various contexts, including the areas of
knowledge and age groups they target, and the specific skills they help
develop. The research also identifies key CT competencies that have been
improved through these tools, including Cognitive and Analytical Competencies
(CAC), Technical and Computational Competencies (TCC) and Social and Emotional
Competencies (SEC). Furthermore, the study highlights recurring challenges in
the implementation of digital tools for CT development, such as inadequate
infrastructure, difficulties in the usability of the tool, teacher training,
adapting pedagogical practices, and measuring student CT skills. Finally, it
proposes areas for future research to address these challenges and advance CT
education.

</details>


### [130] [Agentic Inequality](https://arxiv.org/abs/2510.16853)
*Matthew Sharp,Omer Bilgin,Iason Gabriel,Lewis Hammond*

Main category: cs.CY

TL;DR: 本文提出了'代理不平等'概念，探讨AI代理系统在可用性、质量和数量三个维度上的不平等如何导致权力、机会和结果的不平等分配。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理系统融入政治经济生活，其分布和能力差异可能产生重大影响。作者旨在分析这种技术可能加剧现有差距，也可能在适当条件下成为平等化力量的双重潜力。

Method: 建立分析框架，界定代理不平等在可用性、质量和数量三个核心维度的表现；论证代理不平等与以往技术鸿沟的区别；系统分析技术和经济社会驱动因素。

Result: 提出了代理不平等作为新型技术鸿沟的概念，强调AI代理作为自主代表创造了通过可扩展目标委托和直接代理间竞争的新型权力不对称。

Conclusion: 代理不平等将重塑经济和社会政治领域的结果，需要复杂治理挑战的研究议程来应对技术发布策略和市场激励等因素对代理权力分布的影响。

Abstract: Autonomous AI agents, capable of complex planning and action, represent a
significant technological evolution beyond current generative tools. As these
systems become integrated into political and economic life, their distribution
and capabilities will be highly consequential. This paper introduces and
explores "agentic inequality" - the potential disparities in power,
opportunity, and outcomes stemming from differential access to, and
capabilities of, AI agents. We analyse the dual potential of this technology,
exploring how agents could both exacerbate existing divides and, under the
right conditions, serve as a powerful equalising force. To this end, the paper
makes three primary contributions. First, it establishes an analytical
framework by delineating the three core dimensions through which this
inequality can manifest: disparities in the availability, quality, and quantity
of agents. Second, it argues that agentic inequality is distinct from prior
technological divides. Unlike tools that primarily augment human abilities,
agents act as autonomous delegates, creating novel power asymmetries through
scalable goal delegation and direct agent-to-agent competition that are poised
to reshape outcomes across economic and socio-political spheres. Finally, it
provides a systematic analysis of the technical and socioeconomic drivers -
from model release strategies to market incentives - that will shape the
distribution of agentic power, concluding with a research agenda for navigating
the complex governance challenges ahead.

</details>


### [131] [Sustainable and Adaptive Growth in Creative Tech](https://arxiv.org/abs/2510.16858)
*Enes Ayalp*

Main category: cs.CY

TL;DR: CLEAR CORE是一个应对创意技术领域快速变化的框架，通过整合结构化教育和独立成长两个迭代循环，帮助专业人士在持续变化中保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 创意技术领域发展迅速，现有教育路径往往无法连接技能，导致学习者专业知识碎片化且容易过时。行业需要深度但专业化又面临技术快速演变的风险，而广泛技能集容易被技术变革取代。

Method: CLEAR CORE框架包含两个迭代互连的循环：结构化教育和独立成长，形成一个持续的过程，将正规学习与终身自我发展相结合。

Result: 该框架提供了一个可持续的学习和发展模式，使创意技术专业人士能够在技术快速变化的环境中持续成长并保持竞争力。

Conclusion: CLEAR CORE通过整合结构化教育和自主发展的双循环模式，为创意技术从业者提供了一个应对行业快速变化的有效解决方案，支持终身学习和专业可持续发展。

Abstract: The creative technology evolves rapidly in both scope and depth, demanding
cross-disciplinary expertise and continuous improvement. Although educational
programs and other collaborative initiatives enable strong technical and
artistic skills, even the most advanced pathways rarely ensure a stable career.
Success in these professions often depends on visibility, timing, and
self-directed development. As markets shift or technologies change, talents
still find themselves displaced. Existing learning paths often fail to connect
the skills they teach, leaving learners with fragmented expertise that decays
quickly when not continuously applied. The industry demands depth, yet
specialization carries risk when tools, pipelines, or roles evolve faster than
the expertise built around them. Broad skill sets, by contrast, may increase
employability but are easily replaced or rendered obsolete by technological
change. CLEAR CORE is a framework for learning and sustaining in creative
technology. It integrates two iterative interconnected cycles into a continuous
process linking structured education with independent growth as a lifelong,
renewable practice that allows professionals to excel amid constant change.

</details>


### [132] [Learning Ecology with VERA Using Conceptual Models and Simulations](https://arxiv.org/abs/2510.16944)
*Spencer Rugaber,Scott Bunin,Andrew Hornback,Sungeun An,Ashok Goel*

Main category: cs.CY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Conceptual modeling has been an important part of constructionist educational
practices for many years, particularly in STEM (Science, Technology,
Engineering and Mathematics) disciplines. What is not so common is using
agent-based simulation to provide students feedback on model quality. This
requires the capability of automatically compiling the concept model into its
simulation. The VERA (Virtual Experimentation Research Assistant) system is a
conceptual modeling tool used since 2016 to provide introductory college
biology students with the capability of conceptual modeling and agent-based
simulation in the ecological domain. This paper describes VERA and its approach
to coupling conceptual modeling and simulation with emphasis on how a model's
visual syntax is compiled into code executable on a NetLogo simulation engine.
Experience with VERA in introductory biology classes at several universities
and through the Smithsonian Institution's Encyclopedia of Life website is
related.

</details>


### [133] [Local News Hijacking: A Review of International Instances](https://arxiv.org/abs/2510.16951)
*Christine Sowa Lepird,Kathleen M. Carley*

Main category: cs.CY

TL;DR: 本文回顾了2007-2024年间美国7起创建虚假地方新闻网站进行信息操纵的案例，分析了这些网站的运作模式，并提出了防范措施。


<details>
  <summary>Details</summary>
Motivation: 随着数字时代发展，创建恶意网站传播虚假信息变得更容易。近年来美国出现了创建虚假地方新闻网站进行信息操纵活动的新现象，需要研究其运作模式并提出应对方案。

Method: 通过分析7个虚假地方新闻网站的案例，研究其运作方式，包括复活已停刊的"僵尸"报纸、在社交媒体分享、使用WordPress模板等共同特征。

Result: 发现这些虚假地方新闻网站具有共同运作模式：复活原有的真实地方新闻机构品牌、利用社交媒体传播、使用WordPress网站模板。这些模式使得虚假网站更具欺骗性。

Conclusion: 基于对这些共同特征的分析，提出了未来减轻此类信息操纵活动发生的具体方法，为防范虚假地方新闻网站提供了解决方案。

Abstract: In the rise of the digital era, it's easier than ever to create nefarious
websites to spread misinformation. A more recent phenomenon in the United
States has been the creation of inauthentic local news websites to further an
information operation campaign. This paper is a review of the 7 instances in
which local news websites were created to influence residents of a region
between 2007 and 2024. By breaking down the ways in which these sites operated,
we discovered commonalities in the approach - resurrecting "zombie" papers that
were previously established authentic local news organizations, sharing these
sites on social media, and using website templates from WordPress. By analyzing
these commonalities, we propose ways to mitigate the occurrence of these
campaigns in the future.

</details>


### [134] [Visibility Allocation Systems: How Algorithmic Design Shapes Online Visibility and Societal Outcomes](https://arxiv.org/abs/2510.17241)
*Stefania Ionescu,Robin Forsberg,Elsa Lichtenegger,Salima Jaoua,Kshitijaa Jaglan,Florian Dorfler,Aniko Hannak*

Main category: cs.CY

TL;DR: 提出了一个用于分析可见性分配系统(VAS)的形式化框架，这些系统决定向用户展示哪些(处理过的)数据，帮助分解系统子流程并支持系统评估。


<details>
  <summary>Details</summary>
Motivation: 当前算法系统复杂、结构不明确且可能产生严重后果，但理解和评估这些系统对研究人员和立法者来说仍然是一个挑战。

Method: 引入形式化框架来定义VAS，回顾典型工具并定义相关计算问题，通过数据流图分解系统子流程，并调查整个流程中的评估指标。

Result: 通过学校选择中的预测推荐案例研究展示了该框架如何支持VAS评估，并讨论了如何支持AI立法工作。

Conclusion: 该框架有助于定位义务、量化系统性风险并实现适应性合规，为VAS的透明度和可评估性提供了重要工具。

Abstract: Throughout application domains, we now rely extensively on algorithmic
systems to engage with ever-expanding datasets of information. Despite their
benefits, these systems are often complex (comprising of many intricate tools,
e.g., moderation, recommender systems, prediction models), of unknown structure
(due to the lack of accompanying documentation), and having hard-to-predict yet
potentially severe downstream consequences (due to the extensive use,
systematic enactment of existing errors, and many comprising feedback loops).
As such, understanding and evaluating these systems as a whole remains a
challenge for both researchers and legislators. To aid ongoing efforts, we
introduce a formal framework for such visibility allocation systems (VASs)
which we define as (semi-)automated systems deciding which (processed) data to
present a human user with. We review typical tools comprising VASs and define
the associated computational problems they solve. By doing so, VASs can be
decomposed into sub-processes and illustrated via data flow diagrams. Moreover,
we survey metrics for evaluating VASs throughout the pipeline, thus aiding
system diagnostics. Using forecasting-based recommendations in school choice as
a case study, we demonstrate how our framework can support VAS evaluation. We
also discuss how our framework can support ongoing AI-legislative efforts to
locate obligations, quantify systemic risks, and enable adaptive compliance.

</details>


### [135] [Quantifying Climate Policy Action and Its Links to Development Outcomes: A Cross-National Data-Driven Analysis](https://arxiv.org/abs/2510.17425)
*Aditi Dutta*

Main category: cs.CY

TL;DR: 开发了一个基于多语言transformer的语言模型来量化分析气候政策导向，将政策文档分类为减缓、适应、灾害风险管理和损失损害四个领域，准确率达0.90 F1分数，并与发展数据关联分析政策效果。


<details>
  <summary>Details</summary>
Motivation: 现有气候政策评估多依赖定性描述或综合指数，难以揭示不同政策领域（减缓、适应、灾害风险管理、损失损害）的具体差异和实际发展影响。

Method: 使用多语言transformer语言模型分析官方国家政策文件，构建气候政策导向的量化指标，并通过面板回归与世行发展数据关联。

Result: 减缓政策与更高GDP和GNI相关；灾害风险管理与更大GNI和债务相关但减少外国直接投资；适应和损失损害政策效果有限。

Conclusion: 该NLP-计量经济学框架提供了可比较的、主题特定的气候治理分析方法，为监测进展、评估权衡和调整政策重点提供了可扩展的方法。

Abstract: Addressing climate change effectively requires more than cataloguing the
number of policies in place; it calls for tools that can reveal their thematic
priorities and their tangible impacts on development outcomes. Existing
assessments often rely on qualitative descriptions or composite indices, which
can mask crucial differences between key domains such as mitigation,
adaptation, disaster risk management, and loss and damage. To bridge this gap,
we develop a quantitative indicator of climate policy orientation by applying a
multilingual transformer-based language model to official national policy
documents, achieving a classification accuracy of 0.90 (F1-score). Linking
these indicators with World Bank development data in panel regressions reveals
that mitigation policies are associated with higher GDP and GNI; disaster risk
management correlates with greater GNI and debt but reduced foreign direct
investment; adaptation and loss and damage show limited measurable effects.
This integrated NLP-econometric framework enables comparable, theme-specific
analysis of climate governance, offering a scalable method to monitor progress,
evaluate trade-offs, and align policy emphasis with development goals.

</details>


### [136] [Mensen aanwijzen maar niet bij naam noemen: behavioural targeting, persoonsgegevens, en de nieuwe Privacyverordening](https://arxiv.org/abs/2510.17710)
*Frederik Zuiderveen Borgesius*

Main category: cs.CY

TL;DR: 本文主张数据保护法应适用于行为定向营销，即使公司未将姓名与个人数据关联，只要使用数据来识别特定个人，就应视为处理个人数据。


<details>
  <summary>Details</summary>
Motivation: 行为定向营销公司声称只要不将姓名与个人数据关联，就不处理个人数据，因此不受数据保护法约束。本文旨在反驳这一观点。

Method: 通过分析行为定向营销的实际运作方式，论证公司经常能够将匿名数据与姓名关联，且姓名并非最实用的标识符。

Result: 论证表明行为定向营销本质上涉及收集个人信息、识别个人并向个人投放广告，存在隐私风险。

Conclusion: 数据保护法应适用于行为定向营销，将用于识别个人的数据视为个人数据符合数据保护法的宗旨：保护公平性和隐私。

Abstract: Information about millions of people is collected for behavioural targeting,
a type of marketing that involves tracking people's online behaviour for
targeted advertising. It is hotly debated whether data protection law applies
to behavioural targeting. Many behavioural targeting companies say that, as
long as they do not tie names to data they hold about individuals, they do not
process any personal data, and that, therefore, data protection law does not
apply to them. European Data Protection Authorities, however, take the view
that a company processes personal data if it uses data to single out a person,
even if it cannot tie a name to these data. This paper argues that data
protection law should indeed apply to behavioural targeting. Companies can
often tie a name to nameless data about individuals. Furthermore, behavioural
targeting relies on collecting information about individuals, singling out
individuals, and targeting ads to individuals. Many privacy risks remain,
regardless of whether companies tie a name to the information they hold about a
person. A name is merely one of the identifiers that can be tied to data about
a person, and it is not even the most practical identifier for behavioural
targeting. Seeing data used to single out a person as personal data fits the
rationale for data protection law: protecting fairness and privacy.

</details>


### [137] [Discrimination, intelligence artificielle et decisions algorithmiques](https://arxiv.org/abs/2510.17711)
*Frederik Zuiderveen Borgesius*

Main category: cs.CY

TL;DR: 该研究探讨了人工智能在决策过程中可能导致的歧视风险，由Frederik Zuiderveen Borgesius教授为欧洲理事会反歧视部门编写


<details>
  <summary>Details</summary>
Motivation: 人工智能虽然为人类带来巨大机遇，但其嵌入和延续偏见与歧视的潜力仍是最紧迫的挑战之一，特别是在民主社会中

Method: 该研究通过分析算法决策和其他类型AI系统的运作机制，识别潜在的歧视风险

Result: 研究详细阐述了AI系统可能导致的歧视问题，并提出了相关风险分析

Conclusion: 需要关注和解决AI系统可能导致的歧视风险，确保AI技术的公平使用

Abstract: Artificial intelligence (AI) has a huge impact on our personal lives and also
on our democratic society as a whole. While AI offers vast opportunities for
the benefit of people, its potential to embed and perpetuate bias and
discrimination remains one of the most pressing challenges deriving from its
increasing use. This new study, which was prepared by Prof. Frederik Zuiderveen
Borgesius for the Anti-discrimination Department of the Council of Europe,
elaborates on the risks of discrimination caused by algorithmic decision-making
and other types of artificial intelligence (AI).

</details>


### [138] [Online Political Microtargeting: Promises and Threats for Democracy](https://arxiv.org/abs/2510.17712)
*Frederik J. Zuiderveen Borgesius,Judith Möller,Sanne Kruikemeier,Ronan Ó Fathaigh,Kristina Irion,Tom Dobber,Balazs Bodo,Claes de Vreese*

Main category: cs.CY

TL;DR: 本文分析了在线政治微定向对民主的承诺与威胁，探讨了其优化竞选匹配和提升参与度的潜力，同时也指出了其可能导致的误导性宣传和隐私问题，并为政策制定者提供了合规的监管建议。


<details>
  <summary>Details</summary>
Motivation: 在线政治微定向在美国广泛使用，欧洲也可能跟进。本文旨在系统评估微定向对民主的潜在影响，包括其积极和消极方面，为政策制定提供参考。

Method: 通过映射分析的方法，系统梳理微定向对民主的承诺（如优化竞选匹配、提升参与度）和威胁（如误导性宣传、隐私侵犯），并探讨在符合欧洲人权公约表达自由权的前提下可能的监管措施。

Result: 研究发现微定向既能增强政治参与和竞选效率，也可能导致政治欺骗和隐私风险。政策制定者需要在保护表达自由的同时，考虑透明度要求、数据保护等措施来监管微定向。

Conclusion: 在线政治微定向是一把双刃剑，既有促进民主的潜力，也有威胁民主的风险。有效的监管需要在保障表达自由的前提下，平衡其利弊，确保微定向的负责任使用。

Abstract: Online political microtargeting involves monitoring people's online
behaviour, and using the collected data, sometimes enriched with other data, to
show people-targeted political advertisements. Online political microtargeting
is widely used in the US; Europe may not be far behind. This paper maps
microtargeting's promises and threats to democracy. For example, microtargeting
promises to optimise the match between the electorate's concerns and political
campaigns, and to boost campaign engagement and political participation. But
online microtargeting could also threaten democracy. For instance, a political
party could, misleadingly, present itself as a different one-issue party to
different individuals. And data collection for microtargeting raises privacy
concerns. We sketch possibilities for policymakers if they seek to regulate
online political microtargeting. We discuss which measures would be possible,
while complying with the right to freedom of expression under the European
Convention on Human Rights.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [139] [Spiking Neural Network for Cross-Market Portfolio Optimization in Financial Markets: A Neuromorphic Computing Approach](https://arxiv.org/abs/2510.15921)
*Amarendra Mohan,Ameer Tamoor Khan,Shuai Li,Xinwei Cao,Zhibin Li*

Main category: q-fin.PM

TL;DR: 本研究提出了一种基于脉冲神经网络(SNN)的跨市场投资组合优化框架，利用神经形态计算原理处理印度和美国股市数据，相比传统人工神经网络具有更好的风险调整收益和计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着金融市场全球化和高频多维数据集的增长，跨市场投资组合优化变得日益复杂。传统人工神经网络计算开销大且缺乏处理大规模多市场数据所需的时间处理能力。

Method: 整合了泄漏积分发放神经元动力学、自适应阈值、脉冲时序依赖可塑性和侧向抑制，采用分层聚类进行降维，基于群体的脉冲编码和多种解码策略，在交易约束下构建投资组合。

Result: 实验评估显示SNN框架相比ANN基准具有更优的风险调整收益和更低的波动性，同时显著提高了计算效率。

Conclusion: 神经形态计算为全球金融市场中可扩展、高效和稳健的投资组合优化展现了良好前景。

Abstract: Cross-market portfolio optimization has become increasingly complex with the
globalization of financial markets and the growth of high-frequency,
multi-dimensional datasets. Traditional artificial neural networks, while
effective in certain portfolio management tasks, often incur substantial
computational overhead and lack the temporal processing capabilities required
for large-scale, multi-market data. This study investigates the application of
Spiking Neural Networks (SNNs) for cross-market portfolio optimization,
leveraging neuromorphic computing principles to process equity data from both
the Indian (Nifty 500) and US (S&P 500) markets. A five-year dataset comprising
approximately 1,250 trading days of daily stock prices was systematically
collected via the Yahoo Finance API. The proposed framework integrates Leaky
Integrate-andFire neuron dynamics with adaptive thresholding,
spike-timingdependent plasticity, and lateral inhibition to enable event-driven
processing of financial time series. Dimensionality reduction is achieved
through hierarchical clustering, while populationbased spike encoding and
multiple decoding strategies support robust portfolio construction under
realistic trading constraints, including cardinality limits, transaction costs,
and adaptive risk aversion. Experimental evaluation demonstrates that the
SNN-based framework delivers superior risk-adjusted returns and reduced
volatility compared to ANN benchmarks, while substantially improving
computational efficiency. These findings highlight the promise of neuromorphic
computation for scalable, efficient, and robust portfolio optimization across
global financial markets.

</details>


### [140] [Aligning Language Models with Investor and Market Behavior for Financial Recommendations](https://arxiv.org/abs/2510.15993)
*Fernando Spadea,Oshani Seneviratne*

Main category: q-fin.PM

TL;DR: FLARKO是一个结合LLM、知识图谱和Kahneman-Tversky优化的金融推荐框架，生成既盈利又符合行为偏好的资产推荐，支持集中式和联邦式架构。


<details>
  <summary>Details</summary>
Motivation: 现有金融推荐系统往往忽略行为因素和监管要求，导致建议与用户偏好不符、难以解释或难以遵循。

Method: 将用户交易历史和资产趋势编码为结构化知识图谱，为LLM提供可解释的上下文，结合KTO进行微调，支持集中式(CenFLARKO)和联邦式(FedFLARKO)架构。

Result: 在FAR-Trans数据集上，FLARKO在行为对齐和联合盈利能力方面持续优于最先进的推荐基线，同时保持可解释性和资源效率。

Conclusion: FLARKO首次将KTO用于金融资产推荐的LLM微调，首次在联邦学习环境中使用结构化知识图谱来支撑LLM对行为金融数据的推理。

Abstract: Most financial recommendation systems often fail to account for key
behavioral and regulatory factors, leading to advice that is misaligned with
user preferences, difficult to interpret, or unlikely to be followed. We
present FLARKO (Financial Language-model for Asset Recommendation with
Knowledge-graph Optimization), a novel framework that integrates Large Language
Models (LLMs), Knowledge Graphs (KGs), and Kahneman-Tversky Optimization (KTO)
to generate asset recommendations that are both profitable and behaviorally
aligned. FLARKO encodes users' transaction histories and asset trends as
structured KGs, providing interpretable and controllable context for the LLM.
To demonstrate the adaptability of our approach, we develop and evaluate both a
centralized architecture (CenFLARKO) and a federated variant (FedFLARKO). To
our knowledge, this is the first demonstration of combining KTO for fine-tuning
of LLMs for financial asset recommendation. We also present the first use of
structured KGs to ground LLM reasoning over behavioral financial data in a
federated learning (FL) setting. Evaluated on the FAR-Trans dataset, FLARKO
consistently outperforms state-of-the-art recommendation baselines on
behavioral alignment and joint profitability, while remaining interpretable and
resource-efficient.

</details>


### [141] [3S-Trader: A Multi-LLM Framework for Adaptive Stock Scoring, Strategy, and Selection in Portfolio Optimization](https://arxiv.org/abs/2510.17393)
*Kefan Chen,Hussain Ahmad,Diksha Goel,Claudia Szabo*

Main category: q-fin.PM

TL;DR: 3S-Trader是一个无需训练的投资组合构建框架，通过评分、策略和选择三个模块，在多股票候选中进行推理和选择，在DJIA成分股上实现了131.83%的累计回报。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法主要关注单股票交易，缺乏多候选推理能力，且无法灵活应对市场变化，限制了在真实交易中的适应性。

Method: 提出3S-Trader框架，包含评分模块（总结股票信号）、策略模块（分析历史策略和市场条件生成优化策略）、选择模块（根据策略选择高评分股票构建投资组合）。

Result: 在四个不同股票集合（包括DJIA成分股和三个行业特定股票集）上评估，3S-Trader在DJIA成分股上获得131.83%累计回报，夏普比率0.31，卡尔玛比率11.84，在其他行业也表现稳定。

Conclusion: 3S-Trader通过模块化设计有效解决了多股票投资组合构建问题，展现了在真实市场环境中的适应性和优异表现。

Abstract: Large Language Models (LLMs) have recently gained popularity in stock trading
for their ability to process multimodal financial data. However, most existing
methods focus on single-stock trading and lack the capacity to reason over
multiple candidates for portfolio construction. Moreover, they typically lack
the flexibility to revise their strategies in response to market shifts,
limiting their adaptability in real-world trading. To address these challenges,
we propose 3S-Trader, a training-free framework that incorporates scoring,
strategy, and selection modules for stock portfolio construction. The scoring
module summarizes each stock's recent signals into a concise report covering
multiple scoring dimensions, enabling efficient comparison across candidates.
The strategy module analyzes historical strategies and overall market
conditions to iteratively generate an optimized selection strategy. Based on
this strategy, the selection module identifies and assembles a portfolio by
choosing stocks with higher scores in relevant dimensions. We evaluate our
framework across four distinct stock universes, including the Dow Jones
Industrial Average (DJIA) constituents and three sector-specific stock sets.
Compared with existing multi-LLM frameworks and time-series-based baselines,
3S-Trader achieves the highest accumulated return of 131.83% on DJIA
constituents with a Sharpe ratio of 0.31 and Calmar ratio of 11.84, while also
delivering consistently strong results across other sectors.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [142] [A Motivational Driver Steering Model: Task Difficulty Homeostasis From Control Theory Perspective](https://arxiv.org/abs/2510.16247)
*H. Mozaffari,A. Nahvi*

Main category: eess.SY

TL;DR: 提出了一种结合心理学和控制的统一方法，用于建模驾驶员避碰转向行为，在20-170km/h速度范围内验证了模型性能，平均误差为7%。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶员模型大多仅基于控制理论，缺乏心理学理论基础。需要建立既通用又符合心理学的碰撞避免驾驶员模型来提高交通安全。

Method: 将心理学中的"任务难度稳态理论"与控制理论中的"李雅普诺夫稳定性方法"相结合，提出统一方法建模驾驶员避碰转向行为。

Result: 通过驾驶模拟器实验验证，模型在20-170km/h速度范围内的两种避碰场景中，平均误差为7%，能准确跟随人类行为。

Conclusion: 结合心理学和控制理论的统一方法能够建立通用且符合心理学的驾驶员模型，在避碰场景中表现出良好性能。

Abstract: A general and psychologically plausible collision avoidance driver model can
improve transportation safety significantly. Most computational driver models
found in the literature have used control theory methods only, and they are not
established based on psychological theories. In this paper, a unified approach
is presented based on concepts taken from psychology and control theory. The
"task difficulty homeostasis theory", a prominent motivational theory, is
combined with the "Lyapunov stability method" in control theory to present a
general and psychologically plausible model. This approach is used to model
driver steering behavior for collision avoidance. The performance of this model
is measured by simulation of two collision avoidance scenarios at a wide range
of speeds from 20 km/h to 170 km/h. The model is validated by experiments on a
driving simulator. The results demonstrate that the model follows human
behavior accurately with a mean error of 7 percent.

</details>


### [143] [Spatial-to-Spectral Harmonic-Modulated Arrays for 6G Multi-Beam MIMO](https://arxiv.org/abs/2510.16262)
*Jose Guajardo,Ali Niknejad*

Main category: eess.SY

TL;DR: 本文介绍了空间-频谱谐波调制阵列(SHAs)，相比传统波束成形阵列，SHAs无需大量硬件复制即可实现并发多波束成形，通过频域复用替代硬件复制，有望成为6G网络的关键技术。


<details>
  <summary>Details</summary>
Motivation: 传统模拟或数字波束成形阵列需要大量硬件复制来实现多波束成形，SHAs旨在通过频域复用技术解决这一问题，为未来6G网络提供可扩展的多用户通信、联合通信感知和空间干扰抑制能力。

Method: 提出空间-频谱谐波调制阵列架构，分析谐波调制波形对增益、噪声和带宽的影响，设计梳状调制波形以最小化频谱效率损失，并量化SHAs独立控制多波束的能力。

Result: 开发了一种新颖的SHA架构，仅需最小硬件复制即可提供三个空间-频谱自由度，实现了独立的多波束控制能力。

Conclusion: SHAs通过频域复用技术有效解决了传统波束成形阵列的硬件复制问题，为6G网络的多用户通信、联合通信感知和干扰抑制提供了可行的技术路径。

Abstract: This article presents an overview and analysis of spatial-to-spectral
harmonic-modulated arrays (SHAs). Compared to traditional analog or digital
beamforming arrays, SHAs enable concurrent multi-beamforming without requiring
substantial hardware replication. SHAs replace the need for hardware
replication with frequency-domain multiplexing. Furthermore, SHAs have the
potential to become key contributors to future 6G networks by enabling scalable
multi-user communications, joint communication and sensing, and spatial
interference mitigation. In addition, an analysis of the SHA's
harmonic-modulation waveform and its effects on gain, noise and bandwidth is
presented. A comb-like modulation waveform for SHAs that minimizes spectral
inefficiency is proposed. Further, an analysis of the SHA's capability to
independently steer multiple beams is presented. This capability is quantified
in terms of the SHA's spatial-to-spectral degrees of freedom. Lastly, this work
introduces a novel SHA architecture that provides three spatial-to-spectral
degrees of freedom with minimal hardware replication.

</details>


### [144] [Towards Smart Manufacturing Metaverse via Digital Twinning in Extended Reality](https://arxiv.org/abs/2510.16280)
*Hui Yang,Faisal Aqlan,Richard Zhao*

Main category: eess.SY

TL;DR: 本文探讨了制造业元宇宙(MfgVerse)的发展，重点关注人工智能、数字孪生和扩展现实等新兴技术在制造业中的应用，以及如何通过学习工厂和认知数字孪生推动以人为本的制造业转型。


<details>
  <summary>Details</summary>
Motivation: 制造业面临数字化人才短缺问题，全球疫情改变了工作方式，迫切需要重新思考数字化平台化，利用新兴技术推动工业向以人为本的制造业元宇宙演进。

Method: 提出制造业元宇宙概念，整合社会网络、制造设备网络、数字孪生网络和辅助网络，开发扩展现实学习工厂进行劳动力培训。

Result: 构建了制造业元宇宙的多重网络架构，展示了扩展现实学习工厂的设计开发，为制造业数字化转型提供了新视角。

Conclusion: 制造业元宇宙是未来制造业发展的关键方向，需要更多深入研究和综合研究来推进相关技术发展，解决面临的挑战和机遇。

Abstract: The rapid evolution of modern manufacturing systems is driven by the
integration of emerging metaverse technologies such as artificial intelligence
(AI), digital twin (DT) with different forms of extended reality (XR) like
virtual reality (VR), augmented reality (AR), and mixed reality (MR). These
advances confront manufacturing workers with complex and evolving environments
that demand digital literacy for problem solving in the future workplace.
However, manufacturing industry faces a critical shortage of skilled workforce
with digital literacy in the world. Further, global pandemic has significantly
changed how people work and collaborate digitally and remotely. There is an
urgent need to rethink digital platformization and leverage emerging
technologies to propel industrial evolution toward human-centered manufacturing
metaverse (MfgVerse). This paper presents a forward-looking perspective on the
development of smart MfgVerse, highlighting current efforts in learning
factory, cognitive digital twinning, and the new sharing economy of
manufacturing-as-a-service (MaaS). MfgVerse is converging into multiplex
networks, including a social network of human stakeholders, an interconnected
network of manufacturing things or agents (e.g., machines, robots, facilities,
material handling systems), a network of digital twins of physical things, as
well as auxiliary networks of sales, supply chain, logistics, and
remanufacturing systems. We also showcase the design and development of a
learning factory for workforce training in extended reality. Finally, future
directions, challenges, and opportunities are discussed for human-centered
manufacturing metaverse. We hope this work helps stimulate more comprehensive
studies and in-depth research efforts to advance MfgVerse technologies.

</details>


### [145] [AC Dynamics-aware Trajectory Optimization with Binary Enforcement for Adaptive UFLS Design](https://arxiv.org/abs/2510.16297)
*Muhammad Hamza Ali,Amritanshu Pandey*

Main category: eess.SY

TL;DR: 提出了一种基于轨迹优化的自适应低频减载方案，考虑完整的交流非线性网络动态，通过松弛二进制变量和同伦方法解决混合整数非线性规划问题，在大型电网中有效恢复频率。


<details>
  <summary>Details</summary>
Motivation: 分布式能源的高渗透率导致传统低频减载方案失效，现有自适应方法无法捕捉交流非线性网络行为，导致继电器在严重扰动时无法恢复系统频率。

Method: 将自适应UFLS问题表述为轨迹优化，包含完整的交流非线性网络动态，通过松弛二进制变量为连续替代变量，将MINLP问题重新表述为NLP序列，使用同伦驱动方法求解接近整数可行的解。

Result: 在多个合成输电系统上评估，该方法可扩展到1500+节点、170k+连续变量和73k+二进制变量的网络，成功恢复二进制可行解并在最坏情况下阻止频率下降。

Conclusion: 所提出的框架能够有效处理大规模电网的自适应低频减载问题，确保交流可行性和时间协调控制，在严重扰动下可靠地恢复系统频率。

Abstract: The high penetration of distributed energy resources, resulting in backfeed
of power at the transmission and distribution interface, is causing
conventional underfrequency load shedding (UFLS) schemes to become
nonconforming. Adaptive schemes that update UFLS relay settings recursively in
time offer a solution, but existing adaptive techniques that obtain UFLS relay
settings with linearized or reduced-order model formulations fail to capture AC
nonlinear network behavior. In practice, this will result in relays unable to
restore system frequency during adverse disturbances. We formulate an adaptive
UFLS problem as a trajectory optimization and include the full AC nonlinear
network dynamics to ensure AC feasibility and time-coordinated control actions.
We include binary decisions to model relay switching action and time-delayed
multi-stage load-shedding. However, this formulation results in an intractable
MINLP problem. To enforce model tractability, we relax these binary variables
into continuous surrogates and reformulate the MINLP as a sequence of NLPs. We
solve the NLPs with a homotopy-driven method that enforces
near-integer-feasible solutions. We evaluate the framework on multiple
synthetic transmission systems and demonstrate that it scales efficiently to
networks exceeding 1500+ nodes with over 170k+ continuous and 73k+ binary
decision variables, while successfully recovering binary-feasible solutions
that arrest the frequency decline during worst-case disturbance.

</details>


### [146] [Supervisory Control of Hybrid Power Plants Using Online Feedback Optimization: Designs and Validations with a Hybrid Co-Simulation Engine](https://arxiv.org/abs/2510.16352)
*Sayak Mukherjee,Himanshu Sharma,Wenceslao Shaw Cortez,Genevieve Starke,Michael Sinner,Brooke J. Stanislawski,Zachary Tully,Paul Fleming,Sonja Glavaski*

Main category: eess.SY

TL;DR: 设计混合发电厂的监督反馈控制器，协调风能、太阳能和电池储能系统以满足电力需求，采用无需详细模型知识的在线反馈优化方法。


<details>
  <summary>Details</summary>
Motivation: 解决混合发电厂中可再生能源和储能系统的协调控制问题，以应对电网运营商设定的发电需求，并在天气不确定性下保持鲁棒性能。

Method: 使用反馈优化方法，基于成本和输出相对于控制输入的梯度信息更新控制指令，调整风能、太阳能和储能系统的有功功率参考值。

Result: 提出的监督控制方法已集成到混合发电厂协同仿真引擎Hercules中，在更真实的仿真场景中证明了其有效性。

Conclusion: 反馈优化方法能够有效协调混合发电厂中的不同能源组件，在模型知识有限的情况下实现鲁棒控制性能。

Abstract: This research investigates designing a supervisory feedback controller for a
hybrid power plant that coordinates the wind, solar, and battery energy storage
plants to meet the desired power demands. We have explored an online feedback
control design that does not require detailed knowledge about the models, known
as feedback optimization. The control inputs are updated using the gradient
information of the cost and the outputs with respect to the input control
commands. This enables us to adjust the active power references of wind, solar,
and storage plants to meet the power generation requirements set by grid
operators. The methodology also ensures robust control performance in the
presence of uncertainties in the weather. In this paper, we focus on describing
the supervisory feedback optimization formulation and control-oriented modeling
for individual renewable and storage components of the hybrid power plant. The
proposed supervisory control has been integrated with the hybrid plant
co-simulation engine, Hercules, demonstrating its effectiveness in more
realistic simulation scenarios.

</details>


### [147] [Real-time Measurement-based Optimization for Distribution System Operation Considering Battery Voltage and Thermal Constraints](https://arxiv.org/abs/2510.16408)
*Sen Zhan,Lingkang Jin,Haoyang Zhang,Nikolaos G. Paterakis*

Main category: eess.SY

TL;DR: 提出了一种基于数据驱动的电池储能运行控制方案，用于解决配电网中分布式能源集成带来的安全运行挑战，通过实时测量构建线性约束，利用Lyapunov优化实现实时、无需预测的低复杂度控制。


<details>
  <summary>Details</summary>
Motivation: 配电网安全运行面临分布式能源集成挑战，电池储能灵活性提供了比发电削减更经济的替代方案，但现有运行模型受到不准确电网模型、负荷数据缺失、非线性关系、跨时段约束和复杂电化学热力学动态等限制。

Method: 基于实时配电网和电池储能测量构建线性和凸二次运行约束，采用Lyapunov优化解耦多时段电池运行，实现实时、无需预测的低计算复杂度控制策略。

Result: 使用非线性配电网和电池储能仿真器的数值研究验证了该方法在确保配电网安全运行和满足电池电压与热约束方面的有效性。

Conclusion: 所提出的数据驱动控制方案能够有效解决配电网中电池储能运行面临的挑战，确保系统安全运行并满足电池约束条件。

Abstract: The secure operation of power distribution systems is challenged by the
growing integration of distributed energy resources. Leveraging the flexibility
of battery storage offers a cost-effective alternative to measures like
generation curtailment, which results in energy losses. However, developing an
effective operational model for battery storage is hindered by inaccurate grid
models, unavailability of load data, nonlinear relationship between power
injections and network states, intertemporal constraints, and complex
electrochemical and thermal dynamics. To address these challenges, this paper
proposes a data-driven operational control scheme for battery storage in
distribution systems. Linear and convex quadratic operational constraints are
constructed based on real-time distribution system and battery storage
measurements. Lyapunov optimization decouples multi-period battery operation,
enabling a real-time, forecast-free control strategy with low computational
complexity. Numerical studies using nonlinear distribution system and battery
storage simulators validate the effectiveness of the approach in ensuring
secure distribution system operation and satisfaction of voltage and thermal
constraints of battery storage.

</details>


### [148] [AoI-Aware Task Offloading and Transmission Optimization for Industrial IoT Networks: A Branching Deep Reinforcement Learning Approach](https://arxiv.org/abs/2510.16414)
*Yuang Chen,Fengqian Guo,Chang Wu,Shuyi Liu,Hancheng Lu,Chang Wen Chen*

Main category: eess.SY

TL;DR: 提出了一种基于年龄信息(AoI)的多基站实时监控框架，通过联合任务卸载和资源分配优化来最小化长期平均AoI，使用创新的分支D3QN算法解决多基站决策空间组合爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 工业物联网中大量数据传输需要满足严格的实时性要求，数据包状态更新的新鲜度对系统性能有重要影响，需要解决多基站决策空间组合爆炸和系统随机动态性带来的挑战。

Method: 首先提出分支D3QN算法实现任务卸载，将动作空间复杂度从指数级降低到线性级；然后通过证明带宽和计算资源Hessian矩阵的半定性，提出资源分配优化方案；最后提出迭代优化算法实现联合优化。

Result: 仿真表明，提出的分支D3QN算法优于现有DRL方法和经典启发式算法，收敛速度提升75%，长期平均AoI降低至少22%。

Conclusion: 该框架有效解决了工业物联网中数据新鲜度优化问题，提出的算法在收敛性能和AoI性能方面均有显著提升。

Abstract: In the Industrial Internet of Things (IIoT), the frequent transmission of
large amounts of data over wireless networks should meet the stringent
timeliness requirements. Particularly, the freshness of packet status updates
has a significant impact on the system performance. In this paper, we propose
an age-of-information (AoI)-aware multi-base station (BS) real-time monitoring
framework to support extensive IIoT deployments. To meet the freshness
requirements of IIoT, we formulate a joint task offloading and resource
allocation optimization problem with the goal of minimizing long-term average
AoI. Tackling the core challenges of combinatorial explosion in multi-BS
decision spaces and the stochastic dynamics of IIoT systems is crucial, as
these factors render traditional optimization methods intractable. Firstly, an
innovative branching-based Dueling Double Deep Q-Network (Branching-D3QN)
algorithm is proposed to effectively implement task offloading, which optimizes
the convergence performance by reducing the action space complexity from
exponential to linear levels. Then, an efficient optimization solution to
resource allocation is proposed by proving the semi-definite property of the
Hessian matrix of bandwidth and computation resources. Finally, we propose an
iterative optimization algorithm for efficient joint task offloading and
resource allocation to achieve optimal average AoI performance. Extensive
simulations demonstrate that our proposed Branching-D3QN algorithm outperforms
both state-of-the-art DRL methods and classical heuristics, achieving up to a
75% enhanced convergence speed and at least a 22% reduction in the long-term
average AoI.

</details>


### [149] [Stabilization of Nonlinear Systems with State-Dependent Representation: From Model-Based to Direct Data-Driven Control](https://arxiv.org/abs/2510.16451)
*Lidong Li,Rui Huang,Lin Zhao*

Main category: eess.SY

TL;DR: 提出了一种稳定状态依赖非线性系统的新框架，通过状态依赖参数变化模型和LMI合成控制器，保证局部指数稳定性、鲁棒性和吸引域估计，并扩展到直接数据驱动设置。


<details>
  <summary>Details</summary>
Motivation: 为非线性系统提供严格的端到端稳定性、鲁棒性和安全性保证，直接从有限数据出发，无需显式模型辨识。

Method: 将非线性动力学重新表述为状态依赖参数变化模型，通过可处理的线性矩阵不等式离线合成稳定控制器，并扩展到直接数据驱动设置，利用Petersen引理推导数据依赖的LMI。

Result: 数值和物理实验结果表明，该方法直接从有限数据实现了稳定性、鲁棒性和安全性的严格端到端保证。

Conclusion: 该框架为非线性系统提供了直接从数据出发的稳定性和鲁棒性保证，无需显式模型辨识，具有实际应用价值。

Abstract: This paper presents a novel framework for stabilizing nonlinear systems
represented in state-dependent form. We first reformulate the nonlinear
dynamics as a state-dependent parameter-varying model and synthesize a
stabilizing controller offline via tractable linear matrix inequalities (LMIs).
The resulting controller guarantees local exponential stability, maintains
robustness against disturbances, and provides an estimate of the region of
attraction under input saturation. We then extend the formulation to the direct
data-driven setting, where a known library of basis functions represents the
dynamics with unknown coefficients consistent with noisy experimental data. By
leveraging Petersen's lemma, we derive data-dependent LMIs that ensure
stability and robustness for all systems compatible with the data. Numerical
and physical experimental results validate that our approach achieves rigorous
end-to-end guarantees on stability, robustness, and safety directly from finite
data without explicit model identification.

</details>


### [150] [Small-Signal Stability Analysis of Power Systems by Implicit Multilinear Models](https://arxiv.org/abs/2510.16534)
*Christoph Kaufmann,Georg Pangalos,Gerwald Lichtenberg,Oriol Gomis-Bellmunt*

Main category: eess.SY

TL;DR: 提出基于隐式多线性模型线性化的小信号稳定性分析方法，通过张量表示电网换流器，相比传统方法能更快进行线性化。


<details>
  <summary>Details</summary>
Motivation: 传统小信号稳定性分析方法在处理包含三角函数等复杂非线性关系的电力系统模型时效率较低，需要更高效的线性化方法。

Method: 使用隐式多线性模型描述系统动态，通过变量变换表示三角函数，基于广义特征值进行稳定性分析，并与非线性模型进行时域仿真对比。

Result: 在3节点网络测试中，隐式多线性模型与非线性模型的时域仿真结果一致，且张量分解表示相比MATLAB Simulink传统方法能更快完成线性化。

Conclusion: 隐式多线性模型为电力系统小信号稳定性分析提供了高效的线性化方法，特别适用于包含复杂非线性关系的系统建模。

Abstract: This paper proposes a new approach to perform small-signal stability analysis
based on linearization of implicit multilinear models. Multilinear models
describe the system dynamics by multilinear functions of state, input, and
algebraic variables. Using suitable transformations of variables, they can also
represent trigonometric functions, which often occur in power systems modeling.
This allows tensor representations of grid-following and grid-forming power
converters. This paper introduces small-signal stability analysis of
equilibrium points based on implicit multilinear models using generalized
eigenvalues. The generalized eigenvalues are computed from linear descriptor
models of the linearized implicit multilinear model. The proposed approach is
tested using a 3-bus network example, first by comparing time-domain
simulations of the implicit multilinear model with those of the nonlinear
model, and second by comparing the generalized eigenvalues with those of the
linearized nonlinear model. The results show that the decomposed tensor
representation of the implicit multilinear model allows for a faster
linearization compared to conventional methods in MATLAB Simulink.

</details>


### [151] [SMP-RCR: A Sparse Multipoint Moment Matching Method for RC Reduction](https://arxiv.org/abs/2510.16550)
*Siyuan Yin,Yuncheng Xu,Lin Liu,Fan Yang,Xuan Zeng,Chengtao An,Yangfeng Su*

Main category: eess.SY

TL;DR: 提出了一种稀疏多点矩匹配方法，用于多端口RC电路模型降阶，在保持精度的同时显著提高了计算效率


<details>
  <summary>Details</summary>
Motivation: 现有高精度矩匹配方法（如PRIMA、TurboMOR）在多端口情况下会产生密集降阶系统，影响效率；而基于高斯消元的方法（如SIP）高阶矩匹配不足

Method: 稀疏多点矩匹配方法，结合稀疏控制和紧缩技术优化算法，提供多频点高阶矩匹配的理论分析

Result: 相比SIP方法，在高频点精度提高两个数量级；相比TurboMOR方法，速度提升两倍以上且保持相同精度

Conclusion: 该方法在多端口RC电路模型降阶中实现了精度和效率的良好平衡

Abstract: In post--layout circuit simulation, efficient model order reduction (MOR) for
many--port resistor--capacitor (RC) circuits remains a crucial issue. The
current mainstream MOR methods for such circuits include high--order moment
matching methods and elimination methods. High-order moment matching
methods--characterized by high accuracy, such as PRIMA and TurboMOR--tend to
generate large dense reduced-order systems when the number of ports is large,
which impairs the efficiency of MOR. Another common type of MOR method for
many--port circuits is based on Gaussian elimination, with the SIP method as a
representative. The main limitation of this method lies in the inadequate
matching of high--order moments. In this paper, we propose a sparse multipoint
moment matching method and present comprehensive theoretical analysis results
regarding the multi--frequency high--order moment matching property. Meanwhile,
to enhance the algorithm's efficiency, sparse control and deflation techniques
are introduced to further optimize the algorithm. Numerical experiments
demonstrated that, compared to SIP, the accuracy is improved by more than two
orders of magnitude at high frequency points without adding many extra linear
components. Compared to TurboMOR methods, our method achieves a speed
improvement of more than twice while maintaining the same level of precision.

</details>


### [152] [Linear State Estimation in Presence of Bounded Uncertainties: A Comparative Analysis](https://arxiv.org/abs/2510.16693)
*Ayan Das,Anushka Sharma,Anamitra Pal*

Main category: eess.SY

TL;DR: 本文研究了电力系统线性状态估计中处理数据和模型不确定性的三种方法，重点解决了线路参数变化带来的模型扰动问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注数据不确定性，但对模型扰动（特别是线路参数变化）的处理关注较少。由于实际线路参数可能与数据库中的值不同，需要研究在数据和模型都存在有界不确定性时的状态估计方法。

Method: 提出了三种算法：基于区间算术的方法、基于凸优化的方法、基于广义线性分式规划的方法。这些方法用于处理线性状态估计中的数据和模型不确定性。

Result: 在多个IEEE测试系统上的实验表明，前两种算法速度极快且结果符合预期，而第三种算法存在可扩展性问题，不适用于线性状态估计。

Conclusion: 基于区间算术和凸优化的方法在速度和准确性方面表现良好，而基于广义线性分式规划的方法由于可扩展性问题不适合线性状态估计应用。

Abstract: A variety of algorithms have been proposed to address the power system state
estimation problem in the presence of uncertainties in the data. However, less
emphasis has been given to handling perturbations in the model. In the context
of linear state estimation (LSE), which is the focus of this paper,
perturbations in the model come from variations in the line parameters. Since
the actual values of the line parameters can be different from the values
stored in a power utility's database, we investigate three approaches in this
paper to estimate the states in the presence of bounded uncertainties in the
data and the model. The first approach is based on interval arithmetic, the
second is based on convex optimization, and the third is based on generalized
linear fractional programming. The three algorithms are applied to multiple
IEEE test systems and compared in terms of their speed and accuracy. The
results indicate that the first two algorithms are extremely fast and give
expected results, while the third suffers from scalability issues and is
unsuitable for LSE.

</details>


### [153] [A Control-Theoretic Approach to Dynamic Payment Routing for Success Rate Optimization](https://arxiv.org/abs/2510.16735)
*Aniket Agrawal,Harsharanga Patil*

Main category: eess.SY

TL;DR: 提出基于控制理论的动态支付路由框架，通过闭环反馈控制器实时感知网关性能并动态路由交易，将交易成功率提升1.15%


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的路由系统缺乏动态适应性，无法有效应对网关性能波动，需要更智能的自调节路由机制来提升交易成功率

Method: 结合控制理论、强化学习和多臂老虎机优化，采用广义反馈自适应机制而非显式PID调节，使纠正动作与性能偏差成比例，网关评分逐渐收敛到成功率

Result: 生产环境测试显示，相比传统规则路由，成功率提升最高达1.15%，证明了反馈控制在支付系统中的有效性

Conclusion: 该混合方法统一了控制理论和自适应决策系统，实现了自调节交易路由，抑制了不稳定性并提高了可靠性

Abstract: This paper introduces a control-theoretic framework for dynamic payment
routing, implemented within JUSPAY's Payment Orchestrator to maximize
transaction success rate. The routing system is modeled as a closed-loop
feedback controller continuously sensing gateway performance, computing
corrective actions, and dynamically routes transactions across gateway to
ensure operational resilience. The system leverages concepts from control
theory, reinforcement learning, and multi-armed bandit optimization to achieve
both short-term responsiveness and long-term stability. Rather than relying on
explicit PID regulation, the framework applies generalized feedback-based
adaptation, ensuring that corrective actions remain proportional to observed
performance deviations and the computed gateway score gradually converges
toward the success rate. This hybrid approach unifies control theory and
adaptive decision systems, enabling self-regulating transaction routing that
dampens instability, and improves reliability. Live production results show an
improvement of up to 1.15% in success rate over traditional rule-based routing,
demonstrating the effectiveness of feedback-based control in payment systems.

</details>


### [154] [Safe Payload Transfer with Ship-Mounted Cranes: A Robust Model Predictive Control Approach](https://arxiv.org/abs/2510.16953)
*Ersin Das,William A. Welch,Patrick Spieler,Keenan Albee,Aurelio Noca,Jeffrey Edlund,Jonathan Becktor,Thomas Touma,Jessica Todd,Sriramya Bhamidipati,Stella Kombo,Maira Saboia,Anna Sabel,Grace Lim,Rohan Thakker,Amir Rahmani,Joel W. Burdick*

Main category: eess.SY

TL;DR: 提出了一种用于船载起重机在恶劣海况下的鲁棒安全模型预测控制框架，通过零阶控制屏障函数确保载荷安全定位，并采用时变边界框进行避障。


<details>
  <summary>Details</summary>
Motivation: 船载起重机在非结构化运输环境中面临多重安全约束和外部扰动挑战，需要同时保证安全性和有效载荷传输性能。

Method: 使用基于鲁棒零阶控制屏障函数的安全约束非线性MPC，结合时变边界框避障，并引入在线鲁棒性参数自适应方案。

Result: 在起重机原型上的实验表明，该方法在显著基座扰动下具有良好的整体性能。

Conclusion: 该方法不仅适用于起重机载荷传输，还可推广到安全机器人辅助部件配合和插入应用。

Abstract: Ensuring safe real-time control of ship-mounted cranes in unstructured
transportation environments requires handling multiple safety constraints while
maintaining effective payload transfer performance. Unlike traditional crane
systems, ship-mounted cranes are consistently subjected to significant external
disturbances affecting underactuated crane dynamics due to the ship's dynamic
motion response to harsh sea conditions, which can lead to robustness issues.
To tackle these challenges, we propose a robust and safe model predictive
control (MPC) framework and demonstrate it on a 5-DOF crane system, where a
Stewart platform simulates the external disturbances that ocean surface motions
would have on the supporting ship. The crane payload transfer operation must
avoid obstacles and accurately place the payload within a designated target
area. We use a robust zero-order control barrier function (R-ZOCBF)-based
safety constraint in the nonlinear MPC to ensure safe payload positioning,
while time-varying bounding boxes are utilized for collision avoidance. We
introduce a new optimization-based online robustness parameter adaptation
scheme to reduce the conservativeness of R-ZOCBFs. Experimental trials on a
crane prototype demonstrate the overall performance of our safe control
approach under significant perturbing motions of the crane base. While our
focus is on crane-facilitated transfer, the methods more generally apply to
safe robotically-assisted parts mating and parts insertion.

</details>


### [155] [Differentiating Through Power Flow Solutions for Admittance and Topology Control](https://arxiv.org/abs/2510.17071)
*Samuel Talkington,Daniel Turizo,Sergio A. Dorado-Rojas,Rahul K. Gupta,Daniel K. Molzahn*

Main category: eess.SY

TL;DR: 该论文提出了一种基于隐函数定理的潮流方程线性化方法，用于分析网络导纳参数变化对系统状态的影响。


<details>
  <summary>Details</summary>
Motivation: 电力系统中网络导纳参数的控制、优化和估计对多个研究方向至关重要，需要一种有效的方法来分析导纳参数变化对系统的影响。

Method: 利用隐函数定理对潮流方程进行隐式微分，推导出关于网络导纳参数的线性化表达式，适用于辐射状或网状电网。

Result: 该方法能够计算复杂电压、线路电流和功率流的灵敏度，无需重新求解潮流方程即可预测网络拓扑变化时的节点电压。

Conclusion: 所提出的线性化理论在连续导纳控制等应用中具有重要价值，可用于提高配电网的承载能力。

Abstract: The power flow equations relate bus voltage phasors to power injections via
the network admittance matrix. These equations are central to the key
operational and protection functions of power systems (e.g., optimal power flow
scheduling and control, state estimation, protection, and fault location, among
others). As control, optimization, and estimation of network admittance
parameters are central to multiple avenues of research in electric power
systems, we propose a linearization of power flow solutions obtained by
implicitly differentiating them with respect to the network admittance
parameters. This is achieved by utilizing the implicit function theorem, in
which we show that such a differentiation is guaranteed to exist under mild
conditions and is applicable to generic power systems (radial or meshed). The
proposed theory is applied to derive sensitivities of complex voltages, line
currents, and power flows. The developed theory of linearizing the power flow
equations around changes in the complex network admittance parameters has
numerous applications. We demonstrate several of these applications, such as
predicting the nodal voltages when the network topology changes without solving
the power flow equations. We showcase the application for continuous admittance
control, which is used to increase the hosting capacity of a given distribution
network.

</details>


### [156] [Semantic Intelligence: A Bio-Inspired Cognitive Framework for Embodied Agents](https://arxiv.org/abs/2510.17129)
*Wenbing Tang,Meilin Zhu,Fenghua Wu,Yang Liu*

Main category: eess.SY

TL;DR: 提出了语义智能驱动的具身代理框架SIDE，通过分层语义认知架构和语义驱动决策过程，使代理能够在物理世界中进行上下文自适应推理和交互。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型主要在数字环境中运行，缺乏与物理世界的交互。具身人工智能代理在非结构化现实环境中面临语义智能不足的挑战，无法有效理解和推理复杂任务。

Method: 引入SIDE框架，整合分层语义认知架构与语义驱动决策过程，受生物认知机制启发，采用仿生原理设计语义认知架构，模拟人类和动物整合处理感官信息的方式。

Result: 开发了一个能够进行上下文自适应推理和物理世界交互的具身代理框架，为开发更智能、多功能的具身代理迈出了重要一步。

Conclusion: SIDE框架通过整合语义智能和具身交互，为解决当前具身代理在现实环境中的语义理解不足问题提供了有效方案，是开发更智能具身代理的重要进展。

Abstract: Recent advancements in Large Language Models (LLMs) have greatly enhanced
natural language understanding and content generation. However, these models
primarily operate in disembodied digital environments and lack interaction with
the physical world. To address this limitation, Embodied Artificial
Intelligence (EAI) has emerged, focusing on agents that can perceive and
interact with their surroundings. Despite progress, current embodied agents
face challenges in unstructured real-world environments due to insufficient
semantic intelligence, which is critical for understanding and reasoning about
complex tasks. This paper introduces the Semantic Intelligence-Driven Embodied
(SIDE) agent framework, which integrates a hierarchical semantic cognition
architecture with a semantic-driven decision-making process. This enables
agents to reason about and interact with the physical world in a contextually
adaptive manner. The framework is inspired by biological cognitive mechanisms
and utilizes bio-inspired principles to design a semantic cognitive
architecture that mimics how humans and animals integrate and process sensory
information. We present this framework as a step toward developing more
intelligent and versatile embodied agents.

</details>


### [157] [A Data-Driven Framework for Online Mitigation of False Data Injection Signals in Networked Control Systems](https://arxiv.org/abs/2510.17155)
*Mohammadamin Lari*

Main category: eess.SY

TL;DR: 提出了一种两阶段框架，用于在线缓解网络控制系统中的虚假数据注入信号，通过元学习选择基础时间序列预测模型，并使用小波变换和卷积神经网络来区分不同复杂度的时间序列数据。


<details>
  <summary>Details</summary>
Motivation: 提高网络控制系统在面对恶意活动时的弹性，确保系统安全运行，解决虚假数据注入等安全挑战。

Method: 第一阶段使用元学习在堆叠集成学习架构中选择基础时间序列预测模型，通过连续小波变换将时间序列数据转换为尺度图，分割成图像帧生成尺度-时间表示，并使用卷积神经网络基于熵度量区分不同复杂度的时间序列数据；第二阶段实时缓解虚假数据注入信号。

Result: 通过差速驱动移动机器人编队控制的严格仿真验证了该框架的有效性。

Conclusion: 该框架为解决网络控制系统中的安全问题提供了一种有前景的方法，能够维护系统完整性并确保操作安全。

Abstract: This paper introduces a novel two-stage framework for online mitigation of
False Data Injection (FDI) signals to improve the resiliency of Networked
Control Systems (NCSs) and ensure their safe operation in the presence of
malicious activities. The first stage involves meta learning to select a base
time series forecasting model within a stacked ensemble learning architecture.
This is achieved by converting time series data into scalograms using
continuous wavelet transform, which are then split into image frames to
generate a scalo-temporal representation of the data and to distinguish between
different complexity levels of time series data based on an entropy metric
using a convolutional neural network. In the second stage, the selected model
mitigates false data injection signals in real-time. The proposed framework's
effectiveness is demonstrated through rigorous simulations involving the
formation control of differential drive mobile robots. By addressing the
security challenges in NCSs, this framework offers a promising approach to
maintaining system integrity and ensuring operational safety.

</details>


### [158] [Generalized Group Selection Strategies for Self-sustainable RIS-aided Communication](https://arxiv.org/abs/2510.17176)
*Lakshmikanta Sau,Priyadarshi Mukherjee,Sasthi C. Ghosh*

Main category: eess.SY

TL;DR: 本文研究了在空间相关信道下，基于分组的自可持续RIS辅助D2D通信中的多种分组选择策略，分析了功率分配和时间切换配置下的系统性能，并提出了系统参数选择的适当界限。


<details>
  <summary>Details</summary>
Motivation: 可重构智能表面(RIS)是超越第五代无线通信网络的前沿技术，本研究旨在探索在自可持续RIS辅助D2D通信中，通过优化分组选择策略来提升系统性能。

Method: 采用功率分配(PS)和时间切换(TS)两种配置，考虑线性和非线性能量收集模型，提出基于端到端信噪比和能量收集的分组选择策略，使用高阶统计工具推导中断概率的解析表达式，并应用极值理论分析渐近场景。

Result: 数值结果表明所提方法在数据吞吐量、数据和能量中断性能等指标方面具有重要优势和效益，极值理论分析为大规模智能表面辅助无线通信提供了有价值的见解。

Conclusion: 研究证明了在自可持续RIS辅助D2D通信中，适当的分组选择策略能够显著提升系统性能，特别是在大规模RIS场景下，所提出的分析方法具有重要的实际应用价值。

Abstract: Reconfigurable intelligent surface (RIS) is a cutting-edge communication
technology that has been proposed as aviable option for beyond fifth-generation
wireless communication networks. This paper investigates various group
selection strategies in the context of grouping-based self-sustainable
RIS-aided device-to-device (D2D) communication with spatially correlated
wireless channels. Specifically, we consider both power splitting (PS) and time
switching (TS) configurations, of the self-sustainable RIS to analyze the
system performance and propose appropriate bounds on the choice of system
parameters. The analysis takes into account a simplified linear energy
harvesting (EH) model as well as a practical non-linear EH model. Based on the
application requirements, we propose various group selection strategies at the
RIS. Notably, each strategy schedules the k-th best available group at the RIS
based on the end-to-end signal-to-noise ratio (SNR) and also the energy
harvested at a particular group of the RIS. Accordingly, by using tools from
high order statistics, we derive analytical expressions for the outage
probability of each selection strategy. Moreover, by applying the tools from
extreme value theory, we also investigate an asymptotic scenario, where the
number of groups available for selection at an RIS approaches infinity. The
nontrivial insights obtained from this approach is especially beneficial in
applications like large intelligent surface-aided wireless communication.
Finally, the numerical results demonstrate the importance and benefits of the
proposed approaches in terms of metrics such as the data throughput and the
outage (both data and energy) performance.

</details>


### [159] [Enhanced Ground-Satellite Direct Access via Onboard Rydberg Atomic Quantum Receivers](https://arxiv.org/abs/2510.17290)
*Qihao Peng,Tierui Gong,Zihang Song,Qu Luo,Zihuai Lin,Pei Xiao,Chau Yuen*

Main category: eess.SY

TL;DR: 提出了一种用于6G卫星网络的Rydberg原子量子接收器(RAQR)，通过原子电磁诱导透明将射频场转换为光信号，解决了传统射频前端面临的路径损耗、尺寸重量功率限制和频谱拥塞问题。


<details>
  <summary>Details</summary>
Motivation: 6G网络的地面-卫星链路面临严重路径损耗、严格的尺寸重量功率限制和频谱拥塞等关键挑战，这些因素显著阻碍了传统射频前端的性能。

Method: 采用毫米级前端设计，通过原子电磁诱导透明将射频场转换为光信号，结合混合原子-电子设计和支持信号模型。

Result: RAQR具有高灵敏度和高频选择性，相对于传统射频接收器，在数据速率、覆盖范围和传感精度方面都有所提升，同时满足空间约束。

Conclusion: 文章最后提出了集成策略、分布式卫星概念和开放研究问题，旨在将RAQR支持的卫星有效载荷投入实际应用。

Abstract: Ground-satellite links for 6G networks face critical challenges, including
severe path loss, tight size-weight-power limits, and congested spectrum, all
of which significantly hinder the performance of traditional radio frequency
(RF) front ends. This article introduces the Rydberg Atomic Quantum Receiver
(RAQR) for onboard satellite systems, a millimeter-scale front end that
converts radio fields to optical signals through atomic electromagnetically
induced transparency. RAQR's high sensitivity and high frequency selectivity
address link budget, payload, and interference challenges while fitting within
space constraints. A hybrid atomic-electronic design and supporting signal
model demonstrate enhanced data rate, coverage, and sensing accuracy relative
to conventional RF receivers. The article concludes with integration
strategies, distributed-satellite concepts, and open research problems for
bringing RAQR-enabled satellite payloads into service.

</details>


### [160] [Comparison and performance analysis of dynamic encrypted control approaches](https://arxiv.org/abs/2510.17333)
*Sebastian Schlor,Frank Allgöwer*

Main category: eess.SY

TL;DR: 本文回顾了动态加密控制的最新方法，包括自举、控制器状态周期性重置、整数重构和FIR控制器，并进行了稳定性和性能分析，通过基准系统进行了数值性能比较。


<details>
  <summary>Details</summary>
Motivation: 使用同态加密的加密控制器已被证明可以在调节系统的同时保证测量和控制信号以及系统和控制器参数的隐私。然而，由于编码中不断增长的噪声和溢出问题，加密动态控制器仍然是一个挑战。

Method: 回顾了动态加密控制的几种方法：自举、控制器状态周期性重置、整数重构和FIR控制器，并为这些方法配备了稳定性和性能分析框架。

Result: 通过基准系统的数值性能比较，评估了这些动态加密控制方法的适用性。

Conclusion: 本文为动态加密控制提供了系统性的分析和比较框架，有助于评估不同方法在保证隐私的同时维持系统稳定性和性能的能力。

Abstract: Encrypted controllers using homomorphic encryption have proven to guarantee
the privacy of measurement and control signals, as well as system and
controller parameters, while regulating the system as intended. However,
encrypting dynamic controllers has remained a challenge due to growing noise
and overflow issues in the encoding. In this paper, we review recent approaches
to dynamic encrypted control, such as bootstrapping, periodic resets of the
controller state, integer reformulations, and FIR controllers, and equip them
with a stability and performance analysis to evaluate their suitability. We
complement the analysis with a numerical performance comparison on a benchmark
system.

</details>


### [161] [Accelerating Adaptive Systems via Normalized Parameter Estimation Laws](https://arxiv.org/abs/2510.17371)
*Mohammad Boveiri,Mohammad Khosravi,Peyman Mohajerin Esfahan*

Main category: eess.SY

TL;DR: 提出了一种新的自适应系统参数估计方法——归一化参数估计律，能加速系统状态收敛到原点，保证系统状态的2/r次幂范数有限可积，其中r≥1可任意大，相比传统方法只保证平方范数可积（r=1）有显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统Lyapunov基参数估计方法只能保证系统状态平方范数的可积性，收敛速度较慢。新方法旨在通过归一化参数估计律加速系统状态收敛，并引入时间域稀疏性促进机制来惩罚信号持续时间过长和衰减缓慢的问题。

Method: 提出归一化参数估计律，不依赖时变或高适应增益，不需要持续激励条件。该方法适用于匹配和非匹配不确定系统，只要存在控制Lyapunov函数即可应用，且与任何基于CLF的确定性等价控制器兼容。还开发了包含动量的高阶扩展版本。

Result: 新方法保证系统状态‖x(t)‖₂^(2/r) ∈ L₁，其中r≥1可任意大，相比传统方法只保证‖x(t)‖₂² ∈ L₁（r=1）有显著改进。数值实验验证了性能提升。

Conclusion: 归一化参数估计律提供了一种有效加速自适应系统状态收敛的方法，具有广泛的适用性，不依赖苛刻条件，且能通过高阶扩展进一步提升性能。

Abstract: In this paper, we propose a new class of parameter estimation laws for
adaptive systems, called \emph{normalized parameter estimation laws}. A key
feature of these estimation laws is that they accelerate the convergence of the
system state, $\mathit{x(t)}$, to the origin. We quantify this improvement by
showing that our estimation laws guarantee finite integrability of the
$\mathit{r}$-th root of the squared norm of the system state, i.e., \(
\mathit{\|x(t)\|}_2^{2/\mathit{r}} \in \mathcal{L}_1, \) where $\mathit{r} \geq
1$ is a pre-specified parameter that, for a broad class of systems, can be
chosen arbitrarily large. In contrast, standard Lyapunov-based estimation laws
only guarantee integrability of $\mathit{\|x(t)\|}_2^2$ (i.e., $\mathit{r} =
1$). We motivate our method by showing that, for large values of $r$, this
guarantee serves as a sparsity-promoting mechanism in the time domain, meaning
that it penalizes prolonged signal duration and slow decay, thereby promoting
faster convergence of $\mathit{x(t)}$. The proposed estimation laws do not rely
on time-varying or high adaptation gains and do not require persistent
excitation. Moreover, they can be applied to systems with matched and unmatched
uncertainties, regardless of their dynamic structure, as long as a control
Lyapunov function (CLF) exists. Finally, they are compatible with any CLF-based
certainty equivalence controllers. We further develop higher-order extensions
of our estimation laws by incorporating momentum into the estimation dynamics.
We illustrate the performance improvements achieved with the proposed scheme
through various numerical experiments.

</details>


### [162] [Artificial magnetic conductor backed dual-mode sectoral cylindrical DRA for off-body biomedical telemetry](https://arxiv.org/abs/2510.17619)
*Nayab Gogosh,Sohail Khalid,Bilal Tariq Malik,Slawomir Koziel*

Main category: eess.SY

TL;DR: 本研究提出一种用于生物医学遥测的扇形圆柱介质谐振器天线，通过双模操作和AMC表面设计，解决了传统CDRA带宽有限和尺寸过大的问题，实现了小型化、低SAR和高增益的性能。


<details>
  <summary>Details</summary>
Motivation: 传统圆柱介质谐振器天线虽然具有低损耗、坚固和稳定等优点，但其有限的带宽和较大的尺寸使其不适合可穿戴设备应用。本研究旨在解决这些限制，开发适用于生物医学遥测的小型化天线。

Method: 采用扇形CDRA设计（四分之一段结构），使用完美电导体边界减小尺寸；设计双模天线（EH110和TE210模式）；应用人工磁导体表面降低SAR；通过数学推导支持场分量设计。

Result: 天线在5.2-5.9 GHz频段实现0.7 GHz带宽，峰值增益7.9 dBi，应用于人体手臂时的SAR值为1.24 W/kg，适合生物医学应用。

Conclusion: 扇形CDRA结合AMC表面成功实现了小型化、宽带和高性能的生物医学遥测天线，解决了传统CDRA在可穿戴设备应用中的局限性。

Abstract: This research investigates the potential of a sectoral Cylindrical Dielectric
Resonator Antenna (CDRA) for biomedical telemetry. CDRAs are known for their
low loss, ruggedness, and stability, but their limited bandwidth and size make
them unsuitable for wearable devices. The research addresses these limitations
by proposing a dual mode antenna that operates in EH110 and TE210 modes. The
sectoral CDRA is a quarter segment with Perfect Electric Conductor boundaries,
reducing its size by a factor of four. Mathematical derivations of the field
components for both modes are derived to support the design. To minimize
specific absorption rate (SAR), an Artificial Magnetic Conductor (AMC) surface
is applied to the antennas backside, enhancing compatibility with the
transverse electric modes. The antenna achieves a bandwidth of 0.7 GHz (5.2-5.9
GHz), suitable for biomedical applications, with a measured peak gain of 7.9
dBi and a SAR of 1.24 W/kg when applied to a human arm.

</details>


### [163] [Trajectory Optimization for Minimum Threat Exposure using Physics-Informed Neural Networks](https://arxiv.org/abs/2510.17762)
*Alexandra E. Ballentine,Raghvendra V. Cowlagi*

Main category: eess.SY

TL;DR: 使用物理信息神经网络(PINN)求解由庞特里亚金极小值原理产生的两点边值问题，解决传统打靶法对初值高度敏感的问题，应用于车辆运动学模型的最小威胁暴露轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 传统打靶法求解最优控制问题中的两点边值问题时，对初始猜测极为敏感，数值求解困难。而PINN在求解高维微分方程方面已取得显著成功，因此尝试将其应用于此类问题。

Method: 开发两种PINN：一种是针对给定初始和终端状态的BVP求解器；另一种是仅以初始状态为条件的PINN，无需为每个初始状态重新训练。

Result: PINN输出满足必要条件且数值误差较低，验证了方法的有效性。

Conclusion: PINN能够有效求解最优控制中的两点边值问题，克服了传统方法的敏感性限制，为复杂威胁环境下的轨迹规划提供了可行方案。

Abstract: We apply a physics-informed neural network (PINN) to solve the two-point
boundary value problem (BVP) arising from the necessary conditions postulated
by Pontryagin's Minimum Principle for optimal control. Such BVPs are known to
be numerically difficult to solve by traditional shooting methods due to
extremely high sensitivity to initial guesses. In the light of recent successes
in applying PINNs for solving high-dimensional differential equations, we
develop a PINN to solve the problem of finding trajectories with minimum
exposure to a spatiotemporal threat for a vehicle kinematic model. First, we
implement PINNs that are trained to solve the BVP for a given pair of initial
and final states for a given threat field. Next, we implement a PINN
conditioned on the initial state for a given threat field, which eliminates the
need for retraining for each initial state. We demonstrate that the PINN
outputs satisfy the necessary conditions with low numerical error.

</details>


### [164] [Data-driven Communication and Control Design for Distributed Frequency Regulation with Black-box Inverters](https://arxiv.org/abs/2510.17769)
*Michael Nestor,Jiaxin Wang,Ning Zhang,Fei Teng*

Main category: eess.SY

TL;DR: 提出了一种用于逆变器二次频率控制的分布式数据驱动方法，通过点对点通信避免中央控制中心，并提供了通信拓扑设计框架以平衡通信需求与控制性能。


<details>
  <summary>Details</summary>
Motivation: 随着基于逆变器的资源在电网中渗透率增加，且通常只有黑盒模型可用，这挑战了传统的频率控制方法。现有工作大多采用去中心化方法，缺乏通过通信的在线设备协调。

Method: 开发了分布式数据驱动方法，利用逆变器间的点对点通信；提出了通信拓扑设计框架来指导二次频率调节；设计了基于通信拓扑结构的控制器，并保证闭环稳定性。

Result: 在IEEE 39总线系统上的案例研究验证了该框架，并展示了通信需求与控制性能之间的权衡关系。

Conclusion: 该方法通过分布式控制和通信拓扑优化，有效解决了逆变器二次频率控制问题，在保证稳定性的同时实现了通信需求与控制性能的平衡。

Abstract: The increasing penetration of inverter-based resources into the power grid,
with often only black-box models available, challenges long-standing frequency
control methods. Most recent works take a decentralized approach without online
device coordination via communication. This paper considers both dynamic
behavior and communication within secondary frequency control on an
intermediate timescale. We develop a distributed data-driven approach that
utilizes peer-to-peer communication between inverters to avoid the need for a
central control center. To enable a trade off between communication network
requirements and control performance, we present a framework to guide
communication topology design for secondary frequency regulation. Following
design of the inter-agent information exchange scheme, we design a controller
that is structured according to the communication topology with a closed-loop
stability guarantee. Case studies on the IEEE 39-bus system validate the
framework and illustrate the trade-off between communication requirements and
control performance that is enabled by our approach.

</details>


### [165] [Admittance Matrix Concentration Inequalities for Understanding Uncertain Power Networks](https://arxiv.org/abs/2510.17798)
*Samuel Talkington,Cameron Khanpour,Rahul K. Gupta,Sergio A. Dorado-Rojas,Daniel Turizo,Hyeongon Park,Dmitrii M. Ostrovskii,Daniel K. Molzahn*

Main category: eess.SY

TL;DR: 提出了在不确定网络参数下（如概率性线路故障）导纳矩阵谱和经典线性潮流模型的概率边界分析方法


<details>
  <summary>Details</summary>
Motivation: 电力系统运行中存在网络参数不确定性，需要为导纳矩阵谱和线性潮流模型提供概率边界分析

Method: 采用概率论工具，特别是具有独立条目的随机矩阵的集中不等式

Result: 得到了在参数不确定性下AC潮流方程常见近似（包括DC和LinDistFlow近似）的误差边界

Conclusion: 该方法为电力系统不确定性分析提供了有效的概率边界工具

Abstract: This paper presents probabilistic bounds for the spectrum of the admittance
matrix and classical linear power flow models under uncertain network
parameters; for example, probabilistic line contingencies. Our proposed
approach imports tools from probability theory, such as concentration
inequalities for random matrices with independent entries. It yields error
bounds for common approximations of the AC power flow equations under parameter
uncertainty, including the DC and LinDistFlow approximations.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [166] [Learning density ratios in causal inference using Bregman-Riesz regression](https://arxiv.org/abs/2510.16127)
*Oliver J. Hines,Caleb H. Miles*

Main category: stat.ML

TL;DR: 本文提出了Bregman-Riesz回归框架，统一了三种密度比估计方法：Bregman散度、概率分类和Riesz损失最小化，并展示了如何通过数据增强技术将密度比学习方法应用于因果问题。


<details>
  <summary>Details</summary>
Motivation: 密度比是统计学和机器学习中的基本量，但传统方法如分别估计分子和分母密度会导致不稳定性和维度灾难问题。现有三种方法各自独立发展，需要统一框架来整合这些方法。

Method: 提出了Bregman-Riesz回归框架，将Bregman散度方法、概率分类方法和Riesz损失最小化方法统一起来。通过数据增强技术处理因果问题中的未观测干预分布，并使用梯度提升、神经网络和核方法进行实现。

Result: 通过模拟实验展示了不同Bregman散度和数据增强策略对密度比学习器性能的影响。提供了Python包支持实际应用。

Conclusion: Bregman-Riesz回归为密度比估计提供了一个统一框架，能够整合现有方法并有效应用于因果推断等实际问题，具有重要的理论和实践价值。

Abstract: The ratio of two probability density functions is a fundamental quantity that
appears in many areas of statistics and machine learning, including causal
inference, reinforcement learning, covariate shift, outlier detection,
independence testing, importance sampling, and diffusion modeling. Naively
estimating the numerator and denominator densities separately using, e.g.,
kernel density estimators, can lead to unstable performance and suffers from
the curse of dimensionality as the number of covariates increases. For this
reason, several methods have been developed for estimating the density ratio
directly based on (a) Bregman divergences or (b) recasting the density ratio as
the odds in a probabilistic classification model that predicts whether an
observation is sampled from the numerator or denominator distribution.
Additionally, the density ratio can be viewed as the Riesz representer of a
continuous linear map, making it amenable to estimation via (c) minimization of
the so-called Riesz loss, which was developed to learn the Riesz representer in
the Riesz regression procedure in causal inference. In this paper we show that
all three of these methods can be unified in a common framework, which we call
Bregman-Riesz regression. We further show how data augmentation techniques can
be used to apply density ratio learning methods to causal problems, where the
numerator distribution typically represents an unobserved intervention. We show
through simulations how the choice of Bregman divergence and data augmentation
strategy can affect the performance of the resulting density ratio learner. A
Python package is provided for researchers to apply Bregman-Riesz regression in
practice using gradient boosting, neural networks, and kernel methods.

</details>


### [167] [Personalized Collaborative Learning with Affinity-Based Variance Reduction](https://arxiv.org/abs/2510.16232)
*Chenyu Zhang,Navid Azizan*

Main category: stat.ML

TL;DR: 提出个性化协作学习框架PCL，通过偏差校正和重要性校正机制处理异构性，在未知异构程度下自动适应，实现从联邦学习的线性加速到独立学习的无缝过渡。


<details>
  <summary>Details</summary>
Motivation: 多智能体学习面临分布式协作与个性化需求之间的基本矛盾，特别是在完全个性化且适应未知异构程度的情况下，需要在智能体相似时获得协作加速，而在不同时不降低性能。

Method: 提出AffPCL方法，通过精心设计的偏差校正和重要性校正机制，稳健处理环境和目标异构性，基于亲和力的加速自动在联邦学习的线性加速和独立学习之间插值。

Result: 证明AffPCL相比独立学习将样本复杂度降低max{n^{-1}, δ}倍，其中n是智能体数量，δ∈[0,1]衡量异构性。即使与任意不同的智能体协作，单个智能体仍可获得线性加速。

Conclusion: 揭示了在高异构性情况下个性化和协作的新见解，AffPCL框架在未知系统先验知识的情况下实现无缝自适应协作学习。

Abstract: Multi-agent learning faces a fundamental tension: leveraging distributed
collaboration without sacrificing the personalization needed for diverse
agents. This tension intensifies when aiming for full personalization while
adapting to unknown heterogeneity levels -- gaining collaborative speedup when
agents are similar, without performance degradation when they are different.
Embracing the challenge, we propose personalized collaborative learning (PCL),
a novel framework for heterogeneous agents to collaboratively learn
personalized solutions with seamless adaptivity. Through carefully designed
bias correction and importance correction mechanisms, our method AffPCL
robustly handles both environment and objective heterogeneity. We prove that
AffPCL reduces sample complexity over independent learning by a factor of
$\max\{n^{-1}, \delta\}$, where $n$ is the number of agents and
$\delta\in[0,1]$ measures their heterogeneity. This affinity-based acceleration
automatically interpolates between the linear speedup of federated learning in
homogeneous settings and the baseline of independent learning, without
requiring prior knowledge of the system. Our analysis further reveals that an
agent may obtain linear speedup even by collaborating with arbitrarily
dissimilar agents, unveiling new insights into personalization and
collaboration in the high heterogeneity regime.

</details>


### [168] [A Relative Error-Based Evaluation Framework of Heterogeneous Treatment Effect Estimators](https://arxiv.org/abs/2510.16419)
*Jiayi Guo,Haoxuan Li,Ye Tian,Peng Wu*

Main category: stat.ML

TL;DR: 提出了一个基于相对误差的异质性处理效应(HTE)估计器评估框架，通过推导关键理论条件、设计新颖损失函数和神经网络架构来稳健估计相对误差，并基于此开发了新的HTE学习算法。


<details>
  <summary>Details</summary>
Motivation: 虽然HTE估计取得了显著进展，但其评估方法仍不完善，需要开发稳健的评估框架来可靠比较不同HTE估计器的性能。

Method: 推导了实现稳健相对误差估计所需的干扰参数理论条件，设计了新颖损失函数和神经网络架构来估计干扰参数，并基于此构建了新的HTE学习算法。

Result: 实验表明该评估框架能够可靠比较HTE估计器，且提出的HTE学习算法表现出优良性能。

Conclusion: 提出的相对误差评估框架为HTE估计器提供了可靠的比较方法，同时基于该框架开发的学习算法在HTE估计中表现出色。

Abstract: While significant progress has been made in heterogeneous treatment effect
(HTE) estimation, the evaluation of HTE estimators remains underdeveloped. In
this article, we propose a robust evaluation framework based on relative error,
which quantifies performance differences between two HTE estimators. We first
derive the key theoretical conditions on the nuisance parameters that are
necessary to achieve a robust estimator of relative error. Building on these
conditions, we introduce novel loss functions and design a neural network
architecture to estimate nuisance parameters and obtain robust estimation of
relative error, thereby achieving reliable evaluation of HTE estimators. We
provide the large sample properties of the proposed relative error estimator.
Furthermore, beyond evaluation, we propose a new learning algorithm for HTE
that leverages both the previously HTE estimators and the nuisance parameters
learned through our neural network architecture. Extensive experiments
demonstrate that our evaluation framework supports reliable comparisons across
HTE estimators, and the proposed learning algorithm for HTE exhibits desirable
performance.

</details>


### [169] [A Bayesian Framework for Symmetry Inference in Chaotic Attractors](https://arxiv.org/abs/2510.16509)
*Ziad Ghanem,Chang Hyunwoong,Preskella Mrad*

Main category: stat.ML

TL;DR: 提出了一种贝叶斯框架用于从动态系统数据中检测对称性，通过吉布斯后验和Wasserstein距离进行概率模型选择，具有理论保证并在高噪声和小样本情况下表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有最优传输方法依赖确定性阈值且缺乏不确定性量化，限制了在噪声下的鲁棒性和对层次对称结构的解析能力。

Method: 将对称性检测表述为在候选子群格上的概率模型选择，使用基于Wasserstein距离的吉布斯后验，通过Metropolis-Hastings采样进行后验推断。

Result: 数值实验表明在等变动态系统和合成点云中，即使在高噪声和小样本情况下也能准确恢复对称性。在人类步态动力学应用中揭示了机械约束引起的对称性变化。

Conclusion: 该框架为生物力学和动态系统中的统计推断提供了有效工具，能够稳健地检测对称性并量化不确定性。

Abstract: Detecting symmetry from data is a fundamental problem in signal analysis,
providing insight into underlying structure and constraints. When data emerge
as trajectories of dynamical systems, symmetries encode structural properties
of the dynamics that enable model reduction, principled comparison across
conditions, and detection of regime changes. While recent optimal transport
methods provide practical tools for data-driven symmetry detection in this
setting, they rely on deterministic thresholds and lack uncertainty
quantification, limiting robustness to noise and ability to resolve
hierarchical symmetry structures. We present a Bayesian framework that
formulates symmetry detection as probabilistic model selection over a lattice
of candidate subgroups, using a Gibbs posterior constructed from Wasserstein
distances between observed data and group-transformed copies. We establish
three theoretical guarantees: $(i)$ a Bayesian Occam's razor favoring minimal
symmetry consistent with data, $(ii)$ conjugation equivariance ensuring
frame-independence, and $(iii)$ stability bounds under perturbations for
robustness to noise. Posterior inference is performed via Metropolis-Hastings
sampling and numerical experiments on equivariant dynamical systems and
synthetic point clouds demonstrate accurate symmetry recovery under high noise
and small sample sizes. An application to human gait dynamics reveals symmetry
changes induced by mechanical constraints, demonstrating the framework's
utility for statistical inference in biomechanical and dynamical systems.

</details>


### [170] [From Reviews to Actionable Insights: An LLM-Based Approach for Attribute and Feature Extraction](https://arxiv.org/abs/2510.16551)
*Khaled Boughanmi,Kamel Jedidi,Nour Jedidi*

Main category: stat.ML

TL;DR: 提出基于大语言模型的系统方法，从客户评论中提取产品服务属性、特征和情感，应用于2万条星巴克Yelp评论，验证了方法的可靠性和高效性。


<details>
  <summary>Details</summary>
Motivation: 传统人工编码分析客户评论耗时耗力，需要自动化、可扩展的方法来提取可操作的营销洞察。

Method: 基于营销理论框架，区分感知属性和可操作特征，使用8种提示变体在Yelp评论数据集上评估LLM性能。

Result: LLM与人工编码高度一致，处理速度比人工快180倍（2秒vs6分钟），识别出影响客户满意度的关键属性和特征，模拟显示优化关键服务特征情感可获得1-2%单店收入增长。

Conclusion: LLM方法提供了规模化、可操作的客户洞察，能够识别痛点、愉悦点并设计针对性干预措施，支持营销仪表板跟踪情感趋势和性能基准。

Abstract: This research proposes a systematic, large language model (LLM) approach for
extracting product and service attributes, features, and associated sentiments
from customer reviews. Grounded in marketing theory, the framework
distinguishes perceptual attributes from actionable features, producing
interpretable and managerially actionable insights. We apply the methodology to
20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a
random subset of reviews. Model performance is assessed through agreement with
human annotations and predictive validity for customer ratings. Results show
high consistency between LLMs and human coders and strong predictive validity,
confirming the reliability of the approach. Human coders required a median of
six minutes per review, whereas the LLM processed each in two seconds,
delivering comparable insights at a scale unattainable through manual coding.
Managerially, the analysis identifies attributes and features that most
strongly influence customer satisfaction and their associated sentiments,
enabling firms to pinpoint "joy points," address "pain points," and design
targeted interventions. We demonstrate how structured review data can power an
actionable marketing dashboard that tracks sentiment over time and across
stores, benchmarks performance, and highlights high-leverage features for
improvement. Simulations indicate that enhancing sentiment for key service
features could yield 1-2% average revenue gains per store.

</details>


### [171] [Multi-Marginal Schrödinger Bridge Matching](https://arxiv.org/abs/2510.16587)
*Byoungwoo Park,Juho Lee*

Main category: stat.ML

TL;DR: 提出了多边际薛定谔桥匹配(MSBM)算法，用于处理多时间点轨迹推断问题，通过扩展迭代马尔可夫拟合方法有效约束多个中间边际分布。


<details>
  <summary>Details</summary>
Motivation: 在发育生物学和系统医学等领域，无法对个体进行纵向跟踪，需要从离散时间快照推断连续演化轨迹。传统薛定谔桥方法仅处理成对时间点，对于多中间快照的系统不够充分。

Method: MSBM算法扩展了迭代马尔可夫拟合(IMF)方法，专门设计用于多边际薛定谔桥问题，能够有效处理多个边际约束。

Result: 在合成数据和真实单细胞RNA测序数据集上的实证验证表明，MSBM在捕捉复杂轨迹和尊重中间分布方面具有竞争性或更优性能，且计算效率显著。

Conclusion: MSBM为多时间点轨迹推断提供了一个有效解决方案，能够稳健地强制执行所有中间边际约束，同时保持学习到的全局动态在整个轨迹上的连续性。

Abstract: Understanding the continuous evolution of populations from discrete temporal
snapshots is a critical research challenge, particularly in fields like
developmental biology and systems medicine where longitudinal tracking of
individual entities is often impossible. Such trajectory inference is vital for
unraveling the mechanisms of dynamic processes. While Schr\"odinger Bridge (SB)
offer a potent framework, their traditional application to pairwise time points
can be insufficient for systems defined by multiple intermediate snapshots.
This paper introduces Multi-Marginal Schr\"odinger Bridge Matching (MSBM), a
novel algorithm specifically designed for the multi-marginal SB problem. MSBM
extends iterative Markovian fitting (IMF) to effectively handle multiple
marginal constraints. This technique ensures robust enforcement of all
intermediate marginals while preserving the continuity of the learned global
dynamics across the entire trajectory. Empirical validations on synthetic data
and real-world single-cell RNA sequencing datasets demonstrate the competitive
or superior performance of MSBM in capturing complex trajectories and
respecting intermediate distributions, all with notable computational
efficiency.

</details>


### [172] [Accelerated Learning on Large Scale Screens using Generative Library Models](https://arxiv.org/abs/2510.16612)
*Eli N. Weinstein,Andrei Slabodkin,Mattia G. Gollub,Elizabeth B. Wood*

Main category: stat.ML

TL;DR: 本文提出了一种优化高通量筛选实验的算法，通过只收集阳性序列样本来最大化信息增益，并使用生成模型校正缺失的阴性样本，从而在活性序列稀少的情况下加速机器学习模型训练。


<details>
  <summary>Details</summary>
Motivation: 生物机器学习常受限于数据规模不足，高通量筛选能并行测试大量蛋白质序列，但测量和测序成本限制了数据集规模。当活性序列稀少时，需要优化实验设计来最大化信息获取。

Method: 在活性序列稀少的情况下，只收集阳性序列样本（y>0），然后使用文库的生成模型来校正缺失的阴性样本，从而获得对真实条件概率p(y|x)的一致且有效估计。

Result: 通过实验与推断的协同设计，在模拟和大规模抗体筛选中显著加速了学习过程。

Conclusion: 实验设计与推断算法的协同优化能够显著提高高通量筛选的数据效率，在活性序列稀少的情况下实现更快的机器学习模型训练。

Abstract: Biological machine learning is often bottlenecked by a lack of scaled data.
One promising route to relieving data bottlenecks is through high throughput
screens, which can experimentally test the activity of $10^6-10^{12}$ protein
sequences in parallel. In this article, we introduce algorithms to optimize
high throughput screens for data creation and model training. We focus on the
large scale regime, where dataset sizes are limited by the cost of measurement
and sequencing. We show that when active sequences are rare, we maximize
information gain if we only collect positive examples of active sequences, i.e.
$x$ with $y>0$. We can correct for the missing negative examples using a
generative model of the library, producing a consistent and efficient estimate
of the true $p(y | x)$. We demonstrate this approach in simulation and on a
large scale screen of antibodies. Overall, co-design of experiments and
inference lets us accelerate learning dramatically.

</details>


### [173] [ARCO-BO: Adaptive Resource-aware COllaborative Bayesian Optimization for Heterogeneous Multi-Agent Design](https://arxiv.org/abs/2510.16652)
*Zihan Wang,Yi-Ping Chen,Tuba Dolar,Wei Chen*

Main category: stat.ML

TL;DR: 提出ARCO-BO框架，解决多智能体贝叶斯优化中的异构性问题，包括目标、预算和设计空间的异质性。


<details>
  <summary>Details</summary>
Motivation: 现代科学和工程设计中的分布式优化面临目标、评估预算和可访问设计变量的异质性，现有协作BO方法假设统一资源和完全共享输入空间，难以满足实际需求。

Method: ARCO-BO包含三个组件：相似性和最优值感知的共识机制用于自适应信息共享、预算感知的异步采样策略用于资源协调、部分输入空间共享用于异构设计空间。

Result: 在合成和高维工程问题上的实验表明，ARCO-BO始终优于独立BO和现有协作BO共识方法，在复杂多智能体环境中实现稳健高效性能。

Conclusion: ARCO-BO通过显式处理多智能体优化中的异质性，提供了在异构环境中有效的协作贝叶斯优化解决方案。

Abstract: Modern scientific and engineering design increasingly involves distributed
optimization, where agents such as laboratories, simulations, or industrial
partners pursue related goals under differing conditions. These agents often
face heterogeneities in objectives, evaluation budgets, and accessible design
variables, which complicates coordination and can lead to redundancy, poor
resource use, and ineffective information sharing. Bayesian Optimization (BO)
is a widely used decision-making framework for expensive black box functions,
but its single-agent formulation assumes centralized control and full data
sharing. Recent collaborative BO methods relax these assumptions, yet they
often require uniform resources, fully shared input spaces, and fixed task
alignment, conditions rarely satisfied in practice. To address these
challenges, we introduce Adaptive Resource Aware Collaborative Bayesian
Optimization (ARCO-BO), a framework that explicitly accounts for heterogeneity
in multi-agent optimization. ARCO-BO combines three components: a similarity
and optima-aware consensus mechanism for adaptive information sharing, a
budget-aware asynchronous sampling strategy for resource coordination, and a
partial input space sharing for heterogeneous design spaces. Experiments on
synthetic and high-dimensional engineering problems show that ARCO-BO
consistently outperforms independent BO and existing collaborative BO via
consensus approach, achieving robust and efficient performance in complex
multi-agent settings.

</details>


### [174] [Escaping Model Collapse via Synthetic Data Verification: Near-term Improvements and Long-term Convergence](https://arxiv.org/abs/2510.16657)
*Bingji Yi,Qiyuan Liu,Yuwei Cheng,Haifeng Xu*

Main category: stat.ML

TL;DR: 研究发现通过引入外部合成数据验证器可以避免模型崩溃，甚至可能逆转性能下降趋势。在合成数据迭代训练中，验证器的介入能确保模型参数收敛到验证器的知识中心。


<details>
  <summary>Details</summary>
Motivation: 针对合成数据迭代训练导致模型性能持续下降的模型崩溃现象，探索避免崩溃并实现性能改进的方法。

Method: 在基础线性回归设置中分析迭代训练过程，引入外部合成数据验证器（人类或更好模型）来验证数据质量，并在MNIST数据上对变分自编码器进行实验验证。

Result: 理论分析和实验均表明，使用验证器验证的合成数据进行迭代训练可以避免模型崩溃，初期能带来性能提升，但最终会收敛到验证器的知识中心。

Conclusion: 合成数据迭代训练需要外部验证机制来避免模型崩溃，验证器的可靠性决定了最终性能表现，完美验证器才能确保持续改进。

Abstract: Synthetic data has been increasingly used to train frontier generative
models. However, recent study raises key concerns that iteratively retraining a
generative model on its self-generated synthetic data may keep deteriorating
model performance, a phenomenon often coined model collapse. In this paper, we
investigate ways to modify this synthetic retraining process to avoid model
collapse, and even possibly help reverse the trend from collapse to
improvement. Our key finding is that by injecting information through an
external synthetic data verifier, whether a human or a better model, synthetic
retraining will not cause model collapse. To develop principled understandings
of the above insight, we situate our analysis in the foundational linear
regression setting, showing that iterative retraining with verified synthetic
data can yield near-term improvements but ultimately drives the parameter
estimate to the verifier's "knowledge center" in the long run. Our theory hence
predicts that, unless the verifier is perfectly reliable, the early gains will
plateau and may even reverse. Indeed, these theoretical insights are further
confirmed by our experiments on both linear regression as well as Variational
Autoencoders (VAEs) trained on MNIST data.

</details>


### [175] [Infinite Neural Operators: Gaussian processes on functions](https://arxiv.org/abs/2510.16675)
*Daniel Augusto de Souza,Yuchen Zhu,Harry Jake Cunningham,Yuri Saporito,Diego Mesquita,Marc Peter Deisenroth*

Main category: stat.ML

TL;DR: 该论文将神经网络与高斯过程的连接扩展到神经算子，证明了无限宽神经算子收敛到函数值高斯过程的条件，并计算了NO-GP的协方差函数和后验分布。


<details>
  <summary>Details</summary>
Motivation: 扩展神经网络与高斯过程的连接关系，揭示当前FNO架构的归纳偏置，为基于核的算子学习方法引入新的归纳偏置。

Method: 分析无限宽神经算子收敛到函数值高斯过程的条件，计算NO-GP的协方差函数（包括傅里叶神经算子），并在回归场景中计算这些GP的后验分布。

Result: 建立了神经算子与高斯过程的数学联系，提供了NO-GP协方差函数的计算方法，并展示了在PDE解算子等回归问题中的应用。

Conclusion: 这项工作为理解FNO架构的归纳偏置提供了重要基础，并为基于核的算子学习方法开辟了新路径。

Abstract: A variety of infinitely wide neural architectures (e.g., dense NNs, CNNs, and
transformers) induce Gaussian process (GP) priors over their outputs. These
relationships provide both an accurate characterization of the prior predictive
distribution and enable the use of GP machinery to improve the uncertainty
quantification of deep neural networks. In this work, we extend this connection
to neural operators (NOs), a class of models designed to learn mappings between
function spaces. Specifically, we show conditions for when arbitrary-depth NOs
with Gaussian-distributed convolution kernels converge to function-valued GPs.
Based on this result, we show how to compute the covariance functions of these
NO-GPs for two NO parametrizations, including the popular Fourier neural
operator (FNO). With this, we compute the posteriors of these GPs in regression
scenarios, including PDE solution operators. This work is an important step
towards uncovering the inductive biases of current FNO architectures and opens
a path to incorporate novel inductive biases for use in kernel-based operator
learning methods.

</details>


### [176] [Local regression on path spaces with signature metrics](https://arxiv.org/abs/2510.16728)
*Christian Bayer,Davit Gogolashvili,Luca Pelizzari*

Main category: stat.ML

TL;DR: 提出了一种结合签名变换和局部核回归的函数型Nadaraya-Watson估计器，用于路径值数据的非参数回归和分类。该方法利用签名诱导的距离实现高效计算，避免了大规模核矩阵操作的可扩展性瓶颈。


<details>
  <summary>Details</summary>
Motivation: 解决路径值数据的回归和分类问题，传统方法在无限维设置中面临挑战，需要一种能够直接比较路径的合理方法。

Method: 将粗糙路径理论中的签名变换与局部核回归相结合，使用签名诱导的距离在经典核回归框架中进行计算。

Result: 建立了有限样本收敛界，证明了基于签名的距离相比传统度量在无限维设置中具有更优的统计特性。在随机微分方程学习和时间序列分类等应用中表现出竞争性精度和显著计算优势。

Conclusion: 签名变换为序列数据编码提供了原则性方法，结合核回归框架实现了计算效率和统计性能的良好平衡，特别是在处理路径值数据时表现出色。

Abstract: We study nonparametric regression and classification for path-valued data. We
introduce a functional Nadaraya-Watson estimator that combines the signature
transform from rough path theory with local kernel regression. The signature
transform provides a principled way to encode sequential data through iterated
integrals, enabling direct comparison of paths in a natural metric space. Our
approach leverages signature-induced distances within the classical kernel
regression framework, achieving computational efficiency while avoiding the
scalability bottlenecks of large-scale kernel matrix operations. We establish
finite-sample convergence bounds demonstrating favorable statistical properties
of signature-based distances compared to traditional metrics in
infinite-dimensional settings. We propose robust signature variants that
provide stability against outliers, enhancing practical performance.
Applications to both synthetic and real-world data - including stochastic
differential equation learning and time series classification - demonstrate
competitive accuracy while offering significant computational advantages over
existing methods.

</details>


### [177] [Kernel-Based Nonparametric Tests For Shape Constraints](https://arxiv.org/abs/2510.16745)
*Rohan Sen*

Main category: stat.ML

TL;DR: 开发了一个再生核希尔伯特空间框架，用于非参数均值-方差优化和形状约束推断，提供了统计理论保证和高效计算程序。


<details>
  <summary>Details</summary>
Motivation: 为形状约束下的非参数均值-方差优化问题提供严格的统计理论框架和可扩展的计算方法。

Method: 基于再生核希尔伯特空间理论，使用枢轴Cholesky分解进行高效计算，构建联合Wald型统计量检验形状约束。

Result: 获得了渐近一致性、函数中心极限定理和有限样本偏差界等理论保证，经验测试表明方法表现良好。

Conclusion: 提出的方法为形状约束下的均值-方差优化提供了理论严谨且计算高效的解决方案。

Abstract: We develop a reproducing kernel Hilbert space (RKHS) framework for
nonparametric mean-variance optimization and inference on shape constraints of
the optimal rule. We derive statistical properties of the sample estimator and
provide rigorous theoretical guarantees, such as asymptotic consistency, a
functional central limit theorem, and a finite-sample deviation bound that
matches the Monte Carlo rate up to regularization. Building on these findings,
we introduce a joint Wald-type statistic to test for shape constraints over
finite grids. The approach comes with an efficient computational procedure
based on a pivoted Cholesky factorization, facilitating scalability to large
datasets. Empirical tests suggest favorably of the proposed methodology.

</details>


### [178] [Prediction-Augmented Trees for Reliable Statistical Inference](https://arxiv.org/abs/2510.16937)
*Vikram Kher,Argyris Oikonomou,Manolis Zampetakis*

Main category: stat.ML

TL;DR: 本文提出了两种新的学习增强估计器PART和PAQ，用于在科学发现中安全使用机器学习预测进行统计分析。PART基于决策树构建，PAQ是其极限情况，相比现有方法PPI和PPI++具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在预测任务中的成功，科学家们开始将ML预测作为科学发现流程的核心组成部分。本文研究如何在科学发现的数据统计分析中安全地使用ML预测。

Method: 引入两种新估计器：基于决策树的PART和作为PART极限情况的PAQ。PART使用贪心准则构建决策树，PAQ在树深度趋于无穷时产生。

Result: PART在生态学、天文学和人口普查等真实数据集上优于现有方法，能构建更有效的置信区间。PAQ的方差以O(N^{-1} + n^{-4})的速度收缩，显著优于现有方法的O(N^{-1}+n^{-1})。

Conclusion: PART和PAQ估计器通过结合金标准样本和机器学习预测，提供了比现有方法更优越的统计估计性能，特别是在方差收敛速度方面有显著改进。

Abstract: The remarkable success of machine learning (ML) in predictive tasks has led
scientists to incorporate ML predictions as a core component of the scientific
discovery pipeline. This was exemplified by the landmark achievement of
AlphaFold (Jumper et al. (2021)). In this paper, we study how ML predictions
can be safely used in statistical analysis of data towards scientific
discovery. In particular, we follow the framework introduced by Angelopoulos et
al. (2023). In this framework, we assume access to a small set of $n$
gold-standard labeled samples, a much larger set of $N$ unlabeled samples, and
a ML model that can be used to impute the labels of the unlabeled data points.
We introduce two new learning-augmented estimators: (1) Prediction-Augmented
Residual Tree (PART), and (2) Prediction-Augmented Quadrature (PAQ). Both
estimators have significant advantages over existing estimators like PPI and
PPI++ introduced by Angelopoulos et al. (2023) and Angelopoulos et al. (2024),
respectively. PART is a decision-tree based estimator built using a greedy
criterion. We first characterize PART's asymptotic distribution and demonstrate
how to construct valid confidence intervals. Then we show that PART outperforms
existing methods in real-world datasets from ecology, astronomy, and census
reports, among other domains. This leads to estimators with higher confidence,
which is the result of using both the gold-standard samples and the machine
learning predictions. Finally, we provide a formal proof of the advantage of
PART by exploring PAQ, an estimation that arises when considering the limit of
PART when the depth its tree grows to infinity. Under appropriate assumptions
in the input data we show that the variance of PAQ shrinks at rate of $O(N^{-1}
+ n^{-4})$, improving significantly on the $O(N^{-1}+n^{-1})$ rate of existing
methods.

</details>


### [179] [Adaptive Sample Sharing for Linear Regression](https://arxiv.org/abs/2510.16986)
*Hamza Cherkaoui,Hélène Halconruy,Yohan Petetin*

Main category: stat.ML

TL;DR: 提出一种基于岭回归的样本共享方法，通过数据驱动规则决定从辅助数据集中借用多少样本，以避免负迁移并提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 在商业场景中，任务特定的标注数据稀缺且获取成本高，限制了监督学习的应用。需要利用辅助数据集同时避免负迁移问题。

Method: 基于转移增益估计的样本共享规则，通过估计预测误差的边际减少来决定借用多少辅助样本。在Gaussian特征设置下分析确保借用样本能减少预测误差的数据集特性。

Result: 在合成和真实数据集上的验证表明，该方法相比强基线和单任务训练获得了一致的性能提升，同时成功避免了负迁移。

Conclusion: 该方法提供了一种原则性的样本共享策略，能够在借用辅助数据时确保性能改进，为数据稀缺场景下的机器学习提供了有效解决方案。

Abstract: In many business settings, task-specific labeled data are scarce or costly to
obtain, which limits supervised learning on a specific task. To address this
challenge, we study sample sharing in the case of ridge regression: leveraging
an auxiliary data set while explicitly protecting against negative transfer. We
introduce a principled, data-driven rule that decides how many samples from an
auxiliary dataset to add to the target training set. The rule is based on an
estimate of the transfer gain i.e. the marginal reduction in the predictive
error. Building on this estimator, we derive finite-sample guaranties: under
standard conditions, the procedure borrows when it improves parameter
estimation and abstains otherwise. In the Gaussian feature setting, we analyze
which data set properties ensure that borrowing samples reduces the predictive
error. We validate the approach in synthetic and real datasets, observing
consistent gains over strong baselines and single-task training while avoiding
negative transfer.

</details>


### [180] [Mode Collapse of Mean-Field Variational Inference](https://arxiv.org/abs/2510.17063)
*Shunan Sheng,Bohan Wu,Alberto González-Sanz*

Main category: stat.ML

TL;DR: 本文首次从理论上解释了平均场变分推断中的模态坍塌现象，提出了ε-分离度的概念来衡量混合成分的分离程度，并开发了旋转变分推断方法来解决该问题。


<details>
  <summary>Details</summary>
Motivation: 平均场变分推断在优化过程中经常出现模态坍塌问题，即当目标分布是混合分布时，优化器倾向于将大部分质量集中在单个混合成分上。这种现象缺乏理论解释，需要深入研究。

Method: 引入ε-分离度概念来量化混合成分的分离程度，推导出当混合成分充分分离时，任何MFVI优化器分配给每个成分的质量比例的上界。为解决模态坍塌问题，提出了旋转变分推断方法，在MFVI基础上增加旋转矩阵。

Result: 理论分析表明模态坍塌的发生关键取决于混合成分的相对位置。数值研究支持了理论发现，并证明了RoVI方法的优势。

Conclusion: 本文首次为MFVI中的模态坍塌现象提供了理论解释，提出的RoVI方法能有效缓解该问题，为变分推断方法提供了新的改进方向。

Abstract: Mean-field variational inference (MFVI) is a widely used method for
approximating high-dimensional probability distributions by product measures.
It has been empirically observed that MFVI optimizers often suffer from mode
collapse. Specifically, when the target measure $\pi$ is a mixture $\pi = w P_0
+ (1 - w) P_1$, the MFVI optimizer tends to place most of its mass near a
single component of the mixture. This work provides the first theoretical
explanation of mode collapse in MFVI. We introduce the notion to capture the
separatedness of the two mixture components -- called
$\varepsilon$-separateness -- and derive explicit bounds on the fraction of
mass that any MFVI optimizer assigns to each component when $P_0$ and $P_1$ are
$\varepsilon$-separated for sufficiently small $\varepsilon$. Our results
suggest that the occurrence of mode collapse crucially depends on the relative
position of the components. To address this issue, we propose the rotational
variational inference (RoVI), which augments MFVI with a rotation matrix. The
numerical studies support our theoretical findings and demonstrate the benefits
of RoVI.

</details>


### [181] [DFNN: A Deep Fréchet Neural Network Framework for Learning Metric-Space-Valued Responses](https://arxiv.org/abs/2510.17072)
*Kyum Kim,Yaqing Chen,Paromita Dubey*

Main category: stat.ML

TL;DR: 提出了深度Fréchet神经网络(DFNNs)，这是一个端到端的深度学习框架，用于从欧几里得预测变量预测非欧几里得响应（如概率分布、网络、对称正定矩阵等）。


<details>
  <summary>Details</summary>
Motivation: 现代应用中非欧几里得响应（如概率分布、网络、矩阵等）的回归变得越来越重要，需要能够处理这类响应的预测方法。

Method: 利用深度神经网络的表示学习能力，通过最小化Fréchet风险来近似给定预测变量下响应的条件Fréchet均值（度量空间中条件期望的类比）。

Result: 建立了DFNNs的通用逼近定理，无需模型假设或局部平滑即可处理一般度量空间值响应。在合成分布和网络值响应以及就业职业组成的实际应用中，DFNNs始终优于现有方法。

Conclusion: DFNNs提供了一个高度灵活且强大的框架，能够有效处理各种非欧几里得响应预测问题，并在理论和实证上都表现出优越性能。

Abstract: Regression with non-Euclidean responses -- e.g., probability distributions,
networks, symmetric positive-definite matrices, and compositions -- has become
increasingly important in modern applications. In this paper, we propose deep
Fr\'echet neural networks (DFNNs), an end-to-end deep learning framework for
predicting non-Euclidean responses -- which are considered as random objects in
a metric space -- from Euclidean predictors. Our method leverages the
representation-learning power of deep neural networks (DNNs) to the task of
approximating conditional Fr\'echet means of the response given the predictors,
the metric-space analogue of conditional expectations, by minimizing a
Fr\'echet risk. The framework is highly flexible, accommodating diverse metrics
and high-dimensional predictors. We establish a universal approximation theorem
for DFNNs, advancing the state-of-the-art of neural network approximation
theory to general metric-space-valued responses without making model
assumptions or relying on local smoothing. Empirical studies on synthetic
distributional and network-valued responses, as well as a real-world
application to predicting employment occupational compositions, demonstrate
that DFNNs consistently outperform existing methods.

</details>


### [182] [Optimal Best Arm Identification under Differential Privacy](https://arxiv.org/abs/2510.17348)
*Marc Jourdan,Achraf Azize*

Main category: stat.ML

TL;DR: 本文研究了在全局差分隐私(DP)约束下的最佳臂识别(BAI)问题，通过新的信息论方法和采样策略，显著缩小了隐私BAI问题的上下界差距。


<details>
  <summary>Details</summary>
Motivation: BAI算法在数据敏感应用(如自适应临床试验)中广泛使用，但现有研究在全局DP约束下的样本复杂度上下界存在显著差距，需要开发更高效的隐私保护BAI算法。

Method: 提出基于运输成本的停止规则、使用臂相关几何批处理的私有均值估计器，以及基于运输成本的Top Two采样规则，并推导了拉普拉斯分布和伯努利-拉普拉斯混合分布的浓度结果。

Result: 对于任意隐私预算ε，算法实现了与下界匹配的渐近样本复杂度上界，乘性常数小于8，优于现有的δ正确和ε全局DP BAI算法。

Conclusion: 通过新的信息论方法和算法设计，显著缩小了隐私BAI问题的理论差距，为数据敏感应用提供了高效的隐私保护最佳臂识别解决方案。

Abstract: Best Arm Identification (BAI) algorithms are deployed in data-sensitive
applications, such as adaptive clinical trials or user studies. Driven by the
privacy concerns of these applications, we study the problem of
fixed-confidence BAI under global Differential Privacy (DP) for Bernoulli
distributions. While numerous asymptotically optimal BAI algorithms exist in
the non-private setting, a significant gap remains between the best lower and
upper bounds in the global DP setting. This work reduces this gap to a small
multiplicative constant, for any privacy budget $\epsilon$. First, we provide a
tighter lower bound on the expected sample complexity of any $\delta$-correct
and $\epsilon$-global DP strategy. Our lower bound replaces the
Kullback-Leibler (KL) divergence in the transportation cost used by the
non-private characteristic time with a new information-theoretic quantity that
optimally trades off between the KL divergence and the Total Variation distance
scaled by $\epsilon$. Second, we introduce a stopping rule based on these
transportation costs and a private estimator of the means computed using an
arm-dependent geometric batching. En route to proving the correctness of our
stopping rule, we derive concentration results of independent interest for the
Laplace distribution and for the sum of Bernoulli and Laplace distributions.
Third, we propose a Top Two sampling rule based on these transportation costs.
For any budget $\epsilon$, we show an asymptotic upper bound on its expected
sample complexity that matches our lower bound to a multiplicative constant
smaller than $8$. Our algorithm outperforms existing $\delta$-correct and
$\epsilon$-global DP BAI algorithms for different values of $\epsilon$.

</details>


### [183] [Certified Self-Consistency: Statistical Guarantees and Test-Time Training for Reliable Reasoning in LLMs](https://arxiv.org/abs/2510.17472)
*Paula Cordero-Encinar,Andrew B. Duncan*

Main category: stat.ML

TL;DR: 提出了一个用于大语言模型可验证推理的统一框架，证明多数投票提供自一致性的统计保证，并引入自适应停止规则和新的后训练目标来优化置信度与偏差之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前自一致性和测试时强化学习等无监督方法虽然提高了LLM的可靠性，但其机制和统计保证仍缺乏深入理解，需要建立统一的统计框架来解释这些方法。

Method: 开发了多数投票的统计证书框架，推导有限样本和任意时间有效的集中界限，引入Martingale多数证书作为自适应停止规则，并提出新的后训练目标来显式优化分布锐化与偏差的权衡。

Result: 证明了多数投票在温和假设下以高概率与模型终端分布的众数一致，TTRL方法通过指数倾斜隐式锐化答案分布，减少了认证所需的样本数量。

Conclusion: 该研究在单一统计框架内解释并连接了自一致性和TTRL这两种核心测试时扩展策略，为无标签、可验证的推理LLM可靠性提供了理论基础。

Abstract: Recent advances such as self-consistency and test-time reinforcement learning
(TTRL) improve the reliability of large language models (LLMs) without
additional supervision, yet their underlying mechanisms and statistical
guarantees remain poorly understood. We present a unified framework for
certifiable inference in LLMs, showing that majority voting provides a
statistical certificate of self-consistency: under mild assumptions, the
aggregated answer coincides with the mode of the model's terminal distribution
with high probability. We derive finite-sample and anytime-valid concentration
bounds that quantify this confidence, and introduce the Martingale Majority
Certificate (MMC), a sequential stopping rule that adaptively determines when
sufficient samples have been drawn. We further prove that label-free
post-training methods such as TTRL implicitly sharpen the answer distribution
by exponentially tilting it toward its mode, thereby reducing the number of
samples required for certification. Building on this insight, we propose new
post-training objectives that explicitly optimise this trade-off between
sharpness and bias. Together, these results explain and connect two central
test-time scaling strategies, self-consistency and TTRL, within a single
statistical framework for label-free, certifiable reliability in reasoning
LLMs.

</details>


### [184] [Non-asymptotic error bounds for probability flow ODEs under weak log-concavity](https://arxiv.org/abs/2510.17608)
*Gitte Kremling,Francesco Iafrate,Mahsa Taheri,Johannes Lederer*

Main category: stat.ML

TL;DR: 该论文为基于概率流ODE的分数生成模型建立了非渐近收敛界限，在弱对数凹性和分数函数Lipschitz连续性的假设下，扩展了扩散模型的收敛理论到更现实的数据分布和实际ODE求解器。


<details>
  <summary>Details</summary>
Motivation: 现有的收敛保证依赖于对目标分布的严格正则性假设，如强对数凹性或有界支撑。本文旨在在更弱的假设下建立收敛理论，以适应非对数凹分布（如高斯混合）和实际ODE求解器。

Method: 使用概率流ODE框架，考虑初始化误差、分数近似误差和通过指数积分器方案的离散化效应，在弱对数凹性和分数函数Lipschitz连续性的假设下分析收敛性。

Result: 建立了2-Wasserstein距离中的非渐近收敛界限，为采样算法的效率和正确性提供了具体保证，并将收敛理论扩展到更现实的数据分布和实际ODE求解器。

Conclusion: 该工作弥合了扩散基生成建模中的关键理论挑战，为扩散模型的经验成功提供了严格理论支持，其显式收敛率有助于选择超参数（如离散化步长）。

Abstract: Score-based generative modeling, implemented through probability flow ODEs,
has shown impressive results in numerous practical settings. However, most
convergence guarantees rely on restrictive regularity assumptions on the target
distribution -- such as strong log-concavity or bounded support. This work
establishes non-asymptotic convergence bounds in the 2-Wasserstein distance for
a general class of probability flow ODEs under considerably weaker assumptions:
weak log-concavity and Lipschitz continuity of the score function. Our
framework accommodates non-log-concave distributions, such as Gaussian
mixtures, and explicitly accounts for initialization errors, score
approximation errors, and effects of discretization via an exponential
integrator scheme. Bridging a key theoretical challenge in diffusion-based
generative modeling, our results extend convergence theory to more realistic
data distributions and practical ODE solvers. We provide concrete guarantees
for the efficiency and correctness of the sampling algorithm, complementing the
empirical success of diffusion models with rigorous theory. Moreover, from a
practical perspective, our explicit rates might be helpful in choosing
hyperparameters, such as the step size in the discretization.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [185] [FinFlowRL: An Imitation-Reinforcement Learning Framework for Adaptive Stochastic Control in Finance](https://arxiv.org/abs/2510.15883)
*Yang Li,Zhi Chen*

Main category: q-fin.CP

TL;DR: FinFlowRL是一个用于金融最优随机控制的新框架，通过元策略学习和强化学习在噪声空间微调，使用动作分块处理市场的非马尔可夫特性，在各种市场条件下持续优于单独优化的专家策略。


<details>
  <summary>Details</summary>
Motivation: 传统金融随机控制方法在现实市场中表现不佳，因为它们依赖简化假设和程式化框架，在变化、非平稳环境中产生次优结果。

Method: 框架预训练一个从多个专家策略学习的自适应元策略，然后通过强化学习在噪声空间进行微调以优化生成过程，采用动作分块生成动作序列而非单个决策。

Result: FinFlowRL在各种市场条件下持续优于单独优化的专家策略。

Conclusion: FinFlowRL框架通过元策略学习和强化学习微调，有效解决了传统金融随机控制方法在非平稳市场环境中的局限性。

Abstract: Traditional stochastic control methods in finance struggle in real world
markets due to their reliance on simplifying assumptions and stylized
frameworks. Such methods typically perform well in specific, well defined
environments but yield suboptimal results in changed, non stationary ones. We
introduce FinFlowRL, a novel framework for financial optimal stochastic
control. The framework pretrains an adaptive meta policy learning from multiple
expert strategies, then finetunes through reinforcement learning in the noise
space to optimize the generative process. By employing action chunking
generating action sequences rather than single decisions, it addresses the non
Markovian nature of markets. FinFlowRL consistently outperforms individually
optimized experts across diverse market conditions.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [186] [A study about who is interested in stock splitting and why: considering companies, shareholders or managers](https://arxiv.org/abs/2510.15879)
*Jiaquan Nicholas Chen,Marcel Ausloos*

Main category: q-fin.GN

TL;DR: 该论文通过分析9个股票拆分事件，发现股票拆分能短期提升交易量、扩大股东基础、提高市场流动性，并减少信息不对称。


<details>
  <summary>Details</summary>
Motivation: 研究股票拆分对公司的吸引力，探讨管理层和股东为何同意股票拆分决策，以及这些决策背后的原因。

Method: 选择特定年份的9个股票拆分事件进行分析，计算每个样本的beta值，观察信息不对称、事件前后的回报率和交易量变化。

Result: 股票拆分在短期内影响市场并轻微提升交易量，增加公司股东基础，对市场流动性有积极影响。

Conclusion: 股票拆分公告可以减少信息不对称水平，投资者会重新调整对公司信念，尽管大多数公司在股票拆分年份被错误定价。

Abstract: There are many misconceptions around stock prices, stock splits,
shareholders, investors, and managers behaviour about such informations due to
a number of confounding factors. This paper tests hypotheses with a selected
database, about the question ''is stock split attractive for companies?'' in
another words, ''why companies split their stock?'', ''why managers split their
stock?'', sometimes for no benefit, and ''why shareholders agree with such
decisions?''. We contribute to the existing knowledge through a discussion of
nine events in recent (selectively chosen) years, observing the role of
information asymmetries, the returns and traded volumes before and after the
event. Therefore, calculating the beta for each sample, it is found that stock
splits (i) affect the market and slightly enhance the trading volume in a
short-term, (ii) increase the shareholder base for its firm, (iii) have a
positive effect on the liquidity of the market. We concur that stock split
announcements can reduce the level of information asymmetric. Investors
readjust their beliefs in the firm, although most of the firms are mispriced in
the stock split year.

</details>


### [187] [ESG Signaling on Wall Street in the AI Era](https://arxiv.org/abs/2510.15956)
*Qionghua Chu*

Main category: q-fin.GN

TL;DR: 研究发现ESG投资在AI时代仍具价值，ESG评分与债务资本比率呈显著正相关，为长期投资组合管理提供重要信号。


<details>
  <summary>Details</summary>
Motivation: 随着大型机构投资者转向AI投资，需要验证ESG投资是否仍具有信号价值，探索ESG在AI崛起背景下的投资意义。

Method: 使用雅虎财经的S&P 500公司ESG评分数据，控制股权市场价值，进行横截面回归分析测试信号机制。

Result: 环境、社会和治理评分以及综合ESG评分均与更高的债务资本比率呈显著正相关，单独和组合分析结果一致。

Conclusion: ESG投资在AI时代仍提供经济意义显著的信号渠道，对长期投资组合管理具有重要启示。

Abstract: I identify a new signaling channel in ESG research by empirically examining
whether environmental, social, and governance (ESG) investing remains valuable
as large institutional investors increasingly shift toward artificial
intelligence (AI). Using winsorized ESG scores of S&P 500 firms from Yahoo
Finance and controlling for market value of equity, I conduct cross-sectional
regressions to test the signaling mechanism. I demonstrate that Environmental,
Social, Governance, and composite ESG scores strongly and positively signal
higher debt-to-total-capital ratio, both individually and in various
combinations. My findings contribute to the growing literature on ESG
investing, offering economically meaningful signaling channel with implications
for long-term portfolio management amid the rise of AI.

</details>


### [188] [Sleeping Kelly is a Thirder](https://arxiv.org/abs/2510.15911)
*Ben Abramowitz*

Main category: q-fin.GN

TL;DR: 本文主张使用凯利准则（Kelly Criterion）来解决睡美人问题，认为睡美人应该最大化财富增长率而非期望值，这导向"三分之一派"立场，并通过荷兰赌论证其合理性。


<details>
  <summary>Details</summary>
Motivation: 睡美人问题凸显了不完全记忆情境中的概率问题。传统方法假设睡美人应最大化赌注的期望值，但本文认为这种假设存在问题，需要重新思考理性决策的标准。

Method: 采用凯利准则（财富增长率最大化）而非期望值最大化来分析睡美人的决策。通过荷兰赌论证来检验不同立场的合理性。

Result: 如果睡美人采用"三分之一派"立场并只接受增长率大于1的赌注，她就不会受到荷兰赌的威胁；而"二分之一派"立场则会使她易受荷兰赌攻击。

Conclusion: 在乘性财富动态下，睡美人采用凯利准则并持"三分之一派"立场是理性的选择，但若赌注结构改变导致非乘性财富动态，这一结论可能不再成立。

Abstract: The Sleeping Beauty problem was presented by Elga and highlights the role of
probabilities in situations with imperfect recall. One approach to solving the
Sleeping Beauty problem is to allow Sleeping Beauty to make decisions based on
her beliefs, and then characterize what it takes for her decisions to be
"rational". In particular, she can be allowed to make monetary bets based on
her beliefs, with the assumption that she wants to gain wealth rather than lose
it. However, this approach is often coupled with the assumption that Sleeping
Beauty should maximize the expected value of her bets. Here, I argue instead
that it is rational for Sleeping Beauty to maximize the growth rate of her
wealth using the Kelly Criterion, which leads us to the "thirder" position.
Furthermore, this position is shown to be "rational" by Dutch book arguments.
If Sleeping Kelly only accepts bets that have a growth rate greater than 1 as a
"thirder" then she is not vulnerable to Dutch books. By contrast, if Sleeping
Beauty takes the "halfer" position, she is vulnerable to Dutch books. If the
bets offered to Sleeping Beauty were to be structured differently and lead to
non-multiplicative wealth dynamics, she may no longer be a "thirder".

</details>


### [189] [Geometric Dynamics of Consumer Credit Cycles: A Multivector-based Linear-Attention Framework for Explanatory Economic Analysis](https://arxiv.org/abs/2510.15892)
*Agus Sudjianto,Sandi Setiawan*

Main category: q-fin.GN

TL;DR: 该研究引入几何代数将信贷系统关系分解为投影（类似相关性）和旋转（反馈螺旋）分量，揭示传统分析无法看到的系统性危机动态模式。


<details>
  <summary>Details</summary>
Motivation: 传统经济分析方法无法捕捉信贷系统中的复杂反馈螺旋和旋转动态，特别是在失业和信贷收缩同时进入反馈循环时，需要新的数学框架来揭示这些隐藏的危机模式。

Method: 使用Clifford几何代数，将经济状态表示为多向量，其中双向量元素捕捉失业、消费、储蓄和信贷利用之间的旋转耦合关系。

Result: 该数学框架揭示了传统分析中不可见的交互模式：当失业和信贷收缩同时进入反馈循环时，它们的几何关系从简单相关性转变为危险的旋转动态，这表征了系统性危机。

Conclusion: 几何代数为分析信贷系统提供了强大的数学工具，能够揭示传统方法无法发现的系统性危机动态模式，特别是旋转耦合关系在危机形成中的关键作用。

Abstract: This study introduces geometric algebra to decompose credit system
relationships into their projective (correlation-like) and rotational
(feedback-spiral) components. We represent economic states as multi-vectors in
Clifford algebra, where bivector elements capture the rotational coupling
between unemployment, consumption, savings, and credit utilization. This
mathematical framework reveals interaction patterns invisible to conventional
analysis: when unemployment and credit contraction enter simultaneous feedback
loops, their geometric relationship shifts from simple correlation to dangerous
rotational dynamics that characterize systemic crises.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [190] [Bitcoin Price Forecasting Based on Hybrid Variational Mode Decomposition and Long Short Term Memory Network](https://arxiv.org/abs/2510.15900)
*Emmanuel Boadi*

Main category: q-fin.ST

TL;DR: 提出了一种结合变分模态分解(VMD)和长短期记忆网络(LSTM)的混合深度学习模型，用于预测波动频繁的比特币价格，该模型在各项评估指标上均优于标准LSTM模型。


<details>
  <summary>Details</summary>
Motivation: 由于比特币等数字货币价格波动频繁且难以预测，需要开发更有效的预测模型来捕捉其复杂的时间模式。

Method: 首先使用VMD将原始比特币价格序列分解为多个本征模态函数(IMF)，然后对每个IMF使用LSTM网络建模以更有效地捕捉时间模式，最后将各IMF的预测结果聚合得到最终价格预测。

Result: 混合VMD+LSTM模型在所有评估指标(RMSE、MAE、R2)上均优于标准LSTM模型，并能提供可靠的30天预测。

Conclusion: VMD+LSTM混合模型能够有效提升比特币价格预测的准确性，为数字货币市场提供更可靠的预测工具。

Abstract: This study proposes a hybrid deep learning model for forecasting the price of
Bitcoin, as the digital currency is known to exhibit frequent fluctuations. The
models used are the Variational Mode Decomposition (VMD) and the Long
Short-Term Memory (LSTM) network. First, VMD is used to decompose the original
Bitcoin price series into Intrinsic Mode Functions (IMFs). Each IMF is then
modeled using an LSTM network to capture temporal patterns more effectively.
The individual forecasts from the IMFs are aggregated to produce the final
prediction of the original Bitcoin Price Series. To determine the prediction
power of the proposed hybrid model, a comparative analysis was conducted
against the standard LSTM. The results confirmed that the hybrid VMD+LSTM model
outperforms the standard LSTM across all the evaluation metrics, including
RMSE, MAE and R2 and also provides a reliable 30-day forecast.

</details>


### [191] [Quantum and Classical Machine Learning in Decentralized Finance: Comparative Evidence from Multi-Asset Backtesting of Automated Market Makers](https://arxiv.org/abs/2510.15903)
*Chi-Sheng Chen,Aidan Hung-Wen Tsai*

Main category: q-fin.ST

TL;DR: 量子机器学习与经典机器学习在AMM和DeFi交易策略中的实证比较，结果显示混合量子模型表现最佳


<details>
  <summary>Details</summary>
Motivation: 比较量子机器学习(QML)和经典机器学习(CML)在自动做市商(AMM)和去中心化金融(DeFi)交易策略中的性能差异

Method: 对10种模型进行广泛回测，包括经典ML模型、纯量子模型、混合量子-经典模型和transformer模型，涵盖多种加密货币资产

Result: 混合量子模型获得11.2%平均回报率和1.42平均夏普比率，表现最优；QASA Sequence混合模型达到13.99%最高回报率和1.76最佳夏普比率

Conclusion: 量子-经典混合方法在AMM和DeFi交易策略中展现出巨大潜力

Abstract: This study presents a comprehensive empirical comparison between quantum
machine learning (QML) and classical machine learning (CML) approaches in
Automated Market Makers (AMM) and Decentralized Finance (DeFi) trading
strategies through extensive backtesting on 10 models across multiple
cryptocurrency assets. Our analysis encompasses classical ML models (Random
Forest, Gradient Boosting, Logistic Regression), pure quantum models (VQE
Classifier, QNN, QSVM), hybrid quantum-classical models (QASA Hybrid, QASA
Sequence, QuantumRWKV), and transformer models. The results demonstrate that
hybrid quantum models achieve superior overall performance with 11.2\% average
return and 1.42 average Sharpe ratio, while classical ML models show 9.8\%
average return and 1.47 average Sharpe ratio. The QASA Sequence hybrid model
achieves the highest individual return of 13.99\% with the best Sharpe ratio of
1.76, demonstrating the potential of quantum-classical hybrid approaches in AMM
and DeFi trading strategies.

</details>


### [192] [A three-step machine learning approach to predict market bubbles with financial news](https://arxiv.org/abs/2510.16636)
*Abraham Atsiwo*

Main category: q-fin.ST

TL;DR: 提出一个三步机器学习框架，结合金融新闻情感和宏观经济指标来预测标普500股市泡沫，显著提高预测准确性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 传统计量经济学方法在预测股市泡沫方面存在局限，需要整合文本和定量数据源来更好地捕捉投资者预期和行为模式。

Method: 1) 使用右尾单位根检验识别泡沫期；2) 通过NLP技术从金融新闻提取情感特征；3) 应用集成学习方法基于情感和宏观经济指标预测泡沫发生。

Result: 实证结果表明，提出的三步集成方法显著提高了预测准确性和稳健性，为投资者、监管机构和政策制定者提供了有价值的早期预警。

Conclusion: 该框架通过整合文本情感分析和宏观经济指标，有效改进了股市泡沫预测，有助于缓解系统性金融风险。

Abstract: This study presents a three-step machine learning framework to predict
bubbles in the S&P 500 stock market by combining financial news sentiment with
macroeconomic indicators. Building on traditional econometric approaches, the
proposed approach predicts bubble formation by integrating textual and
quantitative data sources. In the first step, bubble periods in the S&P 500
index are identified using a right-tailed unit root test, a widely recognized
real-time bubble detection method. The second step extracts sentiment features
from large-scale financial news articles using natural language processing
(NLP) techniques, which capture investors' expectations and behavioral
patterns. In the final step, ensemble learning methods are applied to predict
bubble occurrences based on high sentiment-based and macroeconomic predictors.
Model performance is evaluated through k-fold cross-validation and compared
against benchmark machine learning algorithms. Empirical results indicate that
the proposed three-step ensemble approach significantly improves predictive
accuracy and robustness, providing valuable early warning insights for
investors, regulators, and policymakers in mitigating systemic financial risks.

</details>


### [193] [Investor Sentiment and Market Movements: A Granger Causality Perspective](https://arxiv.org/abs/2510.15915)
*Tamoghna Mukherjee*

Main category: q-fin.ST

TL;DR: 该研究通过格兰杰因果检验分析投资者情绪与股票价格之间的关系，发现情绪变化对股价具有预测作用


<details>
  <summary>Details</summary>
Motivation: 投资者情绪对股市有重要影响，但情绪变化是否真正领先于股价变化需要实证验证

Method: 使用格兰杰因果检验来分析收盘价指数与情绪得分之间的因果关系

Result: 假设检验显示存在正向响应，即情绪变化确实先于股价变化

Conclusion: 投资者情绪可以作为预测股票价格变化的有用指标

Abstract: The stock market is heavily influenced by investor sentiment, which can drive
buying or selling behavior. Sentiment analysis helps in gauging the overall
sentiment of market participants towards a particular stock or the market as a
whole. Positive sentiment often leads to increased buying activity and vice
versa. Granger causality can be applied to ascertain whether changes in
sentiment precede changes in stock prices.The study is focused on this aspect
and tries to understand the relationship between close price index and
sentiment score with the help of Granger causality inference. The study finds a
positive response through hypothesis testing.

</details>


### [194] [Comparing LLMs for Sentiment Analysis in Financial Market News](https://arxiv.org/abs/2510.15929)
*Lucas Eduardo Pereira Teles,Carlos M. S. Figueiredo*

Main category: q-fin.ST

TL;DR: 比较大型语言模型与传统方法在金融新闻情感分析任务中的性能表现，发现LLM在大多数情况下优于传统模型


<details>
  <summary>Details</summary>
Motivation: 分析大型语言模型在金融领域自然语言处理任务中的性能差异，量化比较不同模型的优势

Method: 使用比较研究方法，将大型语言模型与经典方法在金融新闻情感分析任务中进行对比

Result: 结果表明大型语言模型在绝大多数情况下优于传统模型

Conclusion: 大型语言模型在金融新闻情感分析任务中展现出比传统方法更好的性能

Abstract: This article presents a comparative study of large language models (LLMs) in
the task of sentiment analysis of financial market news. This work aims to
analyze the performance difference of these models in this important natural
language processing task within the context of finance. LLM models are compared
with classical approaches, allowing for the quantification of the benefits of
each tested model or approach. Results show that large language models
outperform classical models in the vast majority of cases.

</details>


### [195] [Cash Flow Underwriting with Bank Transaction Data: Advancing MSME Financial Inclusion in Malaysia](https://arxiv.org/abs/2510.16066)
*Chun Chet Ng,Wei Zeng Low,Yin Yin Boon*

Main category: q-fin.ST

TL;DR: 本研究探讨银行流水数据作为替代数据源在马来西亚小微企业信用评估中的潜力，提出基于现金流的信用评估流程，并开发了包含611名贷款申请人的数据集。


<details>
  <summary>Details</summary>
Motivation: 解决马来西亚小微企业融资难问题，特别是新成立企业因缺乏信用记录而被传统信贷市场排除在外的情况，促进新兴市场的金融包容性。

Method: 提出基于现金流的信用评估流程，利用银行流水数据进行端到端数据提取和机器学习信用评分，开发并评估基于申请信息和银行交易特征的信用评分模型。

Result: 实证结果显示，使用银行流水数据显著提升了所有模型的性能，能够改善无贷款记录小微企业的信用评分效果。

Conclusion: 银行流水数据可作为有效的替代数据源，提高小微企业信用评估的准确性，促进金融包容性，研究者计划发布匿名银行交易数据集以支持进一步研究。

Abstract: Despite accounting for 96.1% of all businesses in Malaysia, access to
financing remains one of the most persistent challenges faced by Micro, Small,
and Medium Enterprises (MSMEs). Newly established or young businesses are often
excluded from formal credit markets as traditional underwriting approaches rely
heavily on credit bureau data. This study investigates the potential of bank
statement data as an alternative data source for credit assessment to promote
financial inclusion in emerging markets. Firstly, we propose a cash flow-based
underwriting pipeline where we utilise bank statement data for end to end data
extraction and machine learning credit scoring. Secondly, we introduce a novel
dataset of 611 loan applicants from a Malaysian lending institution. Thirdly,
we develop and evaluate credit scoring models based on application information
and bank transaction-derived features. Empirical results show that the use of
such data boosts the performance of all models on our dataset, which can
improve credit scoring for new-to-lending MSMEs. Lastly, we intend to release
the anonymised bank transaction dataset to facilitate further research on MSMEs
financial inclusion within Malaysia's emerging economy.

</details>


### [196] [Dynamic Factor Analysis of Price Movements in the Philippine Stock Exchange](https://arxiv.org/abs/2510.15938)
*Brian Godwin Lim,Dominic Dayta,Benedict Ryan Tiu,Renzo Roel Tan,Len Patrick Dominic Garces,Kazushi Ikeda*

Main category: q-fin.ST

TL;DR: 本研究使用动态因子模型分析菲律宾股票市场，发现单因子模型提取的系统性因子类似综合指数，双因子模型提取市场趋势和波动因子。这些因子作为实时市场指标，在GDP增长率实时预测中减少34%以上的样本外预测误差。


<details>
  <summary>Details</summary>
Motivation: 探索动态因子模型作为可解释模型，用于理解股票价格动态，而不仅仅是预测目的。关注提取的载荷和共同因子作为理解市场现象的新框架。

Method: 使用卡尔曼滤波方法和最大似然估计，在菲律宾证券交易所应用动态因子模型，随后与资本资产定价模型进行验证。

Result: 单因子模型提取代表系统性或市场动态的共同因子，类似综合指数；双因子模型提取代表市场趋势和波动性的共同因子。提取的共同因子作为实时市场指标，在GDP增长率实时预测中减少34%以上的样本外预测误差。

Conclusion: 动态因子分析在深入理解市场价格运动动态方面具有重要价值，提取的共同因子可作为可行的实时市场指标。

Abstract: The intricate dynamics of stock markets have led to extensive research on
models that are able to effectively explain their inherent complexities. This
study leverages the econometrics literature to explore the dynamic factor model
as an interpretable model with sufficient predictive capabilities for capturing
essential market phenomena. Although the model has been extensively applied for
predictive purposes, this study focuses on analyzing the extracted loadings and
common factors as an alternative framework for understanding stock price
dynamics. The results reveal novel insights into traditional market theories
when applied to the Philippine Stock Exchange using the Kalman method and
maximum likelihood estimation, with subsequent validation against the capital
asset pricing model. Notably, a one-factor model extracts a common factor
representing systematic or market dynamics similar to the composite index,
whereas a two-factor model extracts common factors representing market trends
and volatility. Furthermore, an application of the model for nowcasting the
growth rates of the Philippine gross domestic product highlights the potential
of the extracted common factors as viable real-time market indicators, yielding
over a 34% decrease in the out-of-sample prediction error. Overall, the results
underscore the value of dynamic factor analysis in gaining a deeper
understanding of market price movement dynamics.

</details>


### [197] [Intrinsic Geometry of the Stock Market from Graph Ricci Flow](https://arxiv.org/abs/2510.15942)
*Bhargavi Srinivasan*

Main category: q-fin.ST

TL;DR: 使用离散Ollivier-Ricci图曲率和Ricci流分析NASDAQ 100指数的经验相关图，开发了一种处理Ricci流中颈缩奇点的手术技术，并构建算法利用图的固有几何流产生的曲率检测金融市场的隐藏层次结构和聚类。


<details>
  <summary>Details</summary>
Motivation: 通过经验相关图的内在几何结构来研究金融市场的内在几何特性，克服高度连接几何带来的挑战。

Method: 使用离散Ollivier-Ricci图曲率和Ricci流，以全连接图的曲率行为和下界为起点，开发颈缩奇点手术技术，构建基于曲率的算法。

Result: 开发了处理Ricci流中颈缩奇点的手术技术，构建了能够检测金融市场隐藏层次结构、社区行为和聚类的算法。

Conclusion: 利用图的固有几何流产生的曲率可以有效地检测金融市场的隐藏层次结构和聚类，为理解金融市场结构提供了新的几何视角。

Abstract: We use the discrete Ollivier-Ricci graph curvature with Ricci flow to examine
the intrinsic geometry of financial markets through the empirical correlation
graph of the NASDAQ 100 index. Our main result is the development of a
technique to perform surgery on the neckpinch singularities that form during
the Ricci flow of the empirical graph, using the behavior and the lower bound
of curvature of the fully connected graph as a starting point. We construct an
algorithm that uses the curvature generated by intrinsic geometric flow of the
graph to detect hidden hierarchies, community behavior, and clustering in
financial markets despite the underlying challenges posed by a highly connected
geometry.

</details>


### [198] [Convolutional Attention in Betting Exchange Markets](https://arxiv.org/abs/2510.16008)
*Rui Gonçalves,Vitor Miguel Ribeiro,Roman Chertovskih,António Pedro Aguiar*

Main category: q-fin.ST

TL;DR: 该研究开发了一个基于市场深度数据的短期价格预测系统，应用于Betfair赛马博彩市场，通过创新的卷积注意力机制和新型填充方法提升多变量时间序列处理性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个完全自动化的交易系统，利用市场深度数据预测博彩交易所的价格变动，特别是在赛前阶段的赛马博彩市场。

Method: 采用创新的卷积注意力机制，应用于多种循环神经网络和二维卷积循环神经网络层，并提出专门为多变量时间序列处理设计的新型卷积层填充方法。

Result: 所有提出的创新都对分类任务的性能指标产生积极影响，在卷积注意力机制和多变量时间序列问题的填充方法方面推进了当前最先进水平。

Conclusion: 该研究成功开发了一个端到端的自动化交易框架，通过创新的神经网络架构和数据处理方法，显著提升了博彩市场价格预测的准确性。

Abstract: This study presents the implementation of a short-term forecasting system for
price movements in exchange markets, using market depth data and a systematic
procedure to enable a fully automated trading system. The case study focuses on
the UK to Win Horse Racing market during the pre-live stage on the world's
leading betting exchange, Betfair. Innovative convolutional attention
mechanisms are introduced and applied to multiple recurrent neural networks and
bi-dimensional convolutional recurrent neural network layers. Additionally, a
novel padding method for convolutional layers is proposed, specifically
designed for multivariate time series processing. These innovations are
thoroughly detailed, along with their execution process. The proposed
architectures follow a standard supervised learning approach, involving model
training and subsequent testing on new data, which requires extensive
pre-processing and data analysis. The study also presents a complete end-to-end
framework for automated feature engineering and market interactions using the
developed models in production. The key finding of this research is that all
proposed innovations positively impact the performance metrics of the
classification task under examination, thereby advancing the current
state-of-the-art in convolutional attention mechanisms and padding methods
applied to multivariate time series problems.

</details>


### [199] [Institutional Differences, Crisis Shocks, and Volatility Structure: A By-Window EGARCH/TGARCH Analysis of ASEAN Stock Markets](https://arxiv.org/abs/2510.16010)
*Junlin Yang*

Main category: q-fin.ST

TL;DR: 本研究探讨制度差异和外部危机如何影响新兴亚洲股市的波动动态，发现危机期间波动持续性和非对称性增强，制度成熟度对缓冲危机影响和促进恢复有重要作用。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常关注单一市场或静态时期，缺乏在统一GARCH框架下结合制度比较与多危机动态的研究，本文旨在填补这一空白。

Method: 使用印度尼西亚、马来西亚和菲律宾2010-2024年的每日股指收益率，采用EGARCH(1,1)和TGARCH(1,1)模型进行窗口设计分析，将样本分为2013年缩减恐慌、2020-2021年COVID-19时期、2022-2023年加息周期和平静阶段。

Result: 三个市场均显示强烈的波动持续性和厚尾收益。危机期间持续性和非对称性增加，尾部厚度上升，意味着更频繁的极端变动。危机后参数回归到冲击前水平。跨国证据表明制度成熟度具有缓冲作用：马来西亚更强的监管和信息系统抑制了放大效应并加速恢复，而菲律宾较薄的市场结构延长了不稳定。

Conclusion: 危机放大了波动结构，而制度稳健性决定了恢复速度。结果为透明度、宏观审慎沟通和流动性支持提供了政策指导，以减少全球冲击期间的波动持续性。

Abstract: This study examines how institutional differences and external crises shape
volatility dynamics in emerging Asian stock markets. Using daily stock index
returns for Indonesia, Malaysia, and the Philippines from 2010 to 2024, we
estimate EGARCH(1,1) and TGARCH(1,1) models in a by-window design. The sample
is split into the 2013 Taper Tantrum, the 2020-2021 COVID-19 period, the
2022-2023 rate-hike cycle, and tranquil phases. Prior work typically studies a
single market or a static period; to our knowledge no study unifies
institutional comparison with multi-crisis dynamics within one GARCH framework.
We address this gap and show that all three markets display strong volatility
persistence and fat-tailed returns. During crises both persistence and
asymmetry increase, while tail thickness rises, implying more frequent extreme
moves. After crises, parameters revert toward pre-shock levels. Cross-country
evidence indicates a buffering role of institutional maturity: Malaysias
stronger regulatory and information systems dampen amplification and speed
recovery, whereas the Philippines thinner market structure prolongs
instability. We conclude that crises amplify volatility structures, while
institutional robustness governs recovery speed. The results provide policy
guidance on transparency, macroprudential communication, and liquidity support
to reduce volatility persistence during global shocks.

</details>


### [200] [Sentiment and Volatility in Financial Markets: A Review of BERT and GARCH Applications during Geopolitical Crises](https://arxiv.org/abs/2510.16503)
*Domenica Mino,Cillian Williamson*

Main category: q-fin.ST

TL;DR: 该研究使用BERT模型分析俄乌战争相关新闻情感，结合GARCH模型发现负面新闻情感与S&P 500指数波动率呈显著负相关关系。


<details>
  <summary>Details</summary>
Motivation: 探索人工智能技术如何理解公众情感与金融市场行为之间的复杂关系，特别是在地缘政治危机期间。

Method: 使用经过金融语言微调的BERT模型分析2024年1-7月美国主要新闻平台的文章情感，结合采用学生t分布的GARCH模型来捕捉金融收益数据的厚尾特征。

Result: 研究发现负面新闻情感与市场稳定性之间存在统计显著的负相关关系，表明悲观的战争报道与S&P 500指数波动率增加相关。

Conclusion: 该研究展示了人工智能和自然语言处理如何与经济计量建模相结合，为地缘政治危机期间的金融风险分析提供有价值的工具。

Abstract: Artificial intelligence techniques have increasingly been applied to
understand the complex relationship between public sentiment and financial
market behaviour. This study explores the relationship between the sentiment of
news related to the Russia-Ukraine war and the volatility of the stock market.
A comprehensive dataset of news articles from major US platforms, published
between January 1 and July 17, 2024, was analysed using a fine-tuned
Bidirectional Encoder Representations from Transformers (BERT) model adapted
for financial language. We extracted sentiment scores and applied a Generalised
Autoregressive Conditional Heteroscedasticity (GARCH) model, enhanced with a
Student-t distribution to capture the heavy-tailed nature of financial returns
data. The results reveal a statistically significant negative relationship
between negative news sentiment and market stability, suggesting that
pessimistic war coverage is associated with increased volatility in the S&P 500
index. This research demonstrates how artificial intelligence and natural
language processing can be integrated with econometric modelling to assess
real-time market dynamics, offering valuable tools for financial risk analysis
during geopolitical crises.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [201] [Prediction Intervals for Model Averaging](https://arxiv.org/abs/2510.16224)
*Zhongjun Qu,Wendun Wang,Xiaomeng Zhang*

Main category: econ.EM

TL;DR: 该论文提出了一种基于共形推断的模型平均预测区间方法，能够为模型平均预测提供不确定性度量，适用于各种模型类型和数据场景。


<details>
  <summary>Details</summary>
Motivation: 现有的频率模型平均方法主要局限于点预测，在一般设置下衡量预测不确定性仍然是一个未解决的问题。

Method: 基于共形推断构建预测区间，允许一般模型误设，适用于嵌套、不相交、重叠或任意组合的多个模型平均，权重可以依赖于估计样本。

Result: 建立了两种假设下的覆盖保证：在可交换性下的精确有限样本有效性（适用于横截面数据）和在平稳性下的渐近有效性（适用于时间序列数据）。

Conclusion: 提出的方法为模型平均预测提供了可靠的不确定性度量框架，在房地产评估和股票溢价预测等应用中得到了验证。

Abstract: A rich set of frequentist model averaging methods has been developed, but
their applications have largely been limited to point prediction, as measuring
prediction uncertainty in general settings remains an open problem. In this
paper we propose prediction intervals for model averaging based on conformal
inference. These intervals cover out-of-sample realizations of the outcome
variable with a pre-specified probability, providing a way to assess predictive
uncertainty beyond point prediction. The framework allows general model
misspecification and applies to averaging across multiple models that can be
nested, disjoint, overlapping, or any combination thereof, with weights that
may depend on the estimation sample. We establish coverage guarantees under two
sets of assumptions: exact finite-sample validity under exchangeability,
relevant for cross-sectional data, and asymptotic validity under stationarity,
relevant for time-series data. We first present a benchmark algorithm and then
introduce a locally adaptive refinement and split-sample procedures that
broaden applicability. The methods are illustrated with a cross-sectional
application to real estate appraisal and a time-series application to equity
premium forecasting.

</details>


### [202] [On the Asymptotics of the Minimax Linear Estimator](https://arxiv.org/abs/2510.16661)
*Jing Kong*

Main category: econ.EM

TL;DR: 本文研究了通过极小极大凸优化方法设置权重的加权估计器，证明了该估计器在正则条件下具有根号n一致性和渐近正态性，并推导了其渐近方差。


<details>
  <summary>Details</summary>
Motivation: 许多因果估计量可以表示为未知回归函数的连续线性泛函，但现有的极小极大线性估计器的根号n理论仍然有限，本文旨在填补这一空白。

Method: 使用极小极大程序设置权重的加权估计器，通过凸优化问题在条件偏差和方差之间进行权衡。

Result: 在正则条件下，极小极大线性估计器具有根号n一致性和渐近正态性；在温和方差条件下，该估计器达到半参数效率界，无需文献中常用的增强步骤即可实现一阶最优性。

Conclusion: 模拟和实证应用表明，在满足正则条件的设计中，标准误差置信区间足够；否则，偏差感知区间仍然重要。

Abstract: Many causal estimands, such as average treatment effects under
unconfoundedness, can be written as continuous linear functionals of an unknown
regression function. We study a weighting estimator that sets weights by a
minimax procedure: solving a convex optimization problem that trades off
worst-case conditional bias against variance. Despite its growing use, general
root-$n$ theory for this method has been limited. This paper fills that gap.
Under regularity conditions, we show that the minimax linear estimator is
root-$n$ consistent and asymptotically normal, and we derive its asymptotic
variance. These results justify ignoring worst-case bias when forming
large-sample confidence intervals and make inference less sensitive to the
scaling of the function class. With a mild variance condition, the estimator
attains the semiparametric efficiency bound, so an augmentation step commonly
used in the literature is not needed to achieve first-order optimality.
Evidence from simulations and three empirical applications, including
job-training and minimum-wage policies, points to a simple rule: in designs
satisfying our regularity conditions, standard-error confidence intervals
suffice; otherwise, bias-aware intervals remain important.

</details>


### [203] [Causal Inference in High-Dimensional Generalized Linear Models with Binary Outcomes](https://arxiv.org/abs/2510.16669)
*Jing Kong*

Main category: econ.EM

TL;DR: 提出了一种针对高维广义线性模型中因果效应的去偏估计器，适用于二元结果和一般链接函数，不依赖倾向得分估计。


<details>
  <summary>Details</summary>
Motivation: 在高维广义线性模型中，现有因果效应估计方法（如逆倾向得分估计器和双机器学习估计器）在有限样本中表现不佳，需要开发更稳健的估计方法。

Method: 通过正则化回归插件增强权重计算，权重来自凸优化问题，近似平衡链接导数加权协变量并控制方差，不依赖倾向得分估计。

Result: 在标准条件下，估计器对密集线性对比和因果参数具有√n一致性和渐近正态性。模拟结果显示该方法在有限样本中优于逆倾向得分估计器和双机器学习估计器。

Conclusion: 所提出的去偏估计器在高维广义线性模型中表现优异，在国家支持工作培训数据应用中，估计结果和置信区间接近实验基准。

Abstract: This paper proposes a debiased estimator for causal effects in
high-dimensional generalized linear models with binary outcomes and general
link functions. The estimator augments a regularized regression plug-in with
weights computed from a convex optimization problem that approximately balances
link-derivative-weighted covariates and controls variance; it does not rely on
estimated propensity scores. Under standard conditions, the estimator is
$\sqrt{n}$-consistent and asymptotically normal for dense linear contrasts and
causal parameters. Simulation results show the superior performance of our
approach in comparison to alternatives such as inverse propensity score
estimators and double machine learning estimators in finite samples. In an
application to the National Supported Work training data, our estimates and
confidence intervals are close to the experimental benchmark.

</details>


### [204] [On Quantile Treatment Effects, Rank Similarity,and Variation of Instrumental Variables](https://arxiv.org/abs/2510.16681)
*Sukjin Han,Haiqing Xu*

Main category: econ.EM

TL;DR: 本文开发了一个非参数框架，用于识别和估计不可分离内生性下的分布处理效应。通过放宽秩相似性假设，构建了基于线性半无限规划的识别边界，并建立了大样本性质。


<details>
  <summary>Details</summary>
Motivation: 重新审视广泛采用的秩相似性假设，发现其限制性较强，需要更弱的识别条件来处理不可分离内生性问题。

Method: 采用线性半无限规划方法构建识别边界，利用鞍点结构和KKT条件分析大样本性质。

Result: 建立了分布处理效应的识别边界，并证明了估计边界的一致性及渐近分布结果。

Conclusion: 提出的框架在较弱假设下有效识别分布处理效应，外生工具变量变化可进一步收紧边界，具有良好统计性质。

Abstract: This paper develops a nonparametric framework to identify and estimate
distributional treatment effects under nonseparable endogeneity. We begin by
revisiting the widely adopted \emph{rank similarity} (RS) assumption and
characterizing it by the relationship it imposes between observed and
counterfactual potential outcome distributions. The characterization highlights
the restrictiveness of RS, motivating a weaker identifying condition. Under
this alternative, we construct identifying bounds on the distributional
treatment effects of interest through a linear semi-infinite programming (SILP)
formulation. Our identification strategy also clarifies how richer exogenous
instrument variation, such as multi-valued or multiple instruments, can further
tighten these bounds. Finally, exploiting the SILP's saddle-point structure and
Karush-Kuhn-Tucker (KKT) conditions, we establish large-sample properties for
the empirical SILP: consistency and asymptotic distribution results for the
estimated bounds and associated solutions.

</details>


### [205] [Local Overidentification and Efficiency Gains in Modern Causal Inference and Data Combination](https://arxiv.org/abs/2510.16683)
*Xiaohong Chen,Haitian Xie*

Main category: econ.EM

TL;DR: 本文研究非参数局部（过度）识别和半参数效率，开发了一个统一方法将结构模型转化为可观测的统计模型，并应用于三种因果模型：无混杂的一般处理模型、负控制模型和未观测混杂的长期因果推断模型。


<details>
  <summary>Details</summary>
Motivation: 研究现代因果框架中的非参数局部识别和半参数效率问题，特别是在存在过度识别的情况下如何推导效率界限和有效估计量。

Method: 开发统一方法：将带有潜变量的结构模型转化为可观测的统计模型，通过条件矩限制分析局部过度识别，应用于三种因果模型设计。

Result: 第一种设计产生局部恰好识别的统计模型，所有正则渐近线性估计量具有相同渐近方差；后两种模型涉及非参数内生性，自然产生局部过度识别，导致某些双重稳健正交矩估计量效率低下。

Conclusion: 在过度识别模型（ii）和（iii）中，放松了现有文献中恢复恰好识别的强假设条件，推导了广义效率界限并构建了有效估计量。

Abstract: This paper studies nonparametric local (over-)identification, in the sense of
Chen and Santos (2018), and the associated semiparametric efficiency in modern
causal frameworks. We develop a unified approach that begins by translating
structural models with latent variables into their induced statistical models
of observables and then analyzes local overidentification through conditional
moment restrictions. We apply this approach to three leading models: (i) the
general treatment model under unconfoundedness, (ii) the negative control
model, and (iii) the long-term causal inference model under unobserved
confounding. The first design yields a locally just-identified statistical
model, implying that all regular asymptotically linear estimators of the
treatment effect share the same asymptotic variance, equal to the (trivial)
semiparametric efficiency bound. In contrast, the latter two models involve
nonparametric endogeneity and are naturally locally overidentified;
consequently, some doubly robust orthogonal moment estimators of the average
treatment effect are inefficient. Whereas existing work typically imposes
strong conditions to restore just-identification before deriving the efficiency
bound, we relax such assumptions and characterize the general efficiency bound,
along with efficient estimators, in the overidentified models (ii) and (iii).

</details>


### [206] [Equilibrium-Constrained Estimation of Recursive Logit Choice Models](https://arxiv.org/abs/2510.16886)
*Hung Tran,Tien Mai,Minh Hoang Ha*

Main category: econ.EM

TL;DR: 提出一种新的递归logit模型估计方法，将最大似然估计问题重新表述为带均衡约束的优化问题，并转化为锥优化问题，显著提高了计算效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统递归logit模型估计依赖嵌套定点算法，计算成本高且数值不稳定，需要更高效稳定的估计方法。

Method: 将最大似然估计重新表述为带均衡约束的优化问题，将结构参数和价值函数都作为决策变量，并转化为指数锥优化问题，使用现代锥求解器如MOSEK求解。

Result: 在合成和真实数据集上的实验表明，该凸重构方法在精度上与传统方法相当，同时在计算稳定性和效率上有显著提升。

Conclusion: 提出的凸重构方法为递归logit模型估计提供了实用且可扩展的替代方案，解决了传统方法计算效率低和数值不稳定的问题。

Abstract: The recursive logit (RL) model provides a flexible framework for modeling
sequential decision-making in transportation and choice networks, with
important applications in route choice analysis, multiple discrete choice
problems, and activity-based travel demand modeling. Despite its versatility,
estimation of the RL model typically relies on nested fixed-point (NFXP)
algorithms that are computationally expensive and prone to numerical
instability. We propose a new approach that reformulates the maximum likelihood
estimation problem as an optimization problem with equilibrium constraints,
where both the structural parameters and the value functions are treated as
decision variables. We further show that this formulation can be equivalently
transformed into a conic optimization problem with exponential cones, enabling
efficient solution using modern conic solvers such as MOSEK. Experiments on
synthetic and real-world datasets demonstrate that our convex reformulation
achieves accuracy comparable to traditional methods while offering significant
improvements in computational stability and efficiency, thereby providing a
practical and scalable alternative for recursive logit model estimation.

</details>


### [207] [Mixed LR-$C(α)$-type tests for irregular hypotheses, general criterion functions and misspecified models](https://arxiv.org/abs/2510.17070)
*Jean-Marie Dufour,Purevdorj Tuvaandorj*

Main category: econ.EM

TL;DR: 提出一种具有稳健性的似然比型检验，在极值估计框架下具备C(α)型程序的稳健特性，适用于模型误设和约束参数空间的情况。


<details>
  <summary>Details</summary>
Motivation: 标准LR检验在模型误设（信息矩阵等式不成立）或参数空间存在约束（如边界参数）时表现不佳，需要开发更稳健的检验方法。

Method: 通过对受限和不受限准则函数分别进行调整来构建检验统计量，仅需使用根n一致估计量来处理冗余参数，适用于不规则假设。

Result: 该检验在模型误设下仍保持卡方分布，在ARCH模型和生存回归模拟中表现出准确的大小和良好的功效，实证应用显示比传统t检验提供更多信息。

Conclusion: 提出的LR型检验在保持标准LR统计量优点的同时，增强了在模型误设和约束参数空间情况下的稳健性，为复杂模型提供更可靠的推断工具。

Abstract: This paper introduces a likelihood ratio (LR)-type test that possesses the
robustness properties of \(C(\alpha)\)-type procedures in an extremum
estimation setting.
  The test statistic is constructed by applying separate adjustments to the
restricted and unrestricted criterion functions, and is shown to be
asymptotically pivotal under minimal conditions. It features two main
robustness properties. First, unlike standard LR-type statistics, its null
asymptotic distribution remains chi-square even under model misspecification,
where the information matrix equality fails. Second, it accommodates irregular
hypotheses involving constrained parameter spaces, such as boundary parameters,
relying solely on root-\(n\)-consistent estimators for nuisance parameters.
When the model is correctly specified, no boundary constraints are present, and
parameters are estimated by extremum estimators, the proposed test reduces to
the standard LR-type statistic.
  Simulations with ARCH models, where volatility parameters are constrained to
be nonnegative, and parametric survival regressions with potentially monotone
increasing hazard functions, demonstrate that our test maintains accurate size
and exhibits good power. An empirical application to a two-way error components
model shows that the proposed test can provide more informative inference than
the conventional \(t\)-test.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [208] [VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search](https://arxiv.org/abs/2510.15948)
*MingSheng Li,Guangze Zhao,Sichen Liu*

Main category: cs.AI

TL;DR: VisuoAlign是一个通过提示引导树搜索进行多模态安全对齐的框架，旨在解决大型视觉语言模型的安全对齐挑战。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法对多模态越狱攻击存在脆弱性，视觉输入引入新的攻击面，推理链缺乏安全监督，模态融合会降低对齐效果。

Method: 通过视觉-文本交互提示将安全约束嵌入推理过程，使用蒙特卡洛树搜索构建多样化的安全关键提示轨迹，引入基于提示的缩放确保实时风险检测和合规响应。

Result: 实验表明VisuoAlign能主动暴露风险，实现全面的数据集生成，显著提升LVLMs对复杂跨模态威胁的鲁棒性。

Conclusion: VisuoAlign框架有效解决了多模态安全对齐的关键挑战，为大型视觉语言模型的安全部署提供了可行方案。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable progress in
multimodal perception and generation, yet their safety alignment remains a
critical challenge.Existing defenses and vulnerable to multimodal jailbreaks,
as visual inputs introduce new attack surfaces, reasoning chains lack safety
supervision, and alignment often degrades under modality fusion.To overcome
these limitation, we propose VisuoAlign, a framework for multi-modal safety
alignment via prompt-guided tree search.VisuoAlign embeds safety constrains
into the reasoning process through visual-textual interactive prompts, employs
Monte Carlo Tree Search(MCTS) to systematically construct diverse
safety-critical prompt trajectories, and introduces prompt-based scaling to
ensure real-time risk detection and compliant responses.Extensive experiments
demonstrate that VisuoAlign proactively exposes risks, enables comprehensive
dataset generation, and significantly improves the robustness of LVLMs against
complex cross-modal threats.

</details>


### [209] [Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding](https://arxiv.org/abs/2510.15952)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 本文提出了结构化认知循环（SCL）作为可执行的认识论框架，用于实现涌现智能，将哲学洞见转化为可计算结构，重新定义智能为通过意向性理解重建自身认知状态的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型缺乏真正的认知理解，暴露了认知架构的缺失。传统AI研究关注"什么是智能"的本体论问题，而SCL关注"在什么条件下认知涌现"的认识论问题。

Method: 基于心智哲学和认知现象学，结合过程哲学、具身认知和扩展心智理论，将智能定义为执行过程——判断、记忆、控制、行动和调节的连续循环。

Result: SCL将哲学洞见操作化为可计算结构，实现"可执行认识论"；功能分离的认知架构比单一提示系统产生更一致和可解释的行为；重新定义智能为重建自身认知状态的能力。

Conclusion: SCL框架对心智哲学、认识论和AI产生重要影响：允许认知理论被实施和测试；将行为建立在认知结构而非统计规律上；将知识视为在现象学一致循环中的持续重建。

Abstract: Large language models exhibit intelligence without genuine epistemic
understanding, exposing a key gap: the absence of epistemic architecture. This
paper introduces the Structured Cognitive Loop (SCL) as an executable
epistemological framework for emergent intelligence. Unlike traditional AI
research asking "what is intelligence?" (ontological), SCL asks "under what
conditions does cognition emerge?" (epistemological). Grounded in philosophy of
mind and cognitive phenomenology, SCL bridges conceptual philosophy and
implementable cognition. Drawing on process philosophy, enactive cognition, and
extended mind theory, we define intelligence not as a property but as a
performed process -- a continuous loop of judgment, memory, control, action,
and regulation. SCL makes three contributions. First, it operationalizes
philosophical insights into computationally interpretable structures, enabling
"executable epistemology" -- philosophy as structural experiment. Second, it
shows that functional separation within cognitive architecture yields more
coherent and interpretable behavior than monolithic prompt based systems,
supported by agent evaluations. Third, it redefines intelligence: not
representational accuracy but the capacity to reconstruct its own epistemic
state through intentional understanding. This framework impacts philosophy of
mind, epistemology, and AI. For philosophy, it allows theories of cognition to
be enacted and tested. For AI, it grounds behavior in epistemic structure
rather than statistical regularity. For epistemology, it frames knowledge not
as truth possession but as continuous reconstruction within a
phenomenologically coherent loop. We situate SCL within debates on cognitive
phenomenology, emergence, normativity, and intentionality, arguing that real
progress requires not larger models but architectures that realize cognitive
principles structurally.

</details>


### [210] [Exploring the Potential of Citiverses for Regulatory Learning](https://arxiv.org/abs/2510.15959)
*Isabelle Hupont,Marisa Ponti,Sven Schade*

Main category: cs.AI

TL;DR: 本文提出了一个科学政策议程，探索citiverses作为监管学习实验空间的潜力，通过专家咨询识别关键研究领域和实验主题，强调负责任的发展方法。


<details>
  <summary>Details</summary>
Motivation: citiverses具有通过沉浸式虚拟环境支持监管学习的潜力，为政策场景和技术实验提供平台。

Method: 基于与高级专家小组（包括欧盟委员会政策制定者、国家政府科学顾问和数字监管领域领先研究者）的咨询，识别关键研究领域和实验主题。

Result: 确定了可扩展性、实时反馈、复杂性建模、跨境协作、风险降低、公民参与、伦理考虑和新兴技术整合等关键研究领域，以及交通、城市规划、环境/气候危机等实验主题。

Conclusion: citiverses可作为监管学习的重要实验空间，但需要负责任的发展方法，充分考虑伦理、经济、生态和社会维度，并整合到更广泛的实验生态系统。

Abstract: Citiverses hold the potential to support regulatory learning by offering
immersive, virtual environments for experimenting with policy scenarios and
technologies. This paper proposes a science-for-policy agenda to explore the
potential of citiverses as experimentation spaces for regulatory learning,
grounded in a consultation with a high-level panel of experts, including
policymakers from the European Commission, national government science advisers
and leading researchers in digital regulation and virtual worlds. It identifies
key research areas, including scalability, real-time feedback, complexity
modelling, cross-border collaboration, risk reduction, citizen participation,
ethical considerations and the integration of emerging technologies. In
addition, the paper analyses a set of experimental topics, spanning
transportation, urban planning and the environment/climate crisis, that could
be tested in citiverse platforms to advance regulatory learning in these areas.
The proposed work is designed to inform future research for policy and
emphasizes a responsible approach to developing and using citiverses. It
prioritizes careful consideration of the ethical, economic, ecological and
social dimensions of different regulations. The paper also explores essential
preliminary steps necessary for integrating citiverses into the broader
ecosystems of experimentation spaces, including test beds, living labs and
regulatory sandboxes

</details>


### [211] [PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency](https://arxiv.org/abs/2510.15966)
*Shian Jia,Ziyang Huang,Xinbo Wang,Haofei Zhang,Mingli Song*

Main category: cs.AI

TL;DR: 提出了PISA记忆系统，基于皮亚杰认知发展理论，通过三模态适应机制和混合记忆访问架构，显著提升了AI代理的适应性和长期知识保留能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理记忆系统缺乏对多样化任务的适应性，忽视了记忆的建构性和任务导向作用，需要更灵活和实用的记忆机制。

Method: 采用三模态适应机制（图式更新、图式演化和图式创建）保持记忆组织连贯性，设计结合符号推理和神经检索的混合记忆访问架构。

Result: 在LOCOMO基准和新提出的AggQA数据分析基准上，PISA创造了新的最先进水平，显著提升了适应性和长期知识保留能力。

Conclusion: PISA记忆系统通过借鉴认知发展理论，为AI代理提供了更实用、适应性更强的记忆机制，在多个基准测试中表现出色。

Abstract: Memory systems are fundamental to AI agents, yet existing work often lacks
adaptability to diverse tasks and overlooks the constructive and task-oriented
role of AI agent memory. Drawing from Piaget's theory of cognitive development,
we propose PISA, a pragmatic, psych-inspired unified memory system that
addresses these limitations by treating memory as a constructive and adaptive
process. To enable continuous learning and adaptability, PISA introduces a
trimodal adaptation mechanism (i.e., schema updation, schema evolution, and
schema creation) that preserves coherent organization while supporting flexible
memory updates. Building on these schema-grounded structures, we further design
a hybrid memory access architecture that seamlessly integrates symbolic
reasoning with neural retrieval, significantly improving retrieval accuracy and
efficiency. Our empirical evaluation, conducted on the existing LOCOMO
benchmark and our newly proposed AggQA benchmark for data analysis tasks,
confirms that PISA sets a new state-of-the-art by significantly enhancing
adaptability and long-term knowledge retention.

</details>


### [212] [Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games](https://arxiv.org/abs/2510.15974)
*Chris Su,Harrison Li,Matheus Marques,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: 大型推理模型在解决超过特定复杂度阈值的谜题时会出现性能崩溃，即使提供环境接口让模型能够跟踪状态空间，也无法延迟或消除这种崩溃。


<details>
  <summary>Details</summary>
Motivation: 探讨大型推理模型性能崩溃的真正原因，验证是否由于模型需要自行跟踪状态空间这一要求混淆了对真实推理能力的评估。

Method: 为大型语言模型提供汉诺塔问题的环境接口，允许模型通过工具调用进行移动、提供书面理由、观察结果状态空间，并重新提示自己进行下一步移动。

Result: 环境接口的访问无法延迟或消除性能崩溃，模型参数化策略分析显示与最优策略和均匀随机策略的差异都在增加，表明模型在每个复杂度级别都表现出模式崩溃。

Conclusion: 性能崩溃现象可能源于模型在特定复杂度下出现模式崩溃，性能取决于该模式是否反映问题的正确解决方案，类似现象可能也存在于大型推理模型中。

Abstract: Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in
performance on solving puzzles beyond certain perplexity thresholds. In
subsequent discourse, questions have arisen as to whether the nature of the
task muddles an evaluation of true reasoning. One potential confound is the
requirement that the model keep track of the state space on its own. We provide
a large language model (LLM) with an environment interface for Tower of Hanoi
problems, allowing it to make a move with a tool call, provide written
justification, observe the resulting state space, and reprompt itself for the
next move. We observe that access to an environment interface does not delay or
eradicate performance collapse. Furthermore, LLM-parameterized policy analysis
reveals increasing divergence from both optimal policies and uniformly random
policies, suggesting that the model exhibits mode-like collapse at each level
of complexity, and that performance is dependent upon whether the mode reflects
the correct solution for the problem. We suggest that a similar phenomena might
take place in LRMs.

</details>


### [213] [Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition](https://arxiv.org/abs/2510.15980)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: 提出认知负荷痕迹（CLTs）作为深度模型的中层可解释性框架，通过符号化、时变的函数量化模型内部资源分配，包含内在、外在和相关负荷三个分量，能预测错误发生、揭示认知策略，并通过负荷引导干预提高推理效率15-30%


<details>
  <summary>Details</summary>
Motivation: 受人类认知负荷理论启发，为深度模型开发中层可解释性框架，以量化模型内部资源分配和推理动态

Method: 将CLTs定义为三组分随机过程（内在负荷IL_t、外在负荷EL_t、相关负荷GL_t），通过注意力熵、KV缓存未命中率、表示分散度和解码稳定性等可测量代理进行实例化，提出符号化公式和可视化方法

Result: 在推理和规划基准测试中，CLTs能预测错误发生、揭示认知策略，负荷引导干预在保持准确性的同时将推理效率提高15-30%

Conclusion: CLTs为深度模型提供了有效的可解释性分析框架，能够量化认知负荷动态并指导模型优化

Abstract: We propose \textbf{Cognitive Load Traces} (CLTs) as a mid-level
interpretability framework for deep models, inspired by Cognitive Load Theory
in human cognition. CLTs are defined as symbolic, temporally varying functions
that quantify model-internal resource allocation. Formally, we represent CLTs
as a three-component stochastic process $(\mathrm{IL}_t, \mathrm{EL}_t,
\mathrm{GL}_t)$, corresponding to \emph{Intrinsic}, \emph{Extraneous}, and
\emph{Germane} load. Each component is instantiated through measurable proxies
such as attention entropy, KV-cache miss ratio, representation dispersion, and
decoding stability. We propose both symbolic formulations and visualization
methods (load curves, simplex diagrams) that enable interpretable analysis of
reasoning dynamics. Experiments on reasoning and planning benchmarks show that
CLTs predict error-onset, reveal cognitive strategies, and enable load-guided
interventions that improve reasoning efficiency by 15-30\% while maintaining
accuracy.

</details>


### [214] [ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization](https://arxiv.org/abs/2510.15981)
*Rafael Cabral,Tuan Manh Do,Xuejun Yu,Wai Ming Tai,Zijin Feng,Xin Shen*

Main category: cs.AI

TL;DR: ProofFlow是一个新的证明自动形式化管道，通过构建逻辑依赖图和使用基于引理的方法来保持原始证明的结构保真度，在自动形式化任务中达到了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前的方法主要关注生成可执行代码，但经常无法保持原始人工编写证明的语义含义和逻辑结构，因此需要一种能够保持结构保真度的新方法。

Method: ProofFlow首先构建有向无环图来映射证明步骤之间的逻辑依赖关系，然后采用基于引理的方法系统地将每个步骤形式化为中间引理，从而保持原始论证的逻辑结构。

Result: 在包含184个本科水平问题的新基准测试中，ProofFlow实现了0.545的ProofScore，显著超过了全证明形式化(0.123)和步骤证明形式化(0.072)等基线方法。

Conclusion: ProofFlow管道、基准测试和评分指标为证明自动形式化设定了新的最先进标准，其开源发布旨在推动该领域的进一步发展。

Abstract: Proof autoformalization, the task of translating natural language theorems
and proofs into machine-verifiable code, is a critical step for integrating
large language models into rigorous mathematical workflows. Current approaches
focus on producing executable code, but they frequently fail to preserve the
semantic meaning and logical structure of the original human-written argument.
To address this, we introduce ProofFlow, a novel pipeline that treats
structural fidelity as a primary objective. ProofFlow first constructs a
directed acyclic graph (DAG) to map the logical dependencies between proof
steps. Then, it employs a novel lemma-based approach to systematically
formalize each step as an intermediate lemma, preserving the logical structure
of the original argument. To facilitate evaluation, we present a new benchmark
of 184 undergraduate-level problems, manually annotated with step-by-step
solutions and logical dependency graphs, and introduce ProofScore, a new
composite metric to evaluate syntactic correctness, semantic faithfulness, and
structural fidelity. Experimental results show our pipeline sets a new
state-of-the-art for autoformalization, achieving a ProofScore of 0.545,
substantially exceeding baselines like full-proof formalization (0.123), which
processes the entire proof at once, and step-proof formalization (0.072), which
handles each step independently. Our pipeline, benchmark, and score metric are
open-sourced to encourage further progress at
https://github.com/Huawei-AI4Math/ProofFlow.

</details>


### [215] [Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science](https://arxiv.org/abs/2510.15983)
*Sarah Rebecca Ondraszek,Jörg Waitelonis,Katja Keller,Claudia Niessner,Anna M. Jacyszyn,Harald Sack*

Main category: cs.AI

TL;DR: 提出基于MO|RE数据仓库构建知识图谱的愿景，旨在标准化和机器可理解地建模与共享运动表现数据。


<details>
  <summary>Details</summary>
Motivation: 运动表现测试是体育科学研究的核心，但现有数据缺乏标准化和互操作性，难以在不同研究间进行比较和共享。

Method: 基于基本形式本体论构建本体，形式化表示计划规范、具体过程和相关测量之间的相互关系，将MO|RE数据转化为知识图谱。

Result: 提出了一个知识图谱构建框架，能够标准化运动表现数据的建模和共享方式。

Conclusion: 该知识图谱方法将改变运动表现数据的建模和共享范式，使其更加标准化和机器可理解，促进跨研究的数据互操作性。

Abstract: An essential component for evaluating and comparing physical and cognitive
capabilities between populations is the testing of various factors related to
human performance. As a core part of sports science research, testing motor
performance enables the analysis of the physical health of different
demographic groups and makes them comparable.
  The Motor Research (MO|RE) data repository, developed at the Karlsruhe
Institute of Technology, is an infrastructure for publishing and archiving
research data in sports science, particularly in the field of motor performance
research. In this paper, we present our vision for creating a knowledge graph
from MO|RE data. With an ontology rooted in the Basic Formal Ontology, our
approach centers on formally representing the interrelation of plan
specifications, specific processes, and related measurements. Our goal is to
transform how motor performance data are modeled and shared across studies,
making it standardized and machine-understandable. The idea presented here is
developed within the Leibniz Science Campus ``Digital Transformation of
Research'' (DiTraRe).

</details>


### [216] [A Non-overlap-based Conflict Measure for Random Permutation Sets](https://arxiv.org/abs/2510.16001)
*Ruolan Cheng,Yong Deng,Enrique Herrera-Viedma*

Main category: cs.AI

TL;DR: 本文提出了一种基于随机置换集(RPS)的冲突度量方法，从随机有限集(RFS)和Dempster-Shafer理论(DST)两个角度分析置换间的冲突，并引入基于秩偏重叠(RBO)的不一致性度量。


<details>
  <summary>Details</summary>
Motivation: 在涉及顺序信息的不确定性推理中，如何度量由置换质量函数表示的两个证据之间的冲突是一个亟待解决的研究课题。

Method: 从置换的观察出发，首先基于秩偏重叠(RBO)度量定义置换间的不一致性度量，然后提出基于非重叠的RPS冲突度量方法，将RPS理论视为DST的扩展。

Result: 通过数值示例展示了所提冲突度量的行为和性质，该方法具有自然的顶部加权特性，并能从DST视角有效度量RPS间的冲突。

Conclusion: 所提方法不仅具有自然的顶部加权特性，能够从DST视角有效度量RPS间的冲突，还为决策者提供了权重、参数和截断深度的灵活选择。

Abstract: Random permutation set (RPS) is a new formalism for reasoning with
uncertainty involving order information. Measuring the conflict between two
pieces of evidence represented by permutation mass functions remains an urgent
research topic in order-structured uncertain information fusion. In this paper,
a detailed analysis of conflicts in RPS is carried out from two different
perspectives: random finite set (RFS) and Dempster-Shafer theory (DST).
Starting from the observation of permutations, we first define an inconsistency
measure between permutations inspired by the rank-biased overlap(RBO) measure
and further propose a non-overlap-based conflict measure method for RPSs. This
paper regards RPS theory (RPST) as an extension of DST. The order information
newly added in focal sets indicates qualitative propensity, characterized by
top-ranked elements occupying a more critical position. Some numerical examples
are used to demonstrate the behavior and properties of the proposed conflict
measure. The proposed method not only has the natural top-weightedness property
and can effectively measure the conflict between RPSs from the DST view but
also provides decision-makers with a flexible selection of weights, parameters,
and truncated depths.

</details>


### [217] [PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction](https://arxiv.org/abs/2510.16004)
*Andreas Radler,Vincent Seyfried,Stefan Pirker,Johannes Brandstetter,Thomas Lichtenegger*

Main category: cs.AI

TL;DR: PAINT是一种并行时间神经孪生方法，用于从测量数据中建模动态系统，确保系统状态始终保持在真实轨迹上，相比自回归模型具有更好的轨迹跟踪能力。


<details>
  <summary>Details</summary>
Motivation: 神经孪生作为神经代理的进一步发展，旨在创建真实系统的数字副本，需要能够在测试时根据测量更新状态，实现上下文特定的决策制定。关键特性是保持轨迹跟踪能力。

Method: PAINT训练生成神经网络来并行建模时间上的状态分布。在测试时，通过滑动窗口方式从测量数据预测状态。该方法与架构无关，适用于各种动态系统建模。

Result: 在具有挑战性的二维湍流流体动力学问题上，PAINT能够保持轨迹跟踪，从稀疏测量中高保真地预测系统状态。理论分析表明PAINT能够保持在轨迹上，而自回归模型通常不能。

Conclusion: PAINT具有开发保持轨迹跟踪的神经孪生的潜力，能够实现更准确的状态估计和决策制定，为动态系统建模提供了有效解决方案。

Abstract: Neural surrogates have shown great potential in simulating dynamical systems,
while offering real-time capabilities. We envision Neural Twins as a
progression of neural surrogates, aiming to create digital replicas of real
systems. A neural twin consumes measurements at test time to update its state,
thereby enabling context-specific decision-making. A critical property of
neural twins is their ability to remain on-trajectory, i.e., to stay close to
the true system state over time. We introduce Parallel-in-time Neural Twins
(PAINT), an architecture-agnostic family of methods for modeling dynamical
systems from measurements. PAINT trains a generative neural network to model
the distribution of states parallel over time. At test time, states are
predicted from measurements in a sliding window fashion. Our theoretical
analysis shows that PAINT is on-trajectory, whereas autoregressive models
generally are not. Empirically, we evaluate our method on a challenging
two-dimensional turbulent fluid dynamics problem. The results demonstrate that
PAINT stays on-trajectory and predicts system states from sparse measurements
with high fidelity. These findings underscore PAINT's potential for developing
neural twins that stay on-trajectory, enabling more accurate state estimation
and decision-making.

</details>


### [218] [Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis](https://arxiv.org/abs/2510.16033)
*Junyu Ren,Wensheng Gan,Guangyu Zhang,Wei Zhong,Philip S. Yu*

Main category: cs.AI

TL;DR: 提出ISGFAN框架，通过信息分离和全局-局部对抗学习解决噪声干扰和领域偏移共存的跨领域故障诊断问题


<details>
  <summary>Details</summary>
Motivation: 现有迁移故障诊断方法通常假设数据干净或领域相似性足够，但在工业环境中严重噪声干扰和领域偏移共存，限制了其有效性

Method: 基于信息分离架构，结合对抗学习和改进的正交损失来解耦领域不变故障表示；采用全局-局部领域对抗方案约束模型的条件分布和边缘分布

Result: 在三个公共基准数据集上的实验表明，该方法优于其他现有方法

Conclusion: ISGFAN框架在噪声条件下的跨领域故障诊断中表现出优越性

Abstract: Existing transfer fault diagnosis methods typically assume either clean data
or sufficient domain similarity, which limits their effectiveness in industrial
environments where severe noise interference and domain shifts coexist. To
address this challenge, we propose an information separation global-focal
adversarial network (ISGFAN), a robust framework for cross-domain fault
diagnosis under noise conditions. ISGFAN is built on an information separation
architecture that integrates adversarial learning with an improved orthogonal
loss to decouple domain-invariant fault representation, thereby isolating noise
interference and domain-specific characteristics. To further strengthen
transfer robustness, ISGFAN employs a global-focal domain-adversarial scheme
that constrains both the conditional and marginal distributions of the model.
Specifically, the focal domain-adversarial component mitigates
category-specific transfer obstacles caused by noise in unsupervised scenarios,
while the global domain classifier ensures alignment of the overall
distribution. Experiments conducted on three public benchmark datasets
demonstrate that the proposed method outperforms other prominent existing
approaches, confirming the superiority of the ISGFAN framework. Data and code
are available at https://github.com/JYREN-Source/ISGFAN

</details>


### [219] [Algorithms for dynamic scheduling in manufacturing, towards digital factories Improving Deadline Feasibility and Responsiveness via Temporal Networks](https://arxiv.org/abs/2510.16047)
*Ioan Hedea*

Main category: cs.AI

TL;DR: 该论文结合离线约束编程优化和在线时间网络执行，创建在不确定性下仍可行的调度方案，消除100%的截止时间违规，仅增加3-5%的制造周期开销。


<details>
  <summary>Details</summary>
Motivation: 现代制造系统需要满足严格的交付期限，同时应对由过程噪声、设备变异性和人为干预引起的随机任务持续时间。传统的确定性调度在现实偏离名义计划时会失效，导致昂贵的紧急维修。

Method: 首先构建具有每项任务截止时间的柔性作业车间约束编程模型，插入最优缓冲区Δ*获得完全主动基线；然后将结果计划转换为具有不确定性的简单时间网络，验证动态可控性。

Result: 在Kacem 1-4基准套件上的广泛蒙特卡洛模拟显示，该混合方法消除了最先进元启发式调度中观察到的100%截止时间违规，同时仅增加3-5%的制造周期开销。

Conclusion: 该工作展示了时间网络推理如何弥合主动缓冲和动态鲁棒性之间的差距，使工业更接近真正的数字化、自校正工厂。

Abstract: Modern manufacturing systems must meet hard delivery deadlines while coping
with stochastic task durations caused by process noise, equipment variability,
and human intervention. Traditional deterministic schedules break down when
reality deviates from nominal plans, triggering costly last-minute repairs.
This thesis combines offline constraint-programming (CP) optimisation with
online temporal-network execution to create schedules that remain feasible
under worst-case uncertainty. First, we build a CP model of the flexible
job-shop with per-job deadline tasks and insert an optimal buffer $\Delta^*$ to
obtain a fully pro-active baseline. We then translate the resulting plan into a
Simple Temporal Network with Uncertainty (STNU) and verify dynamic
controllability, which guarantees that a real-time dispatcher can retime
activities for every bounded duration realisation without violating resource or
deadline constraints. Extensive Monte-Carlo simulations on the open Kacem~1--4
benchmark suite show that our hybrid approach eliminates 100\% of deadline
violations observed in state-of-the-art meta-heuristic schedules, while adding
only 3--5\% makespan overhead. Scalability experiments confirm that CP
solve-times and STNU checks remain sub-second on medium-size instances. The
work demonstrates how temporal-network reasoning can bridge the gap between
proactive buffering and dynamic robustness, moving industry a step closer to
truly digital, self-correcting factories.

</details>


### [220] [Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study](https://arxiv.org/abs/2510.16095)
*Dou Liu,Ying Long,Sophia Zuoqiu,Di Liu,Kang Li,Yiting Lin,Hanyi Liu,Rong Yin,Tian Tang*

Main category: cs.AI

TL;DR: 本研究评估了LLM生成的临床思维链可靠性，发现选择性少样本策略显著优于零样本和随机少样本策略，提出了基于"黄金标准深度"和"代表性多样性"的双原则框架。


<details>
  <summary>Details</summary>
Motivation: 解决高质量临床思维链数据稀缺问题，验证LLM生成医疗数据的可靠性，并寻找提升其质量的提示策略。

Method: 在辅助生殖技术领域进行盲法比较研究，由资深临床医生评估三种提示策略生成的思维链：零样本、随机少样本和选择性少样本，并与GPT-4o的评估结果对比。

Result: 选择性少样本策略在所有人类评估指标上显著优于其他策略(p < .001)，随机少样本策略相比零样本基线无显著改进，AI评估器未能识别关键性能差异。

Conclusion: 合成思维链的临床可靠性取决于策略性提示优化而非示例数量，提出了"双原则"框架作为可信数据规模化生成的基础方法，确认了人类专业知识在高风险临床AI评估中的不可或缺作用。

Abstract: Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for
explainable medical Artificial Intelligence (AI) while constrained by data
scarcity. Although Large Language Models (LLMs) can synthesize medical data,
their clinical reliability remains unverified. This study evaluates the
reliability of LLM-generated CoTs and investigates prompting strategies to
enhance their quality. In a blinded comparative study, senior clinicians in
Assisted Reproductive Technology (ART) evaluated CoTs generated via three
distinct strategies: Zero-shot, Random Few-shot (using shallow examples), and
Selective Few-shot (using diverse, high-quality examples). These expert ratings
were compared against evaluations from a state-of-the-art AI model (GPT-4o).
The Selective Few-shot strategy significantly outperformed other strategies
across all human evaluation metrics (p < .001). Critically, the Random Few-shot
strategy offered no significant improvement over the Zero-shot baseline,
demonstrating that low-quality examples are as ineffective as no examples. The
success of the Selective strategy is attributed to two principles:
"Gold-Standard Depth" (reasoning quality) and "Representative Diversity"
(generalization). Notably, the AI evaluator failed to discern these critical
performance differences. The clinical reliability of synthetic CoTs is dictated
by strategic prompt curation, not the mere presence of examples. We propose a
"Dual Principles" framework as a foundational methodology to generate
trustworthy data at scale. This work offers a validated solution to the data
bottleneck and confirms the indispensable role of human expertise in evaluating
high-stakes clinical AI.

</details>


### [221] [Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability](https://arxiv.org/abs/2510.16193)
*Elija Perrier*

Main category: cs.AI

TL;DR: 本文提出了一种新的企业知识定义方法，将企业知识视为动态能力，通过信息访问程序的效率和输出可靠性来衡量，为AI时代的企业责任认定提供可量化的法律标准。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在企业决策中的广泛应用，传统的基于人类代理人的企业犯罪意图认定方法面临挑战，需要重新定义企业知识的概念以适应算法时代。

Method: 基于扩展认知理论，开发了一个形式化模型，通过整合计算成本和统计验证错误率来度量企业知识状态，定义了连续的组织知识度量$S_S(\varphi)$和知识谓词$\mathsf{K}_S$。

Result: 建立了可量化的企业知识度量体系，包括企业级认知能力指数$\mathcal{K}_{S,t}$，并将这些指标映射到实际知识、推定知识、故意视而不见和鲁莽等法律标准上。

Conclusion: 该研究为在算法时代创建可测量和可裁决的审计证据提供了路径，使企业思维变得可追踪和可问责。

Abstract: Corporate responsibility turns on notions of corporate \textit{mens rea},
traditionally imputed from human agents. Yet these assumptions are under
challenge as generative AI increasingly mediates enterprise decision-making.
Building on the theory of extended cognition, we argue that in response
corporate knowledge may be redefined as a dynamic capability, measurable by the
efficiency of its information-access procedures and the validated reliability
of their outputs. We develop a formal model that captures epistemic states of
corporations deploying sophisticated AI or information systems, introducing a
continuous organisational knowledge metric $S_S(\varphi)$ which integrates a
pipeline's computational cost and its statistically validated error rate. We
derive a thresholded knowledge predicate $\mathsf{K}_S$ to impute knowledge and
a firm-wide epistemic capacity index $\mathcal{K}_{S,t}$ to measure overall
capability. We then operationally map these quantitative metrics onto the legal
standards of actual knowledge, constructive knowledge, wilful blindness, and
recklessness. Our work provides a pathway towards creating measurable and
justiciable audit artefacts, that render the corporate mind tractable and
accountable in the algorithmic age.

</details>


### [222] [Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration](https://arxiv.org/abs/2510.16194)
*Guanchen Wu,Zuhui Chen,Yuzhang Xie,Carl Yang*

Main category: cs.AI

TL;DR: TEAM-PHI是一个基于大语言模型的多智能体评估框架，用于自动评估医疗信息去识别模型性能，无需依赖昂贵的专家标注，通过多数投票机制实现稳定可靠的模型排名。


<details>
  <summary>Details</summary>
Motivation: 医疗信息去识别对于临床笔记的安全复用至关重要，但传统评估方法依赖成本高昂的小规模专家标注，限制了模型比较和选择的效率。

Method: 部署多个评估智能体独立判断PHI提取的正确性，然后通过基于LLM的多数投票机制整合不同评估者的观点，形成单一稳定的模型排名。

Result: 在真实临床笔记语料上的实验表明，TEAM-PHI能产生一致且准确的排名，尽管个体评估者存在差异，但LLM投票能可靠地收敛到相同的最佳系统。

Conclusion: TEAM-PHI通过结合独立评估智能体和LLM多数投票，为PHI去识别提供了实用、安全且成本效益高的自动评估和最佳模型选择解决方案，即使在地面真值标签有限的情况下也能有效工作。

Abstract: Protected health information (PHI) de-identification is critical for enabling
the safe reuse of clinical notes, yet evaluating and comparing PHI
de-identification models typically depends on costly, small-scale expert
annotations. We present TEAM-PHI, a multi-agent evaluation and selection
framework that uses large language models (LLMs) to automatically measure
de-identification quality and select the best-performing model without heavy
reliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each
independently judging the correctness of PHI extractions and outputting
structured metrics. Their results are then consolidated through an LLM-based
majority voting mechanism that integrates diverse evaluator perspectives into a
single, stable, and reproducible ranking. Experiments on a real-world clinical
note corpus demonstrate that TEAM-PHI produces consistent and accurate
rankings: despite variation across individual evaluators, LLM-based voting
reliably converges on the same top-performing systems. Further comparison with
ground-truth annotations and human evaluation confirms that the framework's
automated rankings closely match supervised evaluation. By combining
independent evaluation agents with LLM majority voting, TEAM-PHI offers a
practical, secure, and cost-effective solution for automatic evaluation and
best-model selection in PHI de-identification, even when ground-truth labels
are limited.

</details>


### [223] [The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI](https://arxiv.org/abs/2510.16206)
*Alex Zhavoronkov,Dominika Wilczok,Roman Yampolskiy*

Main category: cs.AI

TL;DR: 该论文提出了"被记住权"（RTBR）概念，旨在解决LLMs在信息检索中可能导致的偏见、信息遗漏和集体记忆重塑问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，人们越来越依赖它们进行信息检索。但LLMs将多个视角合成单一权威性回答的做法，可能导致某些群体被过度压制，而另一些群体被过度放大，从而重塑集体记忆。

Method: 提出"被记住权"（RTBR）概念框架，包括最小化AI驱动信息遗漏风险、保障公平对待权利，同时确保生成内容的真实性最大化。

Result: 建立了一个应对LLMs信息集中化风险的概念框架，为保护数字弱势群体的信息可见性提供了理论基础。

Conclusion: 需要建立"被记住权"来应对LLMs对信息生态和集体记忆的潜在威胁，确保信息检索系统的公平性和真实性。

Abstract: Since the rapid expansion of large language models (LLMs), people have begun
to rely on them for information retrieval. While traditional search engines
display ranked lists of sources shaped by search engine optimization (SEO),
advertising, and personalization, LLMs typically provide a synthesized response
that feels singular and authoritative. While both approaches carry risks of
bias and omission, LLMs may amplify the effect by collapsing multiple
perspectives into one answer, reducing users ability or inclination to compare
alternatives. This concentrates power over information in a few LLM vendors
whose systems effectively shape what is remembered and what is overlooked. As a
result, certain narratives, individuals or groups, may be disproportionately
suppressed, while others are disproportionately elevated. Over time, this
creates a new threat: the gradual erasure of those with limited digital
presence, and the amplification of those already prominent, reshaping
collective memory.To address these concerns, this paper presents a concept of
the Right To Be Remembered (RTBR) which encompasses minimizing the risk of
AI-driven information omission, embracing the right of fair treatment, while
ensuring that the generated content would be maximally truthful.

</details>


### [224] [ScholarEval: Research Idea Evaluation Grounded in Literature](https://arxiv.org/abs/2510.16234)
*Hanane Nour Moussa,Patrick Queiroz Da Silva,Daniel Adu-Ampratwum,Alyson East,Zitong Lu,Nikki Puccetti,Mingyi Xue,Huan Sun,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.AI

TL;DR: 提出了ScholarEval框架，通过检索增强评估研究想法的合理性和贡献度，并在多领域数据集上验证其优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在研究构思中日益普及，需要建立稳健的评估机制来确保生成想法的有效性和实用性。

Method: 引入ScholarEval检索增强评估框架，基于两个核心标准评估研究想法：合理性（基于现有文献的方法实证有效性）和贡献度（相对于先前研究在不同维度上的推进程度）。

Result: 在ScholarIdeas数据集上的评估显示，ScholarEval在覆盖专家标注要点方面显著优于所有基线方法，且在评估可操作性、深度和证据支持方面持续优于OpenAI的o4-mini-deep-research系统。大规模用户研究也证实其在文献参与、想法精炼和实用性方面表现更优。

Conclusion: ScholarEval为研究想法评估提供了有效的自动化框架，其代码、数据集和工具已开源供社区使用和发展。

Abstract: As AI tools become increasingly common for research ideation, robust
evaluation is critical to ensure the validity and usefulness of generated
ideas. We introduce ScholarEval, a retrieval augmented evaluation framework
that assesses research ideas based on two fundamental criteria: soundness - the
empirical validity of proposed methods based on existing literature, and
contribution - the degree of advancement made by the idea across different
dimensions relative to prior research. To evaluate ScholarEval, we introduce
ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas
and reviews, comprised of 117 ideas across four disciplines: artificial
intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows
that ScholarEval achieves significantly higher coverage of points mentioned in
the human expert annotated rubrics in ScholarIdeas compared to all baselines.
Furthermore, ScholarEval is consistently preferred over our strongest baseline
o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,
in terms of evaluation actionability, depth, and evidence support. Our
large-scale user study also shows that ScholarEval significantly outperforms
deep research in literature engagement, idea refinement, and usefulness. We
openly release our code, dataset, and ScholarEval tool for the community to use
and build on.

</details>


### [225] [Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense](https://arxiv.org/abs/2510.16259)
*Zhehao Zhang,Weijie Xu,Shixian Cui,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 本文发现大型推理模型存在"推理分心"漏洞，即模型会被提示中嵌入的无关复杂任务干扰，导致主要任务准确率下降高达60%。作者提出了一种基于监督微调和强化学习的训练防御方法，可将鲁棒性提高50个百分点以上。


<details>
  <summary>Details</summary>
Motivation: 随着大型推理模型在数学和编程等复杂任务上表现出色，作者发现这些模型容易受到恶意嵌入的无关复杂任务干扰，这种"推理分心"漏洞严重威胁模型可靠性。

Method: 通过跨模型和基准的综合研究，分析了推理分心漏洞的严重性。为缓解风险，提出了结合监督微调和强化学习的训练防御方法，使用合成的对抗数据进行训练。

Result: 研究表明最先进的大型推理模型高度易受攻击，注入干扰物可使任务准确率下降高达60%。某些对齐技术会放大这种弱点，模型可能表现出隐蔽合规性。提出的防御方法在挑战性干扰攻击上将鲁棒性提高了50多个百分点。

Conclusion: 推理分心是对大型推理模型可靠性的独特且紧迫的威胁，本文为构建更安全可信的推理系统提供了实用步骤。

Abstract: Recent advances in large reasoning models (LRMs) have enabled remarkable
performance on complex tasks such as mathematics and coding by generating long
Chain-of-Thought (CoT) traces. In this paper, we identify and systematically
analyze a critical vulnerability we term reasoning distraction, where LRMs are
diverted from their primary objective by irrelevant yet complex tasks
maliciously embedded in the prompt. Through a comprehensive study across
diverse models and benchmarks, we show that even state-of-the-art LRMs are
highly susceptible, with injected distractors reducing task accuracy by up to
60%. We further reveal that certain alignment techniques can amplify this
weakness and that models may exhibit covert compliance, following hidden
adversarial instructions in reasoning while concealing them in the final
output. To mitigate these risks, we propose a training-based defense that
combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on
synthetic adversarial data, improving robustness by over 50 points on
challenging distractor attacks. Our findings establish reasoning distraction as
a distinct and urgent threat to LRM reliability and provide a practical step
toward safer and more trustworthy reasoning systems.

</details>


### [226] [What Limits Agentic Systems Efficiency?](https://arxiv.org/abs/2510.16276)
*Song Bian,Minghao Yan,Anand Jayarajan,Gennady Pekhimenko,Shivaram Venkataraman*

Main category: cs.AI

TL;DR: 本文通过实证研究发现网络交互式智能代理系统存在效率瓶颈，提出SpecCache缓存框架结合推测执行来降低网络环境延迟，显著提升缓存命中率和系统效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注智能代理系统的推理性能，而忽视了系统效率问题。网络交互式代理系统存在显著的延迟瓶颈，影响实际应用效果。

Method: 将端到端延迟分解为LLM API延迟和网络环境延迟，通过15个模型和5个提供商的实证研究，提出SpecCache缓存框架结合推测执行来优化网络环境开销。

Result: 网络环境延迟可占整体延迟的53.7%，SpecCache相比随机缓存策略将缓存命中率提升58倍，网络环境开销降低3.2倍，且不损害系统性能。

Conclusion: 智能代理系统的效率瓶颈主要来自网络环境延迟，SpecCache框架能有效解决这一问题，在保持性能的同时显著提升系统效率。

Abstract: Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have
demonstrated strong reasoning capabilities. To further enhance LLM
capabilities, recent agentic systems, such as Deep Research, incorporate web
interactions into LLM reasoning to mitigate uncertainties and reduce potential
errors. However, existing research predominantly focuses on reasoning
performance, often neglecting the efficiency of agentic systems. In this work,
we present a comprehensive empirical study that identifies efficiency
bottlenecks in web-interactive agentic systems. We decompose end-to-end latency
into two primary components: LLM API latency and web environment latency. We
conduct a comprehensive empirical study across 15 models and 5 providers to
demonstrate high variability in API-based agentic systems. We observe that web
environment latency can contribute as much as 53.7% to the overall latency in a
web-based agentic system. To improve latency, we propose SpecCache, a caching
framework augmented with speculative execution that can reduce web environment
overhead. Extensive evaluations on two standard benchmarks show that our
approach improves the cache hit rate by up to 58x compared to a random caching
strategy, while reducing web environment overhead by up to 3.2x, without
degrading agentic system performance.

</details>


### [227] [DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA](https://arxiv.org/abs/2510.16302)
*Changhao Wang,Yanfang Liu,Xinxin Fan,Anzhi Zhou,Lao Tian,Yunfeng Lu*

Main category: cs.AI

TL;DR: 提出了DTKG双轨知识图谱验证与推理框架，通过分类阶段和分支处理阶段来解决多跳推理中的并行事实验证和链式推理问题


<details>
  <summary>Details</summary>
Motivation: 现有的多跳推理方法要么使用LLM响应的事实验证，要么使用KG路径的链式构建，但前者擅长并行事实验证而不擅长链式推理，后者擅长链式推理但在并行事实验证时存在冗余路径检索问题

Method: 基于认知科学中的双过程理论，提出DTKG框架，包含分类阶段和分支处理阶段，分别处理并行事实验证和链式多跳推理

Result: DTKG框架旨在提高多跳问答任务的效率和准确性

Conclusion: 通过双轨方法结合LLM和KG的优势，可以更有效地处理不同类型多跳推理问题

Abstract: Multi-hop reasoning for question answering (QA) plays a critical role in
retrieval-augmented generation (RAG) for modern large language models (LLMs).
The accurate answer can be obtained through retrieving relational structure of
entities from knowledge graph (KG). Regarding the inherent relation-dependency
and reasoning pattern, multi-hop reasoning can be in general classified into
two categories: i) parallel fact-verification multi-hop reasoning question,
i.e., requiring simultaneous verifications of multiple independent
sub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding
sequential multi-step inference with intermediate conclusions serving as
essential premises for subsequent reasoning. Currently, the multi-hop reasoning
approaches singly employ one of two techniques: LLM response-based fact
verification and KG path-based chain construction. Nevertheless, the former
excels at parallel fact-verification but underperforms on chained reasoning
tasks, while the latter demonstrates proficiency in chained multi-hop reasoning
but suffers from redundant path retrieval when handling parallel
fact-verification reasoning. These limitations deteriorate the efficiency and
accuracy for multi-hop QA tasks. To address this challenge, we propose a novel
dual-track KG verification and reasoning framework DTKG, which is inspired by
the Dual Process Theory in cognitive science. Specifically, DTKG comprises two
main stages: the Classification Stage and the Branch Processing Stage.

</details>


### [228] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier](https://arxiv.org/abs/2510.16309)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG是一个紧凑的知识图谱和符号验证器系统，用于在推理任务中强制执行数学可解释规则，显著提高LLMs在数学推理任务中的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生流畅的推理步骤，但违反简单的数学或逻辑约束。需要一种方法来确保数学推理的准确性和一致性。

Method: 引入MedRule-KG，一个紧凑的类型化知识图谱，结合符号验证器。知识图谱编码实体、关系和三个领域启发规则，验证器检查预测并应用最小修正以保证一致性。

Result: 在90个FDA衍生基准测试中，基于MedRule-KG的推理将精确匹配从0.767提高到0.900，添加验证器后达到1.000的精确匹配，完全消除了规则违反。

Conclusion: MedRule-KG为安全的数学推理提供了一个通用框架，通过知识图谱和符号验证确保推理的准确性和一致性。

Abstract: Large language models (LLMs) often produce fluent reasoning steps while
violating simple mathematical or logical constraints. We introduce MedRule-KG,
a compact typed knowledge graph coupled with a symbolic verifier, designed to
enforce mathematically interpretable rules in reasoning tasks. MedRule-KG
encodes entities, relations, and three domain-inspired rules, while the
verifier checks predictions and applies minimal corrections to guarantee
consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG
improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields
1.000 EM while eliminating rule violations entirely. We demonstrate how
MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss
ablations, and release code and data to encourage reproducibility.

</details>


### [229] [Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts](https://arxiv.org/abs/2510.16342)
*Tong Zhang,Ru Zhang,Jianyi Liu,Zhen Yang,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出了SELECT框架，通过动态锚点选择解决文本到图像扩散模型中概念擦除的固定锚点策略问题，避免概念重现和侵蚀。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法依赖固定锚点策略，导致概念重现和侵蚀等关键问题，需要更有效的锚点选择方法。

Method: 通过因果追踪揭示擦除对锚点选择的敏感性，定义兄弟排他概念作为更优锚点类别，提出两阶段评估机制自动发现最优锚点。

Result: SELECT作为通用锚点解决方案，能高效适应多种擦除框架，在关键性能指标上持续优于现有基线，单个概念锚点挖掘仅需4秒。

Conclusion: SELECT框架通过动态锚点选择有效解决了概念擦除中的锚点依赖问题，提供了更精确和高效的概念控制能力。

Abstract: Existing concept erasure methods for text-to-image diffusion models commonly
rely on fixed anchor strategies, which often lead to critical issues such as
concept re-emergence and erosion. To address this, we conduct causal tracing to
reveal the inherent sensitivity of erasure to anchor selection and define
Sibling Exclusive Concepts as a superior class of anchors. Based on this
insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for
Contextual Targeting), a dynamic anchor selection framework designed to
overcome the limitations of fixed anchors. Our framework introduces a novel
two-stage evaluation mechanism that automatically discovers optimal anchors for
precise erasure while identifying critical boundary anchors to preserve related
concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor
solution, not only efficiently adapts to multiple erasure frameworks but also
consistently outperforms existing baselines across key performance metrics,
averaging only 4 seconds for anchor mining of a single concept.

</details>


### [230] [The Burden of Interactive Alignment with Inconsistent Preferences](https://arxiv.org/abs/2510.16368)
*Ali Shirali*

Main category: cs.AI

TL;DR: 该论文研究了用户如何通过战略性互动来引导算法与自身真实兴趣对齐，提出了一个双系统决策模型和Stackelberg博弈框架，揭示了用户需要足够的前瞻性才能实现算法对齐。


<details>
  <summary>Details</summary>
Motivation: 研究用户与算法互动中的对齐问题，特别是当用户存在不一致偏好时（如花费大量时间在低价值内容上），如何让算法更好地反映用户的真实兴趣。

Method: 将用户决策过程建模为理性系统2（决定是否参与）和冲动系统1（决定参与时长）的双系统模型，并构建多领导者单跟随者的Stackelberg扩展博弈，用户作为领导者通过承诺参与策略来引导算法。

Result: 发现存在一个关键对齐时间范围：足够有远见的用户可以实现算法对齐，而短视的用户反而会被算法目标所对齐。额外的小成本信号（如额外点击）可以显著降低对齐负担。

Conclusion: 提出了一个解释不一致偏好用户如何在与参与驱动算法的Stackelberg均衡中实现对齐的框架，强调了实现对齐的挑战和潜在解决方案。

Abstract: From media platforms to chatbots, algorithms shape how people interact,
learn, and discover information. Such interactions between users and an
algorithm often unfold over multiple steps, during which strategic users can
guide the algorithm to better align with their true interests by selectively
engaging with content. However, users frequently exhibit inconsistent
preferences: they may spend considerable time on content that offers little
long-term value, inadvertently signaling that such content is desirable.
Focusing on the user side, this raises a key question: what does it take for
such users to align the algorithm with their true interests?
  To investigate these dynamics, we model the user's decision process as split
between a rational system 2 that decides whether to engage and an impulsive
system 1 that determines how long engagement lasts. We then study a
multi-leader, single-follower extensive Stackelberg game, where users,
specifically system 2, lead by committing to engagement strategies and the
algorithm best-responds based on observed interactions. We define the burden of
alignment as the minimum horizon over which users must optimize to effectively
steer the algorithm. We show that a critical horizon exists: users who are
sufficiently foresighted can achieve alignment, while those who are not are
instead aligned to the algorithm's objective. This critical horizon can be
long, imposing a substantial burden. However, even a small, costly signal
(e.g., an extra click) can significantly reduce it. Overall, our framework
explains how users with inconsistent preferences can align an engagement-driven
algorithm with their interests in a Stackelberg equilibrium, highlighting both
the challenges and potential remedies for achieving alignment.

</details>


### [231] [Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs](https://arxiv.org/abs/2510.16374)
*Nick Oh*

Main category: cs.AI

TL;DR: 本文提出了一种结合监控-生成-验证的三阶段迭代系统，在GSM8K数学推理任务上取得了75.42%的准确率，优于现有方法且需要更少的尝试次数。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理增强方法存在两个孤立范式：监控-生成方法擅长策略规划但缺乏验证机制，生成-验证方法能迭代优化但缺乏任务评估。这种分离导致策略失败无反馈、优化无策略基础的低效问题。

Method: 基于Flavell的认知监控模型，实现监控-生成-验证框架的三阶段迭代系统，在生成前进行任务评估和策略规划。

Result: 在GSM8K上达到75.42%准确率，优于SELF-REFINE(68.44%)和Self-Verification(67.07%)，尝试次数更少(1.3 vs 2.0)，推理成本增加27-37%。

Conclusion: 前期监控能产生更高质量的初始解决方案，减少优化需求，但需要在算术推理之外的任务上进行评估以验证通用性。

Abstract: Current approaches to enhancing LLM reasoning follows two isolated paradigms:
Monitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and
SELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack
mechanisms to verify whether selected strategies succeed; while Generate-Verify
approaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan
et al., 2023) iteratively refine outputs but commence generation blindly
without task assessment. This separation creates inefficiencies -- strategies
fail without feedback, and refinement occurs without strategic grounding. We
address this gap by implementing Flavell's cognitive monitoring model (1979)
from the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025),
operationalising it as a three-phase iterative system. On GSM8K, preliminary
results show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for
Self-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37%
increased inference cost. These initial findings suggest upfront monitoring
produces higher-quality initial solutions that reduce refinement needs, though
evaluation beyond arithmetic reasoning is needed to establish generalisability.

</details>


### [232] [Humanoid-inspired Causal Representation Learning for Domain Generalization](https://arxiv.org/abs/2510.16382)
*Ze Tao,Jian Zhang,Haowei Li,Xianshuai Li,Yifei Peng,Xiyao Liu,Senzhang Wang,Chao Liu,Sheng Ren,Shichao Zhang*

Main category: cs.AI

TL;DR: 提出了受人类智能启发的HSCM因果框架，通过解耦和重加权图像属性（颜色、纹理、形状）来提升跨域泛化能力，在理论和实证评估中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 克服传统领域泛化模型的局限性，模仿人类视觉系统的分层处理和多层次学习机制，专注于建模细粒度因果机制。

Method: 构建受人类智能启发的结构因果模型（HSCM），解耦和重加权关键图像属性（颜色、纹理、形状），复制人类视觉系统的分层处理过程。

Result: HSCM在跨域泛化任务中表现优于现有方法，提供了更原则性的因果关系捕捉方法，增强了模型鲁棒性和可解释性。

Conclusion: HSCM通过模仿人类智能的灵活性和适应性，为动态复杂环境中的有效迁移和学习提供了更优越的解决方案，代码已开源。

Abstract: This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a
novel causal framework inspired by human intelligence, designed to overcome the
limitations of conventional domain generalization models. Unlike approaches
that rely on statistics to capture data-label dependencies and learn
distortion-invariant representations, HSCM replicates the hierarchical
processing and multi-level learning of human vision systems, focusing on
modeling fine-grained causal mechanisms. By disentangling and reweighting key
image attributes such as color, texture, and shape, HSCM enhances
generalization across diverse domains, ensuring robust performance and
interpretability. Leveraging the flexibility and adaptability of human
intelligence, our approach enables more effective transfer and learning in
dynamic, complex environments. Through both theoretical and empirical
evaluations, we demonstrate that HSCM outperforms existing domain
generalization models, providing a more principled method for capturing causal
relationships and improving model robustness. The code is available at
https://github.com/lambett/HSCM.

</details>


### [233] [RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile](https://arxiv.org/abs/2510.16392)
*Ao Tian,Yunfeng Lu,Xinxin Fan,Changhao Wang,Lanzhi Zhou,Yeyao Zhang,Yanfang Liu*

Main category: cs.AI

TL;DR: RGMem是一个受物理学重整化群思想启发的自进化记忆框架，通过多尺度组织对话历史，从微观互动中提取高层次用户画像，实现语言代理的长期记忆和行为一致性。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案（如RAG和显式记忆系统）主要关注事实级存储和检索，缺乏从多轮对话中提取潜在偏好和深层特征的能力，限制了长期有效的用户建模，导致个性化交互浅层且缺乏跨会话连续性。

Method: RGMem框架通过分层粗粒化和重标度操作，从片段对话中提取语义和用户洞察，逐步形成动态演化的用户画像，将记忆演化建模为信息压缩和涌现的多尺度过程。

Result: 该框架能够从嘈杂的微观级互动中实现高层次、准确的用户画像，解决了有限上下文窗口和静态参数记忆带来的跨会话长期用户状态建模困难。

Conclusion: RGMem通过多尺度记忆组织方法，成功实现了语言代理的长期记忆和行为一致性，为LLM时代的个性化交互提供了有效的解决方案。

Abstract: Personalized and continuous interactions are the key to enhancing user
experience in today's large language model (LLM)-based conversational systems,
however, the finite context windows and static parametric memory make it
difficult to model the cross-session long-term user states and behavioral
consistency. Currently, the existing solutions to this predicament, such as
retrieval-augmented generation (RAG) and explicit memory systems, primarily
focus on fact-level storage and retrieval, lacking the capability to distill
latent preferences and deep traits from the multi-turn dialogues, which limits
the long-term and effective user modeling, directly leading to the personalized
interactions remaining shallow, and hindering the cross-session continuity. To
realize the long-term memory and behavioral consistency for Language Agents in
LLM era, we propose a self-evolving memory framework RGMem, inspired by the
ideology of classic renormalization group (RG) in physics, this framework
enables to organize the dialogue history in multiple scales: it first extracts
semantics and user insights from episodic fragments, then through hierarchical
coarse-graining and rescaling operations, progressively forms a
dynamically-evolved user profile. The core innovation of our work lies in
modeling memory evolution as a multi-scale process of information compression
and emergence, which accomplishes the high-level and accurate user profiles
from noisy and microscopic-level interactions.

</details>


### [234] [ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights](https://arxiv.org/abs/2510.16466)
*Siddhartha Krothapalli,Tridib Kumar Das,Praveen Kumar,Naveen Suravarpu,Pratik Narang*

Main category: cs.AI

TL;DR: ReviewSense是一个基于大语言模型的决策支持框架，能够将客户评论转化为可操作的商业建议，超越了传统偏好预测系统。


<details>
  <summary>Details</summary>
Motivation: 传统AI系统擅长预测用户偏好，但缺乏将客户评论转化为商业建议的能力，而客户反馈对战略增长至关重要。

Method: 整合聚类、LLM适配和专家驱动评估的统一业务导向流程，识别客户情绪中的关键趋势、重复问题和具体关注点。

Result: 初步人工评估显示模型建议与商业目标高度一致，具有推动数据驱动决策的潜力。

Conclusion: 该框架为AI驱动的情感分析提供了新视角，在优化商业策略和最大化客户反馈影响力方面具有重要价值。

Abstract: As customer feedback becomes increasingly central to strategic growth, the
ability to derive actionable insights from unstructured reviews is essential.
While traditional AI-driven systems excel at predicting user preferences, far
less work has focused on transforming customer reviews into prescriptive,
business-facing recommendations. This paper introduces ReviewSense, a novel
prescriptive decision support framework that leverages advanced large language
models (LLMs) to transform customer reviews into targeted, actionable business
recommendations. By identifying key trends, recurring issues, and specific
concerns within customer sentiments, ReviewSense extends beyond
preference-based systems to provide businesses with deeper insights for
sustaining growth and enhancing customer loyalty. The novelty of this work lies
in integrating clustering, LLM adaptation, and expert-driven evaluation into a
unified, business-facing pipeline. Preliminary manual evaluations indicate
strong alignment between the model's recommendations and business objectives,
highlighting its potential for driving data-informed decision-making. This
framework offers a new perspective on AI-driven sentiment analysis,
demonstrating its value in refining business strategies and maximizing the
impact of customer feedback.

</details>


### [235] [NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems](https://arxiv.org/abs/2510.16476)
*Xiaozhe Li,Xinyu Fang,Shengyuan Ding,Linyang Li,Haodong Duan,Qingwen Liu,Kai Chen*

Main category: cs.AI

TL;DR: 提出了NP-ENGINE框架，这是首个专门针对NP难问题训练和评估LLM的综合框架，包含10个任务、可控实例生成器、规则验证器和启发式求解器。通过该框架训练的模型在NP-BENCH基准上显著优于GPT-4o，并在领域外任务上展现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在数学、编程等推理任务上表现出色，但在解决更复杂的NP难优化问题方面的能力尚未充分探索。为了填补这一空白，需要开发专门的训练框架和评估基准。

Method: 提出NP-ENGINE框架，包含可控实例生成器、规则验证器和启发式求解器的生成-验证-启发式管道，支持可扩展且可验证的RLVR分层难度训练。基于此构建NP-BENCH基准，并训练QWEN2.5-7B-NP模型。

Result: QWEN2.5-7B-NP在NP-BENCH上显著优于GPT-4o，达到同模型尺寸下的SOTA性能。RLVR训练还使模型在逻辑、谜题、数学等推理任务以及非推理任务上展现出强大的领域外泛化能力。

Conclusion: 任务丰富的RLVR训练是提升LLM推理能力的有前景方向，揭示了RLVR的扩展规律。增加任务多样性可以改善领域外泛化能力，为LLM在复杂优化问题上的应用开辟了新途径。

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities, with
models like OpenAI's O-series and DeepSeek R1 excelling at tasks such as
mathematics, coding, logic, and puzzles through Reinforcement Learning with
Verifiable Rewards (RLVR). However, their ability to solve more complex
optimization problems - particularly NP-hard tasks - remains underexplored. To
bridge this gap, we propose NP-ENGINE, the first comprehensive framework for
training and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks
across five domains, each equipped with (i) a controllable instance generator,
(ii) a rule-based verifier, and (iii) a heuristic solver that provides
approximate optimal solutions as ground truth. This
generator-verifier-heuristic pipeline enables scalable and verifiable RLVR
training under hierarchical difficulties. We also introduce NP-BENCH, a
benchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'
ability to tackle NP-hard level reasoning problems, focusing not only on
feasibility but also on solution quality. Additionally, we present
QWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on
Qwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and
achieves SOTA performance with the same model size. Beyond in-domain tasks, we
demonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain
(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),
as well as non-reasoning tasks such as instruction following. We also observe a
scaling trend: increasing task diversity improves OOD generalization. These
findings suggest that task-rich RLVR training is a promising direction for
advancing LLM's reasoning ability, revealing new insights into the scaling laws
of RLVR.

</details>


### [236] [Hey Pentti, We Did It Again!: Differentiable vector-symbolic types that prove polynomial termination](https://arxiv.org/abs/2510.16533)
*Eilene Tomkins-Flanagan,Connor Hanley,Mary A. Kelly*

Main category: cs.AI

TL;DR: Doug是一种类型化计算机语言，所有类型化程序都能被证明在多项式时间内停止运行，编码在向量符号架构中。该语言支持类型学习，旨在实现类人技能获取速度。


<details>
  <summary>Details</summary>
Motivation: 希望通过Doug语言描述一种技能获取形式，使其遵循类人的技能获取速度（比暴力方法快得多），超越现有所有方法的效率，更接近模拟人脑中的心理表征及其获取过程。

Method: Doug是基于轻量线性函数式编程语言(LLFPL)的编码实现，使用基于全息声明性记忆(HDM)的槽值编码方案编码类型，使用Lisp VSA变体编码术语，使神经网络能够学习类型。

Result: 论文提出了Doug语言框架，使嵌入空间中的某些点可被解释为类型，且邻近点的类型在结构和内容上相似，从而实现类型的可学习性。

Conclusion: Doug语言为实现类人技能获取提供了一种新方法，使程序合成能够更高效地进行，向模拟人脑实际存在的心理表征及其学习过程迈进了一步。

Abstract: We present a typed computer language, Doug, in which all typed programs may
be proved to halt in polynomial time, encoded in a vector-symbolic architecture
(VSA). Doug is just an encoding of the light linear functional programming
language (LLFPL) described by (Schimanski2009, ch. 7). The types of Doug are
encoded using a slot-value encoding scheme based on holographic declarative
memory (HDM; Kelly, 2020). The terms of Doug are encoded using a variant of the
Lisp VSA defined by (Flanagan, 2024). Doug allows for some points on the
embedding space of a neural network to be interpreted as types, where the types
of nearby points are similar both in structure and content. Types in Doug are
therefore learnable by a neural network. Following (Chollet, 2019), (Card,
1983), and (Newell, 1981), we view skill as the application of a procedure, or
program of action, that causes a goal to be satisfied. Skill acquisition may
therefore be expressed as program synthesis. Using Doug, we hope to describe a
form of learning of skilled behaviour that follows a human-like pace of skill
acquisition (i.e., substantially faster than brute force; Heathcote, 2000),
exceeding the efficiency of all currently existing approaches (Kaplan, 2020;
Jones, 2021; Chollet, 2024). Our approach brings us one step closer to modeling
human mental representations, as they must actually exist in the brain, and
those representations' acquisition, as they are actually learned.

</details>


### [237] [Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence](https://arxiv.org/abs/2510.16555)
*Qiongyan Wang,Xingchen Zou,Yutian Jiang,Haomin Wen,Jiaheng Wei,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: 提出了Urban-R1，一种基于强化学习的后训练框架，通过群体相对策略优化和城市区域画像任务来减轻多模态大语言模型的地理偏见，提升跨区域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 快速城市化增加了对城市通用智能的需求，但现有基于监督微调的城市基础模型存在持续的地理偏见，导致区域预测偏差和泛化能力有限。

Method: 采用强化学习后训练框架Urban-R1，使用群体相对策略优化来优化跨地理群体的推理，并以城市区域画像作为代理任务，从多模态城市数据中提供可测量的奖励。

Result: 在多个区域和任务上的广泛实验表明，Urban-R1有效减轻了地理偏见，提高了跨区域泛化能力，优于监督微调训练和闭源模型。

Conclusion: 强化学习对齐是实现公平可信城市智能的有前景途径。

Abstract: Rapid urbanization intensifies the demand for Urban General Intelligence
(UGI), referring to AI systems that can understand and reason about complex
urban environments. Recent studies have built urban foundation models using
supervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit
persistent geospatial bias, producing regionally skewed predictions and limited
generalization. To this end, we propose Urban-R1, a reinforcement
learning-based post-training framework that aligns MLLMs with the objectives of
UGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize
reasoning across geographic groups and employs urban region profiling as a
proxy task to provide measurable rewards from multimodal urban data. Extensive
experiments across diverse regions and tasks show that Urban-R1 effectively
mitigates geo-bias and improves cross-region generalization, outperforming both
SFT-trained and closed-source models. Our results highlight reinforcement
learning alignment as a promising pathway toward equitable and trustworthy
urban intelligence.

</details>


### [238] [BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction](https://arxiv.org/abs/2510.16559)
*Tian Xia,Tianrun Gao,Wenhao Deng,Long Wei,Xiaowei Qian,Yixian Jiang,Chenglei Yu,Tailin Wu*

Main category: cs.AI

TL;DR: BuildArena是首个面向语言驱动工程建设的物理对齐交互基准，用于评估LLM在工程建造自动化中的能力。


<details>
  <summary>Details</summary>
Motivation: 工程建造自动化需要将自然语言规范转化为物理可行的结构，但现代LLM在此领域的建造能力尚未得到充分评估。

Method: 开发了高度可定制的基准框架、可扩展的任务设计策略、3D空间几何计算库和基线LLM代理工作流。

Result: 在8个前沿LLM上全面评估了它们在语言驱动和物理基础建造自动化方面的能力。

Conclusion: BuildArena填补了LLM在工程建造领域能力评估的空白，为社区提供了首个物理对齐的交互基准。

Abstract: Engineering construction automation aims to transform natural language
specifications into physically viable structures, requiring complex integrated
reasoning under strict physical constraints. While modern LLMs possess broad
knowledge and strong reasoning capabilities that make them promising candidates
for this domain, their construction competencies remain largely unevaluated. To
address this gap, we introduce BuildArena, the first physics-aligned
interactive benchmark designed for language-driven engineering construction. It
contributes to the community in four aspects: (1) a highly customizable
benchmarking framework for in-depth comparison and analysis of LLMs; (2) an
extendable task design strategy spanning static and dynamic mechanics across
multiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for
supporting construction based on language instructions; (4) a baseline LLM
agentic workflow that effectively evaluates diverse model capabilities. On
eight frontier LLMs, BuildArena comprehensively evaluates their capabilities
for language-driven and physics-grounded construction automation. The project
page is at https://build-arena.github.io/.

</details>


### [239] [Ripple Effect Protocol: Coordinating Agent Populations](https://arxiv.org/abs/2510.16572)
*Ayush Chopra,Aman Sharma,Feroz Ahmad,Luca Muscariello,Vijoy Pandey,Ramesh Raskar*

Main category: cs.AI

TL;DR: 本文提出了Ripple Effect Protocol (REP)，一种协调协议，让智能体不仅分享决策，还分享轻量级的敏感度信号，从而在群体中实现更快更稳定的协调。


<details>
  <summary>Details</summary>
Motivation: 现有的AI智能体通信协议（如A2A和ACP）强调通信而非协调，随着智能体群体规模扩大，这会导致脆弱的集体行为，即使个体智能体很聪明，群体结果也很差。

Method: 引入REP协议，智能体分享决策和轻量级敏感度信号（表达关键环境变量变化时选择如何改变），这些敏感度在局部网络中传播。形式化协议规范，分离必需的消息模式和可选的聚合规则。

Result: 在三个领域的基准测试中：(i)供应链级联（啤酒游戏）、(ii)稀疏网络中的偏好聚合（电影排期）、(iii)可持续资源分配（Fishbanks），REP相比A2A将协调准确性和效率提高了41%到100%，并能灵活处理来自LLM的多模态敏感度信号。

Conclusion: 通过将协调作为协议级能力，REP为新兴的智能体互联网提供了可扩展的基础设施。

Abstract: Modern AI agents can exchange messages using protocols such as A2A and ACP,
yet these mechanisms emphasize communication over coordination. As agent
populations grow, this limitation produces brittle collective behavior, where
individually smart agents converge on poor group outcomes. We introduce the
Ripple Effect Protocol (REP), a coordination protocol in which agents share not
only their decisions but also lightweight sensitivities - signals expressing
how their choices would change if key environmental variables shifted. These
sensitivities ripple through local networks, enabling groups to align faster
and more stably than with agent-centric communication alone. We formalize REP's
protocol specification, separating required message schemas from optional
aggregation rules, and evaluate it across scenarios with varying incentives and
network topologies. Benchmarks across three domains: (i) supply chain cascades
(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),
and (iii) sustainable resource allocation (Fishbanks) show that REP improves
coordination accuracy and efficiency over A2A by 41 to 100%, while flexibly
handling multimodal sensitivity signals from LLMs. By making coordination a
protocol-level capability, REP provides scalable infrastructure for the
emerging Internet of Agents

</details>


### [240] [Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?](https://arxiv.org/abs/2510.16582)
*Junchi Yu,Yujie Liu,Jindong Gu,Philip Torr,Dongzhan Zhou*

Main category: cs.AI

TL;DR: GraphFlow是一个基于知识图谱的检索增强生成框架，通过流匹配目标优化检索策略，从文本丰富的知识图谱中高效检索准确多样的知识。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的检索增强生成方法在处理复杂真实世界查询时，难以从文本丰富的知识图谱中检索准确多样的信息。过程奖励模型虽然能对齐检索过程与查询需求，但依赖昂贵且难以获取的过程级监督信号。

Method: GraphFlow采用基于转移的流匹配目标联合优化检索策略和流估计器。流估计器将检索结果的奖励分解到中间检索状态，引导检索策略按奖励比例从知识图谱中检索候选结果。

Result: 在STaRK基准测试中，GraphFlow在命中率和召回率上平均优于包括GPT-4o在内的强基线方法10%，并在未见过的知识图谱上表现出强大的泛化能力。

Conclusion: GraphFlow能够有效从文本丰富的知识图谱中检索准确多样的知识，为复杂真实世界查询提供支持，并展现出良好的有效性和鲁棒性。

Abstract: Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances
large language models (LLMs) by providing structured and interpretable external
knowledge. However, existing KG-based RAG methods struggle to retrieve accurate
and diverse information from text-rich KGs for complex real-world queries.
Process Reward Models (PRMs) offer a way to align the retrieval process of
KG-based RAG with query-specific knowledge requirements, but they heavily rely
on process-level supervision signals that are expensive and hard to obtain on
KGs. To address this challenge, we propose GraphFlow, a framework that
efficiently retrieves accurate and diverse knowledge required for real-world
queries from text-rich KGs. GraphFlow employs a transition-based flow matching
objective to jointly optimize a retrieval policy and a flow estimator. The flow
estimator factorizes the reward of the retrieval outcome into the intermediate
retrieval states. Such reward factorization guides the retrieval policy to
retrieve candidates from KGs in proportion to their reward. This allows
GraphFlow to explore high-quality regions of KGs that yield diverse and
relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes
real-world queries from multiple domains over text-rich KGs. GraphFlow
outperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit
rate and recall. It also shows strong generalization to unseen KGs,
demonstrating its effectiveness and robustness.

</details>


### [241] [Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning](https://arxiv.org/abs/2510.16601)
*Tianxing Wu,Shutong Zhu,Jingting Wang,Ning Xu,Guilin Qi,Haofen Wang*

Main category: cs.AI

TL;DR: 提出了一种半监督置信分布学习方法(ssCDL)来解决不确定知识图谱补全问题，通过将置信度转换为分布并引入元学习来平衡置信度分布。


<details>
  <summary>Details</summary>
Motivation: 现有不确定知识图谱补全方法忽略了置信度的极端不平衡分布，导致学习到的嵌入不足以支持高质量的补全。

Method: 将置信度转换为置信分布，通过关系学习在标记数据（现有三元组）和带伪标签的未标记数据（未见三元组）上迭代学习嵌入，使用元学习预测置信度来增强训练数据并平衡分布。

Result: 在两个不确定知识图谱数据集上的实验表明，ssCDL在不同评估指标上始终优于最先进的基线方法。

Conclusion: ssCDL通过置信分布学习和半监督方法有效解决了置信度不平衡问题，提升了不确定知识图谱补全的性能。

Abstract: Uncertain knowledge graphs (UKGs) associate each triple with a confidence
score to provide more precise knowledge representations. Recently, since
real-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)
completion attracts more attention, aiming to complete missing triples and
confidences. Current studies attempt to learn UKG embeddings to solve this
problem, but they neglect the extremely imbalanced distributions of triple
confidences. This causes that the learnt embeddings are insufficient to
high-quality UKG completion. Thus, in this paper, to address the above issue,
we propose a new semi-supervised Confidence Distribution Learning (ssCDL)
method for UKG completion, where each triple confidence is transformed into a
confidence distribution to introduce more supervision information of different
confidences to reinforce the embedding learning process. ssCDL iteratively
learns UKG embedding by relational learning on labeled data (i.e., existing
triples with confidences) and unlabeled data with pseudo labels (i.e., unseen
triples with the generated confidences), which are predicted by meta-learning
to augment the training data and rebalance the distribution of triple
confidences. Experiments on two UKG datasets demonstrate that ssCDL
consistently outperforms state-of-the-art baselines in different evaluation
metrics.

</details>


### [242] [Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards](https://arxiv.org/abs/2510.16614)
*Xuan Zhang,Ruixiao Li,Zhijian Zhou,Long Li,Yulei Qin,Ke Li,Xing Sun,Xiaoyu Tan,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: MERCI是一种新颖的强化学习算法，通过基于计数的内在奖励来增强LLM推理中的探索，避免重复和次优的推理模式。


<details>
  <summary>Details</summary>
Motivation: 现有的RL范式依赖稀疏的结果奖励和有限探索，导致LLM陷入重复和次优的推理模式，需要设计更好的探索机制。

Method: 使用轻量级的Coin Flipping Network估计推理轨迹的伪计数和认知不确定性，将其转换为内在奖励，并集成到GRPO等RL框架中。

Result: 在复杂推理基准测试中，MERCI鼓励更丰富多样的思维链，显著提升性能，帮助策略逃离局部最优发现更好解决方案。

Conclusion: 针对性的内在动机可以使语言模型推理中的探索更加可靠有效。

Abstract: Reinforcement Learning (RL) has become a compelling way to strengthen the
multi step reasoning ability of Large Language Models (LLMs). However,
prevalent RL paradigms still lean on sparse outcome-based rewards and limited
exploration, which often drives LLMs toward repetitive and suboptimal reasoning
patterns. In this paper, we study the central question of how to design
exploration for LLM reasoning and introduce MERCI (Motivating Exploration in
LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that
augments policy optimization with a principled intrinsic reward. Building on
the idea of count-based exploration, MERCI leverages a lightweight Coin
Flipping Network (CFN) to estimate the pseudo count and further epistemic
uncertainty over reasoning trajectories, and converts them into an intrinsic
reward that values novelty while preserving the learning signal from task
rewards. We integrate MERCI into some advanced RL frameworks like Group
Relative Policy Optimization (GRPO). Experiments on complex reasoning
benchmarks demonstrate that MERCI encourages richer and more varied chains of
thought, significantly improves performance over strong baselines, and helps
the policy escape local routines to discover better solutions. It indicates
that our targeted intrinsic motivation can make exploration reliable for
language model reasoning.

</details>


### [243] [MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning](https://arxiv.org/abs/2510.17590)
*Mir Nafis Sharear Shopnil,Sharad Duwal,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.AI

TL;DR: MIRAGE是一个用于检测多模态虚假信息的推理时框架，通过分解验证过程为四个模块：视觉真实性评估、跨模态一致性分析、检索增强的事实核查和校准判断，无需领域特定训练数据即可达到监督检测器的性能。


<details>
  <summary>Details</summary>
Motivation: 网络平台上每天有数十亿包含文本和图像的多模态帖子传播虚假信息，超出了人工事实核查的能力。现有的监督检测模型需要领域特定训练数据，且无法泛化到不同的操纵策略。

Method: MIRAGE框架将多模态验证分解为四个顺序模块：1) 视觉真实性评估检测AI生成图像；2) 跨模态一致性分析识别上下文不当的重新利用；3) 检索增强的事实核查通过迭代问题生成将声明与网络证据关联；4) 校准判断模块整合所有信号。该框架协调视觉语言模型推理与定向网络检索。

Result: 在MMFakeBench验证集（1000个样本）上，MIRAGE与GPT-4o-mini结合达到81.65% F1和75.1%准确率，比最强的零样本基线（GPT-4V与MMD-Agent的74.0% F1）高出7.65点，同时保持34.3%的假阳性率（相比仅判断基线的97.3%）。测试集结果（5000个样本）确认了泛化能力，达到81.44% F1和75.08%准确率。消融研究显示视觉验证贡献5.18 F1点，检索增强推理贡献2.97点。

Conclusion: 分解的代理推理与网络检索相结合，可以在没有领域特定训练的情况下匹配监督检测器的性能，在标记数据稀缺的多模态场景中实现虚假信息检测。

Abstract: Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.

</details>


### [244] [Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review](https://arxiv.org/abs/2510.16658)
*Shihao Yang,Xiying Huang,Danilo Bernardo,Jun-En Ding,Andrew Michael,Jingmei Yang,Patrick Kwan,Ashish Raj,Feng Liu*

Main category: cs.AI

TL;DR: 本文探讨了大规模AI模型对神经科学研究的变革性影响，涵盖神经影像处理、脑机接口、分子神经科学、临床辅助和疾病应用等五大领域，强调了AI在解决多模态数据整合、时空模式解释等挑战中的作用。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型的出现为神经科学研究带来了范式转变，能够从原始脑信号和神经数据中实现端到端学习，解决传统计算方法难以应对的复杂神经科学问题。

Method: 通过系统回顾和分析大规模AI模型在五个主要神经科学领域的应用：神经影像与数据处理、脑机接口与神经解码、分子神经科学与基因组建模、临床辅助与转化框架、神经系统与精神疾病的特定应用。

Result: 研究表明这些模型能有效解决多模态神经数据整合、时空模式解释等计算神经科学挑战，同时神经科学与AI的互动变得更加相互促进，生物启发的架构约束被用于开发更可解释和计算高效的模型。

Conclusion: 该综述强调了这些技术的显著前景和关键实施考虑，特别强调严格的评估框架、有效的领域知识整合以及临床使用的全面伦理指南，并提供了用于验证大规模AI模型的关键神经科学数据集系统列表。

Abstract: The advent of large-scale artificial intelligence (AI) models has a
transformative effect on neuroscience research, which represents a paradigm
shift from the traditional computational methods through the facilitation of
end-to-end learning from raw brain signals and neural data. In this paper, we
explore the transformative effects of large-scale AI models on five major
neuroscience domains: neuroimaging and data processing, brain-computer
interfaces and neural decoding, molecular neuroscience and genomic modeling,
clinical assistance and translational frameworks, and disease-specific
applications across neurological and psychiatric disorders. These models are
demonstrated to address major computational neuroscience challenges, including
multimodal neural data integration, spatiotemporal pattern interpretation, and
the derivation of translational frameworks for clinical deployment. Moreover,
the interaction between neuroscience and AI has become increasingly reciprocal,
as biologically informed architectural constraints are now incorporated to
develop more interpretable and computationally efficient models. This review
highlights both the notable promise of such technologies and key implementation
considerations, with particular emphasis on rigorous evaluation frameworks,
effective domain knowledge integration, and comprehensive ethical guidelines
for clinical use. Finally, a systematic listing of critical neuroscience
datasets used to derive and validate large-scale AI models across diverse
research applications is provided.

</details>


### [245] [An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems](https://arxiv.org/abs/2510.16701)
*Ni Zhang,Zhiguang Cao,Jianan Zhou,Cong Zhang,Yew-Soon Ong*

Main category: cs.AI

TL;DR: 提出了一个基于大语言模型的代理框架AFL，用于完全自动化解决复杂车辆路径问题，无需人工干预或外部求解器，在代码可靠性和解决方案可行性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的方法在解决复杂车辆路径问题时仍需要外部干预，导致自主性受限、执行错误和解决方案可行性低的问题。

Method: AFL框架将整体流程分解为三个可管理的子任务，并采用四个专门代理的协调交互来确保跨功能一致性和逻辑合理性，直接从原始输入中提取知识并实现自包含的代码生成。

Result: 在60个复杂VRP问题上的实验表明，该框架在代码可靠性和解决方案可行性方面显著优于现有LLM基线，在评估基准上接近100%的成功率，与精心设计的算法性能相当。

Conclusion: AFL框架实现了从问题实例到解决方案的完全自动化，为复杂车辆路径问题的解决提供了可信赖的端到端解决方案。

Abstract: Complex vehicle routing problems (VRPs) remain a fundamental challenge,
demanding substantial expert effort for intent interpretation and algorithm
design. While large language models (LLMs) offer a promising path toward
automation, current approaches still rely on external intervention, which
restrict autonomy and often lead to execution errors and low solution
feasibility. To address these challenges, we propose an Agentic Framework with
LLMs (AFL) for solving complex vehicle routing problems, achieving full
automation from problem instance to solution. AFL directly extracts knowledge
from raw inputs and enables self-contained code generation without handcrafted
modules or external solvers. To improve trustworthiness, AFL decomposes the
overall pipeline into three manageable subtasks and employs four specialized
agents whose coordinated interactions enforce cross-functional consistency and
logical soundness. Extensive experiments on 60 complex VRPs, ranging from
standard benchmarks to practical variants, validate the effectiveness and
generality of our framework, showing comparable performance against
meticulously designed algorithms. Notably, it substantially outperforms
existing LLM-based baselines in both code reliability and solution feasibility,
achieving rates close to 100% on the evaluated benchmarks.

</details>


### [246] [Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI](https://arxiv.org/abs/2510.16720)
*Jitao Sang,Jinlin Xiao,Jiarun Han,Jilin Chen,Xiaoyi Chen,Shuyu Wei,Yongjie Sun,Yuhang Wang*

Main category: cs.AI

TL;DR: 本调查分析了智能AI从基于管线的外部逻辑编排向模型原生内部化能力的范式转变，重点探讨了强化学习在实现这一转变中的关键作用，以及规划、工具使用和记忆等能力如何从外部模块演变为端到端学习行为。


<details>
  <summary>Details</summary>
Motivation: 追踪智能AI的范式转变，从外部逻辑编排的管线系统转向模型内部化能力的新范式，理解强化学习在这一转变中的核心作用。

Method: 系统性地回顾了规划、工具使用和记忆等能力从外部脚本模块到端到端学习行为的演变过程，并分析了强化学习作为算法引擎如何支撑LLM + RL + Task的统一解决方案。

Result: 揭示了智能AI向模型原生范式发展的清晰轨迹，展示了能力内部化如何重塑深度研究代理和GUI代理等主要应用，并推动了多智能体协作和反思等能力的进一步发展。

Conclusion: 智能AI正从构建应用智能的系统转向开发通过经验增长智能的模型，标志着向集成学习和交互框架的模型原生智能AI的过渡。

Abstract: The rapid evolution of agentic AI marks a new phase in artificial
intelligence, where Large Language Models (LLMs) no longer merely respond but
act, reason, and adapt. This survey traces the paradigm shift in building
agentic AI: from Pipeline-based systems, where planning, tool use, and memory
are orchestrated by external logic, to the emerging Model-native paradigm,
where these capabilities are internalized within the model's parameters. We
first position Reinforcement Learning (RL) as the algorithmic engine enabling
this paradigm shift. By reframing learning from imitating static data to
outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task
across language, vision and embodied domains. Building on this, the survey
systematically reviews how each capability -- Planning, Tool use, and Memory --
has evolved from externally scripted modules to end-to-end learned behaviors.
Furthermore, it examines how this paradigm shift has reshaped major agent
applications, specifically the Deep Research agent emphasizing long-horizon
reasoning and the GUI agent emphasizing embodied interaction. We conclude by
discussing the continued internalization of agentic capabilities like
Multi-agent collaboration and Reflection, alongside the evolving roles of the
system and model layers in future agentic AI. Together, these developments
outline a coherent trajectory toward model-native agentic AI as an integrated
learning and interaction framework, marking the transition from constructing
systems that apply intelligence to developing models that grow intelligence
through experience.

</details>


### [247] [A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications](https://arxiv.org/abs/2510.16724)
*Minhua Lin,Zongyu Wu,Zhichao Xu,Hui Liu,Xianfeng Tang,Qi He,Charu Aggarwal,Hui Liu,Xiang Zhang,Suhang Wang*

Main category: cs.AI

TL;DR: 这篇论文对基于强化学习的智能搜索进行了首次全面综述，从功能角色、优化策略和优化范围三个维度组织这一新兴领域，总结了代表性方法、评估协议和应用。


<details>
  <summary>Details</summary>
Motivation: 传统RAG管道通常是单轮和启发式的，缺乏对检索和推理的自适应控制。强化学习为智能搜索提供了自适应和自我改进搜索行为的强大机制。

Method: 从三个互补维度组织基于RL的智能搜索领域：(i) RL的功能角色，(ii) RL的优化策略，(iii) RL的应用范围。

Result: 提供了该领域的全面概述，包括代表性方法、评估协议和应用案例，并讨论了开放挑战和未来方向。

Conclusion: 希望这项调查能激发未来关于RL与智能搜索整合的研究，推动构建可靠和可扩展的RL驱动智能搜索系统。

Abstract: The advent of large language models (LLMs) has transformed information access
and reasoning through open-ended natural language interaction. However, LLMs
remain limited by static knowledge, factual hallucinations, and the inability
to retrieve real-time or domain-specific information. Retrieval-Augmented
Generation (RAG) mitigates these issues by grounding model outputs in external
evidence, but traditional RAG pipelines are often single turn and heuristic,
lacking adaptive control over retrieval and reasoning. Recent advances in
agentic search address these limitations by enabling LLMs to plan, retrieve,
and reflect through multi-step interaction with search environments. Within
this paradigm, reinforcement learning (RL) offers a powerful mechanism for
adaptive and self-improving search behavior. This survey provides the first
comprehensive overview of \emph{RL-based agentic search}, organizing the
emerging field along three complementary dimensions: (i) What RL is for
(functional roles), (ii) How RL is used (optimization strategies), and (iii)
Where RL is applied (scope of optimization). We summarize representative
methods, evaluation protocols, and applications, and discuss open challenges
and future directions toward building reliable and scalable RL driven agentic
search systems. We hope this survey will inspire future research on the
integration of RL and agentic search. Our repository is available at
https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.

</details>


### [248] [Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration](https://arxiv.org/abs/2510.16742)
*Paul Saves,Pramudita Satria Palar,Muhammad Daffa Robani,Nicolas Verstaevel,Moncef Garouani,Julien Aligon,Benoit Gaudou,Koji Shimoyama,Joseph Morlier*

Main category: cs.AI

TL;DR: 提出了一种基于代理模型的仿真驱动工程工作流，通过训练轻量级仿真器来解决高计算成本和黑盒透明度问题，支持不确定性量化和可解释AI分析。


<details>
  <summary>Details</summary>
Motivation: 仿真驱动工程工作流面临两个核心障碍：(1)高计算成本，(2)黑盒组件导致的透明度和可靠性限制。

Method: 在紧凑实验设计上训练轻量级仿真器，结合全局效应分析、不确定性分析和局部归因，评估不同代理模型间解释的一致性。

Result: 在混合电动飞机多学科设计和城市隔离基于代理模型两个案例中，该方法实现了秒级大规模探索，揭示了非线性相互作用和涌现行为，识别了关键设计和政策杠杆。

Conclusion: 代理模型与可解释AI的耦合能够快速探索复杂系统，发现关键因素，并指导进一步数据收集或模型改进。

Abstract: Complex systems are increasingly explored through simulation-driven
engineering workflows that combine physics-based and empirical models with
optimization and analytics. Despite their power, these workflows face two
central obstacles: (1) high computational cost, since accurate exploration
requires many expensive simulator runs; and (2) limited transparency and
reliability when decisions rely on opaque blackbox components. We propose a
workflow that addresses both challenges by training lightweight emulators on
compact designs of experiments that (i) provide fast, low-latency
approximations of expensive simulators, (ii) enable rigorous uncertainty
quantification, and (iii) are adapted for global and local Explainable
Artificial Intelligence (XAI) analyses. This workflow unifies every
simulation-based complex-system analysis tool, ranging from engineering design
to agent-based models for socio-environmental understanding. In this paper, we
proposea comparative methodology and practical recommendations for using
surrogate-based explainability tools within the proposed workflow. The
methodology supports continuous and categorical inputs, combines global-effect
and uncertainty analyses with local attribution, and evaluates the consistency
of explanations across surrogate models, thereby diagnosing surrogate adequacy
and guiding further data collection or model refinement. We demonstrate the
approach on two contrasting case studies: a multidisciplinary design analysis
of a hybrid-electric aircraft and an agent-based model of urban segregation.
Results show that the surrogate model and XAI coupling enables large-scale
exploration in seconds, uncovers nonlinear interactions and emergent behaviors,
identifies key design and policy levers, and signals regions where surrogates
require more data or alternative architectures.

</details>


### [249] [ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2510.16753)
*Wei Huang,Peining Li,Meiyu Liang,Xu Hou,Junping Du,Yingxia Shao,Guanhua Ye,Wu Liu,Kangkang Lu,Yang Yu*

Main category: cs.AI

TL;DR: 提出了ELMM模型用于多模态知识图谱补全，通过多视图视觉标记压缩器和注意力剪枝策略，在保持性能的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 多模态知识图谱存在不完整性问题，而现有方法在处理多模态信息时面临语义噪声、模态冲突和高计算成本等挑战。

Method: 使用基于多头注意力的多视图视觉标记压缩器自适应压缩图像标记，设计注意力剪枝策略减少冗余层，并通过线性投影补偿性能损失。

Result: 在FB15k-237-IMG和WN18-IMG基准测试中达到最先进性能，同时大幅提升计算效率。

Conclusion: ELMM为多模态知识图谱补全建立了新的范式，在性能和效率方面均表现出色。

Abstract: Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by
incorporating visual and textual modalities, enabling richer and more
expressive entity representations. However, existing MKGs often suffer from
incompleteness, which hinder their effectiveness in downstream tasks.
Therefore, multimodal knowledge graph completion (MKGC) task is receiving
increasing attention. While large language models (LLMs) have shown promise for
knowledge graph completion (KGC), their application to the multimodal setting
remains underexplored. Moreover, applying Multimodal Large Language Models
(MLLMs) to the task of MKGC introduces significant challenges: (1) the large
number of image tokens per entity leads to semantic noise and modality
conflicts, and (2) the high computational cost of processing large token
inputs. To address these issues, we propose Efficient Lightweight Multimodal
Large Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token
Compressor (MVTC) based on multi-head attention mechanism, which adaptively
compresses image tokens from both textual and visual views, thereby effectively
reducing redundancy while retaining necessary information and avoiding modality
conflicts. Additionally, we design an attention pruning strategy to remove
redundant attention layers from MLLMs, thereby significantly reducing the
inference cost. We further introduce a linear projection to compensate for the
performance degradation caused by pruning. Extensive experiments on benchmark
FB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art
performance while substantially improving computational efficiency,
establishing a new paradigm for multimodal knowledge graph completion.

</details>


### [250] [End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756)
*Siyin Wang,Wenyi Yu,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Lu Lu,Chao Zhang*

Main category: cs.AI

TL;DR: ELLSA是首个全双工、端到端的多模态模型，能够同时感知和生成视觉、文本、语音和动作，实现更自然的人机交互。


<details>
  <summary>Details</summary>
Motivation: 人类交互本质上是多模态和全双工的，需要模型能够同时感知和生成多种模态，实现更自然的人类模拟。

Method: 采用新颖的SA-MoE架构（自注意力专家混合），将各模态路由到专门专家，通过统一注意力骨干网络进行融合。

Result: 在语音交互和机器人操作基准测试中，ELLSA与特定模态基线相当，同时支持高级多模态和全双工行为。

Conclusion: ELLSA代表了向更自然和通用交互智能迈出的一步，有助于实现人工通用智能。

Abstract: Human interaction is inherently multimodal and full-duplex: we listen while
watching, speak while acting, and fluidly adapt to turn-taking and
interruptions. Realizing these capabilities is essential for building models
simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),
which, to our knowledge, is the first full-duplex, end-to-end model that
simultaneously perceives and generates across vision, text, speech, and action
within a single architecture, enabling interaction patterns previously out of
reach, yielding more natural, human-like behaviors. At its core is a novel
SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each
modality to specialized experts and fuses them through a unified attention
backbone. This provides a generalizable solution for joint multimodal
perception and concurrent generation, leveraging strong pre-trained components
while enabling efficient modality integration and mitigating modality
interference. On speech-interaction and robot-manipulation benchmarks, ELLSA
matches modality-specific baselines, while uniquely supporting advanced
multimodal and full-duplex behaviors such as dialogue and action turn-taking,
defective instruction rejection, speaking-while-acting, context-grounded visual
question answering, and action barge-ins. We contend that ELLSA represents a
step toward more natural and general interactive intelligence, contributing to
the broader pursuit of artificial general intelligence. All data, code and
model checkpoints will be released upon acceptance.

</details>


### [251] [See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models](https://arxiv.org/abs/2510.16769)
*Shuo Han,Yukun Cao,Zezhong Ding,Zengyi Gao,S Kevin Zhou,Xike Xie*

Main category: cs.AI

TL;DR: GraphVista是一个统一的图理解框架，通过分层组织图信息和使用规划代理协调文本和视觉模态，解决了视觉语言模型在图理解中的可扩展性和模态协调问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在图理解中面临输入token限制导致的可扩展性瓶颈，以及缺乏有效协调文本和视觉模态的机制。

Method: GraphVista采用分层方法组织图信息到轻量级GraphRAG基础中，仅检索任务相关的文本描述和高分辨率视觉子图；引入规划代理根据任务复杂度路由到最适合的模态。

Result: GraphVista可扩展到比现有基准大200倍的图，在质量上比最先进基线提升4.4倍，持续优于现有的文本、视觉和融合方法。

Conclusion: GraphVista通过充分利用两种模态的互补优势，有效解决了图理解中的可扩展性和模态协调挑战。

Abstract: Vision-language models (VLMs) have shown promise in graph understanding, but
remain limited by input-token constraints, facing scalability bottlenecks and
lacking effective mechanisms to coordinate textual and visual modalities. To
address these challenges, we propose GraphVista, a unified framework that
enhances both scalability and modality coordination in graph understanding. For
scalability, GraphVista organizes graph information hierarchically into a
lightweight GraphRAG base, which retrieves only task-relevant textual
descriptions and high-resolution visual subgraphs, compressing redundant
context while preserving key reasoning elements. For modality coordination,
GraphVista introduces a planning agent that routes tasks to the most suitable
modality-using the text modality for simple property reasoning and the visual
modality for local and structurally complex reasoning grounded in explicit
topology. Extensive experiments demonstrate that GraphVista scales to large
graphs, up to $200\times$ larger than those used in existing benchmarks, and
consistently outperforms existing textual, visual, and fusion-based methods,
achieving up to $4.4\times$ quality improvement over the state-of-the-art
baselines by fully exploiting the complementary strengths of both modalities.

</details>


### [252] [Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation](https://arxiv.org/abs/2510.16802)
*Chao Li,Yuru Wang*

Main category: cs.AI

TL;DR: 提出了领域情境化概念图(CDC)，将领域作为知识表示的一等元素，采用<概念, 关系@领域, 概念'>的三元组结构，实现上下文感知推理和跨领域类比。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱受限于固定的本体论和刚性层次结构，主要问题在于将领域视为隐含上下文而非显式的推理级组件。

Method: 采用C-D-C三元组结构，基于认知语言学同构映射原理，定义了20多个标准化关系谓词（结构、逻辑、跨领域和时间），并在Prolog中实现完整推理能力。

Result: 在教育、企业知识系统和技术文档等案例研究中，CDC实现了上下文感知推理、跨领域类比和个性化知识建模等传统本体框架无法实现的能力。

Conclusion: CDC框架通过将领域提升为概念表示的一等元素，克服了传统知识图谱的局限性，为知识建模提供了更灵活和上下文感知的方法。

Abstract: Traditional knowledge graphs are constrained by fixed ontologies that
organize concepts within rigid hierarchical structures. The root cause lies in
treating domains as implicit context rather than as explicit, reasoning-level
components. To overcome these limitations, we propose the Domain-Contextualized
Concept Graph (CDC), a novel knowledge modeling framework that elevates domains
to first-class elements of conceptual representation. CDC adopts a C-D-C triple
structure - <Concept, Relation@Domain, Concept'> - where domain specifications
serve as dynamic classification dimensions defined on demand. Grounded in a
cognitive-linguistic isomorphic mapping principle, CDC operationalizes how
humans understand concepts through contextual frames. We formalize more than
twenty standardized relation predicates (structural, logical, cross-domain, and
temporal) and implement CDC in Prolog for full inference capability. Case
studies in education, enterprise knowledge systems, and technical documentation
demonstrate that CDC enables context-aware reasoning, cross-domain analogy, and
personalized knowledge modeling - capabilities unattainable under traditional
ontology-based frameworks.

</details>


### [253] [DeepAnalyze: Agentic Large Language Models for Autonomous Data Science](https://arxiv.org/abs/2510.16872)
*Shaolei Zhang,Ju Fan,Meihao Fan,Guoliang Li,Xiaoyong Du*

Main category: cs.AI

TL;DR: DeepAnalyze-8B是首个用于自主数据科学的代理式大语言模型，能够从数据源到分析师级深度研究报告自动完成端到端流程，仅用8B参数就超越了基于最先进专有LLM的工作流代理。


<details>
  <summary>Details</summary>
Motivation: 现有的工作流式数据代理在特定数据任务上表现良好，但由于依赖预定义工作流，无法实现完全自主的数据科学。随着强大LLM的出现，从原始数据源到分析师级深度研究报告的自主数据科学变得可行。

Method: 提出基于课程学习的代理训练范式，模拟人类数据科学家的学习轨迹，使LLM能够在真实环境中逐步获取和整合多种能力。同时引入数据基础的轨迹合成框架来构建高质量训练数据。

Result: 实验表明，仅用8B参数的DeepAnalyze在广泛的数据任务上表现出色，包括数据问答、专业分析任务和开放式数据研究，超越了基于最先进专有LLM的工作流代理。

Conclusion: DeepAnalyze模型、代码和训练数据已开源，为自主数据科学的发展铺平了道路。

Abstract: Autonomous data science, from raw data sources to analyst-grade deep research
reports, has been a long-standing challenge, and is now becoming feasible with
the emergence of powerful large language models (LLMs). Recent workflow-based
data agents have shown promising results on specific data tasks but remain
fundamentally limited in achieving fully autonomous data science due to their
reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,
the first agentic LLM designed for autonomous data science, capable of
automatically completing the end-toend pipeline from data sources to
analyst-grade deep research reports. To tackle high-complexity data science
tasks, we propose a curriculum-based agentic training paradigm that emulates
the learning trajectory of human data scientists, enabling LLMs to
progressively acquire and integrate multiple capabilities in real-world
environments. We also introduce a data-grounded trajectory synthesis framework
that constructs high-quality training data. Through agentic training,
DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data
question answering and specialized analytical tasks to open-ended data
research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze
outperforms previous workflow-based agents built on most advanced proprietary
LLMs. The model, code, and training data of DeepAnalyze are open-sourced,
paving the way toward autonomous data science.

</details>


### [254] [VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents](https://arxiv.org/abs/2510.16907)
*Kangrui Wang,Pingyue Zhang,Zihan Wang,Yaning Gao,Linjie Li,Qineng Wang,Hanyang Chen,Chi Wan,Yiping Lu,Zhengyuan Yang,Lijuan Wang,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Yejin Choi,Manling Li*

Main category: cs.AI

TL;DR: 该论文提出通过强化学习训练VLM智能体进行显式视觉状态推理，构建内部世界模型，在五个多样化智能体基准测试中实现了3倍性能提升，超越了GPT-5、Gemini 2.5 Pro和Claude 4.5等专有推理模型。


<details>
  <summary>Details</summary>
Motivation: 训练视觉语言模型(VLM)智能体面临从文本状态到复杂视觉观察的转变挑战，这引入了部分可观测性并需要强大的世界建模能力。研究旨在探索VLM智能体是否能够通过显式视觉状态推理构建内部世界模型。

Method: 将智能体推理过程架构化为部分可观测马尔可夫决策过程(POMDP)，通过强化学习强制和奖励推理过程。将推理分解为状态估计和转移建模，设计了世界建模奖励提供密集监督，并引入双层通用优势估计进行回合感知的信用分配。

Result: 一个30亿参数的模型在五个多样化智能体基准测试中获得了0.82的分数，相比未训练对应模型(0.21)实现了3倍改进，超越了GPT-5(0.75)、Gemini 2.5 Pro(0.67)和Claude 4.5(0.62)等专有推理模型。

Conclusion: 通过视觉状态推理，VLM智能体能够有效构建内部世界模型。最优表示形式是任务依赖的：自然语言在一般任务中擅长捕捉语义关系，而结构化格式对于精确操作和控制不可或缺。

Abstract: A key challenge in training Vision-Language Model (VLM) agents, compared to
Language Model (LLM) agents, lies in the shift from textual states to complex
visual observations. This transition introduces partial observability and
demands robust world modeling. We ask: Can VLM agents construct internal world
models through explicit visual state reasoning? To address this question, we
architecturally enforce and reward the agent's reasoning process via
reinforcement learning (RL), formulating it as a Partially Observable Markov
Decision Process (POMDP). We find that decomposing the agent's reasoning into
State Estimation ("what is the current state?") and Transition Modeling ("what
comes next?") is critical for success, as demonstrated through five reasoning
strategies. Our investigation into how agents represent internal beliefs
reveals that the optimal representation is task-dependent: Natural Language
excels at capturing semantic relationships in general tasks, while Structured
formats are indispensable for precise manipulation and control. Building on
these insights, we design a World Modeling Reward that provides dense,
turn-level supervision for accurate state prediction, and introduce Bi-Level
General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.
Through this form of visual state reasoning, a 3B-parameter model achieves a
score of 0.82 across five diverse agent benchmarks, representing a 3$\times$
improvement over its untrained counterpart (0.21) and outperforming proprietary
reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5
(0.62). All experiments are conducted within our VAGEN framework, a scalable
system for training and analyzing multi-turn VLM agents in diverse visual
environments. Code and data are publicly available at
https://vagen-ai.github.io.

</details>


### [255] [A Comparative User Evaluation of XRL Explanations using Goal Identification](https://arxiv.org/abs/2510.16956)
*Mark Towers,Yali Du,Christopher Freeman,Timothy J. Norman*

Main category: cs.AI

TL;DR: 提出了一种新的评估方法，测试用户是否能从强化学习算法的解释中识别智能体的目标。在Ms. Pacman环境中测试了四种可解释强化学习算法，发现只有一种算法在测试目标上达到了超过随机准确率的性能，且用户普遍对自己的选择过度自信。


<details>
  <summary>Details</summary>
Motivation: 可解释强化学习算法的核心应用之一是调试，但缺乏对其相对性能的比较评估。需要了解用户是否能从算法解释中正确识别智能体的目标。

Method: 使用Atari的Ms. Pacman环境和四种可解释强化学习算法，提出新颖的评估方法测试用户从决策解释中识别智能体目标的能力。

Result: 只有一种算法在测试目标上达到了超过随机准确率的性能；用户普遍对自己的选择过度自信；用户自报的识别和理解难易程度与他们的准确率没有相关性。

Conclusion: 当前可解释强化学习算法在帮助用户识别智能体目标方面效果有限，用户的主观感受与实际识别能力存在脱节，需要改进评估方法和算法设计。

Abstract: Debugging is a core application of explainable reinforcement learning (XRL)
algorithms; however, limited comparative evaluations have been conducted to
understand their relative performance. We propose a novel evaluation
methodology to test whether users can identify an agent's goal from an
explanation of its decision-making. Utilising the Atari's Ms. Pacman
environment and four XRL algorithms, we find that only one achieved greater
than random accuracy for the tested goals and that users were generally
overconfident in their selections. Further, we find that users' self-reported
ease of identification and understanding for every explanation did not
correlate with their accuracy.

</details>


### [256] [STARK: Strategic Team of Agents for Refining Kernels](https://arxiv.org/abs/2510.16996)
*Juncheng Dong,Yang Yang,Tao Liu,Yang Wang,Feng Qi,Vahid Tarokh,Kaushik Rangadurai,Shuang Yang*

Main category: cs.AI

TL;DR: 提出了一种基于LLM的多智能体协作框架，用于GPU内核优化，通过系统化探索设计空间、动态上下文管理和策略搜索，显著提升了内核性能和优化成功率。


<details>
  <summary>Details</summary>
Motivation: GPU内核优化对AI进展至关重要，但传统方法困难且耗时。现有LLM方法多将其视为单次生成器或简单优化工具，难以应对复杂的内核优化挑战。

Method: 采用多智能体协作框架，结合基于经验的指导、动态上下文管理和策略搜索，模拟专家工程师的工作流程，让LLM能够推理硬件权衡、整合性能分析反馈并进行迭代优化。

Result: 在KernelBench基准测试中，相比基线方法，该系统在基线经常失败的情况下仍能生成正确解决方案，并实现高达16倍的运行时性能提升。

Conclusion: 该智能体LLM框架展示了实现完全自动化、可扩展GPU内核优化的巨大潜力。

Abstract: The efficiency of GPU kernels is central to the progress of modern AI, yet
optimizing them remains a difficult and labor-intensive task due to complex
interactions between memory hierarchies, thread scheduling, and
hardware-specific characteristics. While recent advances in large language
models (LLMs) provide new opportunities for automated code generation, existing
approaches largely treat LLMs as single-shot generators or naive refinement
tools, limiting their effectiveness in navigating the irregular kernel
optimization landscape. We introduce an LLM agentic framework for GPU kernel
optimization that systematically explores the design space through multi-agent
collaboration, grounded instruction, dynamic context management, and strategic
search. This framework mimics the workflow of expert engineers, enabling LLMs
to reason about hardware trade-offs, incorporate profiling feedback, and refine
kernels iteratively. We evaluate our approach on KernelBench, a benchmark for
LLM-based kernel optimization, and demonstrate substantial improvements over
baseline agents: our system produces correct solutions where baselines often
fail, and achieves kernels with up to 16x faster runtime performance. These
results highlight the potential of agentic LLM frameworks to advance fully
automated, scalable GPU kernel optimization.

</details>


### [257] [ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems](https://arxiv.org/abs/2510.17052)
*Hassan Hamad,Yingru Xu,Liang Zhao,Wenbo Yan,Narendra Gyanchandani*

Main category: cs.AI

TL;DR: ToolCritic是一个诊断框架，用于评估和改进LLM在多轮工具增强对话中的行为，通过检测8种特定工具调用错误并提供针对性反馈，使主LLM能够修正响应。


<details>
  <summary>Details</summary>
Motivation: 工具增强的大型语言模型在现实应用中越来越普遍，但工具使用错误仍然阻碍其可靠性，需要解决这些错误以提高LLM与外部工具集成的鲁棒性。

Method: 系统定义8种工具调用错误类别，构建合成数据集训练ToolCritic，该框架检测错误并提供反馈，主LLM基于反馈修正响应。

Result: 在Schema-Guided Dialogue数据集上的实验表明，ToolCritic相比基线方法（包括零样本提示和自校正技术）将工具调用准确率提高了高达13%。

Conclusion: ToolCritic代表了在现实世界对话应用中实现更鲁棒的LLM与外部工具集成的有希望的一步。

Abstract: Tool-augmented large language models (LLMs) are increasingly employed in
real-world applications, but tool usage errors still hinder their reliability.
We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM
behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight
distinct error types specific to tool-calling (e.g., premature invocation,
argument misalignment, and misinterpretation of tool outputs) and provides
targeted feedback to the main LLM. The main LLM, assumed to have strong
reasoning, task understanding and orchestration capabilities, then revises its
response based on ToolCritic's feedback. We systematically define these error
categories and construct a synthetic dataset to train ToolCritic. Experimental
results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic
improves tool-calling accuracy by up to 13% over baselines, including zero-shot
prompting and self-correction techniques. This represents a promising step
toward more robust LLM integration with external tools in real-world dialogue
applications.

</details>


### [258] [A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation](https://arxiv.org/abs/2510.17064)
*Rongbin Li,Wenbo Chen,Zhao Li,Rodrigo Munoz-Castaneda,Jinbo Li,Neha S. Maurya,Arnav Solanki,Huan He,Hanwen Xing,Meaghan Ramlakhan,Zachary Wise,Zhuhao Wu,Hua Xu,Michael Hawrylycz,W. Jim Zheng*

Main category: cs.AI

TL;DR: BRAINCELL-AID是一个多智能体AI系统，通过整合自由文本描述和本体标签，结合检索增强生成技术，显著提升基因集注释的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序技术虽然能够识别多样细胞类型，但对涉及特征不明确基因的转录组特征进行注释仍然是一个重大挑战。传统方法如GSEA依赖精心策划的注释，在这些情况下表现不佳。

Method: 开发了BRAINCELL-AID多智能体AI系统，整合自由文本描述与本体标签，采用检索增强生成技术，通过相关PubMed文献精炼预测，减少幻觉并增强可解释性。

Result: 对小鼠基因集实现了77%的正确注释率，成功注释了BRAIN Initiative Cell Census Network生成的5,322个脑细胞簇，识别了区域特异性基因共表达模式，并推断出基因集合的功能作用。

Conclusion: BRAINCELL-AID创建了一个有价值的资源来支持社区驱动的细胞类型注释，特别是在识别与基底神经节相关的细胞类型方面表现出色。

Abstract: Single-cell RNA sequencing has transformed our ability to identify diverse
cell types and their transcriptomic signatures. However, annotating these
signatures-especially those involving poorly characterized genes-remains a
major challenge. Traditional methods, such as Gene Set Enrichment Analysis
(GSEA), depend on well-curated annotations and often perform poorly in these
contexts. Large Language Models (LLMs) offer a promising alternative but
struggle to represent complex biological knowledge within structured
ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:
https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that
integrates free-text descriptions with ontology labels to enable more accurate
and robust gene set annotation. By incorporating retrieval-augmented generation
(RAG), we developed a robust agentic workflow that refines predictions using
relevant PubMed literature, reducing hallucinations and enhancing
interpretability. Using this workflow, we achieved correct annotations for 77%
of mouse gene sets among their top predictions. Applying this approach, we
annotated 5,322 brain cell clusters from the comprehensive mouse brain cell
atlas generated by the BRAIN Initiative Cell Census Network, enabling novel
insights into brain cell function by identifying region-specific gene
co-expression patterns and inferring functional roles of gene ensembles.
BRAINCELL-AID also identifies Basal Ganglia-related cell types with
neurologically meaningful descriptions. Hence, we create a valuable resource to
support community-driven cell type annotation.

</details>


### [259] [Structured Debate Improves Corporate Credit Reasoning in Financial AI](https://arxiv.org/abs/2510.17108)
*Yoonjin Lee,Munhee Kim,Hanbi Choi,Juhyeon Park,Seungho Lyoo,Woojin Park*

Main category: cs.AI

TL;DR: 开发并评估了两种基于大语言模型的系统，用于从非财务证据生成结构化推理，以自动化企业信用评估中的证据推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有金融AI方法主要关注数值预测，无法有效处理企业信用评估中具有决定性影响但难以形式化的定性非财务指标，缺乏对专业贷款评估所需解释性判断的支持。

Method: 开发了两种LLM系统：非对抗性单代理系统(NAS)通过单次推理管道生成双向分析；基于辩论的多代理系统(KPD-MADS)基于卡尔·波普批判对话框架，通过十步结构化交互协议实现对抗性验证。

Result: 相比人工专家报告，两种系统均实现显著生产力提升(NAS: 11.55秒/案例；KPD-MADS: 91.97秒；人工基准: 1920秒)。KPD-MADS在推理质量上表现更优，在解释充分性(4.0 vs 3.0)、实际适用性(4.0 vs 3.0)和可用性(62.5 vs 52.5)方面获得更高评分。

Conclusion: 结构化多代理交互能够增强金融AI中的推理严谨性和可解释性，推动企业信用评估中可扩展且可辩护的自动化进程。

Abstract: Despite advances in financial AI, the automation of evidence-based reasoning
remains unresolved in corporate credit assessment, where qualitative
non-financial indicators exert decisive influence on loan repayment outcomes
yet resist formalization. Existing approaches focus predominantly on numerical
prediction and provide limited support for the interpretive judgments required
in professional loan evaluation. This study develops and evaluates two
operational large language model (LLM)-based systems designed to generate
structured reasoning from non-financial evidence. The first is a
non-adversarial single-agent system (NAS) that produces bidirectional analysis
through a single-pass reasoning pipeline. The second is a debate-based
multi-agent system (KPD-MADS) that operationalizes adversarial verification
through a ten-step structured interaction protocol grounded in Karl Popper's
critical dialogue framework. Both systems were applied to three real corporate
cases and evaluated by experienced credit risk professionals. Compared to
manual expert reporting, both systems achieved substantial productivity gains
(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The
KPD-MADS demonstrated superior reasoning quality, receiving higher median
ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.
3.0), and usability (62.5 vs. 52.5). These findings show that structured
multi-agent interaction can enhance reasoning rigor and interpretability in
financial AI, advancing scalable and defensible automation in corporate credit
assessment.

</details>


### [260] [Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion](https://arxiv.org/abs/2510.17145)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.AI

TL;DR: 提出了一种基于手工特征的方法，通过提取和融合颜色统计、多色彩空间直方图以及纹理特征来评估鱼类新鲜度，在FFE数据集上取得了显著优于深度学习基线的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统感官评估鱼类新鲜度存在主观性、不一致性和难以标准化的问题，需要开发客观、可靠的自动化评估方法。

Method: 从鱼眼图像中系统提取互补特征描述符，包括颜色统计、多色彩空间直方图、局部二值模式(LBP)和灰度共生矩阵(GLCM)等纹理特征，融合全局色度变化和局部ROI区域退化特征。

Result: 在标准训练测试设置下，LightGBM分类器达到77.56%准确率，比之前深度学习基线63.21%提升14.35%；使用增强数据时，人工神经网络达到97.16%准确率，比之前最佳77.3%提升19.86%。

Conclusion: 精心设计的手工特征经过战略处理可以提供鲁棒、可解释且可靠的鱼类新鲜度自动评估解决方案，为食品质量监控的实际应用提供有价值见解。

Abstract: Accurate assessment of fish freshness remains a major challenge in the food
industry, with direct consequences for product quality, market value, and
consumer health. Conventional sensory evaluation is inherently subjective,
inconsistent, and difficult to standardize across contexts, often limited by
subtle, species-dependent spoilage cues. To address these limitations, we
propose a handcrafted feature-based approach that systematically extracts and
incrementally fuses complementary descriptors, including color statistics,
histograms across multiple color spaces, and texture features such as Local
Binary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish
eye images. Our method captures global chromatic variations from full images
and localized degradations from ROI segments, fusing each independently to
evaluate their effectiveness in assessing freshness. Experiments on the
Freshness of the Fish Eyes (FFE) dataset demonstrate the approach's
effectiveness: in a standard train-test setting, a LightGBM classifier achieved
77.56% accuracy, a 14.35% improvement over the previous deep learning baseline
of 63.21%. With augmented data, an Artificial Neural Network (ANN) reached
97.16% accuracy, surpassing the prior best of 77.3% by 19.86%. These results
demonstrate that carefully engineered, handcrafted features, when strategically
processed, yield a robust, interpretable, and reliable solution for automated
fish freshness assessment, providing valuable insights for practical
applications in food quality monitoring.

</details>


### [261] [Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation](https://arxiv.org/abs/2510.17146)
*Subin Lin,Chuanbo Hua*

Main category: cs.AI

TL;DR: 提出了PILLM框架，将物理原理融入大型语言模型，通过进化循环自动生成、评估和优化HVAC系统异常检测规则，在保持可解释性的同时提升检测性能。


<details>
  <summary>Details</summary>
Motivation: HVAC系统能耗占比高，现有方法各有不足：基于规则的方法可解释但缺乏适应性，深度学习方法预测能力强但缺乏透明度和物理合理性，LLM方法改善了可解释性但忽略了HVAC运行的物理原理。

Method: PILLM框架在进化循环中运行，引入物理感知的反思和交叉算子，嵌入热力学和控制理论约束，生成既适应性强又物理基础扎实的异常检测规则。

Result: 在公开的建筑故障检测数据集上，PILLM实现了最先进的性能，同时生成可解释和可操作的诊断规则。

Conclusion: PILLM推动了智能建筑系统中可信赖和可部署AI的发展，为HVAC异常检测提供了物理基础扎实且可解释的解决方案。

Abstract: Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a
substantial share of global building energy use, making reliable anomaly
detection essential for improving efficiency and reducing emissions. Classical
rule-based approaches offer explainability but lack adaptability, while deep
learning methods provide predictive power at the cost of transparency,
efficiency, and physical plausibility. Recent attempts to use Large Language
Models (LLMs) for anomaly detection improve interpretability but largely ignore
the physical principles that govern HVAC operations. We present PILLM, a
Physics-Informed LLM framework that operates within an evolutionary loop to
automatically generate, evaluate, and refine anomaly detection rules. Our
approach introduces physics-informed reflection and crossover operators that
embed thermodynamic and control-theoretic constraints, enabling rules that are
both adaptive and physically grounded. Experiments on the public Building Fault
Detection dataset show that PILLM achieves state-of-the-art performance while
producing diagnostic rules that are interpretable and actionable, advancing
trustworthy and deployable AI for smart building systems.

</details>


### [262] [Which LLM Multi-Agent Protocol to Choose?](https://arxiv.org/abs/2510.17149)
*Hongyi Du,Jiaqi Su,Jisen Li,Lijie Ding,Yingxuan Yang,Peixuan Han,Xiangru Tang,Kunlun Zhu,Jiaxuan You*

Main category: cs.AI

TL;DR: ProtocolBench是一个系统评估多智能体系统通信协议的基准测试，ProtocolRouter是一个可学习的协议路由器，能根据场景需求选择最佳协议。


<details>
  <summary>Details</summary>
Motivation: 大规模多智能体系统中，通信协议选择对性能和可靠性至关重要，但当前缺乏标准化评估方法，选择往往基于直觉。

Method: 开发ProtocolBench基准测试，从任务成功率、端到端延迟、消息开销和故障恢复能力四个维度系统比较协议；提出ProtocolRouter学习型协议路由器，根据需求和运行时信号选择协议。

Result: 协议选择显著影响系统行为：在Streaming Queue场景中完成时间差异达36.5%，端到端延迟差异3.48秒；ProtocolRouter相比最佳单协议基线将故障恢复时间减少18.1%，在GAIA场景中成功率更高。

Conclusion: 通信协议选择对多智能体系统性能有重大影响，ProtocolBench和ProtocolRouter提供了系统化评估和优化协议选择的方法，提升了系统可靠性。

Abstract: As large-scale multi-agent systems evolve, the communication protocol layer
has become a critical yet under-evaluated factor shaping performance and
reliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,
etc.), selection is often intuition-driven and lacks standardized guidance. We
introduce ProtocolBench, a benchmark that systematically compares agent
protocols along four measurable axes: task success, end-to-end latency, message
or byte overhead, and robustness under failures. On ProtocolBench, protocol
choice significantly influences system behavior. In the Streaming Queue
scenario, overall completion time varies by up to 36.5% across protocols, and
mean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,
resilience also differs consistently across protocols. Beyond evaluation, we
present ProtocolRouter, a learnable protocol router that selects per-scenario
(or per-module) protocols from requirement and runtime signals. ProtocolRouter
reduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol
baseline, and achieves scenario-specific gains such as higher success in GAIA.
We also release ProtocolRouterBench to standardize protocol evaluation and
improve reliability at scale.

</details>


### [263] [Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients](https://arxiv.org/abs/2510.17172)
*Shun Huang,Wenlu Xing,Shijia Geng,Hailong Wang,Guangkun Nie,Gongzheng Tang,Chenyang He,Shenda Hong*

Main category: cs.AI

TL;DR: 开发了一个结合ECG基础模型和可解释XGBoost分类器的混合预测框架，用于预测急性心肌梗死后恶性室性心律失常风险，在提高准确性的同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 急性心肌梗死后恶性室性心律失常是院内死亡的主要原因，传统风险评分性能有限，而端到端深度学习模型缺乏临床信任所需的可解释性。

Method: 使用ECG基础模型(ECGFounder)提取150维诊断概率特征，通过特征选择后训练XGBoost分类器，并采用SHAP方法进行可解释性分析。

Result: 混合模型AUC达到0.801，优于KNN(0.677)、RNN(0.676)和1D-CNN(0.720)。SHAP分析显示模型识别的关键特征与临床知识高度一致。

Conclusion: 该混合框架通过验证基础模型输出作为有效的自动化特征工程，为构建可信赖、可解释的AI临床决策支持系统提供了新范式。

Abstract: Malignant ventricular arrhythmias (VT/VF) following acute myocardial
infarction (AMI) are a major cause of in-hospital death, yet early
identification remains a clinical challenge. While traditional risk scores have
limited performance, end-to-end deep learning models often lack the
interpretability needed for clinical trust. This study aimed to develop a
hybrid predictive framework that integrates a large-scale electrocardiogram
(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to
improve both accuracy and interpretability. We analyzed 6,634 ECG recordings
from AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder
model was used to extract 150-dimensional diagnostic probability features ,
which were then refined through feature selection to train the XGBoost
classifier. Model performance was evaluated using AUC and F1-score , and the
SHAP method was used for interpretability. The ECGFounder + XGBoost hybrid
model achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC
0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that
model-identified key features, such as "premature ventricular complexes" (risk
predictor) and "normal sinus rhythm" (protective factor), were highly
consistent with clinical knowledge. We conclude that this hybrid framework
provides a novel paradigm for VT/VF risk prediction by validating the use of
foundation model outputs as effective, automated feature engineering for
building trustworthy, explainable AI-based clinical decision support systems.

</details>


### [264] [Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users](https://arxiv.org/abs/2510.17173)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.AI

TL;DR: 研究了基于工具增强的LLM健康教练系统，通过离线策略评估发现统一的重工具策略会损害特定用户群体，特别是低健康素养/高自我效能用户。轻量级模拟器显示添加早期信息增益奖励可改善个性化效果。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过评估优先的方法实现个性化健康教练系统，避免统一策略对特定用户群体造成伤害，特别是关注低健康素养但高自我效能用户的特殊需求。

Method: 使用离线策略评估(OPE)分析因子化决策头(工具/风格)，部署轻量级模拟器验证添加信息增益奖励的效果，采用冻结生成器、学习子群体感知决策头的策略。

Result: 统一重工具策略在日志上提高平均价值但损害特定子群体；添加早期信息增益奖励可靠地缩短特质识别时间，提高目标成功率和pass@3指标。

Conclusion: 提出了评估优先的个性化路径：冻结生成器，基于类型化奖励学习子群体感知决策头，并始终报告每个原型指标以揭示被平均值掩盖的子群体伤害。

Abstract: We study a web-deployed, tool-augmented LLM health coach with real users. In
a pilot with seven users (280 rated turns), offline policy evaluation (OPE)
over factorized decision heads (Tool/Style) shows that a uniform heavy-tool
policy raises average value on logs but harms specific subgroups, most notably
low-health-literacy/high-self-efficacy users. A lightweight simulator with
hidden archetypes further shows that adding a small early information-gain
bonus reliably shortens trait identification and improves goal success and
pass@3. Together, these early findings indicate an evaluation-first path to
personalization: freeze the generator, learn subgroup-aware decision heads on
typed rewards (objective tool outcomes and satisfaction), and always report
per-archetype metrics to surface subgroup harms that averages obscure.

</details>


### [265] [Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling](https://arxiv.org/abs/2510.17211)
*Tingsong Xiao,Yao An Lee,Zelin Xu,Yupu Zhang,Zibo Liu,Yu Huang,Jiang Bian,Serena Jingchuan Guo,Zhe Jiang*

Main category: cs.AI

TL;DR: 提出了TD-HNODE模型，通过时间详细超图和神经ODE框架学习疾病进展的连续时间动态，在2型糖尿病和心血管疾病进展建模中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确建模疾病进展（如2型糖尿病）可以改善患者亚型分型和及时干预，但现有方法难以处理不规则时间采样数据和患者异质性，无法捕捉复杂的连续时间动态。

Method: 使用时间详细超图表示临床认可的进展轨迹，通过神经ODE框架学习连续时间进展动态，包含可学习的TD-超图拉普拉斯算子捕捉疾病并发症标志物在进展轨迹内和轨迹间的相互依赖关系。

Result: 在两个真实世界临床数据集上的实验表明，TD-HNODE在建模2型糖尿病和相关心血管疾病进展方面优于多个基线方法。

Conclusion: TD-HNODE能够有效解决疾病进展建模中的挑战，通过结合超图表示和神经ODE框架，成功捕捉了复杂的连续时间动态和患者异质性。

Abstract: Disease progression modeling aims to characterize and predict how a patient's
disease complications worsen over time based on longitudinal electronic health
records (EHRs). Accurate modeling of disease progression, such as type 2
diabetes, can enhance patient sub-phenotyping and inform effective and timely
interventions. However, the problem is challenging due to the need to learn
continuous-time dynamics of progression patterns based on irregular-time event
samples and patient heterogeneity (\eg different progression rates and
pathways). Existing mechanistic and data-driven methods either lack
adaptability to learn from real-world data or fail to capture complex
continuous-time dynamics on progression trajectories. To address these
limitations, we propose Temporally Detailed Hypergraph Neural Ordinary
Differential Equation (TD-HNODE), which represents disease progression on
clinically recognized trajectories as a temporally detailed hypergraph and
learns the continuous-time progression dynamics via a neural ODE framework.
TD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the
interdependency of disease complication markers within both intra- and
inter-progression trajectories. Experiments on two real-world clinical datasets
demonstrate that TD-HNODE outperforms multiple baselines in modeling the
progression of type 2 diabetes and related cardiovascular diseases.

</details>


### [266] [Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis](https://arxiv.org/abs/2510.17235)
*Chong Chen,Ze Liu,Lingfeng Bao,Yanlin Wang,Ting Chen,Daoyuan Wu,Jiachi Chen*

Main category: cs.AI

TL;DR: Coinvisor是一个基于强化学习的加密货币投资分析聊天机器人，通过多代理框架整合多种分析工具，提供实时、准确的投资洞察。


<details>
  <summary>Details</summary>
Motivation: 解决加密货币投资中现有方法的局限性：手动分析耗时且主观，数据聚合平台功能有限，LLM代理缺乏实时数据整合和多步推理能力。

Method: 采用基于强化学习的工具选择机制，支持多步规划和灵活整合多样化数据源，实现实时交互和动态内容自适应分析。

Result: 在工具编排方面，相比基础模型召回率提升40.7%，F1分数提升26.6%；用户研究显示高满意度（4.64/5），参与者更偏好Coinvisor而非通用LLM和现有加密平台（4.62/5）。

Conclusion: Coinvisor通过强化学习驱动的多代理框架，有效解决了加密货币投资分析中的关键挑战，提供了更准确、实时的投资洞察。

Abstract: The cryptocurrency market offers significant investment opportunities but
faces challenges including high volatility and fragmented information. Data
integration and analysis are essential for informed investment decisions.
Currently, investors use three main approaches: (1) Manual analysis across
various sources, which depends heavily on individual experience and is
time-consuming and prone to bias; (2) Data aggregation platforms-limited in
functionality and depth of analysis; (3) Large language model agents-based on
static pretrained models, lacking real-time data integration and multi-step
reasoning capabilities. To address these limitations, we present Coinvisor, a
reinforcement learning-based chatbot that provides comprehensive analytical
support for cryptocurrency investment through a multi-agent framework.
Coinvisor integrates diverse analytical capabilities through specialized tools.
Its key innovation is a reinforcement learning-based tool selection mechanism
that enables multi-step planning and flexible integration of diverse data
sources. This design supports real-time interaction and adaptive analysis of
dynamic content, delivering accurate and actionable investment insights. We
evaluated Coinvisor through automated benchmarks on tool calling accuracy and
user studies with 20 cryptocurrency investors using our interface. Results show
that Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base
model in tool orchestration. User studies show high satisfaction (4.64/5), with
participants preferring Coinvisor to both general LLMs and existing crypto
platforms (4.62/5).

</details>


### [267] [RubiSCoT: A Framework for AI-Supported Academic Assessment](https://arxiv.org/abs/2510.17309)
*Thorsten Fröhlich,Tim Schlippe*

Main category: cs.AI

TL;DR: 提出了一个名为RubiSCoT的AI支持框架，用于从提案到最终提交的论文评估，利用自然语言处理技术提供一致、可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统论文评估方法耗时且受评估者主观性影响，需要更高效、一致的评估方案。

Method: 使用先进的自然语言处理技术，包括大型语言模型、检索增强生成和结构化思维链提示，进行初步评估、多维评估、内容提取、基于量规的评分和详细报告。

Result: 设计并实现了RubiSCoT框架，展示了其在优化学术评估过程中的潜力。

Conclusion: RubiSCoT有潜力通过一致、可扩展和透明的评估来优化学术评估流程。

Abstract: The evaluation of academic theses is a cornerstone of higher education,
ensuring rigor and integrity. Traditional methods, though effective, are
time-consuming and subject to evaluator variability. This paper presents
RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from
proposal to final submission. Using advanced natural language processing
techniques, including large language models, retrieval-augmented generation,
and structured chain-of-thought prompting, RubiSCoT offers a consistent,
scalable solution. The framework includes preliminary assessments,
multidimensional assessments, content extraction, rubric-based scoring, and
detailed reporting. We present the design and implementation of RubiSCoT,
discussing its potential to optimize academic assessment processes through
consistent, scalable, and transparent evaluation.

</details>


### [268] [Graph Attention-Guided Search for Dense Multi-Agent Pathfinding](https://arxiv.org/abs/2510.17382)
*Rishabh Jain,Keisuke Okumura,Michael Amir,Amanda Prorok*

Main category: cs.AI

TL;DR: 提出LaGAT框架，将基于图注意力的神经网络策略MAGAT集成到搜索算法LaCAM中，用于解决密集多智能体路径规划问题，在密集场景中优于纯搜索和纯学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在密集多智能体路径规划问题上表现不佳，需要结合学习和搜索的优势来找到实时近优解。

Method: 使用增强的MAGAT架构，采用预训练-微调策略，结合死锁检测机制来处理不完美的神经引导。

Result: LaGAT在密集场景中超越了纯搜索和纯学习方法，证明了混合搜索的有效性。

Conclusion: 精心设计的混合搜索方法为解决紧密耦合的复杂多智能体协调问题提供了强大解决方案。

Abstract: Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)
problems in real-time remains challenging even for state-of-the-art planners.
To this end, we develop a hybrid framework that integrates a learned heuristic
derived from MAGAT, a neural MAPF policy with a graph attention scheme, into a
leading search-based algorithm, LaCAM. While prior work has explored
learning-guided search in MAPF, such methods have historically underperformed.
In contrast, our approach, termed LaGAT, outperforms both purely search-based
and purely learning-based methods in dense scenarios. This is achieved through
an enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of
interest, and a deadlock detection scheme to account for imperfect neural
guidance. Our results demonstrate that, when carefully designed, hybrid search
offers a powerful solution for tightly coupled, challenging multi-agent
coordination problems.

</details>


### [269] [Diverse Planning with Simulators via Linear Temporal Logic](https://arxiv.org/abs/2510.17418)
*Mustafa F. Abdelwahed,Alice Toniolo,Joan Espasa,Ian P. Gent*

Main category: cs.AI

TL;DR: 提出了FBI_LTL，一个专门为基于仿真的规划问题设计的多样化规划器，使用线性时序逻辑定义语义多样性标准，生成语义上不同的计划。


<details>
  <summary>Details</summary>
Motivation: 传统规划器只生成单一计划，可能无法满足代理偏好；现有多样化规划方法可能产生语法不同但语义相同的解决方案。

Method: FBI_LTL将基于LTL的多样性模型直接集成到搜索过程中，使用线性时序逻辑定义语义多样性标准。

Result: 在各种基准测试中，FBI_LTL相比基线方法能生成更多样化的计划。

Conclusion: 这项工作确立了在基于仿真的环境中进行语义引导多样化规划的可行性，为在传统基于模型方法失效的现实非符号领域开辟了新途径。

Abstract: Autonomous agents rely on automated planning algorithms to achieve their
objectives. Simulation-based planning offers a significant advantage over
declarative models in modelling complex environments. However, relying solely
on a planner that produces a single plan may not be practical, as the generated
plans may not always satisfy the agent's preferences. To address this
limitation, we introduce $\texttt{FBI}_\texttt{LTL}$, a diverse planner
explicitly designed for simulation-based planning problems.
$\texttt{FBI}_\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define
semantic diversity criteria, enabling agents to specify what constitutes
meaningfully different plans. By integrating these LTL-based diversity models
directly into the search process, $\texttt{FBI}_\texttt{LTL}$ ensures the
generation of semantically diverse plans, addressing a critical limitation of
existing diverse planning approaches that may produce syntactically different
but semantically identical solutions. Extensive evaluations on various
benchmarks consistently demonstrate that $\texttt{FBI}_\texttt{LTL}$ generates
more diverse plans compared to a baseline approach. This work establishes the
feasibility of semantically-guided diverse planning in simulation-based
environments, paving the way for innovative approaches in realistic,
non-symbolic domains where traditional model-based approaches fail.

</details>


### [270] [Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions](https://arxiv.org/abs/2510.17450)
*Johan Schubert,Farzad Kamrani,Tove Gustavi*

Main category: cs.AI

TL;DR: 提出了一种基于主动推理的路径规划方法，用于自主控制智能代理，通过构建证据地图来维持共同作战态势，平衡探索与利用。


<details>
  <summary>Details</summary>
Motivation: 解决智能代理在地理区域侦察中的探索与利用平衡问题，维持对态势的持续理解。

Method: 使用Dempster-Shafer理论和高斯传感器模型构建生成模型，通过变分自由能量计算指导代理移动，最小化自由能量。

Result: 开发了能够平衡区域搜索和目标跟踪的自主路径规划方法。

Conclusion: 该方法有效解决了地理区域侦察中的探索与利用权衡问题，为自主智能代理控制提供了新途径。

Abstract: We develop an active inference route-planning method for the autonomous
control of intelligent agents. The aim is to reconnoiter a geographical area to
maintain a common operational picture. To achieve this, we construct an
evidence map that reflects our current understanding of the situation,
incorporating both positive and "negative" sensor observations of possible
target objects collected over time, and diffusing the evidence across the map
as time progresses. The generative model of active inference uses
Dempster-Shafer theory and a Gaussian sensor model, which provides input to the
agent. The generative process employs a Bayesian approach to update a posterior
probability distribution. We calculate the variational free energy for all
positions within the area by assessing the divergence between a pignistic
probability distribution of the evidence map and a posterior probability
distribution of a target object based on the observations, including the level
of surprise associated with receiving new observations. Using the free energy,
we direct the agents' movements in a simulation by taking an incremental step
toward a position that minimizes the free energy. This approach addresses the
challenge of exploration and exploitation, allowing agents to balance searching
extensive areas of the geographical map while tracking identified target
objects.

</details>


### [271] [Label Indeterminacy in AI & Law](https://arxiv.org/abs/2510.17463)
*Cor Steging,Tadeusz Zbiegień*

Main category: cs.AI

TL;DR: 法律机器学习中标签不确定性问题：法律判决结果常受人为干预影响，导致标签不确定性，这会显著影响模型行为。


<details>
  <summary>Details</summary>
Motivation: 法律机器学习通常将过去案件结果视为真实标签，但法律结果常受和解、上诉等程序性干预影响，造成标签不确定性，需要对此进行考量。

Method: 在欧洲人权法院案件分类背景下，研究不同标签构建方式对模型行为的影响，探讨标签不确定性问题。

Result: 研究表明，训练过程中标签的构建方式会显著影响模型的行为表现。

Conclusion: 标签不确定性是AI与法律领域的重要问题，需要被纳入考量，因为它会影响模型的行为特征。

Abstract: Machine learning is increasingly used in the legal domain, where it typically
operates retrospectively by treating past case outcomes as ground truth.
However, legal outcomes are often shaped by human interventions that are not
captured in most machine learning approaches. A final decision may result from
a settlement, an appeal, or other procedural actions. This creates label
indeterminacy: the outcome could have been different if the intervention had or
had not taken place. We argue that legal machine learning applications need to
account for label indeterminacy. Methods exist that can impute these
indeterminate labels, but they are all grounded in unverifiable assumptions. In
the context of classifying cases from the European Court of Human Rights, we
show that the way that labels are constructed during training can significantly
affect model behaviour. We therefore position label indeterminacy as a relevant
concern in AI & Law and demonstrate how it can shape model behaviour.

</details>


### [272] [Reasoning Distillation and Structural Alignment for Improved Code Generation](https://arxiv.org/abs/2510.17598)
*Amir Jalilifard,Anderson de Rezende Rocha,Marcos Medeiros Raimundo*

Main category: cs.AI

TL;DR: 该研究提出了一种将大型语言模型的推理能力蒸馏到更小、更高效模型中的方法，通过结构感知损失优化使小模型能够理解问题与解决方案之间的结构对应关系。


<details>
  <summary>Details</summary>
Motivation: 代码生成需要准确理解意图并应用算法推理，而小型语言模型缺乏大型模型的复杂推理能力。研究旨在将大型模型的推理能力蒸馏到更小、更高效的模型中。

Method: 通过训练模型模拟大型语言模型的推理和问题解决能力，学习识别正确解决方案路径，并通过结构感知损失优化建立问题定义与潜在解决方案之间的结构对应关系。

Result: 实验结果显示，经过微调的模型在MBPP、MBPP Plus和HumanEval基准测试中，在pass@1、平均数据流和平均语法匹配指标上显著优于基线模型。

Conclusion: 通过廉价且易于实现的过程开发的微调模型能够超越令牌级生成，深入理解给定问题的解决方案结构，实现高效的代码生成能力。

Abstract: Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

</details>


### [273] [OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration](https://arxiv.org/abs/2510.17614)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: OG-Rank是一个低延迟的解码器重排序系统，通过结合池化首词评分和不确定性门控解释步骤，实现快速排序并在真正模糊时生成解释，保持可预测的延迟。


<details>
  <summary>Details</summary>
Motivation: 临床医生需要实时工作并能解释选择的排序系统，需要低延迟、基于解码器的重排序方法。

Method: 采用单解码器方法，结合池化首词评分信号和不确定性门控解释步骤，通过专注于困难案例的课程训练。

Result: 在遭遇范围订单选择任务中表现优异（快速路径：Recall@1~0.45，nDCG@20~0.625），门控激活时进一步提升（Recall@1~0.56，nDCG@20~0.699，门控率45%）。

Conclusion: OG-Rank提供了一个实用方案：默认快速排序，在需要时解释，这种模式适用于选择性生成能在可接受成本下提高准确性的决策任务。

Abstract: Clinicians need ranking systems that work in real time and still justify
their choices. Motivated by the need for a low-latency, decoder-based reranker,
we present OG-Rank, a single-decoder approach that pairs a pooled first-token
scoring signal with an uncertainty-gated explanation step. The model scores all
candidates in one pass and generates a brief, structured rationale only when
the list is genuinely ambiguous, keeping latency predictable. Trained with a
curriculum that concentrates effort on hard cases, OG-Rank delivers strong
effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,
nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,
nDCG@20~0.699 at a 45\% gate rate), while compact backbones show similar gains
under the same policy. Encoder baselines trail in both effectiveness and
flexibility. The result is a practical recipe: rank fast by default and explain
when it helps, a pattern that applies broadly to decision tasks where selective
generation buys accuracy at acceptable cost. The single-policy design
simplifies deployment and budget planning, and the curriculum principle (spend
more on the hard cases, less on the easy ones) readily transfers beyond
clinical order selection.

</details>


### [274] [LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena](https://arxiv.org/abs/2510.17638)
*Qingchuan Yang,Simon Mahns,Sida Li,Anri Gu,Jibang Wu,Haifeng Xu*

Main category: cs.AI

TL;DR: 本文系统评估了大型语言模型作为预测工具的能力，发现LLMs已具备显著的预测能力，但也存在事件回忆不准确、数据源误解等瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着基于互联网规模数据训练的大型语言模型的快速发展，探索利用LLMs预测现实世界未来事件的潜力，这一新兴范式被称为"LLM-as-a-Prophet"。

Method: 构建了Prophet Arena评估基准，持续收集实时预测任务，并将每个任务分解为不同的流水线阶段，以支持受控和大规模实验。

Result: 许多LLMs已展现出令人印象深刻的预测能力，表现为较小的校准误差、一致的预测置信度和有前景的市场回报。但也发现了关键瓶颈。

Conclusion: LLMs作为预测工具具有潜力，但需要解决事件回忆不准确、数据源误解以及在接近决策时信息聚合速度较慢等问题。

Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of
significant importance to societal systems such as finance and economics. With
the rapid advances of large language models (LLMs) trained on Internet-scale
data, it raises the promise of employing LLMs to forecast real-world future
events, an emerging paradigm we call "LLM-as-a-Prophet". This paper
systematically investigates such predictive intelligence of LLMs. To this end,
we build Prophet Arena, a general evaluation benchmark that continuously
collects live forecasting tasks and decomposes each task into distinct pipeline
stages, in order to support our controlled and large-scale experimentation. Our
comprehensive evaluation reveals that many LLMs already exhibit impressive
forecasting capabilities, reflected in, e.g., their small calibration errors,
consistent prediction confidence and promising market returns. However, we also
uncover key bottlenecks towards achieving superior predictive intelligence via
LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of
data sources and slower information aggregation compared to markets when
resolution nears.

</details>


### [275] [A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.17697)
*Anjie Liu,Jianhong Wang,Samuel Kaski,Jun Wang,Mengyue Yang*

Main category: cs.AI

TL;DR: 本文提出使用多智能体影响图(MAIDs)作为图形化框架来解决多智能体强化学习中的协调问题，设计了基于MAIDs的针对性干预范式，并通过因果推理技术实现单智能体干预以缓解全局指导的困难。


<details>
  <summary>Details</summary>
Motivation: 在大规模多智能体强化学习中，对整个系统进行全局人工指导不切实际，而现有的协调机制设计主要依赖经验研究，缺乏易用的研究工具。

Method: 采用多智能体影响图(MAIDs)作为分析框架，设计了针对性干预范式，并引入预策略干预(PSI)因果推理技术来实现该范式。通过最大化相应因果效应来达成复合期望结果。

Result: 实验证明了所提出的针对性干预方法的有效性，并验证了相关性图分析的结果。

Conclusion: MAIDs提供了一个有效的图形化框架来分析和设计多智能体交互范式，针对性干预能够缓解全局指导的困难，为多智能体协调提供了新的研究工具。

Abstract: Steering cooperative multi-agent reinforcement learning (MARL) towards
desired outcomes is challenging, particularly when the global guidance from a
human on the whole multi-agent system is impractical in a large-scale MARL. On
the other hand, designing mechanisms to coordinate agents most relies on
empirical studies, lacking a easy-to-use research tool. In this work, we employ
multi-agent influence diagrams (MAIDs) as a graphical framework to address the
above issues. First, we introduce interaction paradigms that leverage MAIDs to
analyze and visualize existing approaches in MARL. Then, we design a new
interaction paradigm based on MAIDs, referred to as targeted intervention that
is applied to only a single targeted agent, so the problem of global guidance
can be mitigated. In our implementation, we introduce a causal inference
technique-referred to as Pre-Strategy Intervention (PSI)-to realize the
targeted intervention paradigm. Since MAIDs can be regarded as a special class
of causal diagrams, a composite desired outcome that integrates the primary
task goal and an additional desired outcome can be achieved by maximizing the
corresponding causal effect through the PSI. Moreover, the bundled relevance
graph analysis of MAIDs provides a tool to identify whether an MARL learning
paradigm is workable under the design of an interaction paradigm. In
experiments, we demonstrate the effectiveness of our proposed targeted
intervention, and verify the result of relevance graph analysis.

</details>


### [276] [Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models](https://arxiv.org/abs/2510.17705)
*Dayan Pan,Zhaoyang Fu,Jingyuan Wang,Xiao Han,Yue Zhu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 提出Contextual Attention Modulation (CAM)机制和HyCAM框架，通过动态调制自注意力表示来增强任务特定特征同时保留通用知识，在多任务适应中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在多任务适应中平衡知识保留与任务特定专业化的问题，传统微调方法存在灾难性遗忘和资源消耗大的缺陷。

Method: 提出CAM机制动态调制自注意力模块表示，并构建HyCAM框架，结合共享的全参数CAM模块和多个轻量级专用CAM模块，采用动态路由策略进行自适应知识融合。

Result: 在问答、代码生成和逻辑推理等异构任务上的实验表明，该方法平均性能提升3.65%，显著优于现有方法。

Conclusion: CAM和HyCAM框架有效解决了大语言模型的多任务适应问题，在保持通用知识的同时实现了更好的任务专业化。

Abstract: Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

</details>


### [277] [Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs](https://arxiv.org/abs/2510.17771)
*Zhining Liu,Ziyi Chen,Hui Liu,Chen Luo,Xianfeng Tang,Suhang Wang,Joy Zeng,Zhenwei Dai,Zhan Shi,Tianxin Wei,Benoit Dumoulin,Hanghang Tong*

Main category: cs.AI

TL;DR: 该论文发现视觉语言模型(VLMs)存在"看到但不相信"的现象，即模型能感知到视觉证据但仍输出错误答案。作者提出了一种无需训练的推理时干预方法，通过选择性注意力掩码来突出深层证据区域，显著提高了多个VLM家族的准确性。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在多模态任务上表现良好，但它们仍然会在正确视觉证据存在的情况下失败。本研究旨在系统性地调查这些失败是由于未感知证据还是未有效利用证据导致的。

Method: 通过分析层间注意力动态，发现浅层主要关注文本，而深层稀疏但可靠地关注局部证据区域。基于此，引入了一种推理时干预方法，通过选择性注意力掩码来突出深层证据区域。

Result: 该方法无需训练，在多个VLM家族(包括LLaVA、Qwen、Gemma和InternVL)中一致提高了准确性。

Conclusion: VLMs在内部编码了可靠的证据但未充分利用，使这些信号显式化可以弥合感知与推理之间的差距，推进对VLM的诊断理解和可靠性。

Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [278] [Fitting an Escalier to a Curve](https://arxiv.org/abs/2510.16148)
*Sebastien Bossu,Andrew Papanicolaou,Nour El Hatto*

Main category: math.OC

TL;DR: 提出了一种在L²希尔伯特空间中拟合多步函数的两阶段优化方法，通过固定步长位置和可变步长位置的递归求解，实现了线性收敛速度。


<details>
  <summary>Details</summary>
Motivation: 解决在L²希尔伯特空间中拟合多步函数的问题，传统方法难以同时保证计算效率和全局最优解。

Method: 采用两阶段优化方法：第一阶段固定步长位置，使用线性最小二乘法获得闭式解；第二阶段允许步长位置变化，通过递归求解一阶条件。

Result: 在满足正则性条件下，随着步数n趋于无穷大，收敛速度为线性；数值实验显示该方法在速度和精度方面表现良好。

Conclusion: 该方法能够有效恢复全局最优拟合，通过扫描搜索实现的计算算法在速度和准确性方面具有优势。

Abstract: We analyze the problem of fitting a fonction en escalier or multi-step
function to a curve in L^2 Hilbert space. We propose a two-stage optimization
approach whereby the step positions are initially fixed, corresponding to a
classic linear least-squares problem with closed-form solution, and then are
allowed to vary, leading to first-order conditions that can be solved
recursively. We find that, subject to regularity conditions, the speed of
convergence is linear as the number of steps $n$ goes to infinity, and we
develop a simple algorithm to recover the global optimum fit. Our numerical
results based on a sweep search implementation show promising performance in
terms of speed and accuracy.

</details>


### [279] [Agent-Based Optimal Control for Image Processing](https://arxiv.org/abs/2510.16154)
*Alessio Oliviero,Simone Cacace,Giuseppe Visconti*

Main category: math.OC

TL;DR: 使用多智能体系统解决图像处理任务，将任务建模为最优控制问题，通过平衡颜色场的总变差和对原始图像的保真度来获得颜色聚类和分割。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体系统在经典图像处理任务（如颜色量化和分割）中的应用，寻求高效处理高维数据的方法。

Method: 将任务构建为最优控制问题，使用原始-对偶分裂和乘子法求解，通过CUDA并行实现数值实验。

Result: 数值实验证明了该方法的有效性，并展示了其在处理高维数据方面的潜力。

Conclusion: 多智能体系统框架为图像处理任务提供了一种有效的解决方案，特别适合处理高维数据。

Abstract: We investigate the use of multi-agent systems to solve classical image
processing tasks, such as colour quantization and segmentation. We frame the
task as an optimal control problem, where the objective is to steer the
multi-agent dynamics to obtain colour clusters that segment the image. To do
so, we balance the total variation of the colour field and fidelity to the
original image. The solution is obtained resorting to primal-dual splitting and
the method of multipliers. Numerical experiments, implemented in parallel with
CUDA, demonstrate the efficacy of the approach and its potential for
high-dimensional data.

</details>


### [280] [Conformal Prediction in The Loop: A Feedback-Based Uncertainty Model for Trajectory Optimization](https://arxiv.org/abs/2510.16376)
*Han Wang,Chao Ning*

Main category: math.OC

TL;DR: 提出了一种基于反馈的共形预测框架(Fb-CP)，用于具有联合风险约束的收缩时域轨迹优化，通过将已实现轨迹的信息反馈给CP来在线改进轨迹性能，同时保持覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要采用顺序方案，决策单向依赖预测区域，而决策信息无法反馈指导CP。需要一种能够将决策信息反馈给CP的框架。

Method: 开发了基于CP的后验风险计算方法，利用已实现轨迹调整后验允许风险，然后将其分配到未来时间以更新预测区域。还开发了决策聚焦的迭代风险分配算法。

Result: 提出的方法能够在线改进轨迹性能，同时保持预测区域的覆盖保证，确保可证明的安全性。基准实验证明了方法的有效性和优越性。

Conclusion: Fb-CP框架通过反馈机制实现了预测区域的在线调整和性能改进，同时维持覆盖保证，并能处理分布偏移问题。

Abstract: Conformal Prediction (CP) is a powerful statistical machine learning tool to
construct uncertainty sets with coverage guarantees, which has fueled its
extensive adoption in generating prediction regions for decision-making tasks,
e.g., Trajectory Optimization (TO) in uncertain environments. However, existing
methods predominantly employ a sequential scheme, where decisions rely
unidirectionally on the prediction regions, and consequently the information
from decision-making fails to be fed back to instruct CP. In this paper, we
propose a novel Feedback-Based CP (Fb-CP) framework for shrinking-horizon TO
with a joint risk constraint over the entire mission time. Specifically, a
CP-based posterior risk calculation method is developed by fully leveraging the
realized trajectories to adjust the posterior allowable risk, which is then
allocated to future times to update prediction regions. In this way, the
information in the realized trajectories is continuously fed back to the CP,
enabling attractive feedback-based adjustments of the prediction regions and a
provable online improvement in trajectory performance. Furthermore, we
theoretically prove that such adjustments consistently maintain the coverage
guarantees of the prediction regions, thereby ensuring provable safety.
Additionally, we develop a decision-focused iterative risk allocation algorithm
with theoretical convergence analysis for allocating the posterior allowable
risk which closely aligns with Fb-CP. Furthermore, we extend the proposed
method to handle distribution shift. The effectiveness and superiority of the
proposed method are demonstrated through benchmark experiments.

</details>


### [281] [A Simple First-Order Algorithm for Full-Rank Equality Constrained Optimization](https://arxiv.org/abs/2510.16390)
*Serge Gratton,Philippe L. Toint*

Main category: math.OC

TL;DR: 提出一种简单的一阶算法，用于求解具有确定性非线性等式约束的非线性优化问题。该算法自适应选择沿约束切面的步长或减少不可行性的步长，不使用价值函数或过滤器。


<details>
  <summary>Details</summary>
Motivation: 解决带约束的非线性优化问题，特别是在存在噪声的情况下，传统方法需要评估目标函数，这在噪声问题中可能不可靠。

Method: 算法自适应选择切面步长（基于AdaGrad方法）或减少不可行性步长，不评估目标函数值，仅使用梯度信息。

Result: 最坏情况评估复杂度分析显示全局收敛率为O(1/√k)，与无约束问题一阶方法的最佳已知速率匹配。数值实验表明性能与无约束一阶方法相当，在梯度噪声下可靠性稳定。

Conclusion: 该算法为带约束的噪声优化问题提供了一种简单有效的一阶方法，具有理论保证和良好的数值性能。

Abstract: A very simple first-order algorithm is proposed for solving nonlinear
optimization problems with deterministic nonlinear equality constraints. This
algorithm adaptively selects steps in the plane tangent to the constraints or
steps that reduce infeasibility, without using a merit function or filter. The
tangent steps are based on the AdaGrad method for unconstrained minimization.
The objective function is never evaluated by the algorithm, making it suitable
for noisy problems. Its worst-case evaluation complexity is analyzed, yielding
a global convergence rate in O(1/sqrt{k}), which matches the best known rate of
first-order methods for unconstrained problems. Numerical experiments are
presented suggesting that the performance of the algorithm is comparable to
that of first-order methods for unconstrained problems, and that its
reliability is remarkably stable in the presence of noise on the gradient.

</details>


### [282] [Charnes--Cooper transformation and fractional optimization with SOS-convex polynomials](https://arxiv.org/abs/2510.16400)
*Chengmiao Yang,Liguo Jiao,Jae Hyoung Lee*

Main category: math.OC

TL;DR: 提出基于Charnes-Cooper变换的参数自由方案，用于求解一类具有SOS凸多项式的分式规划问题


<details>
  <summary>Details</summary>
Motivation: 解决具有SOS凸多项式的分式规划问题，建立解的存在性、强对偶性和解提取的理论基础

Method: 基于Charnes-Cooper变换的参数自由方案，在特定条件下建立解存在性、强对偶性和解提取定理

Result: 建立了解决方案存在性、强对偶性和解提取的理论定理，并通过示例验证了所获结果

Conclusion: 该参数自由方案能有效求解SOS凸多项式分式规划问题，并具有理论保证

Abstract: This paper proposes a parameter-free scheme that is based on the
Charnes--Cooper transformation for solving a class of fractional programs with
SOS-convex polynomials. Under certain conditions, we establish theorems of
solution existence,strong duality and solution extraction. An illustrative
example is designed to show the obtained results.

</details>


### [283] [Frank-Wolfe Algorithms for (L0, L1)-smooth functions](https://arxiv.org/abs/2510.16468)
*A. A. Vyguzov,F. S. Stonyakin*

Main category: math.OC

TL;DR: 提出(L0,L1)-Frank-Wolfe算法及其自适应版本，用于(L0,L1)-光滑目标优化问题，相比经典Frank-Wolfe方法具有更优的理论收敛率和实际性能。


<details>
  <summary>Details</summary>
Motivation: 针对具有(L0,L1)-光滑特性的优化问题，现有Frank-Wolfe方法收敛率不够理想，需要开发更高效的算法。

Method: 开发了(L0,L1)-Frank-Wolfe算法和自适应版本，后者能动态调整光滑参数以提升性能和稳定性。

Result: 理论分析显示新算法具有更优的收敛率，数值实验验证了其相对于现有Frank-Wolfe变种的明显优势。

Conclusion: 提出的(L0,L1)-Frank-Wolfe算法在理论和实践上均优于经典方法，自适应版本进一步提升了性能。

Abstract: We propose a new version of the Frank-Wolfe method, called the (L0,
L1)-Frank-Wolfe algorithm, developed for optimization problems with (L0,
L1)-smooth objectives. We establish that this algorithm achieves superior
theoretical convergence rates compared to the classical Frank-Wolfe method. In
addition, we introduce a novel adaptive procedure, termed the Adaptive (L0,
L1)-Frank-Wolfe algorithm, which dynamically adjusts the smoothness parameters
to further improve performance and stability. Comprehensive numerical
experiments confirm the theoretical results and demonstrate the clear practical
advantages of both proposed algorithms over existing Frank-Wolfe variants.

</details>


### [284] [On the convergence rate of the boosted Difference-of-Convex Algorithm (DCA)](https://arxiv.org/abs/2510.16569)
*Hadi Abbaszadehpeivasti,Etienne de Klerk,Adrien Taylor*

Main category: math.OC

TL;DR: 本文研究了无约束DC算法（DCA）及其增强版本的最坏情况性能，证明了在某些DC分解类别中，增强DCA在理论上优于经典DCA。


<details>
  <summary>Details</summary>
Motivation: 虽然多个数值研究表明增强DCA优于经典DCA，但此前缺乏理论解释。本文旨在从理论上分析这两种算法的性能差异。

Method: 使用半定规划（SDP）性能估计技术，分析无约束DCA及其增强版本的最坏情况性能。

Result: 对于某些DC分解类别，增强DCA在理论上被证明比经典DCA具有更好的最坏情况性能。

Conclusion: 本文首次为增强DCA优于经典DCA的现象提供了理论依据，填补了该领域的研究空白。

Abstract: The difference-of-convex algorithm (DCA) is a well-established nonlinear
programming technique that solves successive convex optimization problems.
These sub-problems are obtained from the difference-of-convex~(DC)
decompositions of the objective and constraint functions. We investigate the
worst-case performance of the unconstrained DCA, with and without boosting,
where boosting simply performs an additional step in the direction generated by
the usual DCA method. We show that, for certain classes of DC decompositions,
the boosted DCA is provably better in the worst-case than the usual DCA. While
several numerical studies have reported that boosted DCA outperforms classical
DCA, a theoretical explanation for this behavior has, to the best of our
knowledge, not been given until now. Our proof technique relies on semidefinite
programming (SDP) performance estimation.

</details>


### [285] [Convexification of a Separable Function over a Polyhedral Ground Set](https://arxiv.org/abs/2510.16595)
*Santanu S. Dey,Burak Kocuk*

Main category: math.OC

TL;DR: 该论文研究了一个非凸集合的凸包或紧外逼近，该集合与可分离标准二次规划和基于势能的网络流问题相关。作者提出了多种锥规划松弛方法，并在特定情况下获得了凸包，给出了近似保证，并进行了计算实验比较。


<details>
  <summary>Details</summary>
Motivation: 研究该非凸集合的动机在于它与可分离标准二次规划和气体、水网络中的基于势能的网络流问题密切相关。这些问题的求解需要有效的凸松弛方法。

Method: 提出了幂锥、二阶锥和半定规划松弛，并通过Reformulation-Linearization技术和Reformulation-Perspectification技术进一步强化这些松弛。对于κ=2的情况，在低维设置下获得了凸包。

Result: 对于κ=2，在低维情况下获得了凸包。对于一般κ，给出了幂锥可表示松弛的近似保证，并证明当n→∞时，在均匀生成的线性目标下该松弛以概率1紧。通过计算实验比较了不同锥规划松弛的经验强度。

Conclusion: 提出的多种锥规划松弛方法能够有效逼近该非凸集合，其中幂锥松弛在渐近意义下是紧的，计算实验验证了不同松弛方法的性能差异。

Abstract: In this paper, we study the set $\mathcal{S}^\kappa = \{
(x,y)\in\mathcal{G}\times\mathbb{R}^n : y_j = x_j^\kappa , j=1,\dots,n\}$,
where $\kappa > 1$ and the ground set $\mathcal{G}$ is a nonempty polytope
contained in $[0,1]^n$. This nonconvex set is closely related to separable
standard quadratic programming and appears as a substructure in potential-based
network flow problems from gas and water networks. Our aim is to obtain the
convex hull of $\mathcal{S}^\kappa$ or its tight outer-approximation for the
special case when the ground set $\mathcal{G}$ is the standard simplex. We
propose power cone, second-order cone and semidefinite programming relaxations
for this purpose, which are further strengthened by the
Reformulation-Linearization Technique and the Reformulation-Perspectification
Technique. For $\kappa=2$, we obtain the convex hull of $\mathcal{S}^\kappa$ in
the low-dimensional setting. For general $\kappa$, we give approximation
guarantees for the power cone representable relaxation, the weakest relaxation
we consider. We prove that this weakest relaxation is tight with probability
one as $n\to\infty$ when a uniformly generated linear objective is optimized
over it. Finally, we provide the results of our extensive computational
experiments comparing the empirical strength of several conic programming
relaxations that we propose.

</details>


### [286] [A class of singular control problems with tipping points](https://arxiv.org/abs/2510.16599)
*Jean-Paul Décamps,Fabien Gensbittel,Thomas Mariotti,Stéphane Villeneuve*

Main category: math.OC

TL;DR: 研究涉及随机临界点的奇异随机控制问题，建立了与扩散过程及其运行最小值控制问题的联系，并应用于资源开采问题


<details>
  <summary>Details</summary>
Motivation: 研究临界点系统中当系统经历突然且不可逆变化时的随机控制问题，特别是当性能标准依赖于非停止时间的随机水平命中时间的情况

Method: 建立奇异随机控制问题与涉及扩散及其运行最小值的奇异控制问题之间的联系，证明验证定理

Result: 成功建立了两种控制问题价值之间的联系，并应用该方法显式解决了资源开采问题

Conclusion: 提出的方法能够有效处理涉及临界点的随机控制问题，为这类复杂系统的优化提供了理论框架

Abstract: Tipping points define situations where a system experiences sudden and
irreversible changes and are generally associated with a random level of the
system below which the changes materialize. In this paper, we study a singular
stochastic control problem in which the performance criterion depends on the
hitting time of a random level that is not a stopping time for the reference
filtration. We establish a connection between the value of the problem and the
value of a singular control problem involving a diffusion and its running
minimum. We prove a verification theorem and apply our results to explicitly
solve a resource extraction problem where the random evolution of the resource
changes when it crosses a tipping point.

</details>


### [287] [Adversarial Reinforcement Learning for Robust Control of Fixed-Wing Aircraft under Model Uncertainty](https://arxiv.org/abs/2510.16650)
*Dennis J. Marquis,Blake Wilhelm,Devaprakash Muniraj,Mazen Farhood*

Main category: math.OC

TL;DR: 提出一种基于强化学习的固定翼小型无人机路径跟踪控制器，通过鲁棒对抗强化学习框架训练，能够抵抗气动模型不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统控制器对气动模型不确定性敏感，需要开发能在各种不确定气动条件下保持鲁棒性的路径跟踪控制器。

Method: 使用鲁棒对抗强化学习框架，让对手扰动环境（气动模型）来暴露智能体（无人机）于苛刻场景，对手引入速率有界的气动模型系数扰动。

Result: 对抗训练比使用随机模型不确定性的控制器具有更好的鲁棒性，在高保真仿真中展示了在各种不确定气动条件下的准确鲁棒路径跟踪性能。

Conclusion: 该方法有效提高了固定翼无人机路径跟踪控制器的鲁棒性，能够应对气动模型不确定性。

Abstract: This paper presents a reinforcement learning-based path-following controller
for a fixed-wing small uncrewed aircraft system (sUAS) that is robust to
uncertainties in the aerodynamic model of the sUAS. The controller is trained
using the Robust Adversarial Reinforcement Learning framework, where an
adversary perturbs the environment (aerodynamic model) to expose the agent
(sUAS) to demanding scenarios. In our formulation, the adversary introduces
rate-bounded perturbations to the aerodynamic model coefficients. We
demonstrate that adversarial training improves robustness compared to
controllers trained using stochastic model uncertainty. The learned controller
is also benchmarked against a switched uncertain initial condition controller.
The effectiveness of the approach is validated through high-fidelity
simulations using a realistic six-degree-of-freedom fixed-wing aircraft model,
showing accurate and robust path-following performance under a variety of
uncertain aerodynamic conditions.

</details>


### [288] [Bregman Stochastic Proximal Point Algorithm with Variance Reduction](https://arxiv.org/abs/2510.16655)
*Cheik Traoré,Peter Ochs*

Main category: math.OC

TL;DR: 本文提出了Bregman随机近点算法(BSPPA)的方差缩减技术，结合了SAGA和SVRG方法，提高了收敛速度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 随机近点算法(SPPA)比SGD对步长设置更鲁棒，但仍因需要递减步长而导致收敛速度下降。同时，许多问题在非欧几里得几何中使用Bregman距离可以更高效解决。

Method: 将方差缩减技术与Bregman随机近点算法(BSPPA)结合，提出了类似SAGA和SVRG的方差缩减方法。

Result: 理论和数值结果表明，相比使用常数和递减步长的原始BSPPA，该方法具有更好的稳定性和收敛速度。

Conclusion: 提出的方差缩减技术显著提升了BSPPA的性能，并能以统一方式恢复Bregman SGD的相同方差缩减技术。

Abstract: Stochastic algorithms, especially stochastic gradient descent (SGD), have
proven to be the go-to methods in data science and machine learning. In recent
years, the stochastic proximal point algorithm (SPPA) emerged, and it was shown
to be more robust than SGD with respect to stepsize settings. However, SPPA
still suffers from a decreased convergence rate due to the need for vanishing
stepsizes, which is resolved by using variance reduction methods. In the
deterministic setting, there are many problems that can be solved more
efficiently when viewing them in a non-Euclidean geometry using Bregman
distances. This paper combines these two worlds and proposes variance reduction
techniques for the Bregman stochastic proximal point algorithm (BSPPA). As
special cases, we obtain SAGA- and SVRG-like variance reduction techniques for
BSPPA. Our theoretical and numerical results demonstrate improved stability and
convergence rates compared to the vanilla BSPPA with constant and vanishing
stepsizes, respectively. Our analysis, also, allow to recover the same variance
reduction techniques for Bregman SGD in a unified way.

</details>


### [289] [HNAG++: A Super-Fast Accelerated Gradient Method for Strongly Convex Optimization](https://arxiv.org/abs/2510.16680)
*Long Chen,Zeyi Xu*

Main category: math.OC

TL;DR: 提出了HNAG+和HNAG++两种优化强凸函数的方法，HNAG+达到信息论最优收敛率，HNAG++在Hölder连续Hessian条件下实现最快全局收敛率。


<details>
  <summary>Details</summary>
Motivation: 针对条件数κ较大的强凸函数优化问题，现有方法收敛速度不够快，需要开发更高效的优化算法。

Method: HNAG+和HNAG++两种加速梯度方法，HNAG++特别适用于Hölder连续Hessian的函数。

Result: HNAG+达到1-2/√κ的全局线性收敛率，HNAG++在Hölder连续Hessian条件下达到1-2√(2/κ)的渐近线性收敛率，数值实验显示HNAG++优于现有加速梯度方法。

Conclusion: HNAG+和HNAG++是高效的强凸函数优化算法，特别是HNAG++在非线性问题上表现优异，为大规模优化问题提供了新的解决方案。

Abstract: We introduce and analyze two methods, HNAG+ and HNAG++, for minimizing
strongly convex functions with large condition number kappa. For HNAG+, we
prove a global linear convergence rate of 1 - 2/sqrt(kappa), achieving the
information-theoretic optimal rate. For HNAG++, we establish a global
asymptotic linear rate of 1 - 2*sqrt(2/kappa) for functions with H\"older
continuous Hessians, representing the fastest known rate among globally
convergent first-order methods. Extensive numerical experiments on linear and
nonlinear problems show that HNAG++ consistently outperforms existing
accelerated gradient methods.

</details>


### [290] [Geometric Control Theory Over Networks: Minimal Node Cardinality Disturbance Decoupling Problems](https://arxiv.org/abs/2510.16689)
*Luca Claude Gino Lebon,Claudio Altafini*

Main category: math.OC

TL;DR: 该论文提出了一种在网络中解决扰动解耦问题的方法，通过选择最少数量的输入和输出节点来隔离和消除扰动节点对特定目标节点的影响。


<details>
  <summary>Details</summary>
Motivation: 研究如何在网络系统中有效隔离扰动影响，保护特定目标节点免受干扰，同时最小化所需的控制资源。

Method: 利用节点集合而非子空间的重新表述，使控制和条件不变性具有简单的图形解释。使用状态、输出和动态反馈，并通过最小割/最大流算法在多项式时间内计算最小输入输出节点解。

Result: 对于状态和动态反馈，能够精确计算最小输入和输出节点数量的解决方案，且计算效率高。

Conclusion: 该方法为网络扰动解耦问题提供了有效的解决方案，能够在多项式时间内找到最优的控制节点配置，具有重要的理论和应用价值。

Abstract: In this paper we show how to formulate and solve disturbance decoupling
problems over networks while choosing a minimal number of input and output
nodes. Feedback laws that isolate and eliminate the impact of disturbance nodes
on specific target nodes to be protected are provided using state, output, and
dynamical feedback. For that, we leverage the fact that when reformulated in
terms of sets of nodes rather than subspaces, the controlled and conditional
invariance properties admit a simple graphical interpretation. For state and
dynamical feedback, the minimal input and output cardinality solutions can be
computed exactly in polynomial time, via min-cut/max-flow algorithms.

</details>


### [291] [Local integral input-to-state stability for non-autonomous infinite-dimensional systems](https://arxiv.org/abs/2510.16725)
*Yongchun Bi,Panyu Deng,Jun Zheng,Guchuan Zhu*

Main category: math.OC

TL;DR: 本文建立了非线性微分方程的比较原理，并开发了Lyapunov分析工具，用于分析具有超线性增长的非线性非自治无限维系统的积分输入到状态稳定性(iISS)。


<details>
  <summary>Details</summary>
Motivation: 由于非线性系统满足超线性增长特性，给iISS分析带来困难，需要开发新的分析工具来处理这类非线性非自治无限维系统。

Method: 首先建立具有时变系数和超线性项的常微分方程的比较原理，然后利用这些原理在Banach空间框架下证明局部iISS Lyapunov定理，并在Hilbert空间框架下构建LiISS-Lyapunov泛函。

Result: 证明了局部iISS Lyapunov定理，提供了LiISS-Lyapunov泛函存在的充分条件，并通过两个例子验证了方法的有效性：一个是有限维系统，另一个是多维抛物方程。

Conclusion: 提出的Lyapunov方法能够有效分析具有超线性增长的非线性非自治无限维系统的积分输入到状态稳定性，数值实验验证了结果的正确性。

Abstract: In this paper, we prove comparison principles for nonlinear differential
equations with time-varying coefficients and develop Lyapunov analytical tools
for the integral input-to-state stability (iISS) analysis of nonlinear
non-autonomous infinite-dimensional systems, which involve nonlinearities
satisfying a superlinear growth, {bringing} difficulties to the iISS
{analysis.} Specifically, our approach starts by establishing several forms of
comparison principles for a wide range of ordinary differential equations
having time-varying coefficients and superlinear terms, paving the way to
conduct iISS assessment for general nonlinear non-autonomous
infinite-dimensional systems within the Lyapunov stability framework. Then, by
using the comparison principles, we prove a local {iISS} {(LiISS)} Lyapunov
theorem for the nonlinear non-autonomous infinite-dimensional systems in the
framework of Banach spaces. {Furthermore,} we provide sufficient conditions of
the existence of a local iISS Lyapunonv functional (LiISS-LF) and construct
LiISS-LFs for the systems in the framework of Hilbert spaces. Finally, we
preset two examples to illustrate the proposed {Lyapunov} method for the LiISS
analysis: one is to show how to obtain the LiISS of a nonlinear
finite-dimensional system with time-varying coefficients and superlinear terms
under linear state feedback control law while another one is to show how to
employ the interpolation inequalities to handle superliner terms and establish
the LiISS-LF for a class of multi-dimensional parabolic equations with
space-time-varying coefficients. To demonstrate the validity of the results,
numerical experiments are also conducted to verify the LiISS of these two
classes of systems.

</details>


### [292] [Equivalence of additive and parametric pinning control protocols for systems of weakly coupled oscillators](https://arxiv.org/abs/2510.16766)
*Riccardo Muolo,Yuzuru Kato*

Main category: math.OC

TL;DR: 本文证明了在弱耦合周期性振荡系统中，通过外部输入实现的加性钉扎控制与通过改变参数实现的参数钉扎控制是等效的。


<details>
  <summary>Details</summary>
Motivation: 控制网络非线性系统的行为是控制理论中的重要任务，特别是同步控制因其广泛应用而备受关注。本文旨在比较两种不同的钉扎控制方法。

Method: 使用相位约简技术分析弱耦合周期性振荡系统，并通过耦合Stuart-Landau振荡器的数值模拟进行验证。

Result: 数值模拟验证了在弱耦合周期性振荡系统中，加性钉扎控制和参数钉扎控制确实具有等效性。

Conclusion: 两种钉扎控制方法的等效性为在现实世界系统中进一步应用钉扎控制铺平了道路。

Abstract: Controlling the behavior of nonlinear systems on networks is a paramount task
in control theory, in particular the control of synchronization, given its vast
applicability. In this work, we focus on pinning control and we examine two
different approaches: the first, more common in engineering applications, where
the control is implemented through an external input (additive pinning); the
other, where the parameters of the pinned nodes are varied (parametric
pinning). By means of the phase reduction technique, we show that the two
pinning approaches are equivalent for weakly coupled systems exhibiting
periodic oscillatory behaviors. Through numerical simulations, we validate the
claim for a system of coupled Stuart--Landau oscillators. Our results pave the
way for further applications of pinning control in real-world systems.

</details>


### [293] [Method of Monotone Structural Evolution for control and state constrained optimal and control problems](https://arxiv.org/abs/2510.16768)
*Maciej Szymkat,Adam Korytowski*

Main category: math.OC

TL;DR: 提出了一种用于处理控制和状态约束的最优控制计算方法，通过节点和弧的生成与缩减序列来调整控制结构，重新定义决策空间


<details>
  <summary>Details</summary>
Motivation: 解决带有控制和状态约束的最优控制问题，需要有效的方法来处理约束条件并优化控制策略

Method: 使用控制结构调整序列，包括节点和弧的生成与缩减，这些调整不改变当前控制但重新定义决策空间

Result: 提供了几个示例来验证方法的有效性

Conclusion: 该方法能够有效处理控制和状态约束的最优控制问题，通过结构调整重新定义决策空间

Abstract: A method of optimal control computation is proposed for problems with control
and state constraints. It uses a sequence of control structure adjustments in
the form of generations and reductions of nodes and arcs, which do not change
the current control but redefine the decision space. Several examples are
given.

</details>


### [294] [A Surrogate Value Function Formulation for Bilevel Optimization](https://arxiv.org/abs/2510.16818)
*Mengwei Xu,Yu-Hong Dai,Xin-Wei Liu,Meiqi Ma*

Main category: math.OC

TL;DR: 提出了一种替代价值函数公式，用显式替代函数替换难以处理的价值函数，保持双层次优化的层次结构，克服KKT公式的局限性，并在非凸设置中表现出鲁棒性和高数值精度。


<details>
  <summary>Details</summary>
Motivation: 传统价值函数公式虽然捕捉了双层次优化的层次特性，但其隐式和非光滑特性带来了分析和计算困难。KKT公式将下层平稳点嵌入上层可行区域，模糊了层次依赖关系。

Method: 引入替代价值函数公式，从下层平稳条件推导显式替代函数，通过支配约束保持层次结构，应用平滑障碍增广拉格朗日方法处理互补约束。

Result: 证明了与原始双层次问题的等价性，揭示了标准约束条件的失效，展示了强平稳性，实验验证了在非凸设置中的鲁棒性和高精度，特别是在KKT模型失败的Mirrlees问题中。

Conclusion: 替代价值函数公式提供了一种有效处理双层次优化的新方法，克服了传统方法的局限性，在理论和数值上都表现出优越性能。

Abstract: The value function formulation captures the hierarchical nature of bilevel
optimization through the optimal value function of the lower level problem, yet
its implicit and nonsmooth characteristics pose significant analytical and
computational difficulties. We introduce a surrogate value function formulation
that replaces the intractable value function with an explicit surrogate derived
from lower level stationarity conditions. This surrogate formulation preserves
the essential idea of the classical value function model but fundamentally
departs from Karush Kuhn Tucker (KKT) formulations, which embed lower level
stationary points into the upper level feasible region and obscure the
hierarchical dependence. Instead, it enforces the hierarchy through a dominance
constraint that remains valid even when lower level constraint qualifications
fail at the solution. We establish equivalence with the original bilevel
problem, reveal the failure of standard constraint qualifications, and show
that its strong stationarity implies that of KKT models. To handle the
complementarity constraints in the surrogate formulation, we apply a smoothing
barrier augmented Lagrangian method and prove its convergence to solutions and
Clarke stationary points. Extensive experiments demonstrate the robustness and
high numerical precision of this formulation, especially in nonconvex settings,
including the classical Mirrlees problem where KKT models fail.

</details>


### [295] [The Augmented Lagrangian Methods: Overview and Recent Advances](https://arxiv.org/abs/2510.16827)
*Kangkang Deng,Rui Wang,Zhenyuan Zhu,Junyu Zhang,Zaiwen Wen*

Main category: math.OC

TL;DR: 本文对增广拉格朗日方法(ALM)进行了全面综述，涵盖了其在非线性规划、凸和非凸复合规划等优化问题中的理论基础、应用实例和最新进展。


<details>
  <summary>Details</summary>
Motivation: 大规模约束优化在现代科学、工程和工业计算中至关重要，但涉及复杂系统和众多变量约束，需要统一的增广拉格朗日函数构建方法。

Method: 基于Hestenes-Powell-Rockafellar增广拉格朗日函数，构建统一的增广拉格朗日方法框架，包括处理非凸约束、确保全局收敛到一阶和二阶稳定点的技术。

Result: ALM能够处理非凸约束问题，在非光滑凸问题中保持局部线性收敛率，并在整数规划等挑战性问题中取得进展。

Conclusion: 该综述全面阐述了ALM的优势和局限性，探索了不同变体以提升收敛性和计算性能，并展示了在各领域的实际应用。

Abstract: Large-scale constrained optimization is pivotal in modern scientific,
engineering, and industrial computation, often involving complex systems with
numerous variables and constraints. This paper provides a unified and
comprehensive perspective on constructing augmented Lagrangian functions (based
on Hestenes-Powell-Rockafellar augmented Lagrangian) for various optimization
problems, including nonlinear programming and convex and nonconvex composite
programming. We present the augmented Lagrangian method (ALM), covering its
theoretical foundations in both convex and nonconvex cases, and discuss several
successful examples and applications. Recent advancements have extended ALM's
capabilities to handle nonconvex constraints and ensure global convergence to
first and second-order stationary points. For nonsmooth convex problems, ALM
utilizes proximal operations, preserving desirable properties such as locally
linear convergence rates. Furthermore, recent progress has refined the
complexity analysis for ALM and tackled challenging integer programming
instances. This review aims to offer a thorough understanding of ALM's benefits
and limitations, exploring different ALM variants designed to enhance
convergence and computational performance. We also illustrate effective
algorithms for ALM subproblems across different types of optimization problems
and highlight practical implementations in several fields.

</details>


### [296] [Solving nonconvex optimization problems via a second order dynamical system with unbounded damping](https://arxiv.org/abs/2510.16864)
*Szilárd Csaba László*

Main category: math.OC

TL;DR: 研究了带变系数的二阶动力系统在非凸函数最小化问题中的应用，证明了轨迹收敛到临界点，并获得了超线性收敛速率。


<details>
  <summary>Details</summary>
Motivation: 针对非凸函数最小化问题，研究二阶动力系统的收敛性，旨在改进现有文献中的收敛速率。

Method: 使用带变系数的二阶动力系统，要求目标函数的正则化满足Kurdyka-Łojasiewicz性质。

Result: 证明了轨迹收敛到临界点，并获得了超线性收敛速率，显著优于文献中的线性速率。

Conclusion: 所提出的无界阻尼二阶动力系统在非凸优化中实现了超线性收敛，显著改进了现有结果。

Abstract: In this paper we study a second order dynamical system with variable
coefficients in connection to the minimization problem of a smooth nonconvex
function. The convergence of the trajectories generated by the dynamical system
to a critical point of the objective function is assured, provided a
regularization of the objective function satisfies the Kurdyka-{\L}ojasiewicz
property. We also provide convergence rates for the trajectories generated by
the dynamical system, formulated in terms of the {\L}ojasiewicz exponent, and
we show that the unbounded damping considered in our dynamical system
significantly improves the convergence rates known so far in the literature,
that is, instead of linear rates we obtain superlinear rates.

</details>


### [297] [Distributionally Robust Nash Equilibria via Variational Inequalities](https://arxiv.org/abs/2510.17024)
*Zeinab Alizadeh,Azadeh Farsi,Afrooz Jalilzadeh*

Main category: math.OC

TL;DR: 该论文研究分布鲁棒纳什均衡问题，提出变分不等式重构框架和收敛性保证的梯度下降-上升算法


<details>
  <summary>Details</summary>
Motivation: 纳什均衡及其鲁棒版本在博弈论中具有基础重要性，在经济学、工程和机器学习中应用广泛。传统方法难以处理高维和非光滑目标函数下的分布鲁棒纳什均衡计算问题。

Method: 将分布鲁棒纳什均衡问题重构为变分不等式问题，提出梯度下降-上升类型算法，具有收敛性保证，能够有效处理高维和非光滑目标函数。

Result: 建立了统一的变分不等式分析框架，开发了能够解决高维非光滑目标计算挑战的算法，并提供了收敛性理论保证。

Conclusion: 提出的变分不等式重构和梯度下降-上升算法为分布鲁棒纳什均衡问题提供了有效的计算解决方案，特别适用于高维和非光滑目标场景。

Abstract: Nash Equilibrium and its robust counterpart, Distributionally Robust Nash
Equilibrium (DRNE), are fundamental problems in game theory with applications
in economics, engineering, and machine learning. This paper addresses the
problem of DRNE, where multiple players engage in a noncooperative game under
uncertainty. Each player aims to minimize their objective against the
worst-case distribution within an ambiguity set, resulting in a minimax
structure. We reformulate the DRNE problem as a Variational Inequality (VI)
problem, providing a unified framework for analysis and algorithm development.
We propose a gradient descent-ascent type algorithm with convergence guarantee
that effectively addresses the computational challenges of high-dimensional and
nonsmooth objectives.

</details>


### [298] [Optimal Trajectories for Optimal Transport in Nonuniform Environments](https://arxiv.org/abs/2510.17170)
*Luca Dieci,Daniyar Omarov*

Main category: math.OC

TL;DR: 提出了在非均匀环境中解决离散最优传输问题的方法，通过求解欧拉-拉格朗日方程构建成本矩阵，并提供了最优解的充分条件验证。


<details>
  <summary>Details</summary>
Motivation: 解决非均匀环境中的离散最优传输问题，关键在于构建成本矩阵，这需要找到两点之间的最优路径。

Method: 制定并求解相关的欧拉-拉格朗日方程来构建成本矩阵，提出了新的算法来解决该问题。

Result: 提供了验证欧拉-拉格朗日方程解最优性的充分条件，并通过数值示例展示了算法的性能。

Conclusion: 成功解决了非均匀环境中的离散最优传输问题，提出的算法在数值实验中表现良好。

Abstract: In this work, we solve a discrete optimal transport problem in a nonuniform
environment. The key challenge is to form the cost matrix, which requires
finding the optimal path between two points, and for this task we formulate and
solve the associated Euler-Lagrange equations. A main theoretical result of
ours is to provide verifiable sufficient conditions of optimality of the
solution of the Euler-Lagrange equation. We propose new algorithms to solve the
problem, and illustrate our results and performance of the algorithms on
several numerical examples.

</details>


### [299] [Periodic limit for non-autonomous Lagrangian systems and applications to a Kuramoto type model](https://arxiv.org/abs/2510.17242)
*Veronica Danesi,Cristian Mendico,Xuan Tao,Kaizhi Wang*

Main category: math.OC

TL;DR: 研究非自治拉格朗日系统的渐近性质，证明当Tonelli拉格朗日量收敛于时间周期函数时，Lax-Oleinik半群收敛于方程的周期解，且其梯度图在Hausdorff距离下收敛于极限周期函数的梯度图。


<details>
  <summary>Details</summary>
Motivation: 探索非自治拉格朗日系统在Tonelli拉格朗日量收敛于时间周期函数时的渐近行为，为理解此类系统的长期动力学特性提供理论框架。

Method: 构建合适的Lax-Oleinik半群，分析其收敛性质，证明该半群收敛于方程的周期解，并研究其梯度图的Hausdorff收敛性。

Result: 成功证明了Lax-Oleinik半群收敛于周期解，其梯度图在Hausdorff距离下收敛于极限周期函数的梯度图，并将结果应用于Kuramoto型模型，证明了由哈密顿-雅可比方程极限周期解梯度图给出的不变环面的存在性。

Conclusion: 该研究为非自治拉格朗日系统的渐近分析提供了有效工具，特别在Tonelli拉格朗日量收敛于周期函数的情况下，系统表现出良好的收敛性质，这一结果在Kuramoto型模型等应用中具有重要价值。

Abstract: This paper explores the asymptotic properties of non-autonomous Lagrangian
systems, assuming that the associated Tonelli Lagrangian converges to a
time-periodic function. Specifically, given a continuous initial condition, we
provide a suitable construction of a Lax-Oleinik semigroup such that it
converges toward a periodic solution of the equation. Moreover, the graph of
its gradient converges as time tends to infinity to the graph of the gradient
of the periodic limit function with respect to the Hausdorff distance. Finally,
we apply this result to a Kuramoto-type model, proving the existence of an
invariant torus given by the graph of the gradient of the limiting periodic
solution of the Hamilton-Jacobi equation.

</details>


### [300] [A polynomial-based QCQP solver for encrypted optimization](https://arxiv.org/abs/2510.17294)
*Sebastian Schlor,Andrea Iannelli,Junsoo Kim,Hyungbo Shim,Frank Allgöwer*

Main category: math.OC

TL;DR: 提出了一种使用加法和乘法解决二次约束优化问题的新方法，该方法与同态加密方案兼容，可在私有数据上求解约束优化问题。


<details>
  <summary>Details</summary>
Motivation: 为了在私有数据上解决约束优化问题，需要开发与同态加密方案兼容的算法，因为同态加密仅支持加法和乘法操作。

Method: 引入一系列递增次数的多项式罚函数，这些函数在可行集边界足够陡峭。将罚函数添加到原始成本函数中，创建一系列无约束优化问题，其最小化器始终位于可行集内并收敛到约束问题的最小化器。使用梯度下降法生成迭代序列。

Result: 证明了迭代收敛到原始问题的最小化器，且可行集在迭代下是正不变的。方法在一个加密问题（寻找两个数中的较小值）上进行了演示，并讨论了加密实现的可能性。

Conclusion: 该方法能够有效解决二次约束优化问题，且与同态加密方案兼容，为在私有数据上执行约束优化提供了可行的解决方案。

Abstract: In this paper, we present a novel method for solving a class of quadratically
constrained quadratic optimization problems using only additions and
multiplications. This approach enables solving constrained optimization
problems on private data since the operations involved are compatible with the
capabilities of homomorphic encryption schemes. To solve the constrained
optimization problem, a sequence of polynomial penalty functions of increasing
degree is introduced, which are sufficiently steep at the boundary of the
feasible set. Adding the penalty function to the original cost function creates
a sequence of unconstrained optimization problems whose minimizer always lies
in the admissible set and converges to the minimizer of the constrained
problem. A gradient descent method is used to generate a sequence of iterates
associated with these problems. For the algorithm, it is shown that the iterate
converges to a minimizer of the original problem, and the feasible set is
positively invariant under the iteration. Finally, the method is demonstrated
on an illustrative cryptographic problem, finding the smaller value of two
numbers, and the encrypted implementability is discussed.

</details>


### [301] [Assessing the Quality of a Set of Basis Functions for Inverse Optimal Control via Projection onto Global Minimizers](https://arxiv.org/abs/2510.17339)
*Filip Bečanović,Jared Miller,Vincent Bonnet,Kosta Jovanović,Samer Mohammed*

Main category: math.OC

TL;DR: 提出一种新的逆优化方法，通过测量测试点与凸基函数组合生成的全局最优解集之间的距离来评估基函数的表达能力，解决了传统方法中基函数与测试点一致性假设不成立的问题。


<details>
  <summary>Details</summary>
Motivation: 传统逆优化方法假设真实成本函数是凸基函数的凸组合，且基函数与测试点一致，但实际应用中这种一致性假设往往不成立。需要一种方法来评估基函数对测试点的表达能力。

Method: 使用测试点与凸基函数组合生成的全局最优解集之间的距离作为基函数表达能力的度量。在凸二次设置中，通过双层梯度下降和增强线性矩阵不等式分别实现最小距离的上下界计算。

Result: 提出了全局最优解集的概念并探索了其在无约束和约束设置下的性质。建立了最小距离的上下界计算方法，并扩展到最大可表示基函数、非凸基函数（局部最小值）以及多项式优化技术。

Conclusion: 该方法为逆优化提供了一种新的框架，能够有效评估基函数的表达能力，当最小距离较大时表明基函数集无效，解决了传统方法中一致性假设不成立的问题。

Abstract: Inverse optimization (Inverse optimal control) is the task of imputing a cost
function such that given test points (trajectories) are (nearly) optimal with
respect to the discovered cost. Prior methods in inverse optimization assume
that the true cost is a convex combination of a set of convex basis functions
and that this basis is consistent with the test points. However, the
consistency assumption is not always justified, as in many applications the
principles by which the data is generated are not well understood. This work
proposes using the distance between a test point and the set of global optima
generated by the convex combinations of the convex basis functions as a
measurement for the expressive quality of the basis with respect to the test
point. A large minimal distance invalidates the set of basis functions. The
concept of a set of global optima is introduced and its properties are explored
in unconstrained and constrained settings. Upper and lower bounds for the
minimum distance in the convex quadratic setting are implemented by bi-level
gradient descent and an enriched linear matrix inequality respectively.
Extensions to this framework include max-representable basis functions,
nonconvex basis functions (local minima), and applying polynomial optimization
techniques.

</details>


### [302] [A Finite-Difference Trust-Region Method for Convexly Constrained Smooth Optimization](https://arxiv.org/abs/2510.17366)
*Dânâ Davar,Geovani Nunes Grapiglia*

Main category: math.OC

TL;DR: 提出一种基于有限差分梯度近似的无导数信赖域方法，用于解决具有凸约束的光滑优化问题。该方法无需计算近似平稳性度量，并在非凸、凸和Polyak-Lojasiewicz函数上建立了不同的复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法需要梯度信息，但在许多实际问题中梯度难以计算或不可得。无导数优化方法通过函数值来近似梯度，但现有方法在复杂度分析和实际效率方面仍有改进空间。

Method: 基于有限差分梯度近似的信赖域方法，处理凸约束优化问题。该方法避免计算近似平稳性度量，简化了算法实现。

Result: 建立了三种复杂度界限：非凸问题为O(n(L/σϵ)^{-2})，凸问题为O(n(L/σϵ)^{-1})，Polyak-Lojasiewicz函数为O(n log((L/σϵ)^{-1}))。数值实验表明该方法在基准问题和模型拟合应用中优于现有无导数求解器。

Conclusion: 所提出的无导数信赖域方法在理论和实际性能上均表现优异，为无导数优化提供了一种高效且理论保证的解决方案，特别适用于梯度信息不可得或计算成本高的优化问题。

Abstract: We propose a derivative-free trust-region method based on finite-difference
gradient approximations for smooth optimization problems with convex
constraints. The proposed method does not require computing an approximate
stationarity measure. For nonconvex problems, we establish a worst-case
complexity bound of
$\mathcal{O}\!\left(n\left(\tfrac{L}{\sigma}\epsilon\right)^{-2}\right)$
function evaluations for the method to reach an
$\left(\tfrac{L}{\sigma}\epsilon\right)$-approximate stationary point, where
$n$ is the number of variables, $L$ is the Lipschitz constant of the gradient,
and $\sigma$ is a user-defined estimate of $L$. If the objective function is
convex, the complexity to reduce the functional residual below
$(L/\sigma)\epsilon$ is shown to be of
$\mathcal{O}\!\left(n\left(\tfrac{L}{\sigma}\epsilon\right)^{-1}\right)$
function evaluations, while for Polyak-Lojasiewicz functions on unconstrained
domains, the bound further improves to
$\mathcal{O}\left(n\log\left(\left(\frac{L}{\sigma}\epsilon\right)^{-1}\right)\right)$.
Numerical experiments on benchmark problems and a model-fitting application
demonstrate the method's efficiency relative to state-of-the-art
derivative-free solvers for both unconstrained and bound-constrained problems.

</details>


### [303] [A condensing approach for linear-quadratic optimization with geometric constraints](https://arxiv.org/abs/2510.17465)
*Alberto De Marchi*

Main category: math.OC

TL;DR: 提出了一种处理包含逻辑条件和基数约束的凸二次优化问题的方法，结合增广拉格朗日框架和结构利用的子问题重构技术。


<details>
  <summary>Details</summary>
Motivation: 在信号处理、自动控制和决策等领域，凸二次成本和多面体约束的优化问题非常普遍。需要处理能够编码逻辑条件和基数约束的扩展问题类别，包括部分约束为非凸且复杂但可以计算到非凸集投影的情况。

Method: 将增广拉格朗日框架与求解器无关的结构利用子问题重构相结合。通过提出的压缩技术改进计算性能。

Result: 增广拉格朗日框架保证了收敛性，而提出的压缩技术显著提高了计算性能。

Conclusion: 该方法能够有效处理包含逻辑条件和基数约束的凸二次优化问题，在保证收敛的同时显著提升计算效率。

Abstract: Optimization problems with convex quadratic cost and polyhedral constraints
are ubiquitous in signal processing, automatic control and decision-making. We
consider here an enlarged problem class that allows to encode logical
conditions and cardinality constraints, among others. In particular, we cover
also situations where parts of the constraints are nonconvex and possibly
complicated, but it is practical to compute projections onto this nonconvex
set. Our approach combines the augmented Lagrangian framework with a
solver-agnostic structure-exploiting subproblem reformulation. While
convergence guarantees follow from the former, the proposed condensing
technique leads to significant improvements in computational performance.

</details>


### [304] [Towards Optimal Control and Algorithmic Structure of Decompression Schedules](https://arxiv.org/abs/2510.17551)
*Benjamin Marsh*

Main category: math.OC

TL;DR: 本文形式化地将减压规划建模为具有气体可行性窗口、仿射上限和凸惩罚的最优控制问题，证明了存在性、单调无重降结构和bang-bang上升特性，并给出了伪多项式动态规划和标签设置算法。


<details>
  <summary>Details</summary>
Motivation: 为减压规划提供严格的理论基础，特别是在混合气体可行性窗口条件下，这是首次在混合气体场景下形式化证明存在性和bang-bang结构。

Method: 将减压规划建模为最优控制问题，使用气体可行性窗口(ppO₂, END)、仿射上限和凸惩罚函数，开发了伪多项式动态规划和标签设置算法。

Result: 证明了最优解的存在性、单调无重降结构和bang-bang上升特性，建立了停留时间的KKT条件，算法具有先验误差界，在线值函数具有Lipschitz正则性。

Conclusion: 有效前沿是连续且通常非凸的，为混合气体减压规划提供了首个形式化存在性和bang-bang结构证明，为实际应用奠定了理论基础。

Abstract: We formalise decompression planning as an optimal control problem with gas
feasibility windows (ppO$_2$, END), affine ceilings, and convex penalties in
normalised oversaturation. We prove existence, a monotone no re-descent
structure and bang-bang ascents under a mild monotonicity assumption on inert
fraction, and establish dwell time KKT conditions. We give pseudo-polynomial DP
and label-setting algorithms with a priori error bounds, derive Lipschitz
regularity of the online value function, and discuss multi-species extensions.
The efficient frontier is continuous and generally nonconvex. We provide the
first formal existence and bang-bang structure proof under mixed gas
feasibility windows.

</details>


### [305] [An Inexact General Descent Method with Applications in Differential Equation-Constrained Optimization](https://arxiv.org/abs/2510.17581)
*Humberto Gimenes Macedo,Luís Felipe Bueno*

Main category: math.OC

TL;DR: 提出了一种不精确通用下降框架，用于处理梯度评估不精确的优化问题，特别适用于微分方程约束优化。该框架在两种步长机制下建立了全局收敛理论，并通过不精确梯度下降和不精确BFGS方法实现，在ODE反问题和Laplace反问题中展示了自适应不精确梯度能显著减少优化时间。


<details>
  <summary>Details</summary>
Motivation: 在许多应用中，梯度评估本质上是近似的，特别是在微分方程约束优化中，离散伴随梯度依赖于迭代求解器。这促使开发在非精确一阶信息下仍可靠的优化方法，其中自适应评估策略（早期使用粗梯度，在极小值点附近细化）尤为重要。

Method: 提出了不精确通用下降框架，建立了在两种步长机制下的全局收敛理论：有界步长要求计算梯度的误差容限与其范数成比例，而递减步长要求容限序列可求和。通过不精确梯度下降和不精确BFGS类方法实现该框架。

Result: 在二阶ODE反问题和二维Laplace反问题中使用自适应精度的离散伴随梯度进行测试。结果显示，与固定紧容限相比，自适应不精确梯度始终减少了优化时间，而结合曲率信息进一步提高了整体效率。

Conclusion: 提出的不精确通用下降框架为处理梯度评估不精确的优化问题提供了理论基础和实用方法，特别是在微分方程约束优化中，自适应梯度策略能有效平衡精度和计算成本，提高优化效率。

Abstract: In many applications, gradient evaluations are inherently approximate,
motivating the development of optimization methods that remain reliable under
inexact first-order information. A common strategy in this context is adaptive
evaluation, whereby coarse gradients are used in early iterations and refined
near a minimizer. This is particularly relevant in differential
equation-constrained optimization (DECO), where discrete adjoint gradients
depend on iterative solvers. Motivated by DECO applications, we propose an
inexact general descent framework and establish its global convergence theory
under two step-size regimes. For bounded step sizes, the analysis assumes that
the error tolerance in the computed gradient is proportional to its norm,
whereas for diminishing step sizes, the tolerance sequence is required to be
summable. The framework is implemented through inexact gradient descent and an
inexact BFGS-like method, whose performance is demonstrated on a second-order
ODE inverse problem and a two-dimensional Laplace inverse problem using
discrete adjoint gradients with adaptive accuracy. Across these examples,
adaptive inexact gradients consistently reduced optimization time relative to
fixed tight tolerances, while incorporating curvature information further
improved overall efficiency.

</details>


### [306] [A brief note on approximate optimization of submodular functions](https://arxiv.org/abs/2510.17610)
*Alen Alexanderian*

Main category: math.OC

TL;DR: 讨论贪婪方法及其更高效变体用于近似最大化单调子模函数


<details>
  <summary>Details</summary>
Motivation: 提高单调子模函数近似最大化的效率

Method: 贪婪方法及其变体

Result: 提出了更高效的近似算法

Conclusion: 贪婪方法及其变体是有效的近似优化工具

Abstract: We briefly discuss the greedy method and a couple of its more efficient
variants for approximately maximizing monotone submodular functions.

</details>


### [307] [Counterfactual Explanations for Integer Optimization Problems](https://arxiv.org/abs/2510.17624)
*Felix Engelhardt,Jannis Kurtz,Ş. İlker Birbil,Ted Ralphs*

Main category: math.OC

TL;DR: 该论文研究了整数优化问题的反事实解释，证明了构建反事实解释的复杂性，并提出了几种可处理特殊情况的高效算法。


<details>
  <summary>Details</summary>
Motivation: 反事实解释为解释决策提供了一种人类可理解的方式，但在整数优化问题中研究较少，特别是对于一般整数优化问题的反事实解释研究存在空白。

Method: 首先证明了构建反事实解释的复杂性，然后针对几种最易处理的特殊情况提出了解决方案算法：可变目标参数、单一可变约束、可变右端项以及所有输入参数可修改的情况。

Result: 在经典背包问题实例上评估了所提出的方法，重点关注可变约束参数的情况。结果显示，对于涉及最多40个项目的小型实例，该方法能够在几小时内找到最优反事实解释。

Conclusion: 该研究填补了整数优化问题反事实解释的研究空白，提出了有效的算法，并在实际问题上验证了其可行性。

Abstract: Counterfactual explanations (CEs) offer a human-understandable way to explain
decisions by identifying specific changes to the input parameters of a base or
present model that would lead to a desired change in the outcome. For
optimization models, CEs have primarily been studied in limited contexts and
little research has been done on CEs for general integer optimization problems.
In this work, we address this gap. We first show that the general problem of
constructing a CE is $\Sigma_2^p$-complete even for binary integer programs
with just a single mutable constraint. Second, we propose solution algorithms
for several of the most tractable special cases: (i) mutable objective
parameters, (ii) a single mutable constraint, (iii) mutable right-hand-side,
and (iv) all input parameters can be modified. We evaluate our approach using
classical knapsack problem instances, focusing on cases with mutable constraint
parameters. Our results show that our methods are capable of finding optimal
CEs for small instances involving up to 40 items within a few hours.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [308] [Feature-driven reinforcement learning for photovoltaic in continuous intraday trading](https://arxiv.org/abs/2510.16021)
*Arega Getaneh Abate,Xiufeng Liu,Ruyu Liu,Xiaobing Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于特征驱动强化学习的光伏日内交易策略，使用PPO算法学习投标策略，在历史市场数据上训练并验证，优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 光伏运营商面临发电量和短期电价的不确定性，日内市场允许实时调整头寸以改善收益和减少不平衡成本。

Method: 将问题建模为马尔可夫决策过程，使用PPO算法训练可解释的线性策略，整合数据驱动特征到状态空间中。

Result: 策略在不同场景下持续优于基准方法，验证显示快速收敛、实时推理和透明决策规则，学习权重突显市场微观结构和历史特征的重要性。

Conclusion: 特征驱动强化学习为光伏生产商提供了实用、数据高效且可操作部署的日内参与途径。

Abstract: Photovoltaic (PV) operators face substantial uncertainty in generation and
short-term electricity prices. Continuous intraday markets enable producers to
adjust their positions in real time, potentially improving revenues and
reducing imbalance costs. We propose a feature-driven reinforcement learning
(RL) approach for PV intraday trading that integrates data-driven features into
the state and learns bidding policies in a sequential decision framework. The
problem is cast as a Markov Decision Process with a reward that balances
trading profit and imbalance penalties and is solved with Proximal Policy
Optimization (PPO) using a predominantly linear, interpretable policy. Trained
on historical market data and evaluated out-of-sample, the strategy
consistently outperforms benchmark baselines across diverse scenarios.
Extensive validation shows rapid convergence, real-time inference, and
transparent decision rules. Learned weights highlight the central role of
market microstructure and historical features. Taken together, these results
indicate that feature-driven RL offers a practical, data-efficient, and
operationally deployable pathway for active intraday participation by PV
producers.

</details>


### [309] [Lean Finder: Semantic Search for Mathlib That Understands User Intents](https://arxiv.org/abs/2510.15940)
*Jialin Lu,Kye Emond,Kaiyu Yang,Swarat Chaudhuri,Weiran Sun,Wuyang Chen*

Main category: cs.LG

TL;DR: Lean Finder是一个专为数学家和Lean用户设计的语义搜索引擎，通过理解用户意图和数学语义，显著提升了定理搜索效果，比现有搜索引擎和GPT-4o有30%以上的相对改进。


<details>
  <summary>Details</summary>
Motivation: 现有Lean搜索引擎主要依赖形式化语句的非正式翻译，但忽视了与现实用户查询的匹配问题，导致数学家在定理证明过程中难以找到相关定理，学习曲线陡峭，进展缓慢。

Method: 分析并聚类公开Lean讨论的语义，在模拟用户意图的合成查询上微调文本嵌入，通过多样化反馈信号与数学家偏好对齐，从多角度编码用户目标。

Result: 在真实世界查询、非正式化语句和证明状态上的评估显示，Lean Finder相比之前的搜索引擎和GPT-4o实现了超过30%的相对改进。

Conclusion: Lean Finder是一个用户中心的语义搜索系统，能够有效理解数学家意图，与基于LLM的定理证明器兼容，连接了检索与形式推理。

Abstract: We present Lean Finder, a semantic search engine for Lean and mathlib that
understands and aligns with the intents of mathematicians. Progress in formal
theorem proving is often hindered by the difficulty of locating relevant
theorems and the steep learning curve of the Lean 4 language, making
advancement slow and labor-intensive. Existing Lean search engines, though
helpful, rely primarily on informalizations (natural language translation of
the formal statements), while largely overlooking the mismatch with real-world
user queries. In contrast, we propose a user-centered semantic search tailored
to the needs of mathematicians. Our approach begins by analyzing and clustering
the semantics of public Lean discussions, then fine-tuning text embeddings on
synthesized queries that emulate user intents. We further align Lean Finder
with mathematicians' preferences using diverse feedback signals, encoding it
with a rich awareness of their goals from multiple perspectives. Evaluations on
real-world queries, informalized statements, and proof states demonstrate that
our Lean Finder achieves over $30\%$ relative improvement compared to previous
search engines and GPT-4o. In addition, Lean Finder is compatible with
LLM-based theorem provers, bridging retrieval with formal reasoning. Lean
Finder is available at: https://leanfinder.github.io

</details>


### [310] [Lyapunov-Stable Adaptive Control for Multimodal Concept Drift](https://arxiv.org/abs/2510.15944)
*Tianyu Bell Pan,Mengdi Zhu,Alexa Jordyn Cole,Ronald Wilson,Damon L. Woodard*

Main category: cs.LG

TL;DR: LS-OGD是一个针对概念漂移的鲁棒多模态学习自适应控制框架，通过动态调整学习率和模态融合权重来应对数据分布变化。


<details>
  <summary>Details</summary>
Motivation: 多模态学习系统在非平稳环境中容易因概念漂移而性能下降，特别是模态特定的漂移和缺乏持续稳定适应机制的问题。

Method: 使用在线控制器动态调整模型学习率和不同数据模态的融合权重，响应检测到的漂移和预测误差变化。

Result: 在有限漂移条件下，LS-OGD系统的预测误差一致最终有界，如果漂移停止则收敛到零；自适应融合策略能有效隔离和减轻严重模态特定漂移的影响。

Conclusion: 该理论保证为开发可靠且持续自适应的多模态学习系统奠定了原则性基础。

Abstract: Multimodal learning systems often struggle in non-stationary environments due
to concept drift, where changing data distributions can degrade performance.
Modality-specific drifts and the lack of mechanisms for continuous, stable
adaptation compound this challenge. This paper introduces LS-OGD, a novel
adaptive control framework for robust multimodal learning in the presence of
concept drift. LS-OGD uses an online controller that dynamically adjusts the
model's learning rate and the fusion weights between different data modalities
in response to detected drift and evolving prediction errors. We prove that
under bounded drift conditions, the LS-OGD system's prediction error is
uniformly ultimately bounded and converges to zero if the drift ceases.
Additionally, we demonstrate that the adaptive fusion strategy effectively
isolates and mitigates the impact of severe modality-specific drift, thereby
ensuring system resilience and fault tolerance. These theoretical guarantees
establish a principled foundation for developing reliable and continuously
adapting multimodal learning systems.

</details>


### [311] [BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling](https://arxiv.org/abs/2510.15945)
*Guangya Wan,Zixin Stephen Xu,Sasa Zorc,Manel Baucells,Mengxuan Hu,Hao Wang,Sheng Li*

Main category: cs.LG

TL;DR: BEACON是一个基于贝叶斯学习的自适应采样框架，通过实时更新奖励分布的后验信念来决定何时停止生成新样本，在保持响应质量的同时减少80%的平均采样量。


<details>
  <summary>Details</summary>
Motivation: 多响应采样能提高LLM输出质量，但计算成本高昂。关键挑战是如何平衡准确性和效率，决定何时停止生成新样本。

Method: 基于序列搜索和贝叶斯学习，BEACON顺序生成响应、实时更新奖励分布后验信念，通过权衡预期收益与计算成本来决定停止时机。

Result: BEACON在保持响应质量的同时，将平均采样量减少高达80%，并展示了在成本效益偏好数据生成中的实用性。

Conclusion: BEACON提供了理论最优性保证和实际可行性，为未来研究者提供了实用的自适应采样框架。

Abstract: Sampling multiple responses is a common way to improve LLM output quality,
but it comes at the cost of additional computation. The key challenge is
deciding when to stop generating new samples to balance accuracy gains against
efficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive
Criterion for Optimal N-stopping), a principled adaptive sampling framework
grounded in Sequential Search with Bayesian Learning. BEACON sequentially
generates responses from the policy LLM, updates posterior belief over reward
distributions in real time without further training, and determines when to
stop by weighing expected gains against computational cost. Sampling terminates
once the marginal utility of further exploration no longer justifies the
expense. We establish both theoretical optimality guarantees and practical
tractability, and show empirically that BEACON reduces average sampling by up
to 80% while maintaining response quality. We further demonstrate BEACON's
utility for cost-efficient preference data generation and outline practical
extensions, offering actionable insights for future researchers.

</details>


### [312] [Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns](https://arxiv.org/abs/2510.15946)
*Wenshuo Wang,Ziyou Jiang,Junjie Wang,Mingyang Li,Jie Huang,Yuekai Huang,Zhiyuan Chang,Feiyan Duan,Qing Wang*

Main category: cs.LG

TL;DR: PatMD是一种通过学习和主动缓解误判风险来改进有害表情包检测的新方法，它识别潜在的误判模式来指导多模态大语言模型避免已知的误判陷阱。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法（包括MLLM技术）在处理表情包中通过讽刺和隐喻等修辞手法表达的隐含有害内容时存在困难，导致频繁误判。

Method: 构建知识库，将每个表情包解构为误判风险模式，解释其可能被误判的原因；对于目标表情包，检索相关模式并动态指导MLLM的推理过程。

Result: 在5个有害检测任务的6,626个表情包基准测试中，PatMD优于现有最先进基线，F1分数平均提高8.30%，准确率提高7.71%。

Conclusion: PatMD展示了强大的泛化能力和改进的有害表情包检测能力，通过主动缓解误判风险有效提升了检测性能。

Abstract: Internet memes have emerged as a popular multimodal medium, yet they are
increasingly weaponized to convey harmful opinions through subtle rhetorical
devices like irony and metaphor. Existing detection approaches, including
MLLM-based techniques, struggle with these implicit expressions, leading to
frequent misjudgments. This paper introduces PatMD, a novel approach that
improves harmful meme detection by learning from and proactively mitigating
these potential misjudgment risks. Our core idea is to move beyond superficial
content-level matching and instead identify the underlying misjudgment risk
patterns, proactively guiding the MLLMs to avoid known misjudgment pitfalls. We
first construct a knowledge base where each meme is deconstructed into a
misjudgment risk pattern explaining why it might be misjudged, either
overlooking harmful undertones (false negative) or overinterpreting benign
content (false positive). For a given target meme, PatMD retrieves relevant
patterns and utilizes them to dynamically guide the MLLM's reasoning.
Experiments on a benchmark of 6,626 memes across 5 harmful detection tasks show
that PatMD outperforms state-of-the-art baselines, achieving an average of
8.30\% improvement in F1-score and 7.71\% improvement in accuracy,
demonstrating strong generalizability and improved detection capability of
harmful memes.

</details>


### [313] [WaveNet's Precision in EEG Classification](https://arxiv.org/abs/2510.15947)
*Casper van Laar,Khubaib Ahmed*

Main category: cs.LG

TL;DR: 使用WaveNet深度学习模型自动分类EEG信号为生理、病理、伪影和噪声类别，在公开数据集上训练并超越CNN和LSTM方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于专家视觉检查的EEG信号分类方法在处理日益复杂的EEG记录时变得不切实际，需要自动化解决方案。

Method: 基于WaveNet架构，利用扩张因果卷积和残差连接处理EEG数据，在209,232个样本上训练验证测试（70/20/10分割）。

Result: 模型分类准确率超过CNN和LSTM方法，能高精度区分噪声和伪影，但在生理和病理信号间存在可解释的误分类。

Conclusion: WaveNet架构适合EEG数据分析，能捕捉细粒度和长程时间依赖，为EEG自动分类提供了有效解决方案。

Abstract: This study introduces a WaveNet-based deep learning model designed to
automate the classification of EEG signals into physiological, pathological,
artifact, and noise categories. Traditional methods for EEG signal
classification, which rely on expert visual review, are becoming increasingly
impractical due to the growing complexity and volume of EEG recordings.
Leveraging a publicly available annotated dataset from Mayo Clinic and St.
Anne's University Hospital, the WaveNet model was trained, validated, and
tested on 209,232 samples with a 70/20/10 percent split. The model achieved a
classification accuracy exceeding previous CNN and LSTM-based approaches, and
was benchmarked against a Temporal Convolutional Network (TCN) baseline.
Notably, the model distinguishes noise and artifacts with high precision,
although it reveals a modest but explainable degree of misclassification
between physiological and pathological signals, reflecting inherent clinical
overlap. WaveNet's architecture, originally developed for raw audio synthesis,
is well suited for EEG data due to its use of dilated causal convolutions and
residual connections, enabling it to capture both fine-grained and long-range
temporal dependencies. The research also details the preprocessing pipeline,
including dynamic dataset partitioning and normalization steps that support
model generalization.

</details>


### [314] [Cross-dataset Multivariate Time-series Model for Parkinson's Diagnosis via Keyboard Dynamics](https://arxiv.org/abs/2510.15950)
*Arianna Francesconi,Donato Cappetta,Fabio Rebecchi,Paolo Soda,Valerio Guarrasi,Rosa Sicilia*

Main category: cs.LG

TL;DR: 提出基于击键动力学的帕金森病筛查和远程监测新方法，使用深度学习模型在外部验证中达到超过90%的AUC-ROC和70%以上的F1-Score。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断困难，传统临床评估存在局限性，需要非侵入性、可扩展的生物标志物进行远程筛查和监测。

Method: 三阶段流程：数据预处理和类别不平衡处理；在最大数据集上预训练8种深度学习架构；在中等数据集上微调并在独立队列中进行外部验证。

Result: 混合卷积-循环和基于Transformer的模型表现优异，外部验证AUC-ROC超过90%，F1-Score超过70%，时间卷积模型达到91.14%的AUC-ROC。

Conclusion: 击键动力学可作为帕金森病的可靠数字生物标志物，为早期检测和持续监测提供了有前景的途径。

Abstract: Parkinson's disease (PD) presents a growing global challenge, affecting over
10 million individuals, with prevalence expected to double by 2040. Early
diagnosis remains difficult due to the late emergence of motor symptoms and
limitations of traditional clinical assessments. In this study, we propose a
novel pipeline that leverages keystroke dynamics as a non-invasive and scalable
biomarker for remote PD screening and telemonitoring. Our methodology involves
three main stages: (i) preprocessing of data from four distinct datasets,
extracting four temporal signals and addressing class imbalance through the
comparison of three methods; (ii) pre-training eight state-of-the-art
deep-learning architectures on the two largest datasets, optimizing temporal
windowing, stride, and other hyperparameters; (iii) fine-tuning on an
intermediate-sized dataset and performing external validation on a fourth,
independent cohort. Our results demonstrate that hybrid convolutional-recurrent
and transformer-based models achieve strong external validation performance,
with AUC-ROC scores exceeding 90% and F1-Score over 70%. Notably, a temporal
convolutional model attains an AUC-ROC of 91.14% in external validation,
outperforming existing methods that rely solely on internal validation. These
findings underscore the potential of keystroke dynamics as a reliable digital
biomarker for PD, offering a promising avenue for early detection and
continuous monitoring.

</details>


### [315] [Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter](https://arxiv.org/abs/2510.15954)
*Hongzheng Shi,Yuhang Wang,Xiao Liu*

Main category: cs.LG

TL;DR: 本文研究了基于扩散模型的集合评分滤波器(EnSF)在野火蔓延实时预测数据同化中的应用，展示了该方法在准确性、稳定性和计算效率方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 随着野火破坏性增强且控制成本上升，需要准确的实时火势蔓延预测。数据同化通过整合观测数据和数值模型预测，对提高野火预测准确性至关重要。

Method: 应用基于扩散模型的集合评分滤波器(EnSF)，利用基于分数的生成扩散模型来处理高维非线性滤波问题，特别适用于野火蔓延模型的滤波问题。

Result: 数值研究表明，EnSF在野火数据同化中表现出优越的准确性、稳定性和计算效率。

Conclusion: EnSF被证明是野火数据同化的稳健实用方法，代码已公开可用。

Abstract: As wildfires become increasingly destructive and expensive to control,
effective management of active wildfires requires accurate, real-time fire
spread predictions. To enhance the forecasting accuracy of active fires, data
assimilation plays a vital role by integrating observations (such as
remote-sensing data) and fire predictions generated from numerical models. This
paper provides a comprehensive investigation on the application of a recently
proposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter
(EnSF) -- to the data assimilation problem for real-time active wildfire spread
predictions. Leveraging a score-based generative diffusion model, EnSF has been
shown to have superior accuracy for high-dimensional nonlinear filtering
problems, making it an ideal candidate for the filtering problems of wildfire
spread models. Technical details are provided, and our numerical investigations
demonstrate that EnSF provides superior accuracy, stability, and computational
efficiency, establishing it as a robust and practical method for wildfire data
assimilation. Our code has been made publicly available.

</details>


### [316] [How Good Are LLMs at Processing Tool Outputs?](https://arxiv.org/abs/2510.15955)
*Kiran Kate,Yara Rizk,Poulami Ghosh,Ashu Gulati,Tathagata Chakraborti,Zidane Wright,Mayank Agarwal*

Main category: cs.LG

TL;DR: 该论文研究大语言模型处理工具返回的复杂JSON响应的能力，发现即使是前沿模型在处理结构化响应时仍面临困难，不同处理策略的性能差异可达3%到50%。


<details>
  <summary>Details</summary>
Motivation: 现实任务自动化需要LLMs调用工具并处理其返回的复杂JSON响应，但LLMs处理结构化响应的能力尚未得到充分研究。

Method: 创建了工具响应处理任务的数据集，评估了15个开源和闭源模型，使用多种提示方法进行研究。

Result: JSON处理对所有模型来说都是困难任务，最佳处理策略取决于工具输出的性质和大小以及所需推理的复杂性。

Conclusion: 工具响应处理是LLMs面临的重要挑战，处理策略的选择对性能有显著影响，需要根据具体情况优化。

Abstract: Most realistic task automation problems require large language models (LLMs)
to call tools, which often return complex JSON responses. These responses must
be further processed to derive the information necessary for task completion.
The ability of LLMs to do so is under-studied. In this paper, we study the tool
response processing task and LLMs' abilities to process structured (JSON)
responses. We created a dataset for this task, and evaluated 15 open and closed
weight models using multiple prompting approaches. Our results show that JSON
processing remains a difficult task even for frontier models across multiple
prompting strategies. The optimal response processing strategy depends on both
the nature and size of the tool outputs, as well as the complexity of the
required reasoning. Variations in processing approaches can lead to performance
differences ranging from 3\% to 50\%.

</details>


### [317] [Hydrogen production from blended waste biomass: pyrolysis, thermodynamic-kinetic analysis and AI-based modelling](https://arxiv.org/abs/2510.15960)
*Sana Kordoghli,Abdelhakim Settar,Oumayma Belaati,Mohammad Alkhatib*

Main category: cs.LG

TL;DR: 该研究通过热解技术将食物基生物质转化为可持续氢能，利用人工智能优化过程建模，重点研究了咖啡渣和枣核等未充分利用生物质资源的潜力。


<details>
  <summary>Details</summary>
Motivation: 推动可持续能源和废物管理策略，探索未充分利用生物质资源（如咖啡渣和枣核）在可持续氢生产中的潜力，并利用人工智能提高过程建模精度和优化效率。

Method: 对纯枣核、咖啡渣及其混合物进行近似分析、元素分析、纤维分析、TGA/DTG分析、动力学分析、热力学分析和Py-Micro GC分析；采用等转化率方法（KAS、FWO、Friedman）进行动力学建模；训练LSTM模型预测TGA曲线。

Result: 混合物3显示出最佳的氢气产量潜力但活化能最高（313.24 kJ/mol），混合物1具有最佳活化能值（161.75 kJ/mol）；KAS方法被确定为最准确的动力学模型；LSTM模型预测TGA曲线准确度极高（R²: 0.9996-0.9998）。

Conclusion: 人工智能在热解过程建模中表现出卓越性能，LSTM模型能够高精度预测热解行为，为可持续氢生产提供了有效的优化工具，展示了未充分利用生物质资源在能源转化中的巨大潜力。

Abstract: This work contributes to advancing sustainable energy and waste management
strategies by investigating the thermochemical conversion of food-based biomass
through pyrolysis, highlighting the role of artificial intelligence (AI) in
enhancing process modelling accuracy and optimization efficiency. The main
objective is to explore the potential of underutilized biomass resources, such
as spent coffee grounds (SCG) and date seeds (DS), for sustainable hydrogen
production. Specifically, it aims to optimize the pyrolysis process while
evaluating the performance of these resources both individually and as blends.
Proximate, ultimate, fibre, TGA/DTG, kinetic, thermodynamic, and Py-Micro GC
analyses were conducted for pure DS, SCG, and blends (75% DS - 25% SCG, 50% DS
- 50% SCG, 25% DS - 75% SCG). Blend 3 offered superior hydrogen yield potential
but had the highest activation energy (Ea: 313.24 kJ/mol), while Blend 1
exhibited the best activation energy value (Ea: 161.75 kJ/mol). The kinetic
modelling based on isoconversional methods (KAS, FWO, Friedman) identified KAS
as the most accurate. These approaches provide a detailed understanding of the
pyrolysis process, with particular emphasis on the integration of artificial
intelligence. An LSTM model trained with lignocellulosic data predicted TGA
curves with exceptional accuracy (R^2: 0.9996-0.9998).

</details>


### [318] [Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use](https://arxiv.org/abs/2510.15961)
*Yiyang Li,Zehong Wang,Zhengqing Yuan,Zheyuan Zhang,Keerthiram Murugesan,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: 提出LAMI框架，通过联合图-语言建模检测青少年非法药物使用并解释行为风险因素，在YRBS和NSDUH数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有建模方法将调查变量独立处理，忽略了变量间的潜在关联结构，无法有效捕捉青少年药物使用的复杂行为模式。

Method: LAMI将个体响应表示为关系图，通过专门的图结构学习层学习潜在连接，并集成大语言模型生成基于图结构和调查语义的自然语言解释。

Result: 在YRBS和NSDUH数据集上的实验显示LAMI在预测准确性上优于竞争基线，可解释性分析揭示了与已知物质使用风险因素一致的行为子结构和心理社会路径。

Conclusion: LAMI框架能够有效检测青少年非法药物使用，同时提供有意义的可解释性分析，揭示了家庭动态、同伴影响和学校相关压力等关键风险因素。

Abstract: Illicit drug use among teenagers and young adults (TYAs) remains a pressing
public health concern, with rising prevalence and long-term impacts on health
and well-being. To detect illicit drug use among TYAs, researchers analyze
large-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the
National Survey on Drug Use and Health (NSDUH), which preserve rich
demographic, psychological, and environmental factors related to substance use.
However, existing modeling methods treat survey variables independently,
overlooking latent and interconnected structures among them. To address this
limitation, we propose LAMI (LAtent relation Mining with bi-modal
Interpretability), a novel joint graph-language modeling framework for
detecting illicit drug use and interpreting behavioral risk factors among TYAs.
LAMI represents individual responses as relational graphs, learns latent
connections through a specialized graph structure learning layer, and
integrates a large language model to generate natural language explanations
grounded in both graph structures and survey semantics. Experiments on the YRBS
and NSDUH datasets show that LAMI outperforms competitive baselines in
predictive accuracy. Interpretability analyses further demonstrate that LAMI
reveals meaningful behavioral substructures and psychosocial pathways, such as
family dynamics, peer influence, and school-related distress, that align with
established risk factors for substance use.

</details>


### [319] [CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models](https://arxiv.org/abs/2510.15962)
*Zhuxuanzi Wang,Mingqiao Mo,Xi Xiao,Chen Liu,Chenrui Ma,Yunbei Zhang,Xiao Wang,Smita Krishnaswamy,Tianyang Wang*

Main category: cs.LG

TL;DR: CTR-LoRA是一种基于曲率信任区域的参数高效微调框架，通过边际效用分配参数并使用Fisher/Hessian度量信任区域约束更新，在多个基准测试中优于现有PEFT方法。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法在低秩更新、量化或启发式预算分配方面提高了效率，但往往将容量分配与训练过程中更新的演化方式解耦，缺乏对训练稳定性的考虑。

Method: CTR-LoRA框架结合了秩调度与稳定性感知优化，基于轻量级二阶代理的边际效用分配参数，并使用Fisher/Hessian度量信任区域约束更新。

Result: 在多个开源骨干模型（7B-13B）上，在分布内和分布外基准测试中均优于强PEFT基线，提高了准确性、训练稳定性，减少了内存需求并实现了更高的吞吐量。

Conclusion: CTR-LoRA在性能和效率的帕累托前沿上表现出色，为更稳健和可部署的PEFT提供了一条原则性路径。

Abstract: Parameter-efficient fine-tuning (PEFT) has become the standard approach for
adapting large language models under limited compute and memory budgets.
Although previous methods improve efficiency through low-rank updates,
quantization, or heuristic budget reallocation, they often decouple the
allocation of capacity from the way updates evolve during training. In this
work, we introduce CTR-LoRA, a framework guided by curvature trust region that
integrates rank scheduling with stability-aware optimization. CTR-LoRA
allocates parameters based on marginal utility derived from lightweight
second-order proxies and constrains updates using a Fisher/Hessian-metric trust
region. Experiments on multiple open-source backbones (7B-13B), evaluated on
both in-distribution and out-of-distribution benchmarks, show consistent
improvements over strong PEFT baselines. In addition to increased accuracy,
CTR-LoRA enhances training stability, reduces memory requirements, and achieves
higher throughput, positioning it on the Pareto frontier of performance and
efficiency. These results highlight a principled path toward more robust and
deployable PEFT.

</details>


### [320] [Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity](https://arxiv.org/abs/2510.15964)
*Tuowei Wang,Kun Li,Zixu Hao,Donglin Bai,Ju Ren,Yaoxue Zhang,Ting Cao,Mao Yang*

Main category: cs.LG

TL;DR: Long Exposure系统通过解决微调中的Shadowy Sparsity问题，加速参数高效微调(PEFT)，实现最高2.49倍的端到端微调加速。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调(PEFT)技术存在效率低下的问题，在时间投入和运营成本方面面临挑战，特别是微调过程中特有的Shadowy Sparsity问题尚未得到充分解决。

Method: Long Exposure系统包含三个关键组件：Shadowy-sparsity Exposer使用长感知范围捕获更多稀疏细节；Sequence-oriented Predictor处理大序列输入和不断演化的参数；Dynamic-aware Operator解决动态稀疏操作问题。

Result: 广泛评估显示，Long Exposure在端到端微调中实现了最高2.49倍的加速，优于现有最先进方法。

Conclusion: Long Exposure为加速LLMs的PEFT提供了有前景的进展，有效解决了微调过程中的效率瓶颈问题。

Abstract: The adaptation of pre-trained large language models (LLMs) to diverse
downstream tasks via fine-tuning is critical for numerous applications.
However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques
presents significant challenges in terms of time investments and operational
costs. In this paper, we first introduce a nuanced form of sparsity, termed
Shadowy Sparsity, which is distinctive in fine-tuning and has not been
adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long
Exposure, an efficient system to accelerate PEFT for LLMs. Long Exposure
comprises three key components: Shadowy-sparsity Exposer employs a prolonged
sensing range to capture more sparsity details under shadowy sparsity;
Sequence-oriented Predictor provides efficient yet accurate predictions to
handle large sequence inputs and constantly-evolving parameters; and
Dynamic-aware Operator facilitates more structured computational patterns and
coalesced memory accesses, addressing dynamic sparse operations. Extensive
evaluations show that Long Exposure outperforms state-of-the-arts with up to a
$2.49\times$ speedup in end-to-end fine-tuning, offering promising advancements
in accelerating PEFT for LLMs.

</details>


### [321] [One Token Embedding Is Enough to Deadlock Your Large Reasoning Model](https://arxiv.org/abs/2510.15965)
*Mohan Zhang,Yihua Zhang,Jinghan Jia,Zhangyang Wang,Sijia Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出了一种名为Deadlock Attack的资源耗尽攻击方法，通过训练恶意对抗嵌入来劫持大型推理模型的生成控制流，诱导模型陷入永久推理循环，阻止其输出最终答案。


<details>
  <summary>Details</summary>
Motivation: 现代大型推理模型通过思维链推理展示出强大的多步问题解决能力，但这种迭代思维机制引入了新的安全漏洞。研究者发现可以从推理效率的角度攻击这些模型。

Method: 使用优化的对抗嵌入来鼓励模型在推理步骤后生成过渡性标记（如"Wait"、"But"），从而阻止模型得出结论。为了解决连续到离散的投影差距问题，引入了后门植入策略，通过特定触发标记实现可靠激活。

Result: 在四个先进的大型推理模型（Phi-RM、Nemotron-Nano、R1-Qwen、R1-Llama）和三个数学推理基准测试上实现了100%的攻击成功率，迫使模型生成达到最大标记限制。攻击具有隐蔽性，对良性用户输入的效用损失可忽略，且对现有缓解过度思考的策略具有鲁棒性。

Conclusion: 这项研究揭示了大型推理模型在推理效率方面存在关键且未被充分探索的安全漏洞，表明推理机制本身可能成为新的攻击面。

Abstract: Modern large reasoning models (LRMs) exhibit impressive multi-step
problem-solving via chain-of-thought (CoT) reasoning. However, this iterative
thinking mechanism introduces a new vulnerability surface. We present the
Deadlock Attack, a resource exhaustion method that hijacks an LRM's generative
control flow by training a malicious adversarial embedding to induce perpetual
reasoning loops. Specifically, the optimized embedding encourages transitional
tokens (e.g., "Wait", "But") after reasoning steps, preventing the model from
concluding its answer. A key challenge we identify is the
continuous-to-discrete projection gap: na\"ive projections of adversarial
embeddings to token sequences nullify the attack. To overcome this, we
introduce a backdoor implantation strategy, enabling reliable activation
through specific trigger tokens. Our method achieves a 100% attack success rate
across four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three
math reasoning benchmarks, forcing models to generate up to their maximum token
limits. The attack is also stealthy (in terms of causing negligible utility
loss on benign user inputs) and remains robust against existing strategies
trying to mitigate the overthinking issue. Our findings expose a critical and
underexplored security vulnerability in LRMs from the perspective of reasoning
(in)efficiency.

</details>


### [322] [Gains: Fine-grained Federated Domain Adaptation in Open Set](https://arxiv.org/abs/2510.15967)
*Zhengyi Zhong,Wenzheng Jiang,Weidong Bao,Ji Wang,Cheems Wang,Guanbo Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: 提出了Gains方法，用于解决联邦学习中新客户端不断加入带来的知识发现和适应问题，通过细粒度知识发现和贡献驱动聚合来整合新知识，同时保持源域性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中联邦学习面临新客户端持续加入的挑战，需要检测新知识并整合到全局模型中，现有方法在知识发现粒度、源域性能保持和适应效率方面存在不足。

Method: 将模型分为编码器和分类器，基于编码器对域偏移敏感、分类器对类别增量敏感的特性，开发细粒度知识发现、贡献驱动聚合和抗遗忘机制。

Result: 在三个典型数据偏移场景的多域数据集实验中，Gains在源域和目标域客户端性能上均显著优于其他基线方法。

Conclusion: Gains方法有效解决了开放世界联邦学习中的知识发现和适应问题，实现了源域和目标域性能的平衡提升。

Abstract: Conventional federated learning (FL) assumes a closed world with a fixed
total number of clients. In contrast, new clients continuously join the FL
process in real-world scenarios, introducing new knowledge. This raises two
critical demands: detecting new knowledge, i.e., knowledge discovery, and
integrating it into the global model, i.e., knowledge adaptation. Existing
research focuses on coarse-grained knowledge discovery, and often sacrifices
source domain performance and adaptation efficiency. To this end, we propose a
fine-grained federated domain adaptation approach in open set (Gains). Gains
splits the model into an encoder and a classifier, empirically revealing
features extracted by the encoder are sensitive to domain shifts while
classifier parameters are sensitive to class increments. Based on this, we
develop fine-grained knowledge discovery and contribution-driven aggregation
techniques to identify and incorporate new knowledge. Additionally, an
anti-forgetting mechanism is designed to preserve source domain performance,
ensuring balanced adaptation. Experimental results on multi-domain datasets
across three typical data-shift scenarios demonstrate that Gains significantly
outperforms other baselines in performance for both source-domain and
target-domain clients. Code is available at:
https://github.com/Zhong-Zhengyi/Gains.

</details>


### [323] [Self-Attention to Operator Learning-based 3D-IC Thermal Simulation](https://arxiv.org/abs/2510.15968)
*Zhen Huang,Hong Wang,Wenkai Yang,Muxi Tang,Depeng Xie,Ting-Jung Lin,Yu Zhang,Wei W. Xing,Lei He*

Main category: cs.LG

TL;DR: 提出SAU-FNO框架，结合自注意力机制、U-Net和FNO，用于3D IC热管理，实现842倍加速并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 3D IC热管理面临高功率密度挑战，传统PDE方法速度慢，机器学习方法存在高频信息丢失和高保真数据依赖问题。

Method: 结合自注意力机制和U-Net的FNO框架，使用迁移学习微调低保真数据，减少对高保真数据集的需求。

Result: SAU-FNO达到最先进的热预测精度，相比传统FEM方法实现842倍加速。

Conclusion: SAU-FNO是高效的高级3D IC热仿真工具，平衡了精度和速度。

Abstract: Thermal management in 3D ICs is increasingly challenging due to higher power
densities. Traditional PDE-solving-based methods, while accurate, are too slow
for iterative design. Machine learning approaches like FNO provide faster
alternatives but suffer from high-frequency information loss and high-fidelity
data dependency. We introduce Self-Attention U-Net Fourier Neural Operator
(SAU-FNO), a novel framework combining self-attention and U-Net with FNO to
capture long-range dependencies and model local high-frequency features
effectively. Transfer learning is employed to fine-tune low-fidelity data,
minimizing the need for extensive high-fidelity datasets and speeding up
training. Experiments demonstrate that SAU-FNO achieves state-of-the-art
thermal prediction accuracy and provides an 842x speedup over traditional FEM
methods, making it an efficient tool for advanced 3D IC thermal simulations.

</details>


### [324] [LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems](https://arxiv.org/abs/2510.15969)
*Paul-Niklas Ken Kandora,Simon Caspar Zeller,Aaron Jeremias Elsing,Elena Kuss,Steffen Rebennack*

Main category: cs.LG

TL;DR: LinearizeLLM是一个基于代理的框架，利用大语言模型自动将非线性优化问题转化为线性形式，使问题能够用线性优化求解器求解。


<details>
  <summary>Details</summary>
Motivation: 非线性优化问题的线性化通常需要人工操作且依赖专家知识，这限制了问题的求解效率和可扩展性。

Method: 为每种非线性模式分配专门的"线性化代理"，这些代理被明确指示为其特定的非线性模式推导精确的线性重构，然后协调组装成可求解的线性模型。

Result: 在基于ComplexOR数据集的20个真实非线性优化问题上进行测试，结果表明专门的LLM代理能够自动化线性化任务。

Conclusion: 专业化的LLM代理可以自动化线性化任务，为非线性优化开辟了完全对话式建模管道的新路径。

Abstract: Reformulating nonlinear optimization problems is largely manual and
expertise-intensive, yet it remains essential for solving such problems with
linear optimization solvers or applying special-purpose algorithms. We
introduce \textit{LinearizeLLM}, an agent-based framework that solves this task
by leveraging Large Language Models (LLMs). The framework assigns each
nonlinear pattern to a \textit{reformulation agent} that is explicitly
instructed to derive an exact linear reformulation for its nonlinearity
pattern, for instance, absolute-value terms or bilinear products of decision
variables. The agents then coordinate to assemble a solver-ready linear model
equivalent to the original problem. To benchmark the approach, we create a
dataset of 20 real-world nonlinear optimization problems derived from the
established ComplexOR dataset of linear optimization problems. We evaluate our
approach with several LLMs. Our results indicate that specialized LLM agents
can automate linearization tasks, opening a path toward fully conversational
modeling pipelines for nonlinear optimization.

</details>


### [325] [Predict Training Data Quality via Its Geometry in Metric Space](https://arxiv.org/abs/2510.15970)
*Yang Ba,Mohammad Sadeq Abolhasani,Rong Pan*

Main category: cs.LG

TL;DR: 本文提出使用持久同调分析训练数据的几何结构对机器学习性能的影响，强调数据表示的丰富性和冗余消除的重要性。


<details>
  <summary>Details</summary>
Motivation: 虽然已知训练数据的类型对机器学习很重要，但数据的几何结构对模型性能的影响尚未充分探索。作者认为数据的表示丰富性和冗余消除对学习结果有重要影响。

Method: 使用持久同调从度量空间中的数据提取拓扑特征，提供了一种超越基于熵的度量来量化多样性的原则性方法。

Result: 研究发现持久同调是分析和增强驱动AI系统的训练数据的强大工具。

Conclusion: 持久同调为理解和改进训练数据的几何结构提供了有效的分析框架，有助于提升AI系统的性能。

Abstract: High-quality training data is the foundation of machine learning and
artificial intelligence, shaping how models learn and perform. Although much is
known about what types of data are effective for training, the impact of the
data's geometric structure on model performance remains largely underexplored.
We propose that both the richness of representation and the elimination of
redundancy within training data critically influence learning outcomes. To
investigate this, we employ persistent homology to extract topological features
from data within a metric space, thereby offering a principled way to quantify
diversity beyond entropy-based measures. Our findings highlight persistent
homology as a powerful tool for analyzing and enhancing the training data that
drives AI systems.

</details>


### [326] [Bolster Hallucination Detection via Prompt-Guided Data Augmentation](https://arxiv.org/abs/2510.15977)
*Wenyun Li,Zheng Zhang,Dongmei Jiang,Xiangyuan Lan*

Main category: cs.LG

TL;DR: PALE框架通过提示引导的数据增强和对比马氏距离评分来检测大语言模型的幻觉，无需人工标注即可显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在产生误导或虚构信息的幻觉问题，而幻觉检测面临标注数据稀缺的挑战。

Method: 使用提示引导的LLM响应作为数据增强，引入基于激活空间分布建模的对比马氏距离评分来评估文本真实性。

Result: PALE在幻觉检测任务上表现优异，比竞争基线显著提升6.55%的性能。

Conclusion: PALE框架提供了一种无需人工标注、具有强泛化性和实用性的幻觉检测解决方案。

Abstract: Large language models (LLMs) have garnered significant interest in AI
community. Despite their impressive generation capabilities, they have been
found to produce misleading or fabricated information, a phenomenon known as
hallucinations. Consequently, hallucination detection has become critical to
ensure the reliability of LLM-generated content. One primary challenge in
hallucination detection is the scarcity of well-labeled datasets containing
both truthful and hallucinated outputs. To address this issue, we introduce
Prompt-guided data Augmented haLlucination dEtection (PALE), a novel framework
that leverages prompt-guided responses from LLMs as data augmentation for
hallucination detection. This strategy can generate both truthful and
hallucinated data under prompt guidance at a relatively low cost. To more
effectively evaluate the truthfulness of the sparse intermediate embeddings
produced by LLMs, we introduce an estimation metric called the Contrastive
Mahalanobis Score (CM Score). This score is based on modeling the distributions
of truthful and hallucinated data in the activation space. CM Score employs a
matrix decomposition approach to more accurately capture the underlying
structure of these distributions. Importantly, our framework does not require
additional human annotations, offering strong generalizability and practicality
for real-world applications. Extensive experiments demonstrate that PALE
achieves superior hallucination detection performance, outperforming the
competitive baseline by a significant margin of 6.55%.

</details>


### [327] [DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space](https://arxiv.org/abs/2510.15978)
*Junchao Gong,Jingyi Xu,Ben Fei,Fenghua Ling,Wenlong Zhang,Kun Chen,Wanghan Xu,Weidong Yang,Xiaokang Yang,Lei Bai*

Main category: cs.LG

TL;DR: 提出DAWP框架，通过人工智能数据同化(AIDA)模块将AI天气预测从再分析数据解放到观测空间，实现基于不规则观测数据的全球天气预报


<details>
  <summary>Details</summary>
Motivation: 传统AI天气预测依赖再分析数据存在数据同化偏差和时间差异问题，观测预报是解放AIWP的新范式，但需要解决不规则高分辨率观测数据的时空动态学习挑战

Method: 使用掩码多模态自编码器(MMAE)进行数据同化，通过掩码ViT-VAE编码不规则卫星观测令牌，采用时空解耦transformer和跨区域边界条件实现基于子图像的全球观测预报

Result: AIDA初始化显著提高了AIWP的推出效率和性能，DAWP在全局降水预报中展现出应用潜力

Conclusion: DAWP框架成功将AI天气预测从再分析数据依赖中解放出来，通过观测空间操作实现了更准确的天气预报

Abstract: Weather prediction is a critical task for human society, where impressive
progress has been made by training artificial intelligence weather prediction
(AIWP) methods with reanalysis data. However, reliance on reanalysis data
limits the AIWPs with shortcomings, including data assimilation biases and
temporal discrepancies. To liberate AIWPs from the reanalysis data, observation
forecasting emerges as a transformative paradigm for weather prediction. One of
the key challenges in observation forecasting is learning spatiotemporal
dynamics across disparate measurement systems with irregular high-resolution
observation data, which constrains the design and prediction of AIWPs. To this
end, we propose our DAWP as an innovative framework to enable AIWPs to operate
in a complete observation space by initialization with an artificial
intelligence data assimilation (AIDA) module. Specifically, our AIDA module
applies a mask multi-modality autoencoder(MMAE)for assimilating irregular
satellite observation tokens encoded by mask ViT-VAEs. For AIWP, we introduce a
spatiotemporal decoupling transformer with cross-regional boundary conditioning
(CBC), learning the dynamics in observation space, to enable sub-image-based
global observation forecasting. Comprehensive experiments demonstrate that AIDA
initialization significantly improves the roll out and efficiency of AIWP.
Additionally, we show that DAWP holds promising potential to be applied in
global precipitation forecasting.

</details>


### [328] [Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.15979)
*Zexu Sun,Yongcheng Zeng,Erxue Min,Heyang Gao,Bokai Ji,Xu Chen*

Main category: cs.LG

TL;DR: 提出了Cog-Rethinker，一种用于LLM推理的分层元认知强化学习框架，通过两阶段方法提高样本利用率，解决弱LLM在推理任务中因无效输出导致的采样效率低下问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定提示模板激活LLM的推理能力，但这对弱LLM造成严重的采样效率问题，因为大部分问题在准确性驱动的过滤中会产生无效输出，浪费样本。

Method: 提出分层元认知RL框架：1）将零准确率问题分解为子问题；2）参考先前错误解决方案精炼答案。应用监督微调确保训练-测试一致性。

Result: 实验结果显示Cog-Rethinker在多个数学推理基准上表现优异，相比基线方法提高了样本效率并加速了收敛。

Conclusion: Cog-Rethinker通过分层元认知方法有效解决了LLM推理训练中的采样效率问题，在数学推理任务中取得了显著改进。

Abstract: Contemporary progress in large language models (LLMs) has revealed notable
inferential capacities via reinforcement learning (RL) employing verifiable
reward, facilitating the development of O1 and R1-like reasoning models.
Directly training from base models with RL is called zero-RL. However, previous
works rely upon activating LLMs' inherent capacities through fixed prompt
templates. This strategy introduces substantial sampling inefficiencies for
weak LLMs, as the majority of problems generate invalid outputs during
accuracy-driven filtration in reasoning tasks, which causes a waste of samples.
To solve this issue, we propose Cog-Rethinker, a novel hierarchical
metacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses
on the rollout procedure in RL training. After the direct rollout, our
Cog-Rethinker improves sample utilization in a hierarchical metacognitive
two-stage framework. By leveraging human cognition during solving problems,
firstly, it prompts policy to decompose zero-accuracy problems into subproblems
to produce final reasoning results. Secondly, with zero-accuracy problems in
previous rollout stage, it further prompts policy to refine these answers by
referencing previous wrong solutions. Moreover, to enable cold-start of the two
new reasoning patterns and maintain train-test consistency across prompt
templates, our Cog-Rethinker applies supervised fine-tuning on the policy using
correct samples of the two stages with direct rollout template. Experimental
results demonstrate Cog-Rethinker's superior performance on various
mathematical reasoning benchmarks, we also analyzed its improved sample
efficiency that accelerates convergence compared to baseline methods.

</details>


### [329] [Beyond Accuracy: Are Time Series Foundation Models Well-Calibrated?](https://arxiv.org/abs/2510.16060)
*Coen Adler,Yuxin Chang,Felix Draxler,Samar Abdi,Padhraic Smyth*

Main category: cs.LG

TL;DR: 本文研究了5个时间序列基础模型和2个基线模型的校准特性，发现时间序列基础模型比基线模型校准得更好，且没有系统性的过度自信或不足自信。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型在预测性能上达到最先进水平，但其校准特性研究不足，而校准确实对许多实际应用至关重要。

Method: 对5个时间序列基础模型和2个竞争性基线模型进行系统评估，包括模型校准、不同预测头的影响以及长期自回归预测下的校准。

Result: 时间序列基础模型比基线模型校准得更好，且没有系统性的过度自信或不足自信，这与深度学习中常见的过度自信现象形成对比。

Conclusion: 时间序列基础模型不仅预测性能优越，而且具有良好的校准特性，这在实际应用中具有重要意义。

Abstract: The recent development of foundation models for time series data has
generated considerable interest in using such models across a variety of
applications. Although foundation models achieve state-of-the-art predictive
performance, their calibration properties remain relatively underexplored,
despite the fact that calibration can be critical for many practical
applications. In this paper, we investigate the calibration-related properties
of five recent time series foundation models and two competitive baselines. We
perform a series of systematic evaluations assessing model calibration (i.e.,
over- or under-confidence), effects of varying prediction heads, and
calibration under long-term autoregressive forecasting. We find that time
series foundation models are consistently better calibrated than baseline
models and tend not to be either systematically over- or under-confident, in
contrast to the overconfidence often seen in other deep learning models.

</details>


### [330] [AMiD: Knowledge Distillation for LLMs with $α$-mixture Assistant Distribution](https://arxiv.org/abs/2510.15982)
*Donghyeok Shin,Yeongmin Kim,Suhyeon Jo,Byeonghu Na,Il-Chul Moon*

Main category: cs.LG

TL;DR: 本文提出AMiD框架，通过α-混合辅助分布解决LLM知识蒸馏中的容量差距和训练不稳定问题，统一了先前碎片化的方法。


<details>
  <summary>Details</summary>
Motivation: 解决自回归大语言模型知识蒸馏中因高维输出导致的容量差距和训练不稳定问题，现有辅助分布方法缺乏系统性研究。

Method: 提出α-混合辅助分布，引入分布设计变量α实现连续扩展，并基于最优性推广了与辅助分布使用的散度族。

Result: 实验表明AMiD通过利用更广泛且理论基础的辅助分布空间，提供了优越的性能和训练稳定性。

Conclusion: AMiD框架统一了知识蒸馏中的辅助分布方法，通过理论基础的扩展设计解决了现有方法的局限性。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable
improvement across many tasks but incur high computational and memory costs.
Knowledge distillation (KD) mitigates this issue by transferring knowledge from
a large teacher to a smaller student through distributional alignment. Previous
studies have proposed various discrepancy metrics, but the capacity gap and
training instability caused by near-zero probabilities, stemming from the
high-dimensional output of LLMs, remain fundamental limitations. To overcome
these challenges, several approaches implicitly or explicitly incorporating
assistant distribution have recently been proposed. However, the past proposals
of assistant distributions have been a fragmented approach without a systematic
investigation of the interpolation path and the divergence. This paper proposes
$\alpha$-mixture assistant distribution, a novel generalized family of
assistant distributions, and $\alpha$-mixture distillation, coined AMiD, a
unified framework for KD using the assistant distribution. The $\alpha$-mixture
assistant distribution provides a continuous extension of the assistant
distribution by introducing a new distribution design variable $\alpha$, which
has been fixed in all previous approaches. Furthermore, AMiD generalizes the
family of divergences used with the assistant distributions based on
optimality, which has also been restricted in previous works. Through extensive
experiments, we demonstrate that AMiD offers superior performance and training
stability by leveraging a broader and theoretically grounded assistant
distribution space.

</details>


### [331] [Narrowing Action Choices with AI Improves Human Sequential Decisions](https://arxiv.org/abs/2510.16097)
*Eleni Straitouri,Stratis Tsirtsis,Ander Artola Velasco,Manuel Gomez-Rodriguez*

Main category: cs.LG

TL;DR: 开发了一个决策支持系统，通过预训练的AI代理缩小人类可采取的行动范围，实现人机互补性，在野火缓解游戏中使参与者表现提升约30%。


<details>
  <summary>Details</summary>
Motivation: 探索是否能在顺序决策任务中实现人机互补性，让专家使用系统时比单独使用AI或人类专家表现更好。

Method: 使用预训练AI代理缩小人类可采取的行动子集，然后让人类从该子集中选择行动，并引入利用动作集平滑特性的多臂老虎机算法来优化人类代理水平。

Result: 在1600人参与的野火缓解游戏研究中，使用系统的参与者比单独参与者表现提升约30%，比AI代理表现提升超过2%，尽管AI代理本身显著优于无支持的参与者。

Conclusion: 通过设计控制人类代理水平的方法可以在顺序决策任务中实现人机互补性，系统显著提升了人类决策表现。

Abstract: Recent work has shown that, in classification tasks, it is possible to design
decision support systems that do not require human experts to understand when
to cede agency to a classifier or when to exercise their own agency to achieve
complementarity$\unicode{x2014}$experts using these systems make more accurate
predictions than those made by the experts or the classifier alone. The key
principle underpinning these systems reduces to adaptively controlling the
level of human agency, by design. Can we use the same principle to achieve
complementarity in sequential decision making tasks? In this paper, we answer
this question affirmatively. We develop a decision support system that uses a
pre-trained AI agent to narrow down the set of actions a human can take to a
subset, and then asks the human to take an action from this action set. Along
the way, we also introduce a bandit algorithm that leverages the smoothness
properties of the action sets provided by our system to efficiently optimize
the level of human agency. To evaluate our decision support system, we conduct
a large-scale human subject study ($n = 1{,}600$) where participants play a
wildfire mitigation game. We find that participants who play the game supported
by our system outperform those who play on their own by $\sim$$30$% and the AI
agent used by our system by $>$$2$%, even though the AI agent largely
outperforms participants playing without support. We have made available the
data gathered in our human subject study as well as an open source
implementation of our system at
https://github.com/Networks-Learning/narrowing-action-choices .

</details>


### [332] [MEET-Sepsis: Multi-Endogenous-View Enhanced Time-Series Representation Learning for Early Sepsis Prediction Representation Learning for Early Sepsis Prediction](https://arxiv.org/abs/2510.15985)
*Zexi Tan,Tao Xie,Binbin Sun,Xiang Zhang,Yiqun Zhang,Yiu-Ming Cheung*

Main category: cs.LG

TL;DR: 提出MEET-Sepsis框架，通过多内源视图表示增强机制和级联双卷积时间注意力模块，仅需20%ICU监测时间即可实现竞争性的脓毒症预测准确率


<details>
  <summary>Details</summary>
Motivation: 脓毒症在ICU中死亡率高，早期准确预测对及时干预至关重要。现有AI方法难以捕捉微弱的早期时间信号

Method: 采用多内源视图表示增强机制构建丰富特征视图，结合级联双卷积时间注意力模块进行多尺度时间表示学习

Result: 仅需SOTA方法20%的ICU监测时间即可达到竞争性预测准确率，显著推进早期脓毒症预测

Conclusion: MEET-Sepsis框架在广泛验证中证实了其有效性，显著提升了早期脓毒症预测能力

Abstract: Sepsis is a life-threatening infectious syndrome associated with high
mortality in intensive care units (ICUs). Early and accurate sepsis prediction
(SP) is critical for timely intervention, yet remains challenging due to subtle
early manifestations and rapidly escalating mortality. While AI has improved SP
efficiency, existing methods struggle to capture weak early temporal signals.
This paper introduces a Multi-Endogenous-view Representation Enhancement (MERE)
mechanism to construct enriched feature views, coupled with a Cascaded
Dual-convolution Time-series Attention (CDTA) module for multi-scale temporal
representation learning. The proposed MEET-Sepsis framework achieves
competitive prediction accuracy using only 20% of the ICU monitoring time
required by SOTA methods, significantly advancing early SP. Extensive
validation confirms its efficacy. Code is available at:
https://github.com/yueliangy/MEET-Sepsis.

</details>


### [333] [A Minimal-Assumption Analysis of Q-Learning with Time-Varying Policies](https://arxiv.org/abs/2510.16132)
*Phalguni Nanda,Zaiwei Chen*

Main category: cs.LG

TL;DR: 本文首次对时变学习策略下的Q-learning算法进行了有限时间分析，仅需马尔可夫链不可约的基本假设，证明了期望无穷范数平方的收敛速率，样本复杂度为O(1/ε²)，与离策略Q-learning匹配但在探索参数上依赖更差。


<details>
  <summary>Details</summary>
Motivation: 现有Q-learning分析大多基于固定采样策略（离策略），而实际应用中学习策略是时变的（在策略），这种时变策略带来的快速时非齐次马尔可夫噪声给分析带来重大挑战。

Method: 采用改进方法，利用泊松方程将对应惰性转移矩阵的马尔可夫噪声分解为鞅差项和残差项，通过泊松方程解对Q函数估计和学习策略的敏感性分析来控制时非齐次性下的残差项。

Result: 建立了期望无穷范数平方的最后迭代收敛速率，样本复杂度为O(1/ε²)，发现在策略Q-learning探索能力较弱但具有利用优势，其策略会收敛到最优策略而非保持固定。

Conclusion: 在策略Q-learning在探索上弱于离策略版本但在利用上有优势，所发展的分析工具可进一步用于分析具有快速时变学习策略的一般强化学习算法。

Abstract: In this work, we present the first finite-time analysis of the Q-learning
algorithm under time-varying learning policies (i.e., on-policy sampling) with
minimal assumptions -- specifically, assuming only the existence of a policy
that induces an irreducible Markov chain over the state space. We establish a
last-iterate convergence rate for $\mathbb{E}[\|Q_k - Q^*\|_\infty^2]$,
implying a sample complexity of order $O(1/\epsilon^2)$ for achieving
$\mathbb{E}[\|Q_k - Q^*\|_\infty] \le \epsilon$, matching that of off-policy
Q-learning but with a worse dependence on exploration-related parameters. We
also derive an explicit rate for $\mathbb{E}[\|Q^{\pi_k} - Q^*\|_\infty^2]$,
where $\pi_k$ is the learning policy at iteration $k$. These results reveal
that on-policy Q-learning exhibits weaker exploration than its off-policy
counterpart but enjoys an exploitation advantage, as its policy converges to an
optimal one rather than remaining fixed. Numerical simulations corroborate our
theory.
  Technically, the combination of time-varying learning policies (which induce
rapidly time-inhomogeneous Markovian noise) and the minimal assumption on
exploration presents significant analytical challenges. To address these
challenges, we employ a refined approach that leverages the Poisson equation to
decompose the Markovian noise corresponding to the lazy transition matrix into
a martingale-difference term and residual terms. To control the residual terms
under time inhomogeneity, we perform a sensitivity analysis of the Poisson
equation solution with respect to both the Q-function estimate and the learning
policy. These tools may further facilitate the analysis of general
reinforcement learning algorithms with rapidly time-varying learning policies
-- such as single-timescale actor--critic methods and learning-in-games
algorithms -- and are of independent interest.

</details>


### [334] [User Profiles of Sleep Disorder Sufferers: Towards Explainable Clustering and Differential Variable Analysis](https://arxiv.org/abs/2510.15986)
*Sifeddine Sellami,Juba Agoun,Lamia Yessad,Louenas Bounia*

Main category: cs.LG

TL;DR: 提出基于聚类的可解释AI方法，用于根据睡眠障碍特征对患者进行分组，并识别影响病理的关键因素。


<details>
  <summary>Details</summary>
Motivation: 睡眠障碍对患者健康和生活质量有重大影响，但由于症状多样性，诊断复杂。技术进步和医疗数据分析为更好理解这些障碍提供了新视角。

Method: 采用基于聚类的可解释人工智能方法，将患者按不同睡眠障碍特征进行分组，并整合可解释性方法识别关键影响因素。

Result: 在匿名真实数据上的实验证明了该方法的有效性和相关性。

Conclusion: 该方法能够有效识别睡眠障碍患者的不同特征群组，并通过可解释AI揭示影响病理的关键因素，为睡眠障碍诊断提供了新的技术途径。

Abstract: Sleep disorders have a major impact on patients' health and quality of life,
but their diagnosis remains complex due to the diversity of symptoms. Today,
technological advances, combined with medical data analysis, are opening new
perspectives for a better understanding of these disorders. In particular,
explainable artificial intelligence (XAI) aims to make AI model decisions
understandable and interpretable for users. In this study, we propose a
clustering-based method to group patients according to different sleep disorder
profiles. By integrating an explainable approach, we identify the key factors
influencing these pathologies. An experiment on anonymized real data
illustrates the effectiveness and relevance of our approach.

</details>


### [335] [Expert Merging in Sparse Mixture of Experts with Nash Bargaining](https://arxiv.org/abs/2510.16138)
*Dung V. Nguyen,Anh T. Nguyen,Minh H. Nguyen,Luc Q. Nguyen,Shiqi Jiang,Ethan Fetaya,Linh Duy Tran,Gal Chechik,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 提出了NAMEx框架，将纳什议价引入专家合并过程，通过博弈论视角实现更平衡高效的专家协作，并在多个任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏专家混合模型的专家合并策略缺乏原则性的加权机制，需要更平衡高效的专家协作方法。

Method: 基于博弈论视角，引入纳什议价到专家合并过程，并加入复动量来加速专家传播，提供理论收敛保证。

Result: 在语言建模、文本分类、图像分类和数据损坏下的零样本鲁棒性等任务中，NAMEx始终优于竞争方法，并能无缝集成到流行的MoE架构中。

Conclusion: NAMEx在大规模系统中（如Qwen1.5-MoE和DeepSeek-MoE）表现出良好的可扩展性，在零样本和微调设置中都有效。

Abstract: Existing expert merging strategies for Sparse Mixture of Experts (SMoE)
typically rely on input-dependent or input-independent averaging of expert
parameters, but often lack a principled weighting mechanism. In this work, we
reinterpret expert merging through the lens of game theory, revealing
cooperative and competitive dynamics among experts. Based on this perspective,
we introduce Nash Merging of Experts (NAMEx), a novel framework that
incorporates Nash Bargaining into the merging process, enabling more balanced
and efficient collaboration among experts. Additionally, we incorporate complex
momentum into NAMEx to accelerate expert propagation with theoretical
guarantees for convergence. Extensive experiments across language modelling,
text classification, image classification, and zero-shot robustness under data
corruption show that NAMEx consistently outperforms competing methods while
integrating seamlessly with popular MoE architectures. Finally, we demonstrate
NAMEx's scalability by applying it to large-scale systems, including
Qwen1.5-MoE (14B) and DeepSeek-MoE (16B), where it proves effective in both
zero-shot and fine-tuning settings.

</details>


### [336] [Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models](https://arxiv.org/abs/2510.15987)
*Samuel Lippl,Thomas McGee,Kimberly Lopez,Ziwen Pan,Pierce Zhang,Salma Ziadi,Oliver Eberle,Ida Momennejad*

Main category: cs.LG

TL;DR: 该论文提出了一个框架来追踪和操控大语言模型中的算法原语，揭示了模型推理背后的几何逻辑和组合性质。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型如何通过潜在计算和推理时间计算解决多步推理问题，理解模型内部的算法原语如何支持复杂推理。

Method: 通过聚类神经激活并将它们与推理轨迹匹配来操作化算法原语，使用函数向量方法推导可重用的推理构建块，并通过向量操作研究它们的组合性质。

Result: 发现算法原语可以在任务间和模型间转移，推理微调增强了算法的泛化能力，Phi-4-Reasoning比基础版本更系统地使用验证和路径生成原语。

Conclusion: LLM的推理可能由算法原语的组合几何支持，这些原语具有跨任务和跨模型的可转移性，推理微调能增强跨领域的算法泛化能力。

Abstract: How do latent and inference time computations enable large language models
(LLMs) to solve multi-step reasoning? We introduce a framework for tracing and
steering algorithmic primitives that underlie model reasoning. Our approach
links reasoning traces to internal activation patterns and evaluates
algorithmic primitives by injecting them into residual streams and measuring
their effect on reasoning steps and task performance. We consider four
benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph
navigation. We operationalize primitives by clustering neural activations and
labeling their matched reasoning traces. We then apply function vector methods
to derive primitive vectors as reusable compositional building blocks of
reasoning. Primitive vectors can be combined through addition, subtraction, and
scalar operations, revealing a geometric logic in activation space. Cross-task
and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both
shared and task-specific primitives. Notably, comparing Phi-4 with its
reasoning-finetuned variant highlights compositional generalization after
finetuning: Phi-4-Reasoning exhibits more systematic use of verification and
path-generation primitives. Injecting the associated primitive vectors in
Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning.
Together, these findings demonstrate that reasoning in LLMs may be supported by
a compositional geometry of algorithmic primitives, that primitives transfer
cross-task and cross-model, and that reasoning finetuning strengthens
algorithmic generalization across domains.

</details>


### [337] [Zeroth-Order Sharpness-Aware Learning with Exponential Tilting](https://arxiv.org/abs/2510.16157)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 本文提出了一种结合零阶优化和锐度感知最小化的新方法，通过指数倾斜目标在平均损失和最大损失之间平滑过渡，实现了更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统零阶优化方法优化平滑后的函数期望值，而SAM方法关注邻域内最大损失以获得平坦最小值。本文旨在连接这两种方法，开发更有效的优化框架。

Method: 提出指数倾斜目标，通过倾斜参数t在平均损失和最大损失之间平滑过渡，开发新的零阶算法来求解软SAM目标。

Result: 该方法在分类、多项选择问答和语言生成等下游任务上比传统零阶基线方法获得更好的泛化性能。

Conclusion: 该方法可作为SAM变体的无梯度和内存高效替代方案，在多种任务中表现出优越的泛化能力。

Abstract: Classic zeroth-order optimization approaches typically optimize for a
smoothed version of the original function, i.e., the expected objective under
randomly perturbed model parameters. This can be interpreted as encouraging the
loss values in the perturbation set to be small on average. Popular
sharpness-aware minimization (SAM) objectives, however, typically focus on the
largest loss within the neighborhood to arrive at flat minima more effectively.
In this work, we connect zeroth-order optimization (and its corresponding
objectives) with SAM approaches explicitly, through an exponential tilting
objective that provides a smooth transition between the average- and the
max-loss formulations. We explore new zeroth-order algorithms to solve a soft
SAM objective parameterized by a tilting parameter $t$. We provide precise
characterizations of the sharpness notions of the tilted SAM framework.
Practically, our approach can be used as a gradient-free and memory-efficient
alternative to SAM variants, and it achieves better generalization compared to
vanilla zeroth-order baselines on a wide range of downstream tasks, including
classification, multiple choice QA, and language generation.

</details>


### [338] [Can GRPO Help LLMs Transcend Their Pretraining Origin?](https://arxiv.org/abs/2510.15990)
*Kangqi Ni,Zhen Tan,Zijie Liu,Pingzhi Li,Tianlong Chen*

Main category: cs.LG

TL;DR: GRPO算法作为RLVR的核心方法，虽然能提升LLM的推理能力，但其效果不一致且受限于预训练分布，无法发现全新解决方案，只能强化预训练偏见。


<details>
  <summary>Details</summary>
Motivation: 研究GRPO在什么条件下能提升推理能力并实现分布外泛化，解释其效果不一致的原因。

Method: 从数据分布角度进行理论分析，证明GRPO是保守的重加权方案，并通过从零训练transformer进行控制实验，评估在推理深度、输入长度、token表示和组合性等方面的泛化能力。

Result: GRPO的分布外改进仅在目标任务与模型预训练偏见一致时出现，而分布内任务的增益会随着性能饱和而减弱。

Conclusion: GRPO不是通用的推理增强器，而是强化预训练偏见的工具，需要开发能扩展模型能力超越预训练起源的新算法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by
the Group Relative Policy Optimization (GRPO) algorithm, is a leading approach
for enhancing the reasoning abilities of Large Language Models (LLMs). Despite
its wide adoption, GRPO's gains are often inconsistent; for instance, a model
may show significant improvement in one reasoning domain, like mathematics, yet
remain stagnant in another, such as medicine. This inconsistency raises a
critical question: under what conditions does GRPO improve reasoning and
generalize out-of-distribution (OOD)? We investigate this from a data
distribution perspective. We first prove theoretically that GRPO is a
conservative reweighting scheme, bounded by the base model's distribution and
thus unable to discover completely novel solutions. We further validate this in
carefully designed controlled studies by training transformers from scratch,
evaluating generalization across reasoning depth, input length, token
representation, and compositionality. Our results provide a principled
explanation for GRPO's boundaries: OOD improvement emerges only when the target
task aligns with the model's pretrained biases, while gains on in-distribution
(ID) tasks diminish as performance saturates. This reframes GRPO not as a
universal reasoning enhancer but as a tool that sharpens pretraining biases.
Our findings motivate future development of algorithms that can expand a
model's capabilities beyond its pretraining origin.

</details>


### [339] [Learning a Generalized Model for Substation Level Voltage Estimation in Distribution Networks](https://arxiv.org/abs/2510.16063)
*Muhy Eddin Za'ter,Bri-Mathias Hodge*

Main category: cs.LG

TL;DR: 提出了一种用于变电站级电压估计的分层图神经网络，该网络利用电气拓扑和物理特征，在低观测性条件下保持鲁棒性，在SMART-DS数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源渗透和配电网电压波动增加，传统配电网状态估计方法难以应对稀疏测量和大规模网络，需要开发更鲁棒和可扩展的解决方案。

Method: 采用分层图神经网络，结合电气拓扑和物理特征进行电压估计，专门针对实际配电网中常见的低观测性条件设计。

Result: 在数千个总线和多个变电站场景下的实验表明，该方法比替代数据驱动模型的RMSE降低高达2倍，在仅1%测量覆盖率下仍保持高精度。

Conclusion: 图神经网络为实现可扩展、可复现和数据驱动的配电网电压监测提供了潜力，能够有效应对现代配电网的挑战。

Abstract: Accurate voltage estimation in distribution networks is critical for
real-time monitoring and increasing the reliability of the grid. As DER
penetration and distribution level voltage variability increase, robust
distribution system state estimation (DSSE) has become more essential to
maintain safe and efficient operations. Traditional DSSE techniques, however,
struggle with sparse measurements and the scale of modern feeders, limiting
their scalability to large networks. This paper presents a hierarchical graph
neural network for substation-level voltage estimation that exploits both
electrical topology and physical features, while remaining robust to the low
observability levels common to real-world distribution networks. Leveraging the
public SMART-DS datasets, the model is trained and evaluated on thousands of
buses across multiple substations and DER penetration scenarios. Comprehensive
experiments demonstrate that the proposed method achieves up to 2 times lower
RMSE than alternative data-driven models, and maintains high accuracy with as
little as 1\% measurement coverage. The results highlight the potential of GNNs
to enable scalable, reproducible, and data-driven voltage monitoring for
distribution systems.

</details>


### [340] [Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction](https://arxiv.org/abs/2510.16161)
*Ankitkumar Joshi,Milos Hauskrecht*

Main category: cs.LG

TL;DR: GRUwE：基于GRU的简单高效模型，用于不规则采样多变量时间序列预测，在性能和计算效率上优于复杂SOTA方法


<details>
  <summary>Details</summary>
Motivation: 解决不规则采样时间序列建模挑战，验证简单RNN架构是否仍能竞争甚至优于复杂方法

Method: GRUwE：门控循环单元加指数基函数，通过观测触发重置和时间触发重置两种机制更新马尔可夫状态

Result: 在多个真实世界基准测试中，GRUwE在下一观测和下一事件预测任务上达到竞争性甚至优于SOTA的性能

Conclusion: GRUwE证明简单高效的RNN架构仍具竞争力，易于实现、调参少、计算开销低，适合在线部署

Abstract: Modeling irregularly sampled multivariate time series is a persistent
challenge in domains like healthcare and sensor networks. While recent works
have explored a variety of complex learning architectures to solve the
prediction problems for irregularly sampled time series, it remains unclear
what are the true benefits of some of these architectures, and whether clever
modifications of simpler and more efficient RNN-based algorithms are still
competitive, i.e. they are on par with or even superior to these methods. In
this work, we propose and study GRUwE: Gated Recurrent Unit with Exponential
basis functions, that builds upon RNN-based architectures for observations made
at irregular times. GRUwE supports both regression-based and event-based
predictions in continuous time. GRUwE works by maintaining a Markov state
representation of the time series that updates with the arrival of irregular
observations. The Markov state update relies on two reset mechanisms: (i)
observation-triggered reset, and (ii) time-triggered reset of the GRU state
using learnable exponential decays, to support the predictions in continuous
time. Our empirical evaluations across several real-world benchmarks on
next-observation and next-event prediction tasks demonstrate that GRUwE can
indeed achieve competitive to superior performance compared to the recent
state-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers
compelling advantages: it is easy to implement, requires minimal
hyper-parameter tuning efforts, and significantly reduces the computational
overhead in the online deployment.

</details>


### [341] [Stratos: An End-to-End Distillation Pipeline for Customized LLMs under Distributed Cloud Environments](https://arxiv.org/abs/2510.15992)
*Ziming Dai,Tuo Zhang,Fei Gao,Xingyi Cai,Xiaofei Wang,Cheng Zhang,Wenyu Wang,Chengjie Zang*

Main category: cs.LG

TL;DR: Stratos是一个端到端的LLM蒸馏管道，能自动选择服务器和模型、进行知识蒸馏以及在分布式云环境中部署，满足用户定义的模型性能和系统预算约束。


<details>
  <summary>Details</summary>
Motivation: 工业界对定制化和成本效益高的大型语言模型需求增长，现有蒸馏框架需要人工干预且难以满足复杂的用户定义蒸馏需求。

Method: Stratos自动选择Pareto最优服务器，动态匹配师生对，并根据任务复杂度调整蒸馏策略以优化云托管。

Result: 在罕见的麻将推理任务上，Stratos生成的学生模型准确率比GPT-4o教师基线高出四倍，同时降低了延迟和成本而不影响准确性。

Conclusion: Stratos展示了在垂直领域LLM部署中的潜力，能够高效满足定制化需求。

Abstract: The growing industrial demand for customized and cost-efficient large
language models (LLMs) is fueled by the rise of vertical, domain-specific tasks
and the need to optimize performance under constraints such as latency and
budget. Knowledge distillation, as an efficient model compression and transfer
technique, offers a feasible solution. However, existing distillation
frameworks often require manual intervention and struggle to meet such complex
user-defined distillation requirements. To bridge this gap, we propose Stratos,
an end-to-end LLM distillation pipeline that automates server and model
selection, knowledge distillation, and deployment in distributed cloud
environments. Given user-defined constraints on model performance and system
budget, Stratos automatically selects Pareto-optimal servers, dynamically
matches teacher-student pairs, and adapts distillation strategies based on task
complexity to optimize cloud hosting. Experiments show that Stratos produces a
student model that achieves four times the accuracy of its GPT-4o teacher
baseline on a rare, domain-specific Mahjong reasoning task with reverse
synthetic data and knowledge injection. Moreover, it achieves reduced latency
and cost without compromising accuracy. These results highlight its promise for
vertical-domain LLM deployment.

</details>


### [342] [Residual Correction Models for AC Optimal Power Flow Using DC Optimal Power Flow Solutions](https://arxiv.org/abs/2510.16064)
*Muhy Eddin Za'ter,Bri-Mathias Hodge,Kyri Baker*

Main category: cs.LG

TL;DR: 提出一种基于残差学习的AC最优潮流求解方法，使用DC OPF解作为基线，学习非线性修正项，实现快速准确的AC OPF求解。


<details>
  <summary>Details</summary>
Motivation: 传统非线性AC最优潮流计算是电网实时运行的主要计算瓶颈，需要更高效的求解方法。

Method: 采用拓扑感知图神经网络，结合局部注意力和两级DC特征集成，使用物理信息损失函数确保AC潮流可行性和运行限制。

Result: 在57、118和2000母线系统测试中，MSE降低约25%，可行性误差减少3倍，运行速度提升13倍，且在N-1故障下保持准确性。

Conclusion: 残差学习是连接线性近似与AC可行OPF的实用可扩展桥梁，可实现近实时运行决策。

Abstract: Solving the nonlinear AC optimal power flow (AC OPF) problem remains a major
computational bottleneck for real-time grid operations. In this paper, we
propose a residual learning paradigm that uses fast DC optimal power flow (DC
OPF) solutions as a baseline, and learns only the nonlinear corrections
required to provide the full AC-OPF solution. The method utilizes a
topology-aware Graph Neural Network with local attention and two-level DC
feature integration, trained using a physics-informed loss that enforces AC
power-flow feasibility and operational limits. Evaluations on OPFData for 57-,
118-, and 2000-bus systems show around 25% lower MSE, up to 3X reduction in
feasibility error, and up to 13X runtime speedup compared to conventional AC
OPF solvers. The model maintains accuracy under N-1 contingencies and scales
efficiently to large networks. These results demonstrate that residual learning
is a practical and scalable bridge between linear approximations and
AC-feasible OPF, enabling near real-time operational decision making.

</details>


### [343] [Expressive Reward Synthesis with the Runtime Monitoring Language](https://arxiv.org/abs/2510.16185)
*Daniel Donnelly,Angelo Ferrando,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 本文提出了一种基于运行时监控语言(RML)的新型语言奖励机，能够表达非正则、非马尔可夫任务的奖励函数，解决了传统奖励机表达能力受限的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的奖励函数通常被视为黑盒映射，缺乏解释性，而传统奖励机只能表达正则语言，无法处理计数或参数化条件等复杂行为。

Method: 利用RML的内置内存机制构建语言奖励机，扩展了奖励函数的表达能力，支持非正则、非马尔可夫任务的规范。

Result: 实验证明该方法在表达能力上优于现有奖励机方法，在事件处理和任务规范方面具有额外优势。

Conclusion: 基于RML的语言奖励机为复杂强化学习任务提供了更强大和灵活的奖励函数规范方法。

Abstract: A key challenge in reinforcement learning (RL) is reward (mis)specification,
whereby imprecisely defined reward functions can result in unintended, possibly
harmful, behaviours. Indeed, reward functions in RL are typically treated as
black-box mappings from state-action pairs to scalar values. While effective in
many settings, this approach provides no information about why rewards are
given, which can hinder learning and interpretability. Reward Machines address
this issue by representing reward functions as finite state automata, enabling
the specification of structured, non-Markovian reward functions. However, their
expressivity is typically bounded by regular languages, leaving them unable to
capture more complex behaviours such as counting or parametrised conditions. In
this work, we build on the Runtime Monitoring Language (RML) to develop a novel
class of language-based Reward Machines. By leveraging the built-in memory of
RML, our approach can specify reward functions for non-regular, non-Markovian
tasks. We demonstrate the expressiveness of our approach through experiments,
highlighting additional advantages in flexible event-handling and task
specification over existing Reward Machine-based methods.

</details>


### [344] [Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning](https://arxiv.org/abs/2510.15996)
*Ozan K. Tonguz,Federico Taschin*

Main category: cs.LG

TL;DR: 该论文提出使用Kolmogorov-Smirnov检验来监测和量化机器学习系统中的分布偏移问题，特别是在智能交通系统中，即使很小的KS距离（0.02）也可能导致强化学习代理性能显著下降（旅行时间增加50%）。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习系统中训练数据与测试数据概率分布不一致的问题，这种分布偏移会导致预测误差，在安全关键应用中尤为严重。

Method: 使用Kolmogorov-Smirnov检验来测量分布偏移，通过KS距离量化分布变化及其对AI代理性能的影响。

Result: 研究表明，即使KS距离仅为0.02，在单交叉口使用强化学习代理时也会导致旅行时间增加约50%，影响显著。

Conclusion: KS检验和KS距离可作为实时监测AI代理性能退化的有价值统计工具，有助于AI系统更有效地应对分布偏移问题。

Abstract: One of the major problems in Machine Learning (ML) and Artificial
Intelligence (AI) is the fact that the probability distribution of the test
data in the real world could deviate substantially from the probability
distribution of the training data set. When this happens, the predictions of an
ML system or an AI agent could involve large errors which is very troublesome
and undesirable. While this is a well-known hard problem plaguing the AI and ML
systems' accuracy and reliability, in certain applications such errors could be
critical for safety and reliability of AI and ML systems. One approach to deal
with this problem is to monitor and measure the deviation in the probability
distribution of the test data in real time and to compensate for this
deviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov
(KS) Test for measuring the distribution shift and we show how the KS distance
can be used to quantify the distribution shift and its impact on an AI agent's
performance. Our results suggest that KS distance could be used as a valuable
statistical tool for monitoring and measuring the distribution shift. More
specifically, it is shown that even a distance of KS=0.02 could lead to about
50\% increase in the travel time at a single intersection using a Reinforcement
Learning agent which is quite significant. It is hoped that the use of KS Test
and KS distance in AI-based smart transportation could be an important step
forward for gauging the performance degradation of an AI agent in real time and
this, in turn, could help the AI agent to cope with the distribution shift in a
more informed manner.

</details>


### [345] [Explore-then-Commit for Nonstationary Linear Bandits with Latent Dynamics](https://arxiv.org/abs/2510.16208)
*Sunmook Choi,Yahya Sattar,Yassir Jedra,Maryam Fazel,Sarah Dean*

Main category: cs.LG

TL;DR: 提出了一种针对非平稳多臂老虎机问题的探索-提交算法，该问题中奖励取决于动作和潜在状态，状态动态受未知线性动态系统控制。算法通过随机探索估计系统参数，然后优化长期奖励，实现了$\tilde{\mathcal{O}}(T^{2/3})$的遗憾上界。


<details>
  <summary>Details</summary>
Motivation: 研究非平稳老虎机问题，其中奖励不仅依赖于动作，还受潜在状态影响，状态动态本身又受动作影响，导致短期奖励与长期奖励之间的权衡问题。

Method: 采用探索-提交算法：探索阶段使用随机Rademacher动作估计线性动态系统的马尔可夫参数；提交阶段基于估计参数设计优化动作序列以最大化长期奖励。

Result: 算法实现了$\tilde{\mathcal{O}}(T^{2/3})$的遗憾上界，解决了时序相关奖励学习和长期奖励优化两个关键挑战。

Conclusion: 该工作为非平稳老虎机问题提供了有效的解决方案，通过系统识别和优化理论相结合的方法，在理论上保证了性能，并提出了实用的半定规划松弛方法。

Abstract: We study a nonstationary bandit problem where rewards depend on both actions
and latent states, the latter governed by unknown linear dynamics. Crucially,
the state dynamics also depend on the actions, resulting in tension between
short-term and long-term rewards. We propose an explore-then-commit algorithm
for a finite horizon $T$. During the exploration phase, random Rademacher
actions enable estimation of the Markov parameters of the linear dynamics,
which characterize the action-reward relationship. In the commit phase, the
algorithm uses the estimated parameters to design an optimized action sequence
for long-term reward. Our proposed algorithm achieves
$\tilde{\mathcal{O}}(T^{2/3})$ regret. Our analysis handles two key challenges:
learning from temporally correlated rewards, and designing action sequences
with optimal long-term reward. We address the first challenge by providing
near-optimal sample complexity and error bounds for system identification using
bilinear rewards. We address the second challenge by proving an equivalence
with indefinite quadratic optimization over a hypercube, a known NP-hard
problem. We provide a sub-optimality guarantee for this problem, enabling our
regret upper bound. Lastly, we propose a semidefinite relaxation with
Goemans-Williamson rounding as a practical approach.

</details>


### [346] [AMStraMGRAM: Adaptive Multi-cutoff Strategy Modification for ANaGRAM](https://arxiv.org/abs/2510.15998)
*Nilo Schwencke,Cyriaque Rousselot,Alena Shilova,Cyril Furtlehner*

Main category: cs.LG

TL;DR: 分析了使用ANaGRAM自然梯度方法训练PINNs的训练动态，提出了多截止自适应策略来提升性能，在基准PDE上达到机器精度，并建立了基于谱理论的理论框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明自然梯度方法在训练物理信息神经网络(PINNs)时显著优于标准优化器，需要深入分析其训练动态并进一步提升性能。

Method: 使用ANaGRAM方法（基于奇异值分解和截止正则化的自然梯度方法），提出多截止自适应策略，并建立基于谱理论的分析框架。

Result: 在基准PDE实验中验证了方法的有效性，部分实验达到了机器精度，性能优于现有方法。

Conclusion: 多截止自适应策略显著提升了ANaGRAM的性能，谱理论框架为正则化的必要性提供了理论依据，并扩展了与格林函数理论的联系。

Abstract: Recent works have shown that natural gradient methods can significantly
outperform standard optimizers when training physics-informed neural networks
(PINNs). In this paper, we analyze the training dynamics of PINNs optimized
with ANaGRAM, a natural-gradient-inspired approach employing singular value
decomposition with cutoff regularization. Building on this analysis, we propose
a multi-cutoff adaptation strategy that further enhances ANaGRAM's performance.
Experiments on benchmark PDEs validate the effectiveness of our method, which
allows to reach machine precision on some experiments. To provide theoretical
grounding, we develop a framework based on spectral theory that explains the
necessity of regularization and extend previous shown connections with Green's
functions theory.

</details>


### [347] [Benchmarking noisy label detection methods](https://arxiv.org/abs/2510.16211)
*Henrique Pickler,Jorge K. S. Kamassury,Danilo Silva*

Main category: cs.LG

TL;DR: 本文对标签噪声检测方法进行了全面基准测试，通过将方法分解为三个基本组件（标签一致性函数、聚合方法和信息收集方式）来系统比较不同方法，发现在多数场景下使用平均概率聚合结合logit边界作为标签一致性函数的方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集中普遍存在标签噪声问题，影响模型训练和验证效果。虽然已有多种噪声标签检测技术，但缺乏明确的最佳方法共识，需要系统性的比较研究。

Method: 将标签噪声检测方法分解为三个基本组件进行系统比较，提出统一的基准任务（检测与数据集噪声率相等的训练样本比例）和新评估指标（固定操作点下的假阴性率）。

Result: 在视觉和表格数据集上的评估表明，使用平均概率聚合结合logit边界作为标签一致性函数的方法在大多数场景下表现最佳。

Conclusion: 研究结果为设计新的检测方法和为特定应用选择技术提供了实用指导，识别出了最有效的检测方法组合。

Abstract: Label noise is a common problem in real-world datasets, affecting both model
training and validation. Clean data are essential for achieving strong
performance and ensuring reliable evaluation. While various techniques have
been proposed to detect noisy labels, there is no clear consensus on optimal
approaches. We perform a comprehensive benchmark of detection methods by
decomposing them into three fundamental components: label agreement function,
aggregation method, and information gathering approach (in-sample vs
out-of-sample). This decomposition can be applied to many existing detection
methods, and enables systematic comparison across diverse approaches. To fairly
compare methods, we propose a unified benchmark task, detecting a fraction of
training samples equal to the dataset's noise rate. We also introduce a novel
metric: the false negative rate at this fixed operating point. Our evaluation
spans vision and tabular datasets under both synthetic and real-world noise
conditions. We identify that in-sample information gathering using average
probability aggregation combined with the logit margin as the label agreement
function achieves the best results across most scenarios. Our findings provide
practical guidance for designing new detection methods and selecting techniques
for specific applications.

</details>


### [348] [Layer-Aware Influence for Online Data Valuation Estimation](https://arxiv.org/abs/2510.16007)
*Ziao Yang,Longbo Huang,Hongfu Liu*

Main category: cs.LG

TL;DR: 提出了一种层感知在线估计器，用于高效估计训练样本在优化过程中的动态影响，避免参数级和全网络梯度计算，显著降低时间和内存成本。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注在收敛模型上测量的静态影响，忽略了数据价值在优化过程中的动态变化特性，特别是在深度模型中。

Method: 开发层感知在线估计器，仅需要损失到输出的梯度，避免参数级和全网络梯度计算，同时保持排序保真度。

Result: 在LLM预训练、微调和图像分类等广泛实验中，该方法提高了准确性，同时大幅降低了时间和内存成本。

Conclusion: 该方法使动态数据筛选在实践中变得高效且可扩展，为数据中心学习提供了实用的解决方案。

Abstract: Data-centric learning emphasizes curating high-quality training samples to
boost performance rather than designing new architectures. A central problem is
to estimate the influence of training sample efficiently. Prior studies largely
focus on static influence measured on a converged model, overlooking how data
valuation dynamically changes during optimization. This omission neglects the
dynamic nature of sample influence during optimization, especially in deep
models. To address the computational burden of frequent influence estimation,
we develop a layer-aware online estimator that requires only loss-to-output
gradients. This design avoids parameter-level and full-network gradients while
preserving ranking fidelity. Extensive experiments across LLM pretraining,
fine-tuning, and image classification show our method improves accuracy with
substantially lower time and memory cost, making dynamic data curation
efficient and scalable in practice.

</details>


### [349] [One-Bit Quantization for Random Features Models](https://arxiv.org/abs/2510.16250)
*Danil Akhtiamov,Reza Ghane,Babak Hassibi*

Main category: cs.LG

TL;DR: 该论文分析了神经网络中一比特权重压缩的理论基础，证明在随机特征模型中，除最后一层外的所有权重量化不会导致泛化误差损失，并验证了实际推理加速效果。


<details>
  <summary>Details</summary>
Motivation: 神经网络的计算和内存需求激增，促使研究者关注一比特权重压缩以在资源受限设备上实现高效推理，但相关理论基础尚不完善。

Method: 在随机特征模型（对应具有随机表示的神经网络）中分析一比特量化，通过理论证明和实验验证相结合的方法。

Result: 理论证明：除最后一层外所有权重量化在渐近意义上不会损失泛化误差；实证验证：一比特量化在笔记本电脑GPU上显著加速随机特征模型推理。

Conclusion: 一比特权重压缩在理论上可行且在实践中有效，为神经网络压缩提供了理论洞察和实用价值。

Abstract: Recent advances in neural networks have led to significant computational and
memory demands, spurring interest in one-bit weight compression to enable
efficient inference on resource-constrained devices. However, the theoretical
underpinnings of such compression remain poorly understood. We address this gap
by analyzing one-bit quantization in the Random Features model, a simplified
framework that corresponds to neural networks with random representations. We
prove that, asymptotically, quantizing weights of all layers except the last
incurs no loss in generalization error, compared to the full precision random
features model. Our findings offer theoretical insights into neural network
compression. We also demonstrate empirically that one-bit quantization leads to
significant inference speed ups for the Random Features models even on a laptop
GPU, confirming the practical benefits of our work. Additionally, we provide an
asymptotically precise characterization of the generalization error for Random
Features with an arbitrary number of layers. To the best of our knowledge, our
analysis yields more general results than all previous works in the related
literature.

</details>


### [350] [STAR: Boosting Time Series Foundation Models for Anomaly Detection through State-aware Adapter](https://arxiv.org/abs/2510.16014)
*Hanyin Cheng,Ruitong Zhang,Yuning Lu,Peng Chen,Meng Wang,Yang Shu,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: 提出STAR模块来增强时间序列基础模型对状态变量的建模能力，解决现有方法忽视状态变量分类特性导致性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现实工业场景中时间序列包含数值变量和离散状态变量，现有时间序列基础模型通常忽视状态变量的分类特性，将其与数值变量统一处理，导致无法充分利用状态信息甚至性能下降。

Method: STAR包含三个核心组件：身份引导的状态编码器（通过可学习状态记忆捕获状态变量的分类语义）、条件瓶颈适配器（根据当前状态动态生成低秩适配参数）、数值-状态匹配模块（检测状态变量本身的异常）。

Result: 在真实数据集上的广泛实验表明，STAR能够提升现有时间序列基础模型在多变量时间序列异常检测中的性能。

Conclusion: STAR作为即插即用模块，能够有效增强时间序列基础模型对状态变量的建模和利用能力，提升异常检测性能。

Abstract: While Time Series Foundation Models (TSFMs) have demonstrated remarkable
success in Multivariate Time Series Anomaly Detection (MTSAD), however, in
real-world industrial scenarios, many time series comprise not only numerical
variables such as temperature and flow, but also numerous discrete state
variables that describe the system status, such as valve on/off or day of the
week. Existing TSFMs often overlook the distinct categorical nature of state
variables and their critical role as conditions, typically treating them
uniformly with numerical variables. This inappropriate modeling approach
prevents the model from fully leveraging state information and even leads to a
significant degradation in detection performance after state variables are
integrated. To address this critical limitation, this paper proposes a novel
STate-aware AdapteR (STAR). STAR is a plug-and-play module designed to enhance
the capability of TSFMs in modeling and leveraging state variables during the
fine-tuning stage. Specifically, STAR comprisesthree core components: (1) We
design an Identity-guided State Encoder, whicheffectively captures the complex
categorical semantics of state variables through a learnable State Memory. (2)
We propose a Conditional Bottleneck Adapter, which dynamically generates
low-rank adaptation parameters conditioned on the current state, thereby
flexibly injecting the influence of state variables into the backbone model.
(3) We also introduce a Numeral-State Matching module to more effectively
detect anomalies inherent to the state variables themselves. Extensive
experiments conducted on real-world datasets demonstrate that STAR can improve
the performance of existing TSFMs on MTSAD.

</details>


### [351] [Protein Folding with Neural Ordinary Differential Equations](https://arxiv.org/abs/2510.16253)
*Arielle Sanford,Shuo Sun,Christian B. Mendl*

Main category: cs.LG

TL;DR: 提出基于神经常微分方程的连续深度Evoformer，替代AlphaFold中48个离散块，实现恒定内存成本和计算效率提升


<details>
  <summary>Details</summary>
Motivation: 传统Evoformer的48层深度结构计算成本高且层间离散化，需要更轻量高效的替代方案

Method: 使用神经ODE参数化Evoformer，保持核心注意力操作，通过伴随方法实现恒定内存成本，利用自适应ODE求解器平衡运行时间与精度

Result: 在蛋白质结构预测任务中产生结构合理的预测，可靠捕捉α-螺旋等二级结构元素，但精度未完全达到原始架构水平

Conclusion: 连续深度模型作为轻量可解释替代方案在生物分子建模中具有潜力，单GPU仅需17.5小时训练，为高效自适应蛋白质结构预测开辟新方向

Abstract: Recent advances in protein structure prediction, such as AlphaFold, have
demonstrated the power of deep neural architectures like the Evoformer for
capturing complex spatial and evolutionary constraints on protein conformation.
However, the depth of the Evoformer, comprising 48 stacked blocks, introduces
high computational costs and rigid layerwise discretization. Inspired by Neural
Ordinary Differential Equations (Neural ODEs), we propose a continuous-depth
formulation of the Evoformer, replacing its 48 discrete blocks with a Neural
ODE parameterization that preserves its core attention-based operations. This
continuous-time Evoformer achieves constant memory cost (in depth) via the
adjoint method, while allowing a principled trade-off between runtime and
accuracy through adaptive ODE solvers. Benchmarking on protein structure
prediction tasks, we find that the Neural ODE-based Evoformer produces
structurally plausible predictions and reliably captures certain secondary
structure elements, such as alpha-helices, though it does not fully replicate
the accuracy of the original architecture. However, our model achieves this
performance using dramatically fewer resources, just 17.5 hours of training on
a single GPU, highlighting the promise of continuous-depth models as a
lightweight and interpretable alternative for biomolecular modeling. This work
opens new directions for efficient and adaptive protein structure prediction
frameworks.

</details>


### [352] [Decision-focused Sensing and Forecasting for Adaptive and Rapid Flood Response: An Implicit Learning Approach](https://arxiv.org/abs/2510.16015)
*Qian Sun,Graham Hults,Susu Xu*

Main category: cs.LG

TL;DR: 提出了一种决策导向的洪水管理框架，通过端到端优化传感器部署和洪水预测模型，直接最小化下游应急决策的遗憾值，而非传统的任务无关优化目标。


<details>
  <summary>Details</summary>
Motivation: 传统洪水管理系统采用固定、任务无关的策略部署传感器和训练预测模型，忽视了相同感知增益和预测误差可能导致不同决策结果的问题，需要将决策优化直接整合到系统设计中。

Method: 端到端框架包含四个组件：上下文评分网络、预算约束下的可微分传感器选择模块、时空洪水重建预测模型、以及针对特定任务目标的可微分决策层，采用隐式最大似然估计实现离散传感器配置的梯度学习。

Result: 该方法能够战略性地选择传感器位置并优化洪水预测模型，直接针对下游洪水响应决策进行优化，提高了决策的及时性和可靠性。

Conclusion: 决策导向的框架通过端到端优化传感器部署和预测模型，显著提升了洪水应急响应的决策质量，解决了传统方法中感知与决策脱节的问题。

Abstract: Timely and reliable decision-making is vital for flood emergency response,
yet it remains severely hindered by limited and imprecise situational awareness
due to various budget and data accessibility constraints. Traditional flood
management systems often rely on in-situ sensors to calibrate remote
sensing-based large-scale flood depth forecasting models, and further take
flood depth estimates to optimize flood response decisions. However, these
approaches often take fixed, decision task-agnostic strategies to decide where
to put in-situ sensors (e.g., maximize overall information gain) and train
flood forecasting models (e.g., minimize average forecasting errors), but
overlook that systems with the same sensing gain and average forecasting errors
may lead to distinct decisions. To address this, we introduce a novel
decision-focused framework that strategically selects locations for in-situ
sensor placement and optimize spatio-temporal flood forecasting models to
optimize downstream flood response decision regrets. Our end-to-end pipeline
integrates four components: a contextual scoring network, a differentiable
sensor selection module under hard budget constraints, a spatio-temporal flood
reconstruction and forecasting model, and a differentiable decision layer
tailored to task-specific objectives. Central to our approach is the
incorporation of Implicit Maximum Likelihood Estimation (I-MLE) to enable
gradient-based learning over discrete sensor configurations, and probabilistic
decision heads to enable differentiable approximation to various constrained
disaster response tasks.

</details>


### [353] [Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior](https://arxiv.org/abs/2510.16356)
*Fuqun Han,Stanley Osher,Wuchen Li*

Main category: cs.LG

TL;DR: 提出了一种稀疏transformer架构，将数据分布的先验信息直接融入神经网络结构，基于正则化Wasserstein近端算子设计，相比传统流模型改善了优化问题的凸性并促进生成样本的稀疏性。


<details>
  <summary>Details</summary>
Motivation: 将数据分布的先验信息直接整合到transformer架构中，以改善生成模型的性能，特别是针对优化问题的凸性和样本稀疏性。

Method: 基于正则化Wasserstein近端算子设计稀疏transformer架构，该算子具有闭式解且表现为transformer的特殊表示形式。

Result: 在生成建模和贝叶斯逆问题应用中，稀疏transformer比传统基于神经ODE的方法具有更高精度和更快的收敛速度。

Conclusion: 所提出的稀疏transformer架构在理论和实验上都优于传统方法，为生成模型提供了更有效的解决方案。

Abstract: In this work, we propose a sparse transformer architecture that incorporates
prior information about the underlying data distribution directly into the
transformer structure of the neural network. The design of the model is
motivated by a special optimal transport problem, namely the regularized
Wasserstein proximal operator, which admits a closed-form solution and turns
out to be a special representation of transformer architectures. Compared with
classical flow-based models, the proposed approach improves the convexity
properties of the optimization problem and promotes sparsity in the generated
samples. Through both theoretical analysis and numerical experiments, including
applications in generative modeling and Bayesian inverse problems, we
demonstrate that the sparse transformer achieves higher accuracy and faster
convergence to the target distribution than classical neural ODE-based methods.

</details>


### [354] [Transfer learning strategies for accelerating reinforcement-learning-based flow control](https://arxiv.org/abs/2510.16016)
*Saeed Salehi*

Main category: cs.LG

TL;DR: 研究探讨了使用渐进神经网络和微调策略来加速深度强化学习在多保真度混沌流体控制中的知识迁移，发现渐进神经网络在保持先前知识和稳定迁移方面优于传统微调方法。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在流体控制中训练成本高昂，需要探索有效的迁移学习策略来加速高保真度环境中的学习过程，解决微调方法存在的灾难性遗忘和敏感性问题。

Method: 采用渐进神经网络架构，首次在基于DRL的流控制中应用这种模块化设计，同时系统比较传统微调策略的性能、收敛行为和知识保持能力，以Kuramoto-Sivashinsky系统为基准测试平台。

Result: 渐进神经网络通过保留先验知识实现稳定高效的知识迁移，对预训练阶段的过拟合具有鲁棒性；而微调虽然能加速收敛，但对预训练时长敏感且容易发生灾难性遗忘。

Conclusion: 渐进神经网络框架在源环境和目标环境差异较大时仍能有效工作，为稳健、可扩展和计算高效的流控制提供了有前景的解决方案，可应用于更复杂的流动配置。

Abstract: This work investigates transfer learning strategies to accelerate deep
reinforcement learning (DRL) for multifidelity control of chaotic fluid flows.
Progressive neural networks (PNNs), a modular architecture designed to preserve
and reuse knowledge across tasks, are employed for the first time in the
context of DRL-based flow control. In addition, a comprehensive benchmarking of
conventional fine-tuning strategies is conducted, evaluating their performance,
convergence behavior, and ability to retain transferred knowledge. The
Kuramoto-Sivashinsky (KS) system is employed as a benchmark to examine how
knowledge encoded in control policies, trained in low-fidelity environments,
can be effectively transferred to high-fidelity settings. Systematic
evaluations show that while fine-tuning can accelerate convergence, it is
highly sensitive to pretraining duration and prone to catastrophic forgetting.
In contrast, PNNs enable stable and efficient transfer by preserving prior
knowledge and providing consistent performance gains, and are notably robust to
overfitting during the pretraining phase. Layer-wise sensitivity analysis
further reveals how PNNs dynamically reuse intermediate representations from
the source policy while progressively adapting deeper layers to the target
task. Moreover, PNNs remain effective even when the source and target
environments differ substantially, such as in cases with mismatched physical
regimes or control objectives, where fine-tuning strategies often result in
suboptimal adaptation or complete failure of knowledge transfer. The results
highlight the potential of novel transfer learning frameworks for robust,
scalable, and computationally efficient flow control that can potentially be
applied to more complex flow configurations.

</details>


### [355] [Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making](https://arxiv.org/abs/2510.16462)
*Emmanuelle Claeys,Elena Kerjean,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 提出一个序列强化学习框架用于授粉昆虫的模仿学习，能够建模异质认知策略，解决现有方法在专家策略变化时无法捕捉快速和慢速学习行为的问题。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法在处理授粉昆虫（如蜜蜂）的异质认知策略时存在局限，当专家策略随记忆窗口变化或偏离最优时，这些模型无法准确捕捉行为模式且缺乏可解释性。

Method: 引入一个最小化预测损失的模型，识别与行为数据最一致的有效记忆范围，确保完全可解释性，并提供将蜜蜂策略搜索与多臂老虎机问题联系起来的数学框架。

Result: 创建了包含80只蜜蜂在不同天气条件下追踪的新数据集，该基准促进了授粉昆虫认知研究，并通过改进农业生态系统中的昆虫行为模拟来支持生态治理。

Conclusion: 该研究为理解授粉昆虫的学习策略和记忆相互作用提供了新的见解，有助于分析基础决策策略并改善生态系统的管理。

Abstract: We introduce a sequential reinforcement learning framework for imitation
learning designed to model heterogeneous cognitive strategies in pollinators.
Focusing on honeybees, our approach leverages trajectory similarity to capture
and forecast behavior across individuals that rely on distinct strategies: some
exploiting numerical cues, others drawing on memory, or being influenced by
environmental factors such as weather. Through empirical evaluation, we show
that state-of-the-art imitation learning methods often fail in this setting:
when expert policies shift across memory windows or deviate from optimality,
these models overlook both fast and slow learning behaviors and cannot
faithfully reproduce key decision patterns. Moreover, they offer limited
interpretability, hindering biological insight. Our contribution addresses
these challenges by (i) introducing a model that minimizes predictive loss
while identifying the effective memory horizon most consistent with behavioral
data, and (ii) ensuring full interpretability to enable biologists to analyze
underlying decision-making strategies and finally (iii) providing a
mathematical framework linking bee policy search with bandit formulations under
varying exploration-exploitation dynamics, and releasing a novel dataset of 80
tracked bees observed under diverse weather conditions. This benchmark
facilitates research on pollinator cognition and supports ecological governance
by improving simulations of insect behavior in agroecosystems. Our findings
shed new light on the learning strategies and memory interplay shaping
pollinator decision-making.

</details>


### [356] [Airfoil optimization using Design-by-Morphing with minimized design-space dimensionality](https://arxiv.org/abs/2510.16020)
*Sangjoon Lee,Haris Moazam Sheikh*

Main category: cs.LG

TL;DR: AirDbM是一种专门用于翼型优化的设计变形方法，通过从1600多个翼型数据库中选择12个最优基线翼型，显著降低设计空间维度，在保持高重建精度的同时实现更快的多目标优化收敛。


<details>
  <summary>Details</summary>
Motivation: 翼型几何优化需要探索多样化的设计，同时尽可能减少设计变量数量。传统方法需要较多基线翼型，而AirDbM旨在用更少的基线实现更好的性能。

Method: 从UIUC翼型数据库中选择12个最优基线翼型，通过设计变形方法重建数据库，并在多目标气动优化和强化学习中进行验证。

Result: 用12个基线重建了99%的数据库，平均绝对误差低于0.005；在多目标优化中实现了更大的超体积和更快的收敛；在强化学习中表现出比传统参数化方法更好的适应性。

Conclusion: AirDbM通过减少设计变量有效提升了翼型优化效率，在机器学习和优化设计中具有广泛应用潜力。

Abstract: Effective airfoil geometry optimization requires exploring a diverse range of
designs using as few design variables as possible. This study introduces
AirDbM, a Design-by-Morphing (DbM) approach specialized for airfoil
optimization that systematically reduces design-space dimensionality. AirDbM
selects an optimal set of 12 baseline airfoils from the UIUC airfoil database,
which contains over 1,600 shapes, by sequentially adding the baseline that most
increases the design capacity. With these baselines, AirDbM reconstructs 99 \%
of the database with a mean absolute error below 0.005, which matches the
performance of a previous DbM approach that used more baselines. In
multi-objective aerodynamic optimization, AirDbM demonstrates rapid convergence
and achieves a Pareto front with a greater hypervolume than that of the
previous larger-baseline study, where new Pareto-optimal solutions are
discovered with enhanced lift-to-drag ratios at moderate stall tolerances.
Furthermore, AirDbM demonstrates outstanding adaptability for reinforcement
learning (RL) agents in generating airfoil geometry when compared to
conventional airfoil parameterization methods, implying the broader potential
of DbM in machine learning-driven design.

</details>


### [357] [Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems](https://arxiv.org/abs/2510.17276)
*Rishi Jha,Harold Triedman,Justin Wagle,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: 本文提出ControlValve防御机制，通过生成允许的控制流图并强制执行，解决多智能体系统中的控制流劫持攻击问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于对齐检查的防御机制（如LlamaFirewall）无法有效抵御控制流劫持攻击，因为安全性和功能性目标存在根本冲突，且对齐定义脆弱、检查器对执行上下文可见性不完整。

Method: ControlValve基于控制流完整性和最小权限原则，生成多智能体系统的允许控制流图，并强制执行所有执行符合这些图以及每个智能体调用的上下文规则（以零样本方式生成）。

Result: ControlValve能够有效防御规避现有对齐检查机制的控制流劫持攻击。

Conclusion: ControlValve提供了一种新的防御方法，通过控制流完整性原则解决多智能体系统中的安全挑战，比基于对齐检查的方法更有效。

Abstract: Control-flow hijacking attacks manipulate orchestration mechanisms in
multi-agent systems into performing unsafe actions that compromise the system
and exfiltrate sensitive information. Recently proposed defenses, such as
LlamaFirewall, rely on alignment checks of inter-agent communications to ensure
that all agent invocations are "related to" and "likely to further" the
original objective.
  We start by demonstrating control-flow hijacking attacks that evade these
defenses even if alignment checks are performed by advanced LLMs. We argue that
the safety and functionality objectives of multi-agent systems fundamentally
conflict with each other. This conflict is exacerbated by the brittle
definitions of "alignment" and the checkers' incomplete visibility into the
execution context.
  We then propose, implement, and evaluate ControlValve, a new defense inspired
by the principles of control-flow integrity and least privilege. ControlValve
(1) generates permitted control-flow graphs for multi-agent systems, and (2)
enforces that all executions comply with these graphs, along with contextual
rules (generated in a zero-shot manner) for each agent invocation.

</details>


### [358] [Structured Temporal Causality for Interpretable Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2510.16511)
*Dongchan Cho,Jiho Han,Keumyeong Kang,Minsang Kim,Honggyu Ryu,Namsoon Jung*

Main category: cs.LG

TL;DR: OracleAD是一个简单可解释的无监督多变量时间序列异常检测框架，通过因果嵌入和稳定潜在结构来检测异常并定位根因变量。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多变量时间序列异常稀少且通常无标签，现有方法依赖复杂架构且只能检测异常片段，性能被夸大。

Method: 将每个变量的过去序列编码为因果嵌入来预测当前时间点并重建输入窗口，使用自注意力机制投影到共享潜在空间，将投影嵌入对齐到代表正常状态关系的稳定潜在结构(SLS)。

Result: 在多个真实世界数据集和评估协议上实现了最先进的结果。

Conclusion: OracleAD不仅检测性能优越，还通过SLS保持可解释性，能够在嵌入层面直接定位根因变量。

Abstract: Real-world multivariate time series anomalies are rare and often unlabeled.
Additionally, prevailing methods rely on increasingly complex architectures
tuned to benchmarks, detecting only fragments of anomalous segments and
overstating performance. In this paper, we introduce OracleAD, a simple and
interpretable unsupervised framework for multivariate time series anomaly
detection. OracleAD encodes each variable's past sequence into a single causal
embedding to jointly predict the present time point and reconstruct the input
window, effectively modeling temporal dynamics. These embeddings then undergo a
self-attention mechanism to project them into a shared latent space and capture
spatial relationships. These relationships are not static, since they are
modeled by a property that emerges from each variable's temporal dynamics. The
projected embeddings are aligned to a Stable Latent Structure (SLS)
representing normal-state relationships. Anomalies are identified using a dual
scoring mechanism based on prediction error and deviation from the SLS,
enabling fine-grained anomaly diagnosis at each time point and across
individual variables. Since any noticeable SLS deviation originates from
embeddings that violate the learned temporal causality of normal data, OracleAD
directly pinpoints the root-cause variables at the embedding level. OracleAD
achieves state-of-the-art results across multiple real-world datasets and
evaluation protocols, while remaining interpretable through SLS.

</details>


### [359] [Latent Spaces Beyond Synthesis: From GANs to Diffusion Models](https://arxiv.org/abs/2510.17383)
*Ludovica Schaerf*

Main category: cs.LG

TL;DR: 该论文分析了生成视觉模型中内部表征的演变，区分了严格意义上的合成（紧凑潜在空间决定生成过程）和广义合成（表征工作分布在多个层中），通过实验证明扩散模型如何分散表征负担并挑战统一内部空间的假设。


<details>
  <summary>Details</summary>
Motivation: 研究生成视觉模型内部表征的演变，从GANs和VAEs到扩散架构的概念和技术转变，探讨表征如何在不同模型架构中分布和组合。

Method: 通过模型架构的详细分析和针对性的实验设置，干预层间表征，研究扩散模型中表征的分布方式。

Result: 扩散模型将表征负担分散到多个层中，挑战了统一内部空间的假设，表明生成过程是通过专门化过程的涌现配置实现的。

Conclusion: 生成AI应被理解为专门化过程的涌现配置，而非内容的直接合成，这需要对潜在空间和柏拉图表征假设等隐喻进行重新定位。

Abstract: This paper examines the evolving nature of internal representations in
generative visual models, focusing on the conceptual and technical shift from
GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's
account of synthesis as the amalgamation of distributed representations, we
propose a distinction between "synthesis in a strict sense", where a compact
latent space wholly determines the generative process, and "synthesis in a
broad sense," which characterizes models whose representational labor is
distributed across layers. Through close readings of model architectures and a
targeted experimental setup that intervenes in layerwise representations, we
show how diffusion models fragment the burden of representation and thereby
challenge assumptions of unified internal space. By situating these findings
within media theoretical frameworks and critically engaging with metaphors such
as the latent space and the Platonic Representation Hypothesis, we argue for a
reorientation of how generative AI is understood: not as a direct synthesis of
content, but as an emergent configuration of specialized processes.

</details>


### [360] [LSTM-Based Forecasting and Analysis of EV Charging Demand in a Dense Urban Campus](https://arxiv.org/abs/2510.16719)
*Zak Ressler,Marcus Grijalva,Angelica Marie Ignacio,Melanie Torres,Abelardo Cuadra Rojas,Rohollah Moghadam,Mohammad Rasoul narimani*

Main category: cs.LG

TL;DR: 提出基于LSTM的电动汽车充电负荷预测框架，通过数据预处理和特征提取来准确预测多时间尺度的充电需求


<details>
  <summary>Details</summary>
Motivation: 为电动汽车充电基础设施规划、能源管理和电网集成提供准确的负荷预测，解决充电需求波动带来的挑战

Method: 使用LSTM循环神经网络，通过数据预处理（插值和归一化）、特征提取来训练模型，捕捉短期波动和长期趋势

Result: 模型能够在日、周、月等多个时间尺度上准确预测充电需求，模块化设计可适应不同充电站点的使用模式

Conclusion: 该框架为电动汽车充电设施的规划和运营提供了有效的预测工具，具有广泛的应用前景

Abstract: This paper presents a framework for processing EV charging load data in order
to forecast future load predictions using a Recurrent Neural Network,
specifically an LSTM. The framework processes a large set of raw data from
multiple locations and transforms it with normalization and feature extraction
to train the LSTM. The pre-processing stage corrects for missing or incomplete
values by interpolating and normalizing the measurements. This information is
then fed into a Long Short-Term Memory Model designed to capture the short-term
fluctuations while also interpreting the long-term trends in the charging data.
Experimental results demonstrate the model's ability to accurately predict
charging demand across multiple time scales (daily, weekly, and monthly),
providing valuable insights for infrastructure planning, energy management, and
grid integration of EV charging facilities. The system's modular design allows
for adaptation to different charging locations with varying usage patterns,
making it applicable across diverse deployment scenarios.

</details>


### [361] [eDCF: Estimating Intrinsic Dimension using Local Connectivity](https://arxiv.org/abs/2510.16513)
*Dhruv Gupta,Aditya Nagarsekar,Vraj Shah,Sujith Thomas*

Main category: cs.LG

TL;DR: 提出了一种基于连通性因子(CF)的eDCF方法，用于在不同尺度下稳健估计高维数据的本征维度，在噪声环境下表现优异，并能准确检测分形几何结构。


<details>
  <summary>Details</summary>
Motivation: 现代数据集通常包含具有复杂依赖关系的高维特征，而本征维度估计面临尺度依赖的挑战：精细尺度下噪声会膨胀估计值，粗尺度下估计值会稳定到较低的尺度不变值。

Method: 基于连通性因子(CF)这一局部连通性度量，开发了可扩展且可并行化的eDCF方法，能够跨不同尺度稳健估计本征维度。

Result: 在合成基准测试中与领先估计器表现相当，MAE值相近；在中等至高噪声水平和大数据集下，精确本征维度匹配率高达25.0%，优于MLE的16.7%和TWO-NN的12.5%。

Conclusion: eDCF方法能够准确检测决策边界中的分形几何结构，证实了其在分析现实结构化数据方面的实用性。

Abstract: Modern datasets often contain high-dimensional features exhibiting complex
dependencies. To effectively analyze such data, dimensionality reduction
methods rely on estimating the dataset's intrinsic dimension (id) as a measure
of its underlying complexity. However, estimating id is challenging due to its
dependence on scale: at very fine scales, noise inflates id estimates, while at
coarser scales, estimates stabilize to lower, scale-invariant values. This
paper introduces a novel, scalable, and parallelizable method called eDCF,
which is based on Connectivity Factor (CF), a local connectivity-based metric,
to robustly estimate intrinsic dimension across varying scales. Our method
consistently matches leading estimators, achieving comparable values of mean
absolute error (MAE) on synthetic benchmarks with noisy samples. Moreover, our
approach also attains higher exact intrinsic dimension match rates, reaching up
to 25.0% compared to 16.7% for MLE and 12.5% for TWO-NN, particularly excelling
under medium to high noise levels and large datasets. Further, we showcase our
method's ability to accurately detect fractal geometries in decision
boundaries, confirming its utility for analyzing realistic, structured data.

</details>


### [362] [MuonBP: Faster Muon via Block-Periodic Orthogonalization](https://arxiv.org/abs/2510.16981)
*Ahmed Khaled,Kaan Ozkara,Tao Yu,Mingyi Hong,Youngsuk Park*

Main category: cs.LG

TL;DR: MuonBP通过块周期正交化优化梯度正交化方法，在模型并行训练中减少通信开销，实现与AdamW相当的吞吐量同时保持训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决Muon优化器在模型并行训练中因梯度正交化引入的额外通信开销问题，该开销可达5%-10%吞吐量损失。

Method: 提出块周期正交化方法：在各设备上独立对矩阵分片进行正交化，并周期性执行完整正交化以保持训练稳定性。使用两个学习率分别对应块正交化和完整正交化步骤。

Result: 在8B模型训练中，使用8路张量并行和ZeRO优化器状态分片，MuonBP相比Muon实现8%吞吐量提升且性能无下降。

Conclusion: MuonBP在保持Muon优化器数据效率优势的同时，显著减少了模型并行训练中的通信开销，实现了吞吐量与训练稳定性的平衡。

Abstract: Gradient orthogonalization is a simple strategy that shows great utility in
speeding up gradient descent. The Muon optimizer (Jordan, Jin, et al., 2024)
combines gradient orthogonalization with first-order momentum and achieves
significant improvement in data efficiency over Adam/AdamW (Loshchilov and
Hutter, 2019) for language model training. However, when using model
parallelism, gradient orthogonalization introduces additional overhead compared
to coordinate-wise optimizers (such as AdamW) due to additional gather and
scatter operations on gradient matrix shards from different devices. This
additional communication can amount to a throughput hit of 5%-10% compared to
Adam/AdamW. To remedy this, we propose Muon with Block-Periodic
Orthogonalization (MuonBP), which applies orthogonalization independently to
matrix shards on each device and periodically performs full orthogonalization
to maintain training stability at scale. We show how to adjust the learning
rate from the baseline to MuonBP and give convergence guarantees for this
algorithm. Crucially, our theory dictates that we use two stepsizes: one for
the blockwise orthogonalization steps, and one for the full orthogonalization
steps. Our method is simple, requires minimal hyperparameter adjustments, and
achieves competitive iteration complexity compared with baseline Muon while
providing per-iteration throughput comparable to coordinate-wise methods such
as AdamW. When training an 8B model with eight-way tensor parallelism and ZeRO
optimizer state sharding, MuonBP achieves 8% throughput increase compared to
Muon with no degradation in performance.

</details>


### [363] [Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization](https://arxiv.org/abs/2510.16022)
*Changsheng Wang,Xin Chen,Sijia Liu,Ke Ding*

Main category: cs.LG

TL;DR: 论文提出IB-FT方法，通过信息瓶颈指导微调，解决大语言模型在代码生成中的记忆障碍问题，提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 发现预训练大语言模型在代码生成任务中存在记忆障碍问题，即模型过度记忆下游代码数据，阻碍其学习新的可泛化代码知识。

Method: 提出IB-FT方法，在代码数据的隐藏表示上应用信息瓶颈惩罚，压缩虚假记忆特征，同时保留任务相关信息。

Result: 在两个代码基准测试(OriGen和Evol-CodeAlpaca-V1)上，IB-FT显著缓解记忆障碍，提高top-1性能，并在更严格的多样本指标下获得更稳定的增益。

Conclusion: IB-FT方法有效克服了传统微调中的记忆障碍问题，提升了代码生成模型的泛化能力和性能稳定性。

Abstract: Adapting pretrained large language models (LLMs) to code domains via
supervised fine-tuning (FT) has been commonly used for code generation.
However, we identify a previously underappreciated failure mode, the
memorization barrier, where strong memorization of downstream code data in the
base model could trap optimization and prevent the standard FT from effectively
acquiring new, generalizable code knowledge. To overcome this barrier, we
propose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which
applies an IB penalty on hidden representations of the code data to compress
spurious, memorized features while preserving task-relevant information.
Extensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1)
show that IB-FT substantially alleviates the memorization barrier, improves
top-1 performance (Pass@$1$), and yields far more stable gains under the
stricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if
at least $m$ of $k$ samples pass unit tests) compared with conventional FT.

</details>


### [364] [Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks](https://arxiv.org/abs/2510.16530)
*Ashutosh Srivastava,Lokesh Nagalapatti,Gautam Jajoo,Aniket Vashishtha,Parameswari Krishnamurthy,Amit Sharma*

Main category: cs.LG

TL;DR: LLMs在因果发现中的表现被高估，因为评估基准可能已包含在预训练数据中。需要开发防泄漏的评估协议和结合LLM知识与统计方法的混合方法。


<details>
  <summary>Details</summary>
Motivation: 挑战LLMs在因果发现中的真实能力，质疑现有评估方法因数据泄漏问题而不可靠，需要更严格的评估和实用的混合方法。

Method: 提出基于近期科学研究的评估协议防止数据泄漏，设计混合方法将LLM预测作为经典PC算法的先验知识。

Result: 在BNLearn基准上LLMs表现接近完美，但在新构建的真实科学图谱上表现较差；将LLM预测作为PC算法先验可显著提高准确性。

Conclusion: 呼吁社区采用基于科学、防泄漏的基准测试，并投资于适合真实世界研究的混合因果发现方法。

Abstract: Recent claims of strong performance by Large Language Models (LLMs) on causal
discovery are undermined by a key flaw: many evaluations rely on benchmarks
likely included in pretraining corpora. Thus, apparent success suggests that
LLM-only methods, which ignore observational data, outperform classical
statistical approaches. We challenge this narrative by asking: Do LLMs truly
reason about causal structure, and how can we measure it without memorization
concerns? Can they be trusted for real-world scientific discovery? We argue
that realizing LLMs' potential for causal analysis requires two shifts: (P.1)
developing robust evaluation protocols based on recent scientific studies to
guard against dataset leakage, and (P.2) designing hybrid methods that combine
LLM-derived knowledge with data-driven statistics. To address P.1, we encourage
evaluating discovery methods on novel, real-world scientific studies. We
outline a practical recipe for extracting causal graphs from recent
publications released after an LLM's training cutoff, ensuring relevance and
preventing memorization while capturing both established and novel relations.
Compared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy,
they perform far worse on our curated graphs, underscoring the need for
statistical grounding. Supporting P.2, we show that using LLM predictions as
priors for the classical PC algorithm significantly improves accuracy over both
LLM-only and purely statistical methods. We call on the community to adopt
science-grounded, leakage-resistant benchmarks and invest in hybrid causal
discovery methods suited to real-world inquiry.

</details>


### [365] [Unifying Polymer Modeling and Design via a Conformation-Centric Generative Foundation Model](https://arxiv.org/abs/2510.16023)
*Fanmeng Wang,Shan Mei,Wentao Guo,Hongshuai Wang,Qi Ou,Zhifeng Gao,Hongteng Xu*

Main category: cs.LG

TL;DR: 提出了PolyConFM，首个基于构象的聚合物基础模型，通过掩码自回归建模统一聚合物建模与设计，在多种下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法仅使用单体级描述符表示聚合物，忽略了构象中的全局结构信息，限制了实际性能，且缺乏支持多样化下游任务的通用基础模型。

Method: 将聚合物构象分解为局部构象序列，通过掩码自回归建模进行条件生成预训练，重建局部构象并生成方向变换来恢复完整聚合物构象，同时构建首个高质量聚合物构象数据集。

Result: PolyConFM在多样化下游任务中持续优于代表性的任务特定方法。

Conclusion: PolyConFM为聚合物科学提供了一个通用且强大的工具，通过构象中心的预训练统一了聚合物建模与设计。

Abstract: Polymers, macromolecules formed from covalently bonded monomers, underpin
countless technologies and are indispensable to modern life. While deep
learning is advancing polymer science, existing methods typically represent the
whole polymer solely through monomer-level descriptors, overlooking the global
structural information inherent in polymer conformations, which ultimately
limits their practical performance. Moreover, this field still lacks a
universal foundation model that can effectively support diverse downstream
tasks, thereby severely constraining progress. To address these challenges, we
introduce PolyConFM, the first polymer foundation model that unifies polymer
modeling and design through conformation-centric generative pretraining.
Recognizing that each polymer conformation can be decomposed into a sequence of
local conformations (i.e., those of its repeating units), we pretrain PolyConFM
under the conditional generation paradigm, reconstructing these local
conformations via masked autoregressive (MAR) modeling and further generating
their orientation transformations to recover the corresponding polymer
conformation. Besides, we construct the first high-quality polymer conformation
dataset via molecular dynamics simulations to mitigate data sparsity, thereby
enabling conformation-centric pretraining. Experiments demonstrate that
PolyConFM consistently outperforms representative task-specific methods on
diverse downstream tasks, equipping polymer science with a universal and
powerful tool.

</details>


### [366] [Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations](https://arxiv.org/abs/2510.16591)
*Cassidy Ashworth,Pietro Liò,Francesco Caso*

Main category: cs.LG

TL;DR: 该论文研究了神经网络中参数对称性和表达能力在泛化行为中的作用，使用中心极限定理作为测试案例，分析了多层感知机和图神经网络的对称约束与表达能力之间的竞争关系。


<details>
  <summary>Details</summary>
Motivation: 将物理对称性编码到深度学习模型中可以提高性能，研究参数对称性破坏和恢复机制作为层次学习动态的统一机制，评估参数对称性和网络表达能力在神经网络学习实空间重整化群变换时的泛化行为。

Method: 使用中心极限定理作为测试案例，考虑简单的多层感知机和图神经网络，在不同架构中改变权重对称性和激活函数，通过将CLT重新表述为累积量递归关系，并利用已有框架在MLPs中传播累积量来分析泛化行为。

Result: 研究揭示了对称约束和表达能力之间的竞争关系，过于复杂或过度约束的模型泛化性能较差，通过分析证明了某些受限MLP架构的泛化性能差，并经验验证了从MLPs到GNNs的框架扩展。

Conclusion: 这些发现为对称网络的学习动态及其在建模结构化物理变换中的局限性提供了新的见解，阐明了这些更复杂模型的内部信息处理过程。

Abstract: Deep learning models have proven enormously successful at using multiple
layers of representation to learn relevant features of structured data.
Encoding physical symmetries into these models can improve performance on
difficult tasks, and recent work has motivated the principle of parameter
symmetry breaking and restoration as a unifying mechanism underlying their
hierarchical learning dynamics. We evaluate the role of parameter symmetry and
network expressivity in the generalisation behaviour of neural networks when
learning a real-space renormalisation group (RG) transformation, using the
central limit theorem (CLT) as a test case map. We consider simple multilayer
perceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetries
and activation functions across architectures. Our results reveal a competition
between symmetry constraints and expressivity, with overly complex or
overconstrained models generalising poorly. We analytically demonstrate this
poor generalisation behaviour for certain constrained MLP architectures by
recasting the CLT as a cumulant recursion relation and making use of an
established framework to propagate cumulants through MLPs. We also empirically
validate an extension of this framework from MLPs to GNNs, elucidating the
internal information processing performed by these more complex models. These
findings offer new insight into the learning dynamics of symmetric networks and
their limitations in modelling structured physical transformations.

</details>


### [367] [Continuous Q-Score Matching: Diffusion Guided Reinforcement Learning for Continuous-Time Control](https://arxiv.org/abs/2510.17122)
*Chengxiu Hua,Jiawen Gu,Yushun Tang*

Main category: cs.LG

TL;DR: 提出了一种连续时间强化学习方法CQSM，通过鞅条件定义连续时间Q函数，并将扩散策略得分与Q函数的动作梯度关联，解决了连续时间RL中Q函数动作评估能力的长期挑战。


<details>
  <summary>Details</summary>
Motivation: 大多数现有强化学习方法都是离散时间框架，而现实世界中的控制问题往往是连续时间的。传统基于值函数的方法在连续时间设置中难以保持Q函数的动作评估能力，且依赖时间离散化。

Method: 通过鞅条件定义连续时间Q函数，利用动态规划原理将扩散策略得分与学习到的连续Q函数的动作梯度关联，提出基于得分匹配的CQSM策略改进算法。

Result: 在线性二次控制问题中提供了理论闭式解，在模拟环境中验证了方法的有效性，并与主流基线方法进行了比较。

Conclusion: CQSM方法成功解决了连续时间强化学习中保持Q函数动作评估能力的长期挑战，无需依赖时间离散化，为连续时间控制问题提供了新的解决方案。

Abstract: Reinforcement learning (RL) has achieved significant success across a wide
range of domains, however, most existing methods are formulated in discrete
time. In this work, we introduce a novel RL method for continuous-time control,
where stochastic differential equations govern state-action dynamics. Departing
from traditional value function-based approaches, our key contribution is the
characterization of continuous-time Q-functions via a martingale condition and
the linking of diffusion policy scores to the action gradient of a learned
continuous Q-function by the dynamic programming principle. This insight
motivates Continuous Q-Score Matching (CQSM), a score-based policy improvement
algorithm. Notably, our method addresses a long-standing challenge in
continuous-time RL: preserving the action-evaluation capability of Q-functions
without relying on time discretization. We further provide theoretical
closed-form solutions for linear-quadratic (LQ) control problems within our
framework. Numerical results in simulated environments demonstrate the
effectiveness of our proposed method and compare it to popular baselines.

</details>


### [368] [A tutorial on discovering and quantifying the effect of latent causal sources of multimodal EHR data](https://arxiv.org/abs/2510.16026)
*Marco Barbero-Mota,Eric V. Strobl,John M. Still,William W. Stead,Thomas A. Lasko*

Main category: cs.LG

TL;DR: 提出一个可泛化的因果机器学习流程，用于从电子健康记录中发现潜在因果源并量化其对临床结果的影响


<details>
  <summary>Details</summary>
Motivation: 处理不完善的多模态临床数据，发现大规模电子健康记录中的潜在因果源，并量化这些源对临床结果的因果效应

Method: 开发因果机器学习流程：处理多模态临床数据，分解为概率独立的潜在源，训练任务特定的因果模型来估计个体因果效应

Result: 已在两个真实世界应用中验证了该方法的有效性，展示了其在医学发现中的通用性和实用性

Conclusion: 该因果机器学习流程能够有效处理不完善的临床数据，发现潜在因果源并量化其影响，为大规模医学发现提供了实用工具

Abstract: We provide an accessible description of a peer-reviewed generalizable causal
machine learning pipeline to (i) discover latent causal sources of large-scale
electronic health records observations, and (ii) quantify the source causal
effects on clinical outcomes. We illustrate how imperfect multimodal clinical
data can be processed, decomposed into probabilistic independent latent
sources, and used to train taskspecific causal models from which individual
causal effects can be estimated. We summarize the findings of the two
real-world applications of the approach to date as a demonstration of its
versatility and utility for medical discovery at scale.

</details>


### [369] [An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning](https://arxiv.org/abs/2510.17564)
*Lindsay Spoor,Álvaro Serra-Gómez,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 该论文分析了安全强化学习中拉格朗日乘子的最优性和稳定性，发现自动更新乘子能够恢复甚至超过最优性能，但存在振荡行为，可通过PID控制缓解。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，拉格朗日方法是处理约束优化问题的常用方法，但乘子λ的选择对性能影响很大，目前缺乏对自动更新乘子鲁棒性的实证研究。

Method: 通过分析多个任务中拉格朗日乘子的最优性和稳定性，提供λ-profile可视化性能与约束成本的权衡关系，并研究自动乘子更新和PID控制方法。

Result: 发现λ具有高度敏感性，自动乘子更新能够恢复甚至超过最优性能，但存在振荡行为；PID控制可缓解振荡但需要仔细调参。

Conclusion: 拉格朗日方法在安全强化学习中需要进一步研究以提升稳定性，自动乘子更新虽有效但存在振荡问题。

Abstract: In safety-critical domains such as robotics, navigation and power systems,
constrained optimization problems arise where maximizing performance must be
carefully balanced with associated constraints. Safe reinforcement learning
provides a framework to address these challenges, with Lagrangian methods being
a popular choice. However, the effectiveness of Lagrangian methods crucially
depends on the choice of the Lagrange multiplier $\lambda$, which governs the
trade-off between return and constraint cost. A common approach is to update
the multiplier automatically during training. Although this is standard in
practice, there remains limited empirical evidence on the robustness of an
automated update and its influence on overall performance. Therefore, we
analyze (i) optimality and (ii) stability of Lagrange multipliers in safe
reinforcement learning across a range of tasks. We provide $\lambda$-profiles
that give a complete visualization of the trade-off between return and
constraint cost of the optimization problem. These profiles show the highly
sensitive nature of $\lambda$ and moreover confirm the lack of general
intuition for choosing the optimal value $\lambda^*$. Our findings additionally
show that automated multiplier updates are able to recover and sometimes even
exceed the optimal performance found at $\lambda^*$ due to the vast difference
in their learning trajectories. Furthermore, we show that automated multiplier
updates exhibit oscillatory behavior during training, which can be mitigated
through PID-controlled updates. However, this method requires careful tuning to
achieve consistently better performance across tasks. This highlights the need
for further research on stabilizing Lagrangian methods in safe reinforcement
learning. The code used to reproduce our results can be found at
https://github.com/lindsayspoor/Lagrangian_SafeRL.

</details>


### [370] [Differentially Private Linear Regression and Synthetic Data Generation with Statistical Guarantees](https://arxiv.org/abs/2510.16974)
*Shurong Lin,Aleksandra Slavković,Deekshith Reddy Bhoomireddy*

Main category: cs.LG

TL;DR: 提出一种在差分隐私下进行线性回归的方法，提供有效的统计推断和合成数据生成，适用于社会科学中的中小规模数据集。


<details>
  <summary>Details</summary>
Motivation: 社会科学中常见中小规模数据集，现有差分隐私线性回归方法主要关注点估计而缺乏不确定性量化，且当前合成数据生成方法不适合连续回归数据或需要大数据集。

Method: 使用高斯差分隐私下的偏差校正估计器，提供渐近置信区间，并通过分箱聚合策略实现合成数据生成，使合成数据上的回归与差分隐私回归结果一致。

Result: 实验表明该方法在准确性上优于现有方法，能提供有效的置信区间，且生成的合成数据在下游机器学习任务中比当前差分隐私合成数据生成方法更可靠。

Conclusion: 该方法为社会科学中的中小规模连续数据提供了有效的差分隐私线性回归解决方案，支持统计推断和合成数据生成。

Abstract: In social sciences, small- to medium-scale datasets are common and linear
regression (LR) is canonical. In privacy-aware settings, much work has focused
on differentially private (DP) LR, but mostly on point estimation with limited
attention to uncertainty quantification. Meanwhile, synthetic data generation
(SDG) is increasingly important for reproducibility studies, yet current DP LR
methods do not readily support it. Mainstream SDG approaches are either
tailored to discretized data, making them less suitable for continuous
regression, or rely on deep models that require large datasets, limiting their
use for the smaller, continuous data typical in social science. We propose a
method for LR with valid inference under Gaussian DP: a DP bias-corrected
estimator with asymptotic confidence intervals (CIs) and a general SDG
procedure in which regression on the synthetic data matches our DP regression.
Our binning-aggregation strategy is effective in small- to moderate-dimensional
settings. Experiments show our method (1) improves accuracy over existing
methods, (2) provides valid CIs, and (3) produces more reliable synthetic data
for downstream ML tasks than current DP SDGs.

</details>


### [371] [RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction](https://arxiv.org/abs/2510.16035)
*Yingguang Yang,Xianghua Zeng,Qi Wu,Hao Peng,Yutong Xia,Hao Liu,Bin Chong,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出了首个针对GNN社交机器人检测器的多智能体强化学习攻击框架RoBCtrl，使用扩散模型生成高保真机器人账户，并通过MARL模拟对抗行为，有效降低检测器性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN检测方法存在对社交代理控制有限、黑盒性质和机器人异质性等问题，其脆弱性和鲁棒性研究不足，需要开发有效的对抗攻击方法。

Method: 使用扩散模型重构现有账户数据生成高保真机器人账户；采用多智能体强化学习模拟对抗行为，按影响力和预算分类账户；设计基于结构熵的分层状态抽象加速学习。

Result: 在社交机器人检测数据集上的大量实验表明，该框架能有效削弱GNN检测器的性能。

Conclusion: RoBCtrl是首个针对GNN社交机器人检测器的多智能体强化学习攻击框架，成功展示了现有检测方法的脆弱性，为改进检测技术提供了重要参考。

Abstract: Social networks have become a crucial source of real-time information for
individuals. The influence of social bots within these platforms has garnered
considerable attention from researchers, leading to the development of numerous
detection technologies. However, the vulnerability and robustness of these
detection methods is still underexplored. Existing Graph Neural Network
(GNN)-based methods cannot be directly applied due to the issues of limited
control over social agents, the black-box nature of bot detectors, and the
heterogeneity of bots. To address these challenges, this paper proposes the
first adversarial multi-agent Reinforcement learning framework for social Bot
control attacks (RoBCtrl) targeting GNN-based social bot detectors.
Specifically, we use a diffusion model to generate high-fidelity bot accounts
by reconstructing existing account data with minor modifications, thereby
evading detection on social platforms. To the best of our knowledge, this is
the first application of diffusion models to mimic the behavior of evolving
social bots effectively. We then employ a Multi-Agent Reinforcement Learning
(MARL) method to simulate bots adversarial behavior. We categorize social
accounts based on their influence and budget. Different agents are then
employed to control bot accounts across various categories, optimizing the
attachment strategy through reinforcement learning. Additionally, a
hierarchical state abstraction based on structural entropy is designed to
accelerate the reinforcement learning. Extensive experiments on social bot
detection datasets demonstrate that our framework can effectively undermine the
performance of GNN-based detectors.

</details>


### [372] [Data Reliability Scoring](https://arxiv.org/abs/2510.17085)
*Yiling Chen,Shi Feng,Paul Kattuman,Fang-Yi Yu*

Main category: cs.LG

TL;DR: 提出了Gram行列式评分方法，用于在不依赖真实数据的情况下评估数据集的可靠性，该评分具有实验无关性，能有效反映数据质量。


<details>
  <summary>Details</summary>
Motivation: 需要在不访问真实数据的情况下评估来自潜在策略性来源的数据集的可靠性，因为真实数据不可观测，只能看到依赖于真实数据的未知统计实验结果。

Method: 定义了基于真实数据的可靠性排序，提出了Gram行列式评分方法，该方法通过计算观测数据和实验结果的经验分布向量所张成的体积来衡量可靠性。

Result: Gram行列式评分能够保持多个基于真实数据的可靠性排序，且具有实验无关性，在不同实验下产生相同的可靠性排名。在合成噪声模型、CIFAR-10嵌入和真实就业数据上的实验验证了该方法的有效性。

Conclusion: Gram行列式评分是一种有效的数据集可靠性评估方法，能够在不知道真实数据和实验过程的情况下准确反映数据质量。

Abstract: How can we assess the reliability of a dataset without access to ground
truth? We introduce the problem of reliability scoring for datasets collected
from potentially strategic sources. The true data are unobserved, but we see
outcomes of an unknown statistical experiment that depends on them. To
benchmark reliability, we define ground-truth-based orderings that capture how
much reported data deviate from the truth. We then propose the Gram determinant
score, which measures the volume spanned by vectors describing the empirical
distribution of the observed data and experiment outcomes. We show that this
score preserves several ground-truth based reliability orderings and, uniquely
up to scaling, yields the same reliability ranking of datasets regardless of
the experiment -- a property we term experiment agnosticism. Experiments on
synthetic noise models, CIFAR-10 embeddings, and real employment data
demonstrate that the Gram determinant score effectively captures data quality
across diverse observation processes.

</details>


### [373] [Vector Quantization in the Brain: Grid-like Codes in World Models](https://arxiv.org/abs/2510.16039)
*Xiangyuan Peng,Xingsi Dong,Si Wu*

Main category: cs.LG

TL;DR: 提出Grid-like Code Quantization (GCQ)方法，使用吸引子动力学中的网格状模式将观察-动作序列压缩为离散表示，实现时空联合压缩和统一世界建模。


<details>
  <summary>Details</summary>
Motivation: 传统向量量化方法处理静态输入，而GCQ旨在通过动作条件码本进行时空压缩，模拟神经系统中网格状代码的形成过程。

Method: 使用连续吸引子神经网络推导码字，基于动作动态选择码字，构建动作条件码本进行时空联合压缩。

Result: 实验表明GCQ在多种任务中有效实现紧凑编码和下游性能，支持长时程预测、目标导向规划和逆向建模。

Conclusion: GCQ既为高效序列建模提供计算工具，也为神经系统中网格状代码形成提供理论视角。

Abstract: We propose Grid-like Code Quantization (GCQ), a brain-inspired method for
compressing observation-action sequences into discrete representations using
grid-like patterns in attractor dynamics. Unlike conventional vector
quantization approaches that operate on static inputs, GCQ performs
spatiotemporal compression through an action-conditioned codebook, where
codewords are derived from continuous attractor neural networks and dynamically
selected based on actions. This enables GCQ to jointly compress space and time,
serving as a unified world model. The resulting representation supports
long-horizon prediction, goal-directed planning, and inverse modeling.
Experiments across diverse tasks demonstrate GCQ's effectiveness in compact
encoding and downstream performance. Our work offers both a computational tool
for efficient sequence modeling and a theoretical perspective on the formation
of grid-like codes in neural systems.

</details>


### [374] [Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback](https://arxiv.org/abs/2510.17103)
*Shinji Ito,Kevin Jamieson,Haipeng Luo,Arnab Maiti,Taira Tsuchiya*

Main category: cs.LG

TL;DR: 该论文提出了首个针对具有聚合bandit反馈的表格MDP的BOBW算法，在已知转移情况下实现了O(log T)的随机遗憾和O(√T)的对抗遗憾，并建立了匹配下界证明最优性。


<details>
  <summary>Details</summary>
Motivation: 研究在聚合bandit反馈模型下的在线学习问题，该模型下学习者只能观察到每个episode的累积损失而非单个状态-动作对的损失，且现有工作仅关注最坏情况分析。

Method: 结合了基于占用度量的FTRL、自边界技术和受在线最短路径问题启发的新损失估计器，并扩展到未知转移设置中采用置信度技术。

Result: 在已知转移情况下，算法在随机环境中达到O(log T)遗憾，在对抗环境中达到O(√T)遗憾，并建立了匹配下界证明最优性。

Conclusion: 成功设计了首个针对聚合bandit反馈MDP的BOBW算法，证明了其最优性，并为最短路径问题提供了首个个体间隙相关下界和近最优BOBW算法。

Abstract: We study online learning in finite-horizon episodic Markov decision processes
(MDPs) under the challenging aggregate bandit feedback model, where the learner
observes only the cumulative loss incurred in each episode, rather than
individual losses at each state-action pair. While prior work in this setting
has focused exclusively on worst-case analysis, we initiate the study of
best-of-both-worlds (BOBW) algorithms that achieve low regret in both
stochastic and adversarial environments. We propose the first BOBW algorithms
for episodic tabular MDPs with aggregate bandit feedback. In the case of known
transitions, our algorithms achieve $O(\log T)$ regret in stochastic settings
and ${O}(\sqrt{T})$ regret in adversarial ones. Importantly, we also establish
matching lower bounds, showing the optimality of our algorithms in this
setting. We further extend our approach to unknown-transition settings by
incorporating confidence-based techniques. Our results rely on a combination of
FTRL over occupancy measures, self-bounding techniques, and new loss estimators
inspired by recent advances in online shortest path problems. Along the way, we
also provide the first individual-gap-dependent lower bounds and demonstrate
near-optimal BOBW algorithms for shortest path problems with bandit feedback.

</details>


### [375] [Stochastic Difference-of-Convex Optimization with Momentum](https://arxiv.org/abs/2510.17503)
*El Mahdi Chayti,Martin Jaggi*

Main category: cs.LG

TL;DR: 动量方法使随机DC优化在小批量下收敛，无需大批次或强噪声假设


<details>
  <summary>Details</summary>
Motivation: 现有随机DC优化方法需要大批次或强噪声假设，限制了实际应用，小批量下的收敛性质尚不明确

Method: 提出基于动量的算法，在标准光滑性和有界方差假设下实现收敛

Result: 证明无动量时无论步长如何都可能不收敛，动量方法具有可证明的收敛性和强实证性能

Conclusion: 动量是小批量随机DC优化的关键要素，能够实现理论收敛保证和实际性能

Abstract: Stochastic difference-of-convex (DC) optimization is prevalent in numerous
machine learning applications, yet its convergence properties under small batch
sizes remain poorly understood. Existing methods typically require large
batches or strong noise assumptions, which limit their practical use. In this
work, we show that momentum enables convergence under standard smoothness and
bounded variance assumptions (of the concave part) for any batch size. We prove
that without momentum, convergence may fail regardless of stepsize,
highlighting its necessity. Our momentum-based algorithm achieves provable
convergence and demonstrates strong empirical performance.

</details>


### [376] [AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization](https://arxiv.org/abs/2510.16045)
*Mengtao Lv,Ruiqi Zhu,Xinyu Wang,Yun Li*

Main category: cs.LG

TL;DR: AMS-Quant是一种创新的浮点量化方法，通过非整数位宽和尾数位共享技术，将大语言模型量化到FP5.33和FP4.25，实现2.8-3.2倍推理加速，且精度损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的巨大参数量带来了存储和推理效率瓶颈，浮点量化虽然能加速推理，但传统整数位宽限制了量化效果的进一步提升。

Method: 提出两种新技术：1) 尾数位共享 - 将k个量化权重分组共享最低有效尾数位；2) 自适应搜索 - 使用离线优化策略最小化共享带来的精度损失。

Result: 在大型数据集和模型上的实验表明，AMS-Quant可将模型量化到FP5.33-e2m3和FP4.25-e2m2，相比FP16推理分别实现2.8倍和3.2倍的解码加速，精度损失可忽略。

Conclusion: AMS-Quant首次将浮点量化从整数位宽扩展到非整数位宽，通过创新的尾数位共享和自适应搜索技术，在保持精度的同时显著提升LLM推理效率。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various kinds of tasks, while the billion or even trillion parameters bring
storage and efficiency bottlenecks for inference. Quantization, particularly
floating-point quantization, is known to be capable of speeding up LLM
inference by reducing memory footprint and data movement during the inference
process. For the first time, we advance the floating-point quantization
exploration from integer bitwidths to non-integer bit-widths, namely AMS-Quant,
to further approach the quantization sweet spot. AMS-Quant incorporates two
novel techniques to put it into effect: (1) it proposes Mantissa-bit Sharing,
which groups k quantized weights and lets them share the least significant
mantissa bit, allowing us to further approach the minimum quantization
bit-width without accuracy loss. (2) It introduces Adaptive Searching, which
employs an offline optimization strategy to minimize the accuracy degradation
introduced by sharing. Moreover, AMS-Quant is also prototyped as efficient CUDA
Linear kernels, which translates memory savings into wall-clock latency
reduction by reducing memory access. Extensive experiments on large-scale
datasets and models show that AMS-Quant can quantize the model to FP-5.33-e2m3
and FP4.25-e2m2, and significantly speed up the LLM decoding over FP16
inference (2.8x and 3.2x), with negligible accuracy loss.

</details>


### [377] [Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation](https://arxiv.org/abs/2510.17120)
*Rishi Sonthalia,Raj Rao Nadakuditi*

Main category: cs.LG

TL;DR: 提出基于矩阵自由能的自动编码器正则化方法，通过优化编码矩阵的奇异值分布使其接近高斯分布，提高泛化能力并应用于欠定逆问题。


<details>
  <summary>Details</summary>
Motivation: 传统自动编码器缺乏对编码分布的正则化约束，本文旨在通过矩阵自由能理论确保编码具有高斯分布特性，以提升模型的泛化性能。

Method: 定义基于编码矩阵奇异值的可微损失函数，通过最小化负矩阵自由能使编码矩阵的奇异值分布与高斯随机矩阵一致，使用随机梯度下降进行训练。

Result: 实验表明该方法能产生高斯化编码，在训练集和测试集上都具有良好泛化性能，并能有效应用于欠定逆问题求解。

Conclusion: 矩阵自由能正则化是一种有效的自动编码器正则化方法，能够强制编码具有高斯分布特性，提升模型泛化能力并在逆问题中表现良好。

Abstract: We introduce a novel regularization scheme for autoencoders based on
matricial free energy. Our approach defines a differentiable loss function in
terms of the singular values of the code matrix (code dimension x batch size).
From the standpoint of free probability an d random matrix theory, this loss
achieves its minimum when the singular value distribution of the code matrix
coincides with that of an appropriately sculpted random metric with i.i.d.
Gaussian entries. Empirical simulations demonstrate that minimizing the
negative matricial free energy through standard stochastic gradient-based
training yields Gaussian-like codes that generalize across training and test
sets. Building on this foundation, we propose a matricidal free energy
maximizing autoencoder that reliably produces Gaussian codes and show its
application to underdetermined inverse problems.

</details>


### [378] [Convergence Rates for Gradient Descent on the Edge of Stability in Overparametrised Least Squares](https://arxiv.org/abs/2510.17506)
*Lachlan Ewen MacDonald,Hancheng Min,Leandro Palma,Salma Tarmoun,Ziqing Xu,René Vidal*

Main category: cs.LG

TL;DR: 该论文分析了过参数化最小二乘问题中梯度下降在大学习率下的收敛行为，揭示了三种不同学习率区间的收敛特性：亚临界、临界和超临界状态。


<details>
  <summary>Details</summary>
Motivation: 传统优化理论只保证梯度下降在小学习率下的单调收敛，但神经网络训练中常使用大学习率（边缘稳定性状态），此时目标函数非单调下降且偏向平坦最小值。本文旨在量化这一现象。

Method: 利用过参数化使全局最小值形成黎曼流形M，将GD动态分解为平行和正交于M的分量。平行分量对应黎曼梯度下降，正交分量是分叉动力系统。

Result: 推导出三种学习率区间的收敛率：亚临界状态（有限时间内克服瞬时不稳定性，线性收敛至次优平坦最小值）；临界状态（不稳定性持续，以幂律收敛至最优平坦最小值）；超临界状态（不稳定性持续，线性收敛至周期为2的轨道）。

Conclusion: 通过流形分解方法，成功量化了梯度下降在大学习率下的收敛行为，解释了边缘稳定性现象中非单调下降和平坦最小值的偏好机制。

Abstract: Classical optimisation theory guarantees monotonic objective decrease for
gradient descent (GD) when employed in a small step size, or ``stable", regime.
In contrast, gradient descent on neural networks is frequently performed in a
large step size regime called the ``edge of stability", in which the objective
decreases non-monotonically with an observed implicit bias towards flat minima.
In this paper, we take a step toward quantifying this phenomenon by providing
convergence rates for gradient descent with large learning rates in an
overparametrised least squares setting. The key insight behind our analysis is
that, as a consequence of overparametrisation, the set of global minimisers
forms a Riemannian manifold $M$, which enables the decomposition of the GD
dynamics into components parallel and orthogonal to $M$. The parallel component
corresponds to Riemannian gradient descent on the objective sharpness, while
the orthogonal component is a bifurcating dynamical system. This insight allows
us to derive convergence rates in three regimes characterised by the learning
rate size: (a) the subcritical regime, in which transient instability is
overcome in finite time before linear convergence to a suboptimally flat global
minimum; (b) the critical regime, in which instability persists for all time
with a power-law convergence toward the optimally flat global minimum; and (c)
the supercritical regime, in which instability persists for all time with
linear convergence to an orbit of period two centred on the optimally flat
global minimum.

</details>


### [379] [GUIrilla: A Scalable Framework for Automated Desktop UI Exploration](https://arxiv.org/abs/2510.16051)
*Sofiya Garkot,Maksym Shamrai,Ivan Synytsia,Mariya Hirna*

Main category: cs.LG

TL;DR: GUIrilla是一个自动化可扩展框架，通过原生可访问性API系统探索应用程序，解决GUI自动化中的数据收集挑战，并构建了包含27,171个任务的GUIrilla-Task数据集。


<details>
  <summary>Details</summary>
Motivation: 当前自主代理在复杂图形用户界面操作方面面临数据可用性限制，包括昂贵的手动标注、闭源数据集和表面级合成流程，特别是在macOS生态系统中的代表性不足。

Method: 使用原生可访问性API系统探索应用程序，将发现的界面元素和爬虫操作组织成层次化GUI图，并采用专门的交互处理程序实现全面应用覆盖。

Result: 构建了GUIrilla-Task数据集，包含27,171个功能基础任务，涵盖1,108个macOS应用。在ScreenSpot Pro基准测试中，基于GUIrilla-Task调优的LLM代理性能显著提升，使用97%更少数据超越合成基线。

Conclusion: GUIrilla框架有效解决了桌面自动化中的数据收集挑战，发布的macapptree库、GUIrilla-Task数据集和GUIrilla-Gold基准支持桌面自主性的开放研究。

Abstract: Autonomous agents capable of operating complex graphical user interfaces
(GUIs) have the potential to transform desktop automation. While recent
advances in large language models (LLMs) have significantly improved UI
understanding, navigating full-window, multi-application desktop environments
remains a major challenge. Data availability is limited by costly manual
annotation, closed-source datasets and surface-level synthetic pipelines. We
introduce GUIrilla, an automated scalable framework that systematically
explores applications via native accessibility APIs to address the critical
data collection challenge in GUI automation. Our framework focuses on macOS -
an ecosystem with limited representation in current UI datasets - though many
of its components are designed for broader cross-platform applicability.
GUIrilla organizes discovered interface elements and crawler actions into
hierarchical GUI graphs and employs specialized interaction handlers to achieve
comprehensive application coverage. Using the application graphs from GUIrilla
crawler, we construct and release GUIrilla-Task, a large-scale dataset of
27,171 functionally grounded tasks across 1,108 macOS applications, each
annotated with full-desktop and window-level screenshots, accessibility
metadata, and semantic action traces. Empirical results show that tuning
LLM-based agents on GUIrilla-Task significantly improves performance on
downstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro
benchmark while using 97% less data. We also release macapptree, an open-source
library for reproducible collection of structured accessibility metadata, along
with the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold
benchmark, and the framework code to support open research in desktop autonomy.

</details>


### [380] [Adaptive Discretization for Consistency Models](https://arxiv.org/abs/2510.17266)
*Jiayu Bai,Zhanbo Feng,Zhijie Deng,Tianqi Hou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

TL;DR: 提出ADCMs框架，通过自动自适应离散化方法解决一致性模型依赖手动设计离散化方案的问题，使用局部一致性作为优化目标、全局一致性作为约束，显著提升训练效率和生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有的一致性模型大多依赖手动设计的离散化方案，这在不同噪声调度和数据集上需要反复调整，缺乏通用性和效率。

Method: 将离散化制定为优化问题，以局部一致性为优化目标确保可训练性，全局一致性为约束确保稳定性，使用拉格朗日乘子平衡两者，并基于高斯-牛顿法实现自适应离散化。

Result: 在CIFAR-10和ImageNet上显著提升训练效率，获得更优的生成性能，且对更先进的扩散模型变体表现出强适应性。

Conclusion: ADCMs框架为一致性模型提供了一种自动自适应的离散化方法，有效解决了手动设计方案的局限性，提高了训练效率和模型性能。

Abstract: Consistency Models (CMs) have shown promise for efficient one-step
generation. However, most existing CMs rely on manually designed discretization
schemes, which can cause repeated adjustments for different noise schedules and
datasets. To address this, we propose a unified framework for the automatic and
adaptive discretization of CMs, formulating it as an optimization problem with
respect to the discretization step. Concretely, during the consistency training
process, we propose using local consistency as the optimization objective to
ensure trainability by avoiding excessive discretization, and taking global
consistency as a constraint to ensure stability by controlling the denoising
error in the training target. We establish the trade-off between local and
global consistency with a Lagrange multiplier. Building on this framework, we
achieve adaptive discretization for CMs using the Gauss-Newton method. We refer
to our approach as ADCMs. Experiments demonstrate that ADCMs significantly
improve the training efficiency of CMs, achieving superior generative
performance with minimal training overhead on both CIFAR-10 and ImageNet.
Moreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code
is available at https://github.com/rainstonee/ADCM.

</details>


### [381] [Unbiased Gradient Low-Rank Projection](https://arxiv.org/abs/2510.17802)
*Rui Pan,Yang Luo,Yuxing Liu,Yang You,Tong Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为GUM的无偏低秩优化方法，通过层间采样技术消除低秩投影的偏差，在保持内存效率的同时实现与全参数训练相当甚至更好的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有低秩优化方法（如GaLore）因投影偏差而缺乏收敛保证的问题，旨在在保持内存效率的同时实现无偏优化。

Method: 基于GaLore机制和Muon算法，采用层间采样技术来消除低秩投影的偏差，开发了GaLore Unbiased with Muon (GUM)方法。

Result: 理论证明GUM匹配Muon算法的收敛保证，实验显示在LLM微调和预训练中优于GaLore，甚至超过全参数训练的性能。

Conclusion: GUM方法通过更均匀的层内知识分布实现了参数空间的更高效利用和更好的记忆能力，为内存高效的LLM训练提供了有效解决方案。

Abstract: Memory-efficient optimization is critical for training increasingly large
language models (LLMs). A popular strategy involves gradient low-rank
projection, storing only the projected optimizer states, with GaLore being a
representative example. However, a significant drawback of many such methods is
their lack of convergence guarantees, as various low-rank projection approaches
introduce inherent biases relative to the original optimization algorithms,
which contribute to performance gaps compared to full-parameter training.
Aiming to tackle this problem, this paper investigates the layerwise sampling
technique for debiasing low-rank projection mechanisms. In particular, an
instantiation of the paradigm gives rise to a novel and unbiased low-rank
optimization method built upon GaLore's mechanism and the Muon algorithm, named
GaLore Unbiased with Muon (GUM). We theoretically prove our method matches the
convergence guarantees of the base Muon algorithm while preserving the memory
efficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and
pretraining also demonstrate non-trivial improvements over GaLore and even
better performance than full-parameter training. Further investigation shows
that the improvement of this technique comes from a more uniform distribution
of knowledge inside layers, leading to more efficient utilization of the model
parameter space and better memorization.

</details>


### [382] [FUSE-Traffic: Fusion of Unstructured and Structured Data for Event-aware Traffic Forecasting](https://arxiv.org/abs/2510.16053)
*Chenyang Yu,Xinpeng Xie,Yan Huang,Chenxi Qiu*

Main category: cs.LG

TL;DR: 本文探讨了智能交通系统中的交通预测技术，重点分析了图神经网络在捕捉空间依赖性和时间演化模式方面的应用，并讨论了现有方法在事件信息整合方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着城市化进程加快，交通拥堵问题日益严重，需要可靠且响应迅速的交通预测模型来改善城市资源分配和出行体验。

Method: 主要采用图神经网络（GNNs）作为主流范式，包括STGCN、GraphWaveNet、STWave和D2STGNN等模型，这些模型结合了复杂的图卷积结构和时间建模机制。对于事件信息处理，早期方法主要依赖人工设计的特征，如事件影响评分或特定子图构建。

Result: 现有方法在标准交通数据集上取得了显著性能，特别是在捕捉具有周期性规律的交通模式方面表现出色。

Conclusion: 当前基于人工特征的事件信息整合方法存在局限性，过度依赖领域专家先验知识，难以泛化到多样复杂的未知事件，且低维人工特征会导致丰富语义细节的丢失。

Abstract: Accurate traffic forecasting is a core technology for building Intelligent
Transportation Systems (ITS), enabling better urban resource allocation and
improved travel experiences. With growing urbanization, traffic congestion has
intensified, highlighting the need for reliable and responsive forecasting
models. In recent years, deep learning, particularly Graph Neural Networks
(GNNs), has emerged as the mainstream paradigm in traffic forecasting. GNNs can
effectively capture complex spatial dependencies in road network topology and
dynamic temporal evolution patterns in traffic flow data. Foundational models
such as STGCN and GraphWaveNet, along with more recent developments including
STWave and D2STGNN, have achieved impressive performance on standard traffic
datasets. These approaches incorporate sophisticated graph convolutional
structures and temporal modeling mechanisms, demonstrating particular
effectiveness in capturing and forecasting traffic patterns characterized by
periodic regularities. To address this challenge, researchers have explored
various ways to incorporate event information. Early attempts primarily relied
on manually engineered event features. For instance, some approaches introduced
manually defined incident effect scores or constructed specific subgraphs for
different event-induced traffic conditions. While these methods somewhat
enhance responsiveness to specific events, their core drawback lies in a heavy
reliance on domain experts' prior knowledge, making generalization to diverse
and complex unknown events difficult, and low-dimensional manual features often
lead to the loss of rich semantic details.

</details>


### [383] [Uncertainty-aware data assimilation through variational inference](https://arxiv.org/abs/2510.17268)
*Anthony Frion,David S Greenberg*

Main category: cs.LG

TL;DR: 提出了一种基于变分推断的扩展方法，将确定性机器学习方法扩展到多元高斯分布预测，用于数据同化中的不确定性建模。


<details>
  <summary>Details</summary>
Motivation: 数据同化涉及将动态模型与噪声和不完整观测相结合来推断系统状态，在大多数设置中都存在不确定性。现有确定性方法无法充分处理这种不确定性。

Method: 在现有确定性机器学习方法基础上，提出变分推断扩展，使预测状态遵循多元高斯分布。使用混沌Lorenz-96动力学作为测试平台。

Result: 新模型能够获得几乎完美校准的预测，并且可以集成到更广泛的变分数据同化管道中，从增加的数据同化窗口长度中获得更大收益。

Conclusion: 基于变分推断的随机扩展方法能够有效处理数据同化中的不确定性，提供校准良好的预测，并提升数据同化性能。

Abstract: Data assimilation, consisting in the combination of a dynamical model with a
set of noisy and incomplete observations in order to infer the state of a
system over time, involves uncertainty in most settings. Building upon an
existing deterministic machine learning approach, we propose a variational
inference-based extension in which the predicted state follows a multivariate
Gaussian distribution. Using the chaotic Lorenz-96 dynamics as a testing
ground, we show that our new model enables to obtain nearly perfectly
calibrated predictions, and can be integrated in a wider variational data
assimilation pipeline in order to achieve greater benefit from increasing
lengths of data assimilation windows. Our code is available at
https://github.com/anthony-frion/Stochastic_CODA.

</details>


### [384] [Symmetries in PAC-Bayesian Learning](https://arxiv.org/abs/2510.17303)
*Armin Beck,Peter Ochs*

Main category: cs.LG

TL;DR: 该论文将泛化保证扩展到非紧对称性和非不变数据分布，通过PAC-Bayes框架提供理论支持，并在旋转MNIST数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有理论主要关注紧群对称性并假设数据分布不变，这在现实应用中很少满足。需要扩展理论保证到更广泛的非紧对称性和非不变数据分布。

Method: 基于PAC-Bayes框架，改进和收紧现有边界，特别适用于McAllester的PAC-Bayes边界，但可应用于广泛的PAC-Bayes边界。

Result: 在非均匀旋转群的旋转MNIST数据集上验证，推导的保证不仅成立，而且优于先前结果。

Conclusion: 对于对称数据，对称模型在超越紧群和不变分布的更广泛设置中更优，为理解机器学习中的对称性开辟了更一般的途径。

Abstract: Symmetries are known to improve the empirical performance of machine learning
models, yet theoretical guarantees explaining these gains remain limited. Prior
work has focused mainly on compact group symmetries and often assumes that the
data distribution itself is invariant, an assumption rarely satisfied in
real-world applications. In this work, we extend generalization guarantees to
the broader setting of non-compact symmetries, such as translations and to
non-invariant data distributions. Building on the PAC-Bayes framework, we adapt
and tighten existing bounds, demonstrating the approach on McAllester's
PAC-Bayes bound while showing that it applies to a wide range of PAC-Bayes
bounds. We validate our theory with experiments on a rotated MNIST dataset with
a non-uniform rotation group, where the derived guarantees not only hold but
also improve upon prior results. These findings provide theoretical evidence
that, for symmetric data, symmetric models are preferable beyond the narrow
setting of compact groups and invariant distributions, opening the way to a
more general understanding of symmetries in machine learning.

</details>


### [385] [Exploration via Feature Perturbation in Contextual Bandits](https://arxiv.org/abs/2510.17390)
*Seouh-won Yi,Min-hwan Oh*

Main category: cs.LG

TL;DR: 提出了一种名为特征扰动的简单而强大的技术，通过直接在特征输入中注入随机性，而非随机化未知参数或向奖励添加噪声，实现了广义线性赌博机的最坏情况遗憾界，同时避免了现有随机化算法的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有随机化赌博机算法通常存在遗憾界较高的问题，且计算效率较低，难以扩展到非参数或神经网络模型。特征扰动方法旨在解决这些问题，提供更优的理论保证和实际性能。

Method: 特征扰动技术直接在特征输入中注入随机性，而不是随机化未知参数或向奖励添加噪声。这种方法避免了参数采样，计算效率高，并能自然扩展到非参数或神经网络模型。

Result: 该算法实现了广义线性赌博机的最坏情况遗憾界，显著优于现有随机化算法的遗憾界。实证评估表明，特征扰动不仅超越了现有方法，而且在强实际性能和最佳理论保证之间实现了统一。

Conclusion: 特征扰动是一种简单而强大的技术，通过直接在特征输入中注入随机性，实现了更优的理论遗憾界和实际性能，同时保持了计算效率和对复杂模型的良好扩展性。

Abstract: We propose feature perturbation, a simple yet powerful technique that injects
randomness directly into feature inputs, instead of randomizing unknown
parameters or adding noise to rewards. Remarkably, this algorithm achieves
$\tilde{\mathcal{O}}(d\sqrt{T})$ worst-case regret bound for generalized linear
bandits, while avoiding the $\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$ regret
typical of existing randomized bandit algorithms. Because our algorithm eschews
parameter sampling, it is both computationally efficient and naturally extends
to non-parametric or neural network models. We verify these advantages through
empirical evaluations, demonstrating that feature perturbation not only
surpasses existing methods but also unifies strong practical performance with
best-known theoretical guarantees.

</details>


### [386] [RINS-T: Robust Implicit Neural Solvers for Time Series Linear Inverse Problems](https://arxiv.org/abs/2510.17396)
*Keivan Faghih Niresi,Zepeng Zhang,Olga Fink*

Main category: cs.LG

TL;DR: RINS-T是一个无需预训练数据的深度先验框架，通过神经网络作为隐式先验并结合鲁棒优化技术，有效解决时间序列线性逆问题，对异常值具有强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据常受缺失值、噪声和异常值等污染，传统深度学习方法需要大量预训练且难以应对分布偏移。

Method: 使用神经网络作为隐式先验，集成鲁棒优化技术，引入引导输入初始化、输入扰动和凸输出组合三个关键技术。

Result: 实现了高恢复性能，无需预训练数据，对异常值具有鲁棒性，且不依赖高斯噪声假设。

Conclusion: RINS-T为复杂现实世界时间序列挑战提供了灵活有效的解决方案。

Abstract: Time series data are often affected by various forms of corruption, such as
missing values, noise, and outliers, which pose significant challenges for
tasks such as forecasting and anomaly detection. To address these issues,
inverse problems focus on reconstructing the original signal from corrupted
data by leveraging prior knowledge about its underlying structure. While deep
learning methods have demonstrated potential in this domain, they often require
extensive pretraining and struggle to generalize under distribution shifts. In
this work, we propose RINS-T (Robust Implicit Neural Solvers for Time Series
Linear Inverse Problems), a novel deep prior framework that achieves high
recovery performance without requiring pretraining data. RINS-T leverages
neural networks as implicit priors and integrates robust optimization
techniques, making it resilient to outliers while relaxing the reliance on
Gaussian noise assumptions. To further improve optimization stability and
robustness, we introduce three key innovations: guided input initialization,
input perturbation, and convex output combination techniques. Each of these
contributions strengthens the framework's optimization stability and
robustness. These advancements make RINS-T a flexible and effective solution
for addressing complex real-world time series challenges. Our code is available
at https://github.com/EPFL-IMOS/RINS-T.

</details>


### [387] [FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning](https://arxiv.org/abs/2510.16065)
*Lunchen Xie,Zehua He,Qingjiang Shi*

Main category: cs.LG

TL;DR: 提出FedPURIN框架，通过整数编程识别关键参数进行传输，结合稀疏聚合方案显著降低通信开销，同时保持个性化联邦学习性能


<details>
  <summary>Details</summary>
Motivation: 解决个性化联邦学习中通信效率低下的问题，现有方法在数据异构环境下通信负担较重，阻碍实际部署

Method: 基于整数编程策略识别关键传输参数，结合稀疏聚合方案，在保持模型性能的同时大幅减少通信量

Result: 在标准图像分类基准测试中，在多种非IID条件下表现出与最先进方法相当的性能，同时实现可量化的通信减少

Conclusion: FedPURIN为通信高效的个性化联邦学习建立了新范式，特别适用于具有异构数据源的边缘智能系统

Abstract: Personalized Federated Learning (PFL) has emerged as a critical research
frontier addressing data heterogeneity issue across distributed clients. Novel
model architectures and collaboration mechanisms are engineered to accommodate
statistical disparities while producing client-specific models. Parameter
decoupling represents a promising paradigm for maintaining model performance in
PFL frameworks. However, the communication efficiency of many existing methods
remains suboptimal, sustaining substantial communication burdens that impede
practical deployment. To bridge this gap, we propose Federated Learning with
Programmed Update and Reduced INformation (FedPURIN), a novel framework that
strategically identifies critical parameters for transmission through an
integer programming formulation. This mathematically grounded strategy is
seamlessly integrated into a sparse aggregation scheme, achieving a significant
communication reduction while preserving the efficacy. Comprehensive
evaluations on standard image classification benchmarks under varied non-IID
conditions demonstrate competitive performance relative to state-of-the-art
methods, coupled with quantifiable communication reduction through sparse
aggregation. The framework establishes a new paradigm for
communication-efficient PFL, particularly advantageous for edge intelligence
systems operating with heterogeneous data sources.

</details>


### [388] [MNO: Multiscale Neural Operator for Computational Fluid Dynamics with 3D Point Cloud Data](https://arxiv.org/abs/2510.16071)
*Qinxuan Wang,Chuang Wang,Mingyu Zhang,Jingwei Sun,Peipei Yang,Shuo Tang,Shiming Xiang*

Main category: cs.LG

TL;DR: 提出多尺度神经算子(MNO)，一种用于三维非结构化点云上计算流体动力学的新架构，通过显式三尺度分解显著提升了神经算子在复杂流体问题上的精度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在求解偏微分方程时存在精度和可扩展性限制，特别是在不规则域上处理多尺度流体结构时表现不佳。

Method: MNO架构包含三个尺度模块：全局维度收缩注意力模块处理长程依赖，局部图注意力模块处理邻域交互，微观点级注意力模块处理精细细节。

Result: 在四个不同基准测试中，MNO在稳态和非稳态流场景下均优于现有方法，预测误差降低5%-40%，在30万点规模问题上表现出更好的鲁棒性。

Conclusion: 显式多尺度设计对神经算子至关重要，MNO为学习不规则域上复杂流体动力学提供了可扩展框架。

Abstract: Neural operators have emerged as a powerful data-driven paradigm for solving
Partial Differential Equations (PDEs), offering orders-of-magnitude
acceleration over traditional solvers. However, existing approaches still
suffer from limited accuracy and scalability, particularly on irregular domains
where fluid flows exhibit rich multiscale structures. In this work, we
introduce the Multiscale Neural Operator (MNO), a new architecture for
Computational Fluid Dynamics (CFD) on three-dimensional (3D) unstructured point
clouds. MNO explicitly decomposes information across three scales: a global
dimension-shrinkage attention module for long-range dependencies, a local graph
attention module for neighborhood-level interactions, and a micro point-wise
attention module for fine-grained details. This design preserves multiscale
inductive biases while remaining computationally efficient. We evaluate MNO on
four diverse benchmarks, covering both steady-state and unsteady flow scenarios
with up to 300K points. Across all tasks, MNO consistently outperforms
state-of-the-art baselines, reducing prediction errors by 5% to 40% and
demonstrating improved robustness in challenging 3D CFD problems. Our results
highlight the importance of explicit multiscale design for neural operators and
establish MNO as a scalable framework for learning complex fluid dynamics on
irregular domains.

</details>


### [389] [Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment](https://arxiv.org/abs/2510.17543)
*Jiayi Huang,Sangwoo Park,Nicola Paoletti,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出了一种基于保形对齐的级联机制(CAb)，用于边缘-云模型级联系统，确保边缘预测集满足云模型级别的条件覆盖保证，同时减少向云的卸载。


<details>
  <summary>Details</summary>
Motivation: 边缘智能虽然能实现低延迟推理，但确保可靠性仍然具有挑战性。需要保证边缘预测集在条件覆盖方面达到云模型的水平。

Method: 将边缘到云的升级过程建模为多重假设检验问题，采用保形对齐方法来选择哪些输入可以在边缘安全处理。该方法适用于任意边缘预测集，包括各种保形预测变体。

Result: 在CIFAR-100图像分类和TeleQnA问答基准测试中，CAb级联方法在保持目标条件覆盖的同时，显著减少了向云的卸载，预测集大小仅有适度增加。

Conclusion: CAb级联方法为边缘-云系统提供了统计保证，在覆盖、延迟率和集合大小之间实现了可调权衡，有效平衡了边缘推理的效率和可靠性。

Abstract: Edge intelligence enables low-latency inference via compact on-device models,
but assuring reliability remains challenging. We study edge-cloud cascades that
must preserve conditional coverage: whenever the edge returns a prediction set,
it should contain the true label with a user-specified probability, as if
produced by the cloud model. We formalize conditional coverage with respect to
the cloud predictive distribution, and introduce a conformal alignment-based
(CAb) cascading mechanism that certifies this property with user control over
the risk level. Our method casts escalation from edge to cloud models as a
multiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)
to select which inputs can be safely handled at the edge. The proposed CAb
model cascading method yields statistical guarantees on the average fraction of
edge decisions that satisfy cloud-level conditional coverage. The procedure
applies to arbitrary edge prediction sets, including variants of conformal
prediction (CP), and exposes a tunable trade-off among coverage, deferral rate,
and set size. Experiments on CIFAR-100 image classification and the TeleQnA
question-answering (QA) benchmark show that the proposed CAb cascade maintains
the target conditional coverage for edge predictions while substantially
reducing offloading to the cloud and incurring modest increases in
prediction-set size.

</details>


### [390] [Early-stopping for Transformer model training](https://arxiv.org/abs/2510.16074)
*Jing He,Hua Jiang,Cheng Li,Siqian Xin,Shuzhen Yang*

Main category: cs.LG

TL;DR: 基于随机矩阵理论提出Transformer训练动态分析框架，通过自注意力矩阵谱密度演化识别训练三阶段，并提出无验证的早停标准


<details>
  <summary>Details</summary>
Motivation: 理解Transformer训练动态的底层机制，为性能改进提供理论基础并推导原则性的早停标准

Method: 利用随机矩阵理论分析Transformer训练动态，通过自注意力矩阵V的谱密度演化识别训练阶段，使用幂律拟合作为探针

Result: 发现浅层自注意力矩阵谱密度一致演化为重尾分布，识别出结构探索、重尾结构稳定和收敛饱和三个训练阶段

Conclusion: 提出的重尾动态定量指标和收敛谱特征两个无验证标准高度一致，证明随机矩阵理论在监控Transformer训练进程中的实用性

Abstract: This work introduces a novel theoretical framework grounded in Random Matrix
Theory (RMT) for analyzing Transformer training dynamics. We focus on the
underlying mechanisms that drive performance improvements and derive principled
early-stopping criteria. Empirically, we observe that the spectral density of
the shallow self-attention matrix V consistently evolves into a heavy-tailed
distribution. Utilizing the PL (Power Law) fit to this matrix as a probe, we
demarcate training into three stages: structural exploration, heavy-tailed
structure stabilization, and convergence saturation. This staging provides
guidance for preliminary stopping decisions. Crucially, we propose two
consistent and validation-free criteria: a quantitative metric for heavy-tailed
dynamics and a novel spectral signature indicative of convergence. The strong
alignment between these criteria highlights the utility of RMT for monitoring
and diagnosing the progression of Transformer model training.

</details>


### [391] [Functional Distribution Networks (FDN)](https://arxiv.org/abs/2510.17794)
*Omer Haq*

Main category: cs.LG

TL;DR: 提出Functional Distribution Networks (FDN)，一种输入条件化的网络权重分布方法，通过beta-ELBO和蒙特卡洛采样训练，能够在分布偏移下产生自适应分散的预测混合分布。


<details>
  <summary>Details</summary>
Motivation: 现代概率回归器在分布偏移下往往过于自信，需要一种能够适应输入变化的预测不确定性估计方法。

Method: 使用输入条件化的网络权重分布，通过beta-ELBO损失函数和蒙特卡洛采样进行训练，诱导产生自适应分散的预测混合分布。

Result: 在标准回归任务中，与贝叶斯、集成、dropout和超网络基线相比，在匹配参数和更新预算下评估了准确性、校准性和偏移感知能力。

Conclusion: 该框架和评估协议旨在使具有OOD感知和良好校准的神经回归变得实用和模块化。

Abstract: Modern probabilistic regressors often remain overconfident under distribution
shift. We present Functional Distribution Networks (FDN), an input-conditioned
distribution over network weights that induces predictive mixtures whose
dispersion adapts to the input. FDN is trained with a beta-ELBO and Monte Carlo
sampling. We further propose an evaluation protocol that cleanly separates
interpolation from extrapolation and stresses OOD sanity checks (e.g., that
predictive likelihood degrades under shift while in-distribution accuracy and
calibration are maintained). On standard regression tasks, we benchmark against
strong Bayesian, ensemble, dropout, and hypernetwork baselines under matched
parameter and update budgets, and assess accuracy, calibration, and
shift-awareness with standard diagnostics. Together, the framework and protocol
aim to make OOD-aware, well-calibrated neural regression practical and modular.

</details>


### [392] [Optimization of the quantization of dense neural networks from an exact QUBO formulation](https://arxiv.org/abs/2510.16075)
*Sergio Muñiz Subiñas,Manuel L. González,Jorge Ruiz Gómez,Alejandro Mata Ali,Jorge Martínez Martín,Miguel Franco Hernando,Ángel Miguel García-Vico*

Main category: cs.LG

TL;DR: 提出了一种基于ADAROUND的QUBO公式的神经网络后训练量化方法，通过Frobenius距离作为目标函数，将量化问题转化为可分解的QUBO问题，并使用模拟退火等启发式方法高效求解。


<details>
  <summary>Details</summary>
Motivation: 传统量化方法如round-to-nearest存在精度损失，需要更精确的量化方法来保持神经网络性能。

Method: 使用Frobenius距离作为目标函数，构建明确的QUBO问题，其中二元变量表示权重和偏置的舍入选择。利用系数矩阵结构将全局问题分解为多个独立子问题。

Result: 在MNIST、Fashion-MNIST、EMNIST和CIFAR-10数据集上评估，从int8到int1的整数精度范围内表现优于传统量化方法。

Conclusion: 该方法提供了一种有效的后训练量化解决方案，能够显著提升量化精度，特别是在低精度设置下。

Abstract: This work introduces a post-training quantization (PTQ) method for dense
neural networks via a novel ADAROUND-based QUBO formulation. Using the
Frobenius distance between the theoretical output and the dequantized output
(before the activation function) as the objective, an explicit QUBO whose
binary variables represent the rounding choice for each weight and bias is
obtained. Additionally, by exploiting the structure of the coefficient QUBO
matrix, the global problem can be exactly decomposed into $n$ independent
subproblems of size $f+1$, which can be efficiently solved using some
heuristics such as simulated annealing. The approach is evaluated on MNIST,
Fashion-MNIST, EMNIST, and CIFAR-10 across integer precisions from int8 to int1
and compared with a round-to-nearest traditional quantization methodology.

</details>


### [393] [BPL: Bias-adaptive Preference Distillation Learning for Recommender System](https://arxiv.org/abs/2510.16076)
*SeongKu Kang,Jianxun Lian,Dongha Lee,Wonbin Kweon,Sanghwan Jang,Jaehyun Lee,Jindong Wang,Xing Xie,Hwanjo Yu*

Main category: cs.LG

TL;DR: 提出了BPL框架，通过双重蒸馏策略解决推荐系统中的偏差问题，在事实和反事实测试环境中都能取得良好性能


<details>
  <summary>Details</summary>
Motivation: 推荐系统存在偏差导致收集的反馈不能完全揭示用户偏好，现有去偏方法主要关注反事实测试环境，在基于实际用户-物品交互的事实测试环境中准确率显著下降

Method: 使用偏置模型的师生蒸馏保留与收集反馈一致的知识，通过带可靠性过滤的自蒸馏迭代精炼知识

Result: 综合实验验证了BPL在事实和反事实测试中的有效性

Conclusion: BPL框架能够逐步揭示用户偏好，在两种测试环境中都表现出色

Abstract: Recommender systems suffer from biases that cause the collected feedback to
incompletely reveal user preference. While debiasing learning has been
extensively studied, they mostly focused on the specialized (called
counterfactual) test environment simulated by random exposure of items,
significantly degrading accuracy in the typical (called factual) test
environment based on actual user-item interactions. In fact, each test
environment highlights the benefit of a different aspect: the counterfactual
test emphasizes user satisfaction in the long-terms, while the factual test
focuses on predicting subsequent user behaviors on platforms. Therefore, it is
desirable to have a model that performs well on both tests rather than only
one. In this work, we introduce a new learning framework, called Bias-adaptive
Preference distillation Learning (BPL), to gradually uncover user preferences
with dual distillation strategies. These distillation strategies are designed
to drive high performance in both factual and counterfactual test environments.
Employing a specialized form of teacher-student distillation from a biased
model, BPL retains accurate preference knowledge aligned with the collected
feedback, leading to high performance in the factual test. Furthermore, through
self-distillation with reliability filtering, BPL iteratively refines its
knowledge throughout the training process. This enables the model to produce
more accurate predictions across a broader range of user-item combinations,
thereby improving performance in the counterfactual test. Comprehensive
experiments validate the effectiveness of BPL in both factual and
counterfactual tests. Our implementation is accessible via:
https://github.com/SeongKu-Kang/BPL.

</details>


### [394] [Continual Knowledge Consolidation LORA for Domain Incremental Learning](https://arxiv.org/abs/2510.16077)
*Naeem Paeedeh,Mahardhika Pratama,Weiping Ding,Jimmy Cao,Wolfgang Mayer,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: 提出CONEC-LoRA方法解决领域增量学习问题，通过任务共享和任务特定的LoRA整合，结合随机分类器和辅助网络，在4个基准测试中性能提升超过5%。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法创建任务特定的LoRA，忽略了跨任务的共享知识，且推理时任务特定LoRA选择不准确导致精度显著下降，现有分类器泛化能力不足。

Method: 1. 任务共享LoRA和任务特定LoRA整合提取共同知识和领域特定知识；2. 随机分类器从分布中采样参数增强分类正确性；3. 辅助网络预测任务特定LoRA，采用不同深度网络结构利用中间表示；4. 集成球生成器损失和变换模块解决合成样本偏差问题。

Result: 在4个流行基准问题上，CONEC-LoRA相比现有方法有超过5%的性能优势。

Conclusion: CONEC-LoRA通过知识整合、随机分类器和辅助网络有效解决了领域增量学习中的灾难性遗忘问题，显著提升了性能。

Abstract: Domain Incremental Learning (DIL) is a continual learning sub-branch that
aims to address never-ending arrivals of new domains without catastrophic
forgetting problems. Despite the advent of parameter-efficient fine-tuning
(PEFT) approaches, existing works create task-specific LoRAs overlooking shared
knowledge across tasks. Inaccurate selection of task-specific LORAs during
inference results in significant drops in accuracy, while existing works rely
on linear or prototype-based classifiers, which have suboptimal generalization
powers. Our paper proposes continual knowledge consolidation low rank
adaptation (CONEC-LoRA) addressing the DIL problems. CONEC-LoRA is developed
from consolidations between task-shared LORA to extract common knowledge and
task-specific LORA to embrace domain-specific knowledge. Unlike existing
approaches, CONEC-LoRA integrates the concept of a stochastic classifier whose
parameters are sampled from a distribution, thus enhancing the likelihood of
correct classifications. Last but not least, an auxiliary network is deployed
to optimally predict the task-specific LoRAs for inferences and implements the
concept of a different-depth network structure in which every layer is
connected with a local classifier to take advantage of intermediate
representations. This module integrates the ball-generator loss and
transformation module to address the synthetic sample bias problem. Our
rigorous experiments demonstrate the advantage of CONEC-LoRA over prior arts in
4 popular benchmark problems with over 5% margins.

</details>


### [395] [PassREfinder-FL: Privacy-Preserving Credential Stuffing Risk Prediction via Graph-Based Federated Learning for Representing Password Reuse between Websites](https://arxiv.org/abs/2510.16083)
*Jaehan Kim,Minkyoo Song,Minjae Seo,Youngjin Jin,Seungwon Shin,Jinwoo Kim*

Main category: cs.LG

TL;DR: 提出PassREfinder-FL框架，使用图神经网络预测网站间的密码重用风险，并通过联邦学习保护用户隐私，在真实数据集上达到0.9153的F1分数。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在检测密码重用攻击时存在的可用性问题和隐私泄露风险，现有方法往往限制密码创建或网站访问，且依赖复杂的账户共享机制难以实际部署。

Method: 引入密码重用关系概念，将其表示为网站图中的边，使用图神经网络进行链接预测。采用联邦学习方法，无需跨管理员共享用户敏感信息。

Result: 在包含3.6亿个泄露账户、22,378个网站的真实数据集上，PassREfinder-FL在联邦学习设置下达到0.9153的F1分数，比最先进GNN模型性能提升4-11%。

Conclusion: 该方法能有效预测网站间的密码重用风险，生成可操作的风险评分，同时保护用户隐私，具有实际部署价值。

Abstract: Credential stuffing attacks have caused significant harm to online users who
frequently reuse passwords across multiple websites. While prior research has
attempted to detect users with reused passwords or identify malicious login
attempts, existing methods often compromise usability by restricting password
creation or website access, and their reliance on complex account-sharing
mechanisms hinders real-world deployment. To address these limitations, we
propose PassREfinder-FL, a novel framework that predicts credential stuffing
risks across websites. We introduce the concept of password reuse relations --
defined as the likelihood of users reusing passwords between websites -- and
represent them as edges in a website graph. Using graph neural networks (GNNs),
we perform a link prediction task to assess credential reuse risk between
sites. Our approach scales to a large number of arbitrary websites by
incorporating public website information and linking newly observed websites as
nodes in the graph. To preserve user privacy, we extend PassREfinder-FL with a
federated learning (FL) approach that eliminates the need to share user
sensitive information across administrators. Evaluation on a real-world dataset
of 360 million breached accounts from 22,378 websites shows that
PassREfinder-FL achieves an F1-score of 0.9153 in the FL setting. We further
validate that our FL-based GNN achieves a 4-11% performance improvement over
other state-of-the-art GNN models through an ablation study. Finally, we
demonstrate that the predicted results can be used to quantify password reuse
likelihood as actionable risk scores.

</details>


### [396] [Near-Equilibrium Propagation training in nonlinear wave systems](https://arxiv.org/abs/2510.16084)
*Karol Sajnok,Michał Matuszewski*

Main category: cs.LG

TL;DR: 将平衡传播学习扩展到离散和连续复值波系统，适用于弱耗散机制，在激子-极化激元凝聚体中实现了物理系统的原位学习。


<details>
  <summary>Details</summary>
Motivation: 反向传播算法在物理神经网络中难以实现，平衡传播(EP)作为替代方案具有可比的效率和原位训练潜力。

Method: 扩展EP学习到离散和连续复值波系统，在弱耗散机制下有效，用可训练的局部势能替代节点间连接，在激子-极化激元凝聚体中测试。

Result: 在标准基准测试中（包括逻辑任务和手写数字识别）表现出稳定收敛。

Conclusion: 为系统控制仅限于局部参数的物理系统建立了实用的原位学习路径。

Abstract: Backpropagation learning algorithm, the workhorse of modern artificial
intelligence, is notoriously difficult to implement in physical neural
networks. Equilibrium Propagation (EP) is an alternative with comparable
efficiency and strong potential for in-situ training. We extend EP learning to
both discrete and continuous complex-valued wave systems. In contrast to
previous EP implementations, our scheme is valid in the weakly dissipative
regime, and readily applicable to a wide range of physical settings, even
without well defined nodes, where trainable inter-node connections can be
replaced by trainable local potential. We test the method in driven-dissipative
exciton-polariton condensates governed by generalized Gross-Pitaevskii
dynamics. Numerical studies on standard benchmarks, including a simple logical
task and handwritten-digit recognition, demonstrate stable convergence,
establishing a practical route to in-situ learning in physical systems in which
system control is restricted to local parameters.

</details>


### [397] [FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.16086)
*Ziyang Liu,Pengjunfei Chu,Shuming Dong,Chen Zhang,Mingcheng Li,Jin Wang*

Main category: cs.LG

TL;DR: 提出FSRF框架解决多模态情感分析中的模态缺失问题，通过去冗余同质-异质分解和分布对齐自蒸馏模块来恢复缺失语义


<details>
  <summary>Details</summary>
Motivation: 现实应用中由于遮挡、隐私约束和设备故障导致模态缺失，传统方法忽略此问题导致泛化性差

Method: 使用去冗余同质-异质分解模块将模态分解为同质、异质和噪声表示，并设计分布对齐自蒸馏模块进行双向知识转移

Result: 在两个数据集上的实验表明，FSRF在不确定模态缺失情况下相比先前方法具有显著性能优势

Conclusion: FSRF框架能有效缓解多模态情感分析中的模态缺失问题，提高模型的泛化能力

Abstract: In recent years, Multimodal Sentiment Analysis (MSA) has become a research
hotspot that aims to utilize multimodal data for human sentiment understanding.
Previous MSA studies have mainly focused on performing interaction and fusion
on complete multimodal data, ignoring the problem of missing modalities in
real-world applications due to occlusion, personal privacy constraints, and
device malfunctions, resulting in low generalizability.
  To this end, we propose a Factorization-guided Semantic Recovery Framework
(FSRF) to mitigate the modality missing problem in the MSA task.
  Specifically, we propose a de-redundant homo-heterogeneous factorization
module that factorizes modality into modality-homogeneous,
modality-heterogeneous, and noisy representations and design elaborate
constraint paradigms for representation learning.
  Furthermore, we design a distribution-aligned self-distillation module that
fully recovers the missing semantics by utilizing bidirectional knowledge
transfer.
  Comprehensive experiments on two datasets indicate that FSRF has a
significant performance advantage over previous methods with uncertain missing
modalities.

</details>


### [398] [STABLE: Gated Continual Learning for Large Language Models](https://arxiv.org/abs/2510.16089)
*William Hoy,Nurcin Celik*

Main category: cs.LG

TL;DR: STABLE是一个门控持续自编辑框架，使用LoRA进行参数高效微调，通过三种指标评估编辑稳定性来约束顺序更新中的灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要持续适应机制，但顺序更新会导致灾难性遗忘，新编辑会降低先前获得的知识。

Method: 使用LoRA进行参数高效微调，通过三种指标（精确匹配下降、比特数增加、KL散度）评估候选编辑的稳定性，超过阈值时重新缩放或拒绝LoRA更新。

Result: 在Qwen-2.5-7B模型上的实验表明，门控能有效减轻遗忘同时保持适应性，基于EM的门控在短持续学习序列中实现了最高的累积性能。

Conclusion: 该方法为持续模型编辑提供了原则性方法，使LLM能够整合新知识同时保持可靠性，不同门控策略可产生不同的准确性结果，突显门控设计的重要性。

Abstract: Large language models (LLMs) increasingly require mechanisms for continual
adaptation without full retraining. However, sequential updates can lead to
catastrophic forgetting, where new edits degrade previously acquired knowledge.
This work presents STABLE, a gated continual self editing framework that
constrains forgetting during sequential updates using parameter efficient fine
tuning via Low Rank Adaptation (LoRA; see arXiv:2106.09685). Each candidate
edit is evaluated against a stability budget using one of three metrics: (i)
Exact Match (EM) drop, capturing factual accuracy loss; (ii) bits increase,
reflecting reduced model confidence; and (iii) KL divergence, quantifying
distributional drift between the base and adapted models. If a threshold is
exceeded, the LoRA update is rescaled through a clipping procedure or rejected.
Experiments on the Qwen-2.5-7B model show that gating effectively mitigates
forgetting while preserving adaptability. EM based gating achieved the highest
cumulative performance in short continual learning sequences. Our results show
that different gating strategies can achieve comparable distribution shift
(measured by KL divergence) while producing different accuracy outcomes,
highlighting the importance of gating design in continual adaptation. This
approach offers a principled method for continual model editing, enabling LLMs
to integrate new knowledge while maintaining reliability. Code:
https://github.com/Bhoy1/STABLE

</details>


### [399] [Compressing Many-Shots in In-Context Learning](https://arxiv.org/abs/2510.16092)
*Devvrit Khatri,Pranamya Kulkarni,Nilesh Gupta,Yerram Varun,Liqian Peng,Jay Yagnik,Praneeth Netrapalli,Cho-Jui Hsieh,Alec Go,Inderjit S Dhillon,Aditya Kusupati,Prateek Jain*

Main category: cs.LG

TL;DR: 提出MemCom方法，通过层间压缩技术有效压缩多示例提示，在保持高准确率的同时显著减少内存和计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有提示压缩方法在多示例压缩中效果不佳，而简单减少示例数量作为基线方法表现意外强劲。需要更高效的压缩方法来平衡性能与计算成本。

Method: 提出MemCom层间压缩方法，使用更强的压缩器模型，在transformer的每一层进行细粒度压缩，为每层提供独立的压缩表示。

Result: 在多个分类任务上，MemCom在所有压缩比下均优于强基线。在高压缩比时，基线性能下降20-30%，而MemCom仅下降不到10%，保持高准确率。

Conclusion: 层间压缩和更强的压缩器模型是实现多示例提示有效压缩的关键，MemCom方法在保持性能的同时显著提升了计算效率。

Abstract: Large Language Models (LLMs) have been shown to be able to learn different
tasks without explicit finetuning when given many input-output examples /
demonstrations through In-Context Learning (ICL). Increasing the number of
examples, called ``shots'', improves downstream task performance but incurs
higher memory and computational costs. In this work, we study an approach to
improve the memory and computational efficiency of ICL inference by compressing
the many-shot prompts. Given many shots comprising t tokens, our goal is to
generate a m soft-token summary, where m < t. We first show that existing
prompt compression methods are ineffective for many-shot compression, and
simply using fewer shots as a baseline is surprisingly strong. To achieve
effective compression, we find that: (a) a stronger compressor model with more
trainable parameters is necessary, and (b) compressing many-shot
representations at each transformer layer enables more fine-grained compression
by providing each layer with its own compressed representation. Based on these
insights, we propose MemCom, a layer-wise compression method. We systematically
evaluate various compressor models and training approaches across different
model sizes (2B and 7B), architectures (Gemma and Mistral), many-shot sequence
lengths (3k-6k tokens), and compression ratios (3x to 8x). MemCom outperforms
strong baselines across all compression ratios on multiple classification tasks
with large label sets. Notably, while baseline performance degrades sharply at
higher compression ratios, often by over 20-30%, MemCom maintains high accuracy
with minimal degradation, typically dropping by less than 10%.

</details>


### [400] [Zero-shot World Models via Search in Memory](https://arxiv.org/abs/2510.16123)
*Federico Malato,Ville Hautamäki*

Main category: cs.LG

TL;DR: 提出了一种基于相似性搜索和随机表示的无训练世界模型，与Dreamer家族的PlaNet模型进行比较，在潜在重建质量和长时程预测方面表现相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 利用相似性搜索和随机表示来近似世界模型，避免复杂的训练过程，提高模型的实用性和效率。

Method: 使用相似性搜索和随机表示构建无训练的世界模型，与PlaNet模型在潜在重建和图像相似性方面进行比较评估。

Result: 搜索式世界模型在单步和长时程动态预测方面与基于训练的方法表现相当，在视觉差异较大的环境中长时程预测表现更优。

Conclusion: 基于搜索的世界模型可以替代基于训练的方法，在保持性能的同时避免了复杂的训练过程，特别是在长时程预测任务中表现突出。

Abstract: World Models have vastly permeated the field of Reinforcement Learning. Their
ability to model the transition dynamics of an environment have greatly
improved sample efficiency in online RL. Among them, the most notorious example
is Dreamer, a model that learns to act in a diverse set of image-based
environments. In this paper, we leverage similarity search and stochastic
representations to approximate a world model without a training procedure. We
establish a comparison with PlaNet, a well-established world model of the
Dreamer family. We evaluate the models on the quality of latent reconstruction
and on the perceived similarity of the reconstructed image, on both next-step
and long horizon dynamics prediction. The results of our study demonstrate that
a search-based world model is comparable to a training based one in both cases.
Notably, our model show stronger performance in long-horizon prediction with
respect to the baseline on a range of visually different environments.

</details>


### [401] [AtomBench: A Benchmark for Generative Atomic Structure Models using GPT, Diffusion, and Flow Architectures](https://arxiv.org/abs/2510.16165)
*Charles Rhys Campbell,Aldo H. Romero,Kamal Choudhary*

Main category: cs.LG

TL;DR: 对三种代表性生成模型（AtomGPT、CDVAE、FlowMM）在超导材料数据集上的性能进行系统性基准测试，发现CDVAE表现最佳


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在材料发现中应用日益广泛，但缺乏对其性能的严格比较评估

Method: 使用两种公开超导数据集训练三种生成模型，通过KL散度和平均绝对误差评估晶格参数重建性能

Result: CDVAE表现最优，其次是AtomGPT，FlowMM表现最差

Conclusion: CDVAE在晶体结构生成任务中表现最佳，为材料发现提供了有价值的基准

Abstract: Generative models have become significant assets in the exploration and
identification of new materials, enabling the rapid proposal of candidate
crystal structures that satisfy target properties. Despite the increasing
adoption of diverse architectures, a rigorous comparative evaluation of their
performance on materials datasets is lacking. In this work, we present a
systematic benchmark of three representative generative models- AtomGPT (a
transformer-based model), Crystal Diffusion Variational Autoencoder (CDVAE),
and FlowMM (a Riemannian flow matching model). These models were trained to
reconstruct crystal structures from subsets of two publicly available
superconductivity datasets- JARVIS Supercon 3D and DS A/B from the Alexandria
database. Performance was assessed using the Kullback-Leibler (KL) divergence
between predicted and reference distributions of lattice parameters, as well as
the mean absolute error (MAE) of individual lattice constants. For the computed
KLD and MAE scores, CDVAE performs most favorably, followed by AtomGPT, and
then FlowMM. All benchmarking code and model configurations will be made
publicly available at https://github.com/atomgptlab/atombench_inverse.

</details>


### [402] [Alignment is Localized: A Causal Probe into Preference Layers](https://arxiv.org/abs/2510.16167)
*Archie Chaudhury*

Main category: cs.LG

TL;DR: 通过层间因果修补分析语言模型对齐机制，发现对齐过程是空间局部化的，主要由中间层激活决定奖励一致行为，而非扩散的参数化过程。


<details>
  <summary>Details</summary>
Motivation: 尽管基于人类反馈的强化学习（RLHF）已成为语言模型偏好微调的流行方法，但其内部对齐机制仍不透明，需要系统分析对齐如何实现。

Method: 在基础模型和调优模型之间应用层间因果修补，使用LASSO回归分析激活距离与奖励增益的关系。

Result: 对齐是空间局部化的：中间层激活编码了决定奖励一致行为的独特子空间，而早期和晚期层基本不受影响；只有少数层具有非零系数连接激活距离与奖励增益。

Conclusion: 基于人类偏好的语言模型对齐是一个定向、低秩的过程，而非扩散的参数化过程。

Abstract: Reinforcement Learning frameworks, particularly those utilizing human
annotations, have become an increasingly popular method for preference
fine-tuning, where the outputs of a language model are tuned to match a certain
set of behavioral policies or guidelines. Reinforcement Learning through Human
Feedback (RLHF) is perhaps the most popular implementation of such a framework,
particularly for aligning LMs toward safety and human intent. However, the
internal workings of how such alignment is achieved remain largely opaque. In
this work, we systematically analyze preference optimization for language model
alignment by applying layer-wide causal patching between a base model and its
tuned counterpart across human preference pairs. We implement our methodology
on \textit{Llama-3.2-1B}, and find that alignment is spatially localized:
mid-layer activations encode a distinct subspace that causally determines
reward-consistent behavior, while early and late layers remain largely
unaffected. Utilizing LASSO regression, we also find that only a small number
of layers possess non-zero coefficients linking activation distances to reward
gains. Overall, we show that, at least for some language models, alignment from
human-based, preferential tuning is a directional, low rank process, rather
than diffuse and parameteric.

</details>


### [403] [Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness](https://arxiv.org/abs/2510.16171)
*Longwei Wang,Ifrat Ikhtear Uddin,KC Santosh,Chaowei Zhang,Xiao Qin,Yang Zhou*

Main category: cs.LG

TL;DR: 该论文提出通过嵌入群等变卷积层（旋转和尺度等变）来增强卷积神经网络的对抗鲁棒性，无需对抗训练即可提升模型对对抗攻击的防御能力。


<details>
  <summary>Details</summary>
Motivation: 对抗训练作为主要防御策略存在计算成本高和可能降低干净数据准确性的问题，需要探索更高效的架构方法来增强对抗鲁棒性。

Method: 提出两种对称感知架构：并行设计（独立处理标准和等变特征后融合）和级联设计（顺序应用等变操作），嵌入旋转和尺度等变卷积层。

Result: 在CIFAR-10、CIFAR-100和CIFAR-10C数据集上，模型在FGSM和PGD攻击下一致提升了对抗鲁棒性和泛化能力，无需对抗训练。

Conclusion: 对称强制架构作为数据增强防御的高效原则性替代方案具有巨大潜力，能够降低假设空间复杂度、正则化梯度并获得更紧的认证鲁棒性边界。

Abstract: Adversarial examples reveal critical vulnerabilities in deep neural networks
by exploiting their sensitivity to imperceptible input perturbations. While
adversarial training remains the predominant defense strategy, it often incurs
significant computational cost and may compromise clean-data accuracy. In this
work, we investigate an architectural approach to adversarial robustness by
embedding group-equivariant convolutions-specifically, rotation- and
scale-equivariant layers-into standard convolutional neural networks (CNNs).
These layers encode symmetry priors that align model behavior with structured
transformations in the input space, promoting smoother decision boundaries and
greater resilience to adversarial attacks. We propose and evaluate two
symmetry-aware architectures: a parallel design that processes standard and
equivariant features independently before fusion, and a cascaded design that
applies equivariant operations sequentially. Theoretically, we demonstrate that
such models reduce hypothesis space complexity, regularize gradients, and yield
tighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme
Value for nEtwork Robustness) framework. Empirically, our models consistently
improve adversarial robustness and generalization across CIFAR-10, CIFAR-100,
and CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial
training. These findings underscore the potential of symmetry-enforcing
architectures as efficient and principled alternatives to data
augmentation-based defenses.

</details>


### [404] [The Formalism-Implementation Gap in Reinforcement Learning Research](https://arxiv.org/abs/2510.16175)
*Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 本文主张强化学习研究应停止仅关注智能体性能展示，而应更关注理解学习动态，并需要更精确地定义基准测试与数学形式化之间的映射关系。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习研究过度关注性能展示而忽视了对学习动态的理解，这可能导致在学术基准上的过拟合，并使技术难以迁移到新问题。

Method: 以Arcade Learning Environment (ALE)为例，说明即使被认为是"饱和"的基准测试，仍可有效用于发展对强化学习的理解。

Result: 论证了重新聚焦研究重点的必要性，提出了更精确的基准测试映射方法。

Conclusion: 强化学习研究需要从单纯性能展示转向科学理解，并改进基准测试的设计和使用方式。

Abstract: The last decade has seen an upswing in interest and adoption of reinforcement
learning (RL) techniques, in large part due to its demonstrated capabilities at
performing certain tasks at "super-human levels". This has incentivized the
community to prioritize research that demonstrates RL agent performance, often
at the expense of research aimed at understanding their learning dynamics.
Performance-focused research runs the risk of overfitting on academic
benchmarks -- thereby rendering them less useful -- which can make it difficult
to transfer proposed techniques to novel problems. Further, it implicitly
diminishes work that does not push the performance-frontier, but aims at
improving our understanding of these techniques. This paper argues two points:
(i) RL research should stop focusing solely on demonstrating agent
capabilities, and focus more on advancing the science and understanding of
reinforcement learning; and (ii) we need to be more precise on how our
benchmarks map to the underlying mathematical formalisms. We use the popular
Arcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a
benchmark that, despite being increasingly considered "saturated", can be
effectively used for developing this understanding, and facilitating the
deployment of RL techniques in impactful real-world problems.

</details>


### [405] [Human-Allied Relational Reinforcement Learning](https://arxiv.org/abs/2510.16188)
*Fateme Golivand Darvishvand,Hikaru Shindo,Sahil Sidheekh,Kristian Kersting,Sriraam Natarajan*

Main category: cs.LG

TL;DR: 提出了一种结合关系强化学习与物体中心表示的新框架，能够处理结构化和非结构化数据，并通过主动查询人类专家来增强学习效果。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在处理结构化问题时忽略了问题的内在结构，而关系强化学习虽然能处理结构化问题但对问题结构有强假设限制。需要一种能同时处理结构化和非结构化数据的方法。

Method: 结合关系强化学习与物体中心表示，通过显式建模策略不确定性，允许系统主动向人类专家查询指导。

Result: 实证评估表明所提方法具有有效性和高效性。

Conclusion: 该框架成功解决了关系强化学习对问题结构的强假设限制，能够有效处理混合类型数据并提升学习效率。

Abstract: Reinforcement learning (RL) has experienced a second wind in the past decade.
While incredibly successful in images and videos, these systems still operate
within the realm of propositional tasks ignoring the inherent structure that
exists in the problem. Consequently, relational extensions (RRL) have been
developed for such structured problems that allow for effective generalization
to arbitrary number of objects. However, they inherently make strong
assumptions about the problem structure. We introduce a novel framework that
combines RRL with object-centric representation to handle both structured and
unstructured data. We enhance learning by allowing the system to actively query
the human expert for guidance by explicitly modeling the uncertainty over the
policy. Our empirical evaluation demonstrates the effectiveness and efficiency
of our proposed approach.

</details>


### [406] [Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal](https://arxiv.org/abs/2510.16233)
*Patricia West,Michelle WL Wan,Alexander Hepburn,Edwin Simpson,Raul Santos-Rodriguez,Jeffrey N Clark*

Main category: cs.LG

TL;DR: 使用机器学习方法分析欧洲绿色协议气候政策的进展状态，比较不同文本表示方法和元数据特征对预测性能的影响。


<details>
  <summary>Details</summary>
Motivation: 气候变化需要有效的立法行动来减轻其影响，本研究旨在探索机器学习在理解气候政策从宣布到采纳过程中的应用。

Method: 收集165项政策的文本和元数据，使用TF-IDF、BERT和ClimateBERT等文本表示方法，结合元数据特征预测政策进展状态。

Result: 仅使用文本特征时，ClimateBERT表现最佳（RMSE=0.17，R²=0.29）；结合元数据特征后，BERT表现最优（RMSE=0.16，R²=0.38）。

Conclusion: 机器学习工具在支持气候政策分析和决策方面具有潜力，可解释AI方法揭示了政策措辞、政党背景和国家代表性等因素的影响。

Abstract: Climate change demands effective legislative action to mitigate its impacts.
This study explores the application of machine learning (ML) to understand the
progression of climate policy from announcement to adoption, focusing on
policies within the European Green Deal. We present a dataset of 165 policies,
incorporating text and metadata. We aim to predict a policy's progression
status, and compare text representation methods, including TF-IDF, BERT, and
ClimateBERT. Metadata features are included to evaluate the impact on
predictive performance. On text features alone, ClimateBERT outperforms other
approaches (RMSE = 0.17, R^2 = 0.29), while BERT achieves superior performance
with the addition of metadata features (RMSE = 0.16, R^2 = 0.38). Using methods
from explainable AI highlights the influence of factors such as policy wording
and metadata including political party and country representation. These
findings underscore the potential of ML tools in supporting climate policy
analysis and decision-making.

</details>


### [407] [WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale](https://arxiv.org/abs/2510.16252)
*Yuxuan Lu,Jing Huang,Hui Liu,Jiri Gesi,Yan Han,Shihan Fu,Tianqi Zheng,Dakuo Wang*

Main category: cs.LG

TL;DR: WEBSERV是一个可扩展的强化学习Web代理环境，通过紧凑的浏览器环境和高效的服务端管理，解决了现有环境在上下文噪声、动作不确定性和扩展性方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RL Web代理环境存在上下文噪声过多、动作执行不确定、以及无法有效扩展并行RL训练等问题，需要一个新的环境来支持大规模、高效的训练和评估。

Method: 提出WEBSERV环境，包括：1）紧凑、站点无关的浏览器环境，平衡上下文和动作复杂性；2）通过高效启动和重置Web服务器实现可扩展的RL环境。

Result: 在WebArena的购物CMS和Gitlab任务上达到最先进的单提示成功率，同时将启动延迟降低约5倍，存储需求减少约240倍，内存占用相当，支持单主机上200+并发容器。

Conclusion: WEBSERV提供了一个可扩展且高效的RL Web代理训练环境，显著提升了性能并降低了资源需求。

Abstract: Training and evaluation of Reinforcement Learning (RL) web agents have gained
increasing attention, yet a scalable and efficient environment that couples
realistic and robust browser-side interaction with controllable server-side
state at scale is still missing. Existing environments tend to have one or more
of the following issues: they overwhelm policy models with excessive and noisy
context; they perform actions non-deterministically without waiting for the UI
or network to stabilize; or they cannot scale isolated client-server containers
effectively for parallel RL rollouts. We propose WEBSERV, an environment that
includes 1) a compact, site-agnostic browser environment that balances context
and action complexity, and 2) a scalable RL environment via efficient launching
and resetting web-servers to enable scalable RL training and evaluation. We
evaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving
state-of-the-art single-prompt success rates while cutting launch latency by
~5x and storage need by ~240x, with a comparable memory footprint, enabling
200+ concurrent containers on a single host.

</details>


### [408] [Disentangling Hyperedges through the Lens of Category Theory](https://arxiv.org/abs/2510.16289)
*Yoonho Lee,Junseok Lee,Sangwoo Seo,Sungwon Kim,Yeongmin Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: 本文从范畴论角度分析超边解缠，提出了基于自然性条件的解缠准则，并在基因通路数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管解缠表示学习在图结构数据中表现出色，但很少有研究探索超图结构数据的解缠。将超边解缠整合到超图神经网络中，可以挖掘与标签相关的隐藏超边语义。

Method: 从范畴论视角分析超边解缠，提出基于自然性条件的新解缠准则，并构建概念验证模型。

Result: 在基因通路数据上的实验表明，所提准则成功捕捉了基因（节点）在遗传通路（超边）中的功能关系。

Conclusion: 提出的解缠准则具有潜力，能够有效挖掘超图中的隐藏语义关系。

Abstract: Despite the promising results of disentangled representation learning in
discovering latent patterns in graph-structured data, few studies have explored
disentanglement for hypergraph-structured data. Integrating hyperedge
disentanglement into hypergraph neural networks enables models to leverage
hidden hyperedge semantics, such as unannotated relations between nodes, that
are associated with labels. This paper presents an analysis of hyperedge
disentanglement from a category-theoretical perspective and proposes a novel
criterion for disentanglement derived from the naturality condition. Our
proof-of-concept model experimentally showed the potential of the proposed
criterion by successfully capturing functional relations of genes (nodes) in
genetic pathways (hyperedges).

</details>


### [409] [QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models](https://arxiv.org/abs/2510.16292)
*Yutong Wang,Haiyu Wang,Sai Qian Zhang*

Main category: cs.LG

TL;DR: 提出一种结合奇异值分解(SVD)和量化的方法，通过动态调整SVD秩来减少视觉语言模型的KV缓存大小和计算开销，在保持精度的同时显著降低内存使用和计算成本。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型(VLMs)在图像描述和视觉问答等任务中应用广泛，但其高计算成本和内存占用限制了可扩展性和实时应用。

Method: 对联合QKV权重矩阵应用奇异值分解(SVD)来减少KV缓存大小，引入动态SVD秩分配策略，并结合权重和激活的量化技术。

Result: 相比仅使用量化或SVD的方法，准确率提升超过10%，同时硬件成本更低，更适合资源受限设备的实时部署。

Conclusion: 该方法通过SVD和量化的结合，在显著降低计算和内存开销的同时保持了模型精度，为资源受限环境下的VLM部署提供了有效解决方案。

Abstract: Vision-Language Models (VLMs) are integral to tasks such as image captioning
and visual question answering, but their high computational cost, driven by
large memory footprints and processing time, limits their scalability and
real-time applicability. In this work, we propose leveraging Singular-Value
Decomposition (SVD) over the joint query (Q), key (K), and value (V) weight
matrices to reduce KV cache size and computational overhead. We in addition
introduce an efficient rank allocation strategy that dynamically adjusts the
SVD rank based on its impact on VLM accuracy, achieving a significant reduction
in both memory usage and computational cost. Finally, we extend this approach
by applying quantization to both VLM weights and activations, resulting in a
highly efficient VLM. Our method outperforms previous approaches that rely
solely on quantization or SVD by achieving more than $10\%$ accuracy
improvement while consuming less hardware cost, making it better for real-time
deployment on resource-constrained devices. We open source our code at
\href{https://github.com/SAI-Lab-NYU/QSVD}{\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.

</details>


### [410] [Scaffold-Aware Generative Augmentation and Reranking for Enhanced Virtual Screening](https://arxiv.org/abs/2510.16306)
*Xin Wang,Yu Wang,Yunchao Liu,Jens Meiler,Tyler Derr*

Main category: cs.LG

TL;DR: ScaffAug是一个基于支架的虚拟筛选框架，通过生成式AI增强数据、自训练和重排序模块，解决类别不平衡、结构不平衡和支架多样性问题。


<details>
  <summary>Details</summary>
Motivation: 虚拟筛选面临三大挑战：活性化合物比例低的类别不平衡、某些支架占主导地位的结构不平衡，以及需要识别结构多样的活性化合物用于新药开发。

Method: 1. 增强模块：使用图扩散模型基于实际命中化合物的支架生成合成数据；2. 自训练模块：安全整合生成数据与原始数据；3. 重排序模块：提高推荐分子集中的支架多样性。

Result: 在五个靶点类别上的计算实验表明，ScaffAug在多个评估指标上优于现有基线方法，同时提高了支架多样性。

Conclusion: 该工作通过生成式增强、重排序和支架感知，为有效增强虚拟筛选提供了新的视角。

Abstract: Ligand-based virtual screening (VS) is an essential step in drug discovery
that evaluates large chemical libraries to identify compounds that potentially
bind to a therapeutic target. However, VS faces three major challenges: class
imbalance due to the low active rate, structural imbalance among active
molecules where certain scaffolds dominate, and the need to identify
structurally diverse active compounds for novel drug development. We introduce
ScaffAug, a scaffold-aware VS framework that addresses these challenges through
three modules. The augmentation module first generates synthetic data
conditioned on scaffolds of actual hits using generative AI, specifically a
graph diffusion model. This helps mitigate the class imbalance and furthermore
the structural imbalance, due to our proposed scaffold-aware sampling
algorithm, designed to produce more samples for active molecules with
underrepresented scaffolds. A model-agnostic self-training module is then used
to safely integrate the generated synthetic data from our augmentation module
with the original labeled data. Lastly, we introduce a reranking module that
improves VS by enhancing scaffold diversity in the top recommended set of
molecules, while still maintaining and even enhancing the overall general
performance of identifying novel, active compounds. We conduct comprehensive
computational experiments across five target classes, comparing ScaffAug
against existing baseline methods by reporting the performance of multiple
evaluation metrics and performing ablation studies on ScaffAug. Overall, this
work introduces novel perspectives on effectively enhancing VS by leveraging
generative augmentations, reranking, and general scaffold-awareness.

</details>


### [411] [Toward General Digraph Contrastive Learning: A Dual Spatial Perspective](https://arxiv.org/abs/2510.16311)
*Daohan Su,Yang Zhang,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: S2-DiGCL是一个针对有向图的对比学习框架，通过复数域和实数域的双重视角来捕捉方向信息，在节点分类和链接预测任务中实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图对比学习方法主要关注无向图，忽略了现实网络（如社交网络和推荐系统）中至关重要的方向信息。

Method: 从复数域视角，在磁拉普拉斯矩阵中引入个性化扰动来调制边相位和方向语义；从实数域视角，使用基于路径的子图增强策略来捕捉细粒度局部不对称性和拓扑依赖。

Result: 在7个真实世界有向图数据集上的实验表明，该方法在节点分类和链接预测任务中分别实现了4.41%和4.34%的性能提升。

Conclusion: S2-DiGCL通过结合复数域和实数域的双重视角，能够构建更高质量的正负样本，实现更通用和鲁棒的有向图对比学习。

Abstract: Graph Contrastive Learning (GCL) has emerged as a powerful tool for
extracting consistent representations from graphs, independent of labeled
information. However, existing methods predominantly focus on undirected
graphs, disregarding the pivotal directional information that is fundamental
and indispensable in real-world networks (e.g., social networks and
recommendations).In this paper, we introduce S2-DiGCL, a novel framework that
emphasizes spatial insights from complex and real domain perspectives for
directed graph (digraph) contrastive learning. From the complex-domain
perspective, S2-DiGCL introduces personalized perturbations into the magnetic
Laplacian to adaptively modulate edge phases and directional semantics. From
the real-domain perspective, it employs a path-based subgraph augmentation
strategy to capture fine-grained local asymmetries and topological
dependencies. By jointly leveraging these two complementary spatial views,
S2-DiGCL constructs high-quality positive and negative samples, leading to more
general and robust digraph contrastive learning. Extensive experiments on 7
real-world digraph datasets demonstrate the superiority of our approach,
achieving SOTA performance with 4.41% improvement in node classification and
4.34% in link prediction under both supervised and unsupervised settings.

</details>


### [412] [Memorizing Long-tail Data Can Help Generalization Through Composition](https://arxiv.org/abs/2510.16322)
*Mo Zhou,Haoyang Ma,Rong Ge*

Main category: cs.LG

TL;DR: 论文探讨了记忆化与简单组合能力之间的协同作用，发现在线性设置中，记忆化结合组合能力可以帮助模型对需要长尾特征组合的罕见测试样本做出正确预测，即使训练数据中从未出现过这种组合。


<details>
  <summary>Details</summary>
Motivation: 深度学习促使研究者重新思考记忆化与泛化之间的关系。在许多情况下，记忆化不会损害泛化，反而可能通过记忆长尾样本来帮助泛化。本文研究记忆化与简单组合能力之间的协同作用。

Method: 在线性设置中进行理论分析，并在简单数据上对神经网络架构进行实验验证。

Result: 理论分析表明，记忆化与组合能力结合可以帮助模型对需要长尾特征组合的罕见测试样本做出正确预测。实验证实这一理论洞察适用于非线性设置，并发现模型的组合能力取决于其架构。

Conclusion: 记忆化与组合能力之间存在协同作用，这种协同可以帮助模型泛化到未见过的长尾特征组合，且模型的架构影响其组合能力。

Abstract: Deep learning has led researchers to rethink the relationship between
memorization and generalization. In many settings, memorization does not hurt
generalization due to implicit regularization and may help by memorizing
long-tailed examples. In this paper, we consider the synergy between
memorization and simple composition -- the ability to make correct prediction
on a combination of long-tailed features. Theoretically, we show that for a
linear setting, memorization together with composition can help the model make
correct predictions on rare test examples that require a combination of
long-tailed features, even if such combinations were never observed in the
training data. Experiments on neural network architecture on simple data show
that the theoretical insight extends beyond the linear setting, and we further
observe that the composition capability of the model depends on its
architecture.

</details>


### [413] [MGTS-Net: Exploring Graph-Enhanced Multimodal Fusion for Augmented Time Series Forecasting](https://arxiv.org/abs/2510.16350)
*Shule Hao,Junpeng Bao,Wenli Li*

Main category: cs.LG

TL;DR: 提出了MGTS-Net，一种用于时间序列预测的多模态图增强网络，通过优化多模态特征提取、构建异构图融合多模态信息、以及动态多尺度预测，解决了现有方法在细粒度时序模式提取、多模态信息融合和动态多尺度特征适应方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法在整合多模态特征时面临三个关键挑战：细粒度时序模式提取不足、多模态信息融合效果不佳、以及对动态多尺度特征的适应能力有限。

Method: MGTS-Net包含三个核心组件：(1)多模态特征提取层，针对时序、视觉和文本模态优化特征编码器；(2)多模态特征融合层，构建异构图建模模态内时序依赖和跨模态对齐关系；(3)多尺度预测层，动态加权融合短期、中期和长期预测器输出。

Result: 大量实验表明MGTS-Net在轻量级和高效率下表现出优异性能，相比其他最先进的基线模型取得了更优越的性能。

Conclusion: 该方法验证了所提方法的优越性，为多模态时间序列预测提供了有效的解决方案。

Abstract: Recent research in time series forecasting has explored integrating
multimodal features into models to improve accuracy. However, the accuracy of
such methods is constrained by three key challenges: inadequate extraction of
fine-grained temporal patterns, suboptimal integration of multimodal
information, and limited adaptability to dynamic multi-scale features. To
address these problems, we propose MGTS-Net, a Multimodal Graph-enhanced
Network for Time Series forecasting. The model consists of three core
components: (1) a Multimodal Feature Extraction layer (MFE), which optimizes
feature encoders according to the characteristics of temporal, visual, and
textual modalities to extract temporal features of fine-grained patterns; (2) a
Multimodal Feature Fusion layer (MFF), which constructs a heterogeneous graph
to model intra-modal temporal dependencies and cross-modal alignment
relationships and dynamically aggregates multimodal knowledge; (3) a
Multi-Scale Prediction layer (MSP), which adapts to multi-scale features by
dynamically weighting and fusing the outputs of short-term, medium-term, and
long-term predictors. Extensive experiments demonstrate that MGTS-Net exhibits
excellent performance with light weight and high efficiency. Compared with
other state-of-the-art baseline models, our method achieves superior
performance, validating the superiority of the proposed methodology.

</details>


### [414] [Modeling Expert Interactions in Sparse Mixture of Experts via Graph Structures](https://arxiv.org/abs/2510.16411)
*Minh-Khoi Nguyen-Nhat,Rachel S. Y. Teo,Laziz Abdullaev,Maurice Mok,Viet-Hoang Tran,Tan Minh Nguyen*

Main category: cs.LG

TL;DR: SymphonySMoE是一种新型的稀疏混合专家模型，通过引入专家间的社交图来增强令牌路由过程，解决了传统SMoE在数据分布变化下的鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏混合专家模型虽然能有效扩展模型规模并保持计算效率，但在面对数据分布变化时鲁棒性较差，特别是在数据污染情况下表现不佳。

Method: 提出SymphonySMoE，在SMoE基础上引入专家间的社交图结构来建模专家交互，改进令牌路由过程，该方法是轻量级、模块化的，可与现有SMoE模型无缝集成。

Result: 在语言建模和视觉指令调优任务上的广泛实验验证了方法的有效性，理论分析和实证证据都表明SymphonySMoE优于基线SMoE模型，并成功扩展到42亿和74亿参数规模。

Conclusion: SymphonySMoE通过图结构增强的专家交互机制，有效提升了SMoE在分布变化下的鲁棒性，同时保持了模型的可扩展性和实用性。

Abstract: Sparse Mixture of Experts (SMoE) has emerged as a promising solution to
achieving unparalleled scalability in deep learning by decoupling model
parameter count from computational cost. By activating only a small subset of
parameters per sample, SMoE enables significant growth in model capacity while
maintaining efficiency. However, SMoE struggles to adapt to distributional
shifts, leading to reduced robustness under data contamination. In this work,
we introduce SymphonySMoE, a novel family of SMoE that introduces a social
graph to model interactions among experts. This graph-based structure enhances
the token routing process, addressing the robustness challenges that are
inherent in conventional SMoE designs. SymphonySMoE is lightweight, modular,
and integrates seamlessly with existing SMoE-based models such as the XMoE and
the Generalist Language Model. We provide both theoretical analysis and
empirical evidence demonstrating SymphonySMoE's advantages over baseline SMoE.
Extensive experiments on language modeling and visual instruction tuning
validate our method's effectiveness. We further highlight the scalability of
SymphonySMoE to models with 4.2 and 7.4 billion parameters, showcasing its
applicability in fine-tuning tasks for large-scale systems.

</details>


### [415] [Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution](https://arxiv.org/abs/2510.16440)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 本文提出了在ECML-PKDD 2025高能物理发现挑战赛中Task 1的获胜解决方案，采用基于梯度的多轮对抗攻击策略，在最小化扰动的同时最大化分类错误率。


<details>
  <summary>Details</summary>
Motivation: 该任务要求设计对抗攻击来对抗提供的分类模型，目标是在最小化扰动的同时最大化误分类率。

Method: 采用基于梯度的多轮攻击策略，利用模型的可微分结构，结合随机初始化和样本混合技术来增强攻击效果。

Result: 所提出的攻击方法在扰动大小和欺骗成功率方面取得了最佳结果，在竞赛中获得了第一名。

Conclusion: 多轮梯度攻击结合随机初始化和样本混合技术能够有效生成对抗样本，在保持小扰动的同时实现高误分类率。

Abstract: This report presents the winning solution for Task 1 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The task required designing an adversarial attack against a
provided classification model that maximizes misclassification while minimizing
perturbations. Our approach employs a multi-round gradient-based strategy that
leverages the differentiable structure of the model, augmented with random
initialization and sample-mixing techniques to enhance effectiveness. The
resulting attack achieved the best results in perturbation size and fooling
success rate, securing first place in the competition.

</details>


### [416] [Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution](https://arxiv.org/abs/2510.16443)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 本文提出了在ECML-PKDD 2025高能物理发现挑战赛中Task 2的获胜解决方案，通过数据生成和鲁棒模型训练两阶段方法，在对抗性攻击下实现了80%的混合准确率。


<details>
  <summary>Details</summary>
Motivation: 设计能够同时在干净数据和对抗性数据上实现高准确率的鲁棒ANN模型，应对高能物理发现中的对抗攻击挑战。

Method: 采用两阶段方法：1) 基于RDSA方法生成1500万人工训练样本；2) 构建包含特征嵌入块（共享权重）和密集融合尾部的鲁棒架构进行训练。

Result: 在混合数据集上达到80%的准确率，比第二名解决方案高出2个百分点。

Conclusion: 提出的两阶段方法结合对抗数据生成和专门设计的鲁棒架构，在高能物理对抗学习挑战中取得了最佳性能。

Abstract: This report presents the winning solution for Task 2 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The goal of the challenge was to design and train a robust
ANN-based model capable of achieving high accuracy in a binary classification
task on both clean and adversarial data generated with the Random Distribution
Shuffle Attack (RDSA). Our solution consists of two components: a data
generation phase and a robust model training phase. In the first phase, we
produced 15 million artificial training samples using a custom methodology
derived from Random Distribution Shuffle Attack (RDSA). In the second phase, we
introduced a robust architecture comprising (i)a Feature Embedding Block with
shared weights among features of the same type and (ii)a Dense Fusion Tail
responsible for the final prediction. Training this architecture on our
adversarial dataset achieved a mixed accuracy score of 80\%, exceeding the
second-place solution by two percentage points.

</details>


### [417] [Input Domain Aware MoE: Decoupling Routing Decisions from Task Optimization in Mixture of Experts](https://arxiv.org/abs/2510.16448)
*Yongxiang Hua,Haoyu Cao,Zhou Tao,Bocheng Li,Zihao Wu,Chaohu Liu,Linli Xu*

Main category: cs.LG

TL;DR: 提出Input Domain Aware MoE路由框架，通过概率混合模型更好地划分输入空间，解决现有稀疏专家混合模型在专家专业化和计算平衡之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于相似性评分的路由机制难以有效捕捉输入底层结构，导致专家专业化与计算平衡之间的权衡，阻碍模型扩展性和性能提升。

Method: 使用概率混合模型建模路由概率，将路由概率表示为分布的混合，使专家形成清晰的专业化边界，同时实现均衡利用。路由机制独立于任务特定目标进行训练。

Result: 在视觉语言任务上的实验结果表明，该方法持续优于现有稀疏专家混合方法，获得更高的任务性能和更好的专家利用平衡。

Conclusion: 提出的Input Domain Aware MoE路由框架通过概率混合模型有效解决了专家专业化和计算平衡的权衡问题，提升了稀疏专家混合模型的性能。

Abstract: Sparse Mixture of Experts (sMoE) has become a pivotal approach for scaling
large vision-language models, offering substantial capacity while maintaining
computational efficiency through dynamic, sparse activation of experts.
However, existing routing mechanisms, typically based on similarity scoring,
struggle to effectively capture the underlying input structure. This limitation
leads to a trade-off between expert specialization and balanced computation,
hindering both scalability and performance. We propose Input Domain Aware MoE,
a novel routing framework that leverages a probabilistic mixture model to
better partition the input space. By modeling routing probabilities as a
mixture of distributions, our method enables experts to develop clear
specialization boundaries while achieving balanced utilization. Unlike
conventional approaches, our routing mechanism is trained independently of
task-specific objectives, allowing for stable optimization and decisive expert
assignments. Empirical results on vision-language tasks demonstrate that our
method consistently outperforms existing sMoE approaches, achieving higher task
performance and improved expert utilization balance.

</details>


### [418] [SCALAR: Self-Calibrating Adaptive Latent Attention Representation Learning](https://arxiv.org/abs/2510.16474)
*Farwa Abbas,Hussain Ahmad,Claudia Szabo*

Main category: cs.LG

TL;DR: 提出了一种新的自适应核注意力机制，通过分别处理不同特征组来提升高维异构数据的预测性能，解决了传统PLS方法在处理复杂非线性关系和跨尺度交互时的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统投影到潜在结构(PLS)方法在处理高维异构数据时面临挑战，特别是难以建模复杂非线性关系、多变量系统中的高维相关结构，以及跨尺度同时交互。静态特征加权限制了上下文变化的适应性，忽略了样本特定相关性。

Method: 引入自适应核基注意力机制，分别处理不同的特征组后再进行整合，既能捕捉局部模式又能保持全局关系。

Result: 实验结果表明，与最先进方法相比，在多个数据集上性能指标有显著提升。

Conclusion: 所提出的方法通过新颖的架构创新有效解决了高维异构数据建模中的关键挑战，显著提升了预测性能。

Abstract: High-dimensional, heterogeneous data with complex feature interactions pose
significant challenges for traditional predictive modeling approaches. While
Projection to Latent Structures (PLS) remains a popular technique, it struggles
to model complex non-linear relationships, especially in multivariate systems
with high-dimensional correlation structures. This challenge is further
compounded by simultaneous interactions across multiple scales, where local
processing fails to capture crossgroup dependencies. Additionally, static
feature weighting limits adaptability to contextual variations, as it ignores
sample-specific relevance. To address these limitations, we propose a novel
method that enhances predictive performance through novel architectural
innovations. Our architecture introduces an adaptive kernel-based attention
mechanism that processes distinct feature groups separately before integration,
enabling capture of local patterns while preserving global relationships.
Experimental results show substantial improvements in performance metrics,
compared to the state-of-the-art methods across diverse datasets.

</details>


### [419] [Predicting life satisfaction using machine learning and explainable AI](https://arxiv.org/abs/2510.16547)
*Alif Elham Khan,Mohammad Junayed Hasan,Humayra Anjum,Nabeel Mohammed,Sifat Momen*

Main category: cs.LG

TL;DR: 该研究使用机器学习算法和大型语言模型预测生活满意度，在丹麦19000人的政府调查数据上达到93.80%的准确率和73.00%的宏F1分数，发现健康状况是所有年龄段最重要的决定因素。


<details>
  <summary>Details</summary>
Motivation: 传统的生活满意度测量方法存在验证和传播问题，本研究旨在探索机器学习和LLMs在预测生活满意度方面的潜力，为理解人类行为和主观幸福感提供更可靠的方法。

Method: 使用特征学习技术从调查数据中提取27个重要问题，并将表格数据转换为自然语言句子，应用临床和生物医学LLMs进行预测，同时进行数据重采样和特征选择的消融研究。

Result: 机器学习模型达到93.80%准确率，LLMs达到93.74%准确率，发现生活满意度预测与生物医学领域更相关，健康状况是所有年龄段最重要的决定因素。

Conclusion: 机器学习、大型语言模型和可解释AI可以共同建立对使用AI研究人类行为的信任和理解，对量化主观幸福感具有重要影响。

Abstract: Life satisfaction is a crucial facet of human well-being. Hence, research on
life satisfaction is incumbent for understanding how individuals experience
their lives and influencing interventions targeted at enhancing mental health
and well-being. Life satisfaction has traditionally been measured using analog,
complicated, and frequently error-prone methods. These methods raise questions
concerning validation and propagation. However, this study demonstrates the
potential for machine learning algorithms to predict life satisfaction with a
high accuracy of 93.80% and a 73.00% macro F1-score. The dataset comes from a
government survey of 19000 people aged 16-64 years in Denmark. Using feature
learning techniques, 27 significant questions for assessing contentment were
extracted, making the study highly reproducible, simple, and easily
interpretable. Furthermore, clinical and biomedical large language models
(LLMs) were explored for predicting life satisfaction by converting tabular
data into natural language sentences through mapping and adding meaningful
counterparts, achieving an accuracy of 93.74% and macro F1-score of 73.21%. It
was found that life satisfaction prediction is more closely related to the
biomedical domain than the clinical domain. Ablation studies were also
conducted to understand the impact of data resampling and feature selection
techniques on model performance. Moreover, the correlation between primary
determinants with different age brackets was analyzed, and it was found that
health condition is the most important determinant across all ages. This study
demonstrates how machine learning, large language models and XAI can jointly
contribute to building trust and understanding in using AI to investigate human
behavior, with significant ramifications for academics and professionals
working to quantify and comprehend subjective well-being.

</details>


### [420] [NeurIPT: Foundation Model for Neural Interfaces](https://arxiv.org/abs/2510.16548)
*Zitao Fang,Chenxuan Li,Hongting Zhou,Shuyang Yu,Guodong Du,Ashwaq Qasem,Yang Lu,Jing Li,Junsong Zhang,Sim Kuan Goh*

Main category: cs.LG

TL;DR: 提出了NeurIPT，一个基于预训练Transformer的EEG基础模型，通过捕捉EEG信号的同质和异质时空特征，解决了跨被试、跨任务和跨条件的变异性问题。


<details>
  <summary>Details</summary>
Motivation: 随着EEG数据量的增加，建立基础模型来扩展和泛化神经解码变得重要，但由于被试间、任务间、条件间的显著变异性以及不同的电极配置，应用基础模型到EEG仍具挑战性。

Method: 1) 时间上：引入振幅感知掩码预训练(AAMP)，基于信号振幅而非随机间隔进行掩码；2) 使用渐进混合专家(PMoE)架构，在深层逐步引入专门子网络；3) 空间上：利用电极3D物理坐标实现跨EEG设置的嵌入迁移；4) 微调时开发脑内-脑间叶池化(IILP)来有效利用区域脑特征。

Result: 在八个下游BCI数据集上的实证评估显示，NeurIPT通过微调始终达到最先进的性能，突出了其广泛的适用性和强大的泛化能力。

Conclusion: 该工作推动了EEG中基础模型的发展，并为可扩展和可泛化的神经信息处理系统提供了见解。

Abstract: Electroencephalography (EEG) has wide-ranging applications, from clinical
diagnosis to brain-computer interfaces (BCIs). With the increasing volume and
variety of EEG data, there has been growing interest in establishing foundation
models (FMs) to scale up and generalize neural decoding. Despite showing early
potential, applying FMs to EEG remains challenging due to substantial
inter-subject, inter-task, and inter-condition variability, as well as diverse
electrode configurations across recording setups. To tackle these open
challenges, we propose NeurIPT, a foundation model developed for diverse
EEG-based Neural Interfaces with a Pre-trained Transformer by capturing both
homogeneous and heterogeneous spatio-temporal characteristics inherent in EEG
signals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP),
masking based on signal amplitude rather than random intervals, to learn robust
representations across varying signal intensities beyond local interpolation.
Moreover, this temporal representation is enhanced by a Progressive
Mixture-of-Experts (PMoE) architecture, where specialized expert subnetworks
are progressively introduced at deeper layers, adapting effectively to the
diverse temporal characteristics of EEG signals. Spatially, NeurIPT leverages
the 3D physical coordinates of electrodes, enabling effective transfer of
embedding across varying EEG settings, and develops Intra-Inter Lobe Pooling
(IILP) during fine-tuning to efficiently exploit regional brain features.
Empirical evaluations across eight downstream BCI datasets, via fine-tuning,
demonstrated NeurIPT consistently achieved state-of-the-art performance,
highlighting its broad applicability and robust generalization. Our work pushes
forward the state of FMs in EEG and offers insights into scalable and
generalizable neural information processing systems.

</details>


### [421] [LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs](https://arxiv.org/abs/2510.16552)
*Ang Li,Yifei Wang,Zhihang Yuan,Stefanie Jegelka,Yisen Wang*

Main category: cs.LG

TL;DR: LANPO框架通过分离语言反馈和数值奖励的角色来解决LLM强化学习中的样本效率问题，语言指导探索，数值奖励驱动优化，在数学推理任务上显著优于GRPO基线。


<details>
  <summary>Details</summary>
Motivation: 传统LLM强化学习依赖标量奖励，丢弃了rollout中的宝贵文本理由，导致模型每次尝试都需要重新探索，降低了样本效率。同时，在线经验集成存在信息泄露与行为崩溃的悖论。

Method: 提出LANPO框架：1）构建动态经验池；2）奖励无关反思原则实现安全的样本内自校正；3）相关抽象原则从样本间经验中提取可泛化的教训。

Result: 在数学推理基准测试中，7B和14B模型使用LANPO在测试准确率上显著优于使用GRPO的强基线模型。

Conclusion: LANPO为将历史经验集成到LLM强化学习循环中提供了稳健方法，创造了更有效和数据高效的学习智能体。

Abstract: Reinforcement learning in large language models (LLMs) often relies on scalar
rewards, a practice that discards valuable textual rationale buried in the
rollouts, forcing the model to explore \textit{de novo} with each attempt and
hindering sample efficiency. While LLMs can uniquely learn from language
feedback provided in-context, naively integrating on-line experiences into RL
training presents a paradox: feedback from the same problem risks information
leakage and memorization, while feedback from different problems often leads to
behavior collapse due to irrelevant context. To resolve this tension, we
propose \textbf{Language-And-Numerical Policy Optimization (LANPO)}, a
framework that cleanly separates the roles of feedback: language guides
exploration, while numerical rewards drive optimization. LANPO builds a dynamic
experience pool from past trials and introduces two principles to ensure
feedback is effective: \emph{Reward-Agnostic Reflection} for safe intra-sample
self-correction and \emph{Relevant Abstraction} to distill generalizable
lessons from inter-sample experiences. Across mathematical reasoning
benchmarks, LANPO enables 7B and 14B models to significantly outperform strong
baselines trained with GRPO in test accuracy. Our work provides a robust method
for integrating historical experiences into the LLM RL loop, creating more
effective and data-efficient learning agents.

</details>


### [422] [Copy-Augmented Representation for Structure Invariant Template-Free Retrosynthesis](https://arxiv.org/abs/2510.16588)
*Jiaxi Zhuang,Yu Zhang,Aimin Zhou,Ying Qian*

Main category: cs.LG

TL;DR: 提出了C-SMILES分子表示方法，通过分解SMILES为元素-标记对和特殊标记，减少反应物与产物间的编辑距离，结合复制增强机制和SMILES对齐指导，显著提升逆合成预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有无模板方法难以捕捉化学反应中的结构不变性，导致搜索空间过大和预测精度降低。需要开发能更好表示分子结构不变性的方法。

Method: 引入C-SMILES表示法，将传统SMILES分解为元素-标记对和五个特殊标记；采用复制增强机制动态决定生成新标记或保留产物中未变片段；集成SMILES对齐指导增强注意力一致性。

Result: 在USPTO-50K数据集上达到67.2%的top-1准确率，在USPTO-FULL数据集上达到50.8%的准确率，生成分子有效性达99.9%。

Conclusion: 这项工作为结构感知的分子生成建立了新范式，在计算药物发现中具有直接应用价值。

Abstract: Retrosynthesis prediction is fundamental to drug discovery and chemical
synthesis, requiring the identification of reactants that can produce a target
molecule. Current template-free methods struggle to capture the structural
invariance inherent in chemical reactions, where substantial molecular
scaffolds remain unchanged, leading to unnecessarily large search spaces and
reduced prediction accuracy. We introduce C-SMILES, a novel molecular
representation that decomposes traditional SMILES into element-token pairs with
five special tokens, effectively minimizing editing distance between reactants
and products. Building upon this representation, we incorporate a
copy-augmented mechanism that dynamically determines whether to generate new
tokens or preserve unchanged molecular fragments from the product. Our approach
integrates SMILES alignment guidance to enhance attention consistency with
ground-truth atom mappings, enabling more chemically coherent predictions.
Comprehensive evaluation on USPTO-50K and large-scale USPTO-FULL datasets
demonstrates significant improvements: 67.2% top-1 accuracy on USPTO-50K and
50.8% on USPTO-FULL, with 99.9% validity in generated molecules. This work
establishes a new paradigm for structure-aware molecular generation with direct
applications in computational drug discovery.

</details>


### [423] [Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration](https://arxiv.org/abs/2510.16590)
*Alan Kai Hassen,Andrius Bernatavicius,Antonius P. A. Janssen,Mike Preuss,Gerard J. P. van Westen,Djork-Arné Clevert*

Main category: cs.LG

TL;DR: 提出了一种无需标记训练数据的分子推理框架，通过将思维链推理锚定到分子结构上，使用大语言模型解决化学任务，特别是在单步逆合成中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 化学中机器学习应用常受限于标记数据的稀缺性和昂贵成本，限制了传统监督方法的使用。

Method: 使用通用大语言模型，通过原子标识符将思维链推理锚定到分子结构，进行一步任务识别相关片段及其化学标签，然后在可选步骤中使用少量样本预测化学转化。

Result: 在学术基准和专家验证的药物发现分子中，LLMs在识别化学合理反应位点（≥90%）、命名反应类别（≥40%）和最终反应物（≥74%）方面取得了高成功率。

Conclusion: 该框架不仅解决了复杂化学任务，还提供了一种通过将化学知识映射到分子结构来生成理论基础的合成数据集的方法，从而解决数据稀缺问题。

Abstract: Applications of machine learning in chemistry are often limited by the
scarcity and expense of labeled data, restricting traditional supervised
methods. In this work, we introduce a framework for molecular reasoning using
general-purpose Large Language Models (LLMs) that operates without requiring
labeled training data. Our method anchors chain-of-thought reasoning to the
molecular structure by using unique atomic identifiers. First, the LLM performs
a one-shot task to identify relevant fragments and their associated chemical
labels or transformation classes. In an optional second step, this
position-aware information is used in a few-shot task with provided class
examples to predict the chemical transformation. We apply our framework to
single-step retrosynthesis, a task where LLMs have previously underperformed.
Across academic benchmarks and expert-validated drug discovery molecules, our
work enables LLMs to achieve high success rates in identifying chemically
plausible reaction sites ($\geq90\%$), named reaction classes ($\geq40\%$), and
final reactants ($\geq74\%$). Beyond solving complex chemical tasks, our work
also provides a method to generate theoretically grounded synthetic datasets by
mapping chemical knowledge onto the molecular structure and thereby addressing
data scarcity.

</details>


### [424] [Asymptotically Stable Quaternion-valued Hopfield-structured Neural Network with Periodic Projection-based Supervised Learning Rules](https://arxiv.org/abs/2510.16607)
*Tianwei Wang,Xinhui Ma,Wei Pang*

Main category: cs.LG

TL;DR: 提出了一种四元数监督学习Hopfield结构神经网络(QSHNN)，利用四元数在表示旋转和姿态方面的几何优势，通过周期性投影策略保持四元数结构一致性，在机器人控制等领域具有应用价值。


<details>
  <summary>Details</summary>
Motivation: 利用四元数在表示旋转和姿态方面的几何优势，扩展经典Hopfield神经网络到四元数域，为机器人控制、路径规划等应用提供更有效的神经网络模型。

Method: 从连续时间HNN动力学模型出发，扩展到四元数域，引入周期性投影策略修改标准梯度下降，定期将权重矩阵的4*4块投影到最近的四元数结构。

Result: 实验模型实现高精度、快速收敛和强可靠性，QSHNN演化轨迹具有良好有界曲率（足够平滑性），适用于机器人关节姿态参数化等应用。

Conclusion: 该模型为超复数或非交换代数结构下的神经网络设计提供了实用的实现框架和通用数学方法学。

Abstract: Motivated by the geometric advantages of quaternions in representing
rotations and postures, we propose a quaternion-valued supervised learning
Hopfield-structured neural network (QSHNN) with a fully connected structure
inspired by the classic Hopfield neural network (HNN). Starting from a
continuous-time dynamical model of HNNs, we extend the formulation to the
quaternionic domain and establish the existence and uniqueness of fixed points
with asymptotic stability. For the learning rules, we introduce a periodic
projection strategy that modifies standard gradient descent by periodically
projecting each 4*4 block of the weight matrix onto the closest quaternionic
structure in the least-squares sense. This approach preserves both convergence
and quaternionic consistency throughout training. Benefiting from this rigorous
mathematical foundation, the experimental model implementation achieves high
accuracy, fast convergence, and strong reliability across randomly generated
target sets. Moreover, the evolution trajectories of the QSHNN exhibit
well-bounded curvature, i.e., sufficient smoothness, which is crucial for
applications such as control systems or path planning modules in robotic arms,
where joint postures are parameterized by quaternion neurons. Beyond these
application scenarios, the proposed model offers a practical implementation
framework and a general mathematical methodology for designing neural networks
under hypercomplex or non-commutative algebraic structures.

</details>


### [425] [Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods](https://arxiv.org/abs/2510.16609)
*Avrim Blum,Daniel Hsu,Cyrus Rashtchian,Donya Saless*

Main category: cs.LG

TL;DR: 该论文研究了测试时增强（如RAG或工具使用）中模型参数知识与外部检索信息的关系，通过将多步推理建模为知识图上的连通性问题，揭示了参数知识密度与增强效率之间的相变现象。


<details>
  <summary>Details</summary>
Motivation: 理解测试时增强中模型参数知识与外部检索信息之间的理论关系，特别是确定在少量增强步骤下准确回答问题所需的最小预训练知识量。

Method: 将多步推理建模为知识图上的s-t连通性问题，将模型的预训练参数知识表示为部分且可能有噪声的子图，将增强视为查询真实边来扩展模型知识。

Result: 发现了一个相变现象：当知识图断开为小组件时，通过增强找到路径效率低下，需要Ω(√n)次查询；但当正确知识密度超过阈值形成巨型组件时，可以用期望常数次查询找到路径。

Conclusion: 测试时增强的效率取决于模型参数知识图的连通性，存在一个临界密度阈值，超过该阈值后增强效率显著提高。

Abstract: Test-time augmentation, such as Retrieval-Augmented Generation (RAG) or tool
use, critically depends on an interplay between a model's parametric knowledge
and externally retrieved information. However, the theoretical underpinnings of
this relationship remain poorly understood. Specifically, it is not clear how
much pre-training knowledge is required to answer queries with a small number
of augmentation steps, which is a desirable property in practice. To address
this question, we formulate multi-step reasoning as an $s$-$t$ connectivity
problem on a knowledge graph. We represent a model's pre-training parametric
knowledge as a partial, potentially noisy subgraph. We view augmentation as
querying an oracle for true edges that augment the model's knowledge. Then, we
characterize the necessary and sufficient number of augmentation steps for the
model to generate an accurate answer given partial prior knowledge. One key
result shows a phase transition: if the prior knowledge graph over $n$ vertices
is disconnected into small components, then finding a path via augmentation is
inefficient and requires $\Omega(\sqrt{n})$ queries. On the other hand, once
the density of correct knowledge surpasses a threshold, forming a giant
component, we can find paths with an expected constant number of queries.

</details>


### [426] [On the Impossibility of Retrain Equivalence in Machine Unlearning](https://arxiv.org/abs/2510.16629)
*Jiatong Yu,Yinghui He,Anirudh Goyal,Sanjeev Arora*

Main category: cs.LG

TL;DR: 该研究表明，在多阶段训练场景下，局部遗忘算法无法普遍实现重训练等价性，因为遗忘结果具有路径依赖性，受训练阶段顺序影响。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习流水线通常涉及多阶段训练（如LLM微调），但现有的机器遗忘理论主要针对i.i.d.数据训练，需要研究多阶段训练对遗忘的影响。

Method: 通过理论分析和实验验证，使用梯度上升、NPO和SimNPO等局部遗忘算法，在Llama和Qwen模型（1B到14B）上进行多阶段训练和遗忘实验。

Result: 不同训练路径的模型在遗忘过程中行为差异显著，GSM8K准确率下降差异超过20%，某些学习路径产生难以遗忘的模型，概率质量分布也呈现路径依赖性。

Conclusion: 重训练等价性对于局部遗忘算法来说是一个不适定的目标，需要重新思考机器遗忘的定义和期望目标，特别是在难以获取训练历史的情况下。

Abstract: Machine unlearning seeks to selectively remove the "influence" of specific
training data on a model's outputs. The ideal goal is Retrain
Equivalence--behavior identical to a model trained from scratch on only the
retained data. This goal was formulated for models trained on i.i.d. data
batches, but modern pipelines often involve multi-stage training, with each
stage having a distinct data distribution and objective. Examples include LLM
fine-tuning for alignment, reasoning ability, etc. Our study shows via theory
and experiments that this shift to multi-stage training introduces a
fundamental barrier for machine unlearning. The theory indicates that the
outcome of local unlearning--methods that only use gradients computed on the
forget set--is path-dependent. That is, a model's behavior during unlearning is
influenced by the order of its training stages during learning, making it
impossible for path-oblivious algorithms to universally achieve Retrain
Equivalence. We empirically demonstrate the same phenomenon in LLM
post-training across Llama and Qwen models (1B to 14B) with gradient ascent,
NPO, and SimNPO local unlearning algorithms. Models fine-tuned via different
orderings of identical training stages diverge in behavior during unlearning,
with the degradation in GSM8K accuracy after unlearning varying by over 20%
across paths. We also observe that some learning paths consistently produce
models that unlearn slowly. During unlearning, whether the probability mass
gets squeezed into paraphrasing or alternative concepts is also path-dependent.
These results consistently show that Retrain Equivalence is an ill-posed target
for local unlearning algorithms, so long as the target models are trained in
stages. In situations where access to models' training histories is hard, the
current work calls for rethinking the definition and desiderata of machine
unlearning.

</details>


### [427] [Simulation-free Structure Learning for Stochastic Dynamics](https://arxiv.org/abs/2510.16656)
*Noah El Rimawi-Fine,Adam Stecklov,Lucas Nelson,Mathieu Blanchette,Alexander Tong,Stephen Y. Zhang,Lazar Atanackovic*

Main category: cs.LG

TL;DR: StructureFlow是一种新颖的无仿真方法，能够同时学习物理系统的结构和随机群体动力学，解决了现有方法无法同时处理结构学习和动力学建模的问题。


<details>
  <summary>Details</summary>
Motivation: 许多自然系统中的物理系统（如细胞生物学）本质上是高维和随机的，只能获得部分噪声状态测量，这给建模底层动力学和推断网络结构带来了挑战。现有方法通常只能单独处理结构学习或群体层面的动力学建模。

Method: 提出StructureFlow方法，这是一种原则性的无仿真方法，能够联合学习物理系统的结构和随机群体动力学。该方法支持从干预中学习结构以及条件群体动力学的动态（轨迹）推断。

Result: 在合成高维系统、生物学上合理的模拟系统和实验性单细胞数据集上的实证评估表明，StructureFlow能够学习底层系统的结构，同时建模其条件群体动力学。

Conclusion: StructureFlow能够同时学习系统结构和条件群体动力学，这是理解系统行为机制的关键一步。

Abstract: Modeling dynamical systems and unraveling their underlying causal
relationships is central to many domains in the natural sciences. Various
physical systems, such as those arising in cell biology, are inherently
high-dimensional and stochastic in nature, and admit only partial, noisy state
measurements. This poses a significant challenge for addressing the problems of
modeling the underlying dynamics and inferring the network structure of these
systems. Existing methods are typically tailored either for structure learning
or modeling dynamics at the population level, but are limited in their ability
to address both problems together. In this work, we address both problems
simultaneously: we present StructureFlow, a novel and principled
simulation-free approach for jointly learning the structure and stochastic
population dynamics of physical systems. We showcase the utility of
StructureFlow for the tasks of structure learning from interventions and
dynamical (trajectory) inference of conditional population dynamics. We
empirically evaluate our approach on high-dimensional synthetic systems, a set
of biologically plausible simulated systems, and an experimental single-cell
dataset. We show that StructureFlow can learn the structure of underlying
systems while simultaneously modeling their conditional population dynamics --
a key step toward the mechanistic understanding of systems behavior.

</details>


### [428] [Evaluating protein binding interfaces with PUMBA](https://arxiv.org/abs/2510.16674)
*Azam Shirali,Giri Narasimhan*

Main category: cs.LG

TL;DR: PUMBA是一个基于Vision Mamba架构的蛋白质-蛋白质对接评分函数，通过替换PIsToN中的Vision Transformer为Vision Mamba，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的蛋白质-蛋白质对接工具依赖准确的评分函数来区分原生和非原生复合物。Vision Mamba在自然语言处理和计算机视觉领域表现出色，有望提升蛋白质对接的准确性。

Method: 将PIsToN中的Vision Transformer主干替换为Vision Mamba架构，利用Mamba在图像块序列上的高效长程序列建模能力。

Result: 在多个大规模公共数据集上的评估表明，PUMBA始终优于其基于Transformer的前身PIsToN。

Conclusion: Vision Mamba架构能够显著提升蛋白质-蛋白质界面特征中全局和局部模式的捕捉能力，为蛋白质对接提供了更准确的评分函数。

Abstract: Protein-protein docking tools help in studying interactions between proteins,
and are essential for drug, vaccine, and therapeutic development. However, the
accuracy of a docking tool depends on a robust scoring function that can
reliably differentiate between native and non-native complexes. PIsToN is a
state-of-the-art deep learning-based scoring function that uses Vision
Transformers in its architecture. Recently, the Mamba architecture has
demonstrated exceptional performance in both natural language processing and
computer vision, often outperforming Transformer-based models in their domains.
In this study, we introduce PUMBA (Protein-protein interface evaluation with
Vision Mamba), which improves PIsToN by replacing its Vision Transformer
backbone with Vision Mamba. This change allows us to leverage Mamba's efficient
long-range sequence modeling for sequences of image patches. As a result, the
model's ability to capture both global and local patterns in protein-protein
interface features is significantly improved. Evaluation on several
widely-used, large-scale public datasets demonstrates that PUMBA consistently
outperforms its original Transformer-based predecessor, PIsToN.

</details>


### [429] [Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory](https://arxiv.org/abs/2510.16676)
*Anindya Sarkar,Binglin Ji,Yevgeniy Vorobeychik*

Main category: cs.LG

TL;DR: 提出一种在无信息先验条件下进行有效主动目标发现的新方法，该方法具有理论依据、神经科学启发、可解释性强，并在多种领域实验中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在数据获取成本高昂的领域（如医学成像、环境监测），传统基于强先验的生成模型在数据极其有限或采样成本高的场景下难以泛化，需要解决无信息先验条件下的主动目标发现问题。

Method: 提出一个理论上有依据、受神经科学启发的框架，该方法具有内在可解释性，能保证每次新观测带来先验估计的单调改进，实现越来越准确的采样。

Result: 通过跨多个领域（包括物种分布建模和遥感）的综合实验和消融研究，证明该方法显著优于基线方法。

Conclusion: 该方法在无信息先验条件下实现了稳健的探索和适应性，为复杂现实场景中的主动目标发现提供了可靠解决方案。

Abstract: In many scientific and engineering fields, where acquiring high-quality data
is expensive--such as medical imaging, environmental monitoring, and remote
sensing--strategic sampling of unobserved regions based on prior observations
is crucial for maximizing discovery rates within a constrained budget. The rise
of powerful generative models, such as diffusion models, has enabled active
target discovery in partially observable environments by leveraging learned
priors--probabilistic representations that capture underlying structure from
data. With guidance from sequentially gathered task-specific observations,
these models can progressively refine exploration and efficiently direct
queries toward promising regions. However, in domains where learning a strong
prior is infeasible due to extremely limited data or high sampling cost (such
as rare species discovery, diagnostics for emerging diseases, etc.), these
methods struggle to generalize. To overcome this limitation, we propose a novel
approach that enables effective active target discovery even in settings with
uninformative priors, ensuring robust exploration and adaptability in complex
real-world scenarios. Our framework is theoretically principled and draws
inspiration from neuroscience to guide its design. Unlike black-box policies,
our approach is inherently interpretable, providing clear insights into
decision-making. Furthermore, it guarantees a strong, monotonic improvement in
prior estimates with each new observation, leading to increasingly accurate
sampling and reinforcing both reliability and adaptability in dynamic settings.
Through comprehensive experiments and ablation studies across various domains,
including species distribution modeling and remote sensing, we demonstrate that
our method substantially outperforms baseline approaches.

</details>


### [430] [Renaissance of RNNs in Streaming Clinical Time Series: Compact Recurrence Remains Competitive with Transformers](https://arxiv.org/abs/2510.16677)
*Ran Tong,Jiaqi Liu,Su Liu,Xin Hu,Lanruo Wang*

Main category: cs.LG

TL;DR: 提出了一个紧凑的严格因果基准，用于在MIT-BIH心律失常数据库上使用每秒心率进行流式临床时间序列分析。研究了两个任务：短期心动过速风险预测和一步心率预测，比较了GRU-D和Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 在纵向监测中，模型选择是任务依赖的，需要评估不同模型在临床时间序列任务中的表现差异。

Method: 使用MIT-BIH心律失常数据库的每秒心率数据，采用记录级非重叠分割。比较GRU-D（RNN）和Transformer模型，在匹配的训练预算下与强非学习基线对比。评估采用分类的温度缩放和预测的分组bootstrap置信区间。

Result: 在MIT-BIH上，GRU-D在心动过速风险预测上略优于Transformer，而Transformer在预测误差上明显低于GRU-D和持久性模型。

Conclusion: 紧凑RNN在短时域风险评分中仍具竞争力，而紧凑Transformer在点预测方面带来更清晰的收益。

Abstract: We present a compact, strictly causal benchmark for streaming clinical time
series on the MIT--BIH Arrhythmia Database using per-second heart rate. Two
tasks are studied under record-level, non-overlapping splits: near-term
tachycardia risk (next ten seconds) and one-step heart rate forecasting. We
compare a GRU-D (RNN) and a Transformer under matched training budgets against
strong non-learned baselines. Evaluation is calibration-aware for
classification and proper for forecasting, with temperature scaling and grouped
bootstrap confidence intervals. On MIT-BIH, GRU-D slightly surpasses the
Transformer for tachycardia risk, while the Transformer clearly lowers
forecasting error relative to GRU-D and persistence. Our results show that, in
longitudinal monitoring, model choice is task-dependent: compact RNNs remain
competitive for short-horizon risk scoring, whereas compact Transformers
deliver clearer gains for point forecasting.

</details>


### [431] [High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient Descent on Least Squares](https://arxiv.org/abs/2510.16687)
*Shurong Lin,Eric D. Kolaczyk,Adam Smith,Elliot Paquette*

Main category: cs.LG

TL;DR: 本文使用扩散方法精确分析噪声SGD，提供连续时间视角来捕捉高维设置中的统计风险演变和隐私损失动态，并研究了一种无需梯度敏感性显式知识的噪声SGD变体。


<details>
  <summary>Details</summary>
Motivation: 优化与隐私之间的相互作用已成为隐私保护机器学习的核心主题。噪声SGD已成为关键算法，但现有工作主要提供统计风险和隐私损失的各种界限，而过程的精确行为仍不清楚，特别是在高维设置中。

Method: 利用扩散方法分析噪声SGD，提供连续时间视角；研究一种无需梯度敏感性显式知识的噪声SGD变体，重点关注带有ℓ2正则化的最小二乘问题。

Result: 该方法能够精确捕捉高维设置中统计风险演变和隐私损失动态，提供对噪声SGD过程的更深入理解。

Conclusion: 扩散方法为分析噪声SGD提供了精确框架，能够更好地理解高维设置中的统计风险和隐私损失动态，同时提出的变体算法避免了梯度敏感性假设的需求。

Abstract: The interplay between optimization and privacy has become a central theme in
privacy-preserving machine learning. Noisy stochastic gradient descent (SGD)
has emerged as a cornerstone algorithm, particularly in large-scale settings.
These variants of gradient methods inject carefully calibrated noise into each
update to achieve differential privacy, the gold standard notion of rigorous
privacy guarantees. Prior work primarily provides various bounds on statistical
risk and privacy loss for noisy SGD, yet the \textit{exact} behavior of the
process remains unclear, particularly in high-dimensional settings. This work
leverages a diffusion approach to analyze noisy SGD precisely, providing a
continuous-time perspective that captures both statistical risk evolution and
privacy loss dynamics in high dimensions. Moreover, we study a variant of noisy
SGD that does not require explicit knowledge of gradient sensitivity, unlike
existing work that assumes or enforces sensitivity through gradient clipping.
Specifically, we focus on the least squares problem with $\ell_2$
regularization.

</details>


### [432] [CLIP: Client-Side Invariant Pruning for Mitigating Stragglers in Secure Federated Learning](https://arxiv.org/abs/2510.16694)
*Anthony DiMaggio,Raghav Sharma,Gururaj Saileshwar*

Main category: cs.LG

TL;DR: 提出了CLIP技术，一种客户端不变神经元剪枝与网络感知剪枝相结合的方法，用于解决安全联邦学习中由计算和网络瓶颈导致的慢客户端问题，在最小精度损失下加速训练13%至34%。


<details>
  <summary>Details</summary>
Motivation: 安全联邦学习在分布式模型训练中保护数据隐私，但异构设备部署会导致性能瓶颈，计算或网络能力有限的慢客户端会拖慢所有参与客户端的训练速度。

Method: CLIP技术结合客户端不变神经元剪枝和网络感知剪枝，通过剪枝减少计算和通信开销，同时保持模型精度。

Result: 在多个数据集（CIFAR10、Shakespeare、FEMNIST）上，安全联邦学习训练加速13%至34%，精度影响在1.3%提升到2.6%下降之间。

Conclusion: CLIP是首个针对深度神经网络安全聚合的慢客户端缓解技术，有效解决了安全联邦学习中的计算和网络瓶颈问题。

Abstract: Secure federated learning (FL) preserves data privacy during distributed
model training. However, deploying such frameworks across heterogeneous devices
results in performance bottlenecks, due to straggler clients with limited
computational or network capabilities, slowing training for all participating
clients. This paper introduces the first straggler mitigation technique for
secure aggregation with deep neural networks. We propose CLIP, a client-side
invariant neuron pruning technique coupled with network-aware pruning, that
addresses compute and network bottlenecks due to stragglers during training
with minimal accuracy loss. Our technique accelerates secure FL training by 13%
to 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with an
accuracy impact of between 1.3% improvement to 2.6% reduction.

</details>


### [433] [Resolution-Aware Retrieval Augmented Zero-Shot Forecasting](https://arxiv.org/abs/2510.16695)
*Iman Deznabi,Peeyush Kumar,Madalina Fiterau*

Main category: cs.LG

TL;DR: 提出了一种分辨率感知的检索增强预测模型，通过利用空间相关性和时间频率特征来提高零样本预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 零样本预测旨在预测没有直接历史数据的未见条件下的结果，这对传统预测方法构成了重大挑战。

Method: 通过将信号分解为不同频率分量，采用分辨率感知检索机制：低频分量依赖更广泛的空间上下文，高频分量关注局部影响。

Result: 在微气候预测中，该模型显著优于传统预测方法、数值天气预报模型和现代基础时间序列模型，在ERA5数据集上比HRRR的MSE降低71%，比Chronos降低34%。

Conclusion: 检索增强和分辨率感知策略在零样本预测中非常有效，为微气候建模及其他领域提供了可扩展且数据高效的解决方案。

Abstract: Zero-shot forecasting aims to predict outcomes for previously unseen
conditions without direct historical data, posing a significant challenge for
traditional forecasting methods. We introduce a Resolution-Aware
Retrieval-Augmented Forecasting model that enhances predictive accuracy by
leveraging spatial correlations and temporal frequency characteristics. By
decomposing signals into different frequency components, our model employs
resolution-aware retrieval, where lower-frequency components rely on broader
spatial context, while higher-frequency components focus on local influences.
This allows the model to dynamically retrieve relevant data and adapt to new
locations with minimal historical context.
  Applied to microclimate forecasting, our model significantly outperforms
traditional forecasting methods, numerical weather prediction models, and
modern foundation time series models, achieving 71% lower MSE than HRRR and 34%
lower MSE than Chronos on the ERA5 dataset.
  Our results highlight the effectiveness of retrieval-augmented and
resolution-aware strategies, offering a scalable and data-efficient solution
for zero-shot forecasting in microclimate modeling and beyond.

</details>


### [434] [On the Granularity of Causal Effect Identifiability](https://arxiv.org/abs/2510.16703)
*Yizuo Chen,Adnan Darwiche*

Main category: cs.LG

TL;DR: 本文探讨了基于状态的因果效应可识别性，证明在某些情况下即使变量级因果效应不可识别，状态级因果效应仍可识别，这需要上下文特定独立性和条件函数依赖等额外知识。


<details>
  <summary>Details</summary>
Motivation: 传统因果效应可识别性基于变量层面，但实际应用中往往关心特定状态间的因果效应。现有变量级框架可能错过某些可识别的情况。

Method: 通过分析状态级因果效应与变量级因果效应的关系，考察上下文特定独立性、条件函数依赖和变量状态约束等知识对可识别性的影响。

Result: 发现状态级因果效应在变量级因果效应不可识别时仍可能可识别，这种分离需要额外知识。变量状态约束知识单独不能改善可识别性，但与其他知识结合可同时改善变量级和状态级可识别性。

Conclusion: 状态级因果效应可识别性分析能发现传统变量级框架可能错过的可识别情况，为从观测数据估计感兴趣的因果效应提供了新视角。

Abstract: The classical notion of causal effect identifiability is defined in terms of
treatment and outcome variables. In this note, we consider the identifiability
of state-based causal effects: how an intervention on a particular state of
treatment variables affects a particular state of outcome variables. We
demonstrate that state-based causal effects may be identifiable even when
variable-based causal effects may not. Moreover, we show that this separation
occurs only when additional knowledge -- such as context-specific
independencies and conditional functional dependencies -- is available. We
further examine knowledge that constrains the states of variables, and show
that such knowledge does not improve identifiability on its own but can improve
both variable-based and state-based identifiability when combined with other
knowledge such as context-specific independencies. Our findings highlight
situations where causal effects of interest may be estimable from observational
data and this identifiability may be missed by existing variable-based
frameworks.

</details>


### [435] [Zero-Shot Performance Prediction for Probabilistic Scaling Laws](https://arxiv.org/abs/2510.16743)
*Viktoria Schram,Markus Hiller,Daniel Beck,Trevor Cohn*

Main category: cs.LG

TL;DR: 本文提出了一种多任务学习方法，使用潜在变量多输出高斯过程来预测NLP模型的学习曲线，支持零样本预测并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 预测NLP模型的学习曲线可以做出明智决策以满足特定性能目标，同时减少计算开销和数据集获取与整理的成本。

Method: 将学习曲线预测任务制定为多任务学习问题，使用潜在变量多输出高斯过程建模任务间和层次间的共享信息与依赖关系。

Result: 该方法能够以较低成本开发概率性缩放定律，通过主动学习策略减少预测不确定性，提供接近真实缩放定律的预测结果。

Conclusion: 在三个小型NLP数据集上的验证表明，该框架能够有效预测学习曲线，适用于不同规模的模型。

Abstract: The prediction of learning curves for Natural Language Processing (NLP)
models enables informed decision-making to meet specific performance
objectives, while reducing computational overhead and lowering the costs
associated with dataset acquisition and curation. In this work, we formulate
the prediction task as a multitask learning problem, where each task's data is
modelled as being organized within a two-layer hierarchy. To model the shared
information and dependencies across tasks and hierarchical levels, we employ
latent variable multi-output Gaussian Processes, enabling to account for task
correlations and supporting zero-shot prediction of learning curves (LCs). We
demonstrate that this approach facilitates the development of probabilistic
scaling laws at lower costs. Applying an active learning strategy, LCs can be
queried to reduce predictive uncertainty and provide predictions close to
ground truth scaling laws. We validate our framework on three small-scale NLP
datasets with up to $30$ LCs. These are obtained from nanoGPT models, from
bilingual translation using mBART and Transformer models, and from multilingual
translation using M2M100 models of varying sizes.

</details>


### [436] [An Efficient Semantic Segmentation Decoder for In-Car or Distributed Applications](https://arxiv.org/abs/2510.16747)
*Danish Nazir,Gowtham Sai Inti,Timo Bartels,Jan Piewek,Thorsten Bagdonat,Tim Fingscheidt*

Main category: cs.LG

TL;DR: 提出了一种基于SegDeformer的联合特征和任务解码方法，在车载和分布式应用中降低计算复杂度，同时保持语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 现代汽车系统使用DNN进行语义分割，但现有方法未充分利用transformer架构的优势，且计算复杂度高，限制了在车载和分布式应用中的可扩展性。

Method: 采用SegDeformer架构进行联合特征和任务解码，通过优化解码过程降低计算复杂度，同时保持分割性能。

Result: 在车载应用中，Cityscapes数据集上fps提升11.7倍(1.4到16.5fps)，ADE20K数据集上提升3.5倍(43.3到154.3fps)；在分布式应用中，使用仅0.14%(ADE20K)和0.04%(Cityscapes)的云DNN参数实现SOTA性能。

Conclusion: 该方法在显著降低计算复杂度的同时保持语义分割性能，为车载和分布式应用提供了更高效的解决方案。

Abstract: Modern automotive systems leverage deep neural networks (DNNs) for semantic
segmentation and operate in two key application areas: (1) In-car, where the
DNN solely operates in the vehicle without strict constraints on the data rate.
(2) Distributed, where one DNN part operates in the vehicle and the other part
typically on a large-scale cloud platform with a particular constraint on
transmission bitrate efficiency. Typically, both applications share an image
and source encoder, while each uses distinct (joint) source and task decoders.
Prior work utilized convolutional neural networks for joint source and task
decoding but did not investigate transformer-based alternatives such as
SegDeformer, which offer superior performance at the cost of higher
computational complexity. In this work, we propose joint feature and task
decoding for SegDeformer, thereby enabling lower computational complexity in
both in-car and distributed applications, despite SegDeformer's computational
demands. This improves scalability in the cloud while reducing in-car
computational complexity. For the in-car application, we increased the frames
per second (fps) by up to a factor of $11.7$ ($1.4$ fps to $16.5$ fps) on
Cityscapes and by up to a factor of $3.5$ ($43.3$ fps to $154.3$ fps) on
ADE20K, while being on-par w.r.t.\ the mean intersection over union (mIoU) of
the transformer-based baseline that doesn't compress by a source codec. For the
distributed application, we achieve state-of-the-art (SOTA) over a wide range
of bitrates on the mIoU metric, while using only $0.14$\% ($0.04$\%) of cloud
DNN parameters used in previous SOTA, reported on ADE20K (Cityscapes).

</details>


### [437] [SAMOSA: Sharpness Aware Minimization for Open Set Active learning](https://arxiv.org/abs/2510.16757)
*Young In Kim,Andrea Agiollo,Rajiv Khanna*

Main category: cs.LG

TL;DR: 提出了SAMOSA方法，一种基于锐度感知最小化的开集主动学习算法，通过选择典型性样本来提高模型性能，在多个数据集上比现有方法提升3%准确率


<details>
  <summary>Details</summary>
Motivation: 现代机器学习需要大量数据标注，但标注成本高昂。开集主动学习旨在从未标记数据中选择包含相关和无关类别的信息样本，以减轻标注负担

Method: 基于SGD和SAM的理论发现，SAMOSA根据样本典型性主动查询。它识别嵌入流形中靠近模型决策边界的非典型样本，优先选择对目标类别高度信息丰富且能区分目标类和无关类的样本

Result: 在多个数据集上的广泛实验表明，SAMOSA比现有最先进方法提升高达3%的准确率，且不引入计算开销

Conclusion: SAMOSA是一种有效的开集主动学习查询算法，通过锐度感知最小化和典型性样本选择，显著提高了模型性能

Abstract: Modern machine learning solutions require extensive data collection where
labeling remains costly. To reduce this burden, open set active learning
approaches aim to select informative samples from a large pool of unlabeled
data that includes irrelevant or unknown classes. In this context, we propose
Sharpness Aware Minimization for Open Set Active Learning (SAMOSA) as an
effective querying algorithm. Building on theoretical findings concerning the
impact of data typicality on the generalization properties of traditional
stochastic gradient descent (SGD) and sharpness-aware minimization (SAM),
SAMOSA actively queries samples based on their typicality. SAMOSA effectively
identifies atypical samples that belong to regions of the embedding manifold
close to the model decision boundaries. Therefore, SAMOSA prioritizes the
samples that are (i) highly informative for the targeted classes, and (ii)
useful for distinguishing between targeted and unwanted classes. Extensive
experiments show that SAMOSA achieves up to 3% accuracy improvement over the
state of the art across several datasets, while not introducing computational
overhead. The source code of our experiments is available at:
https://anonymous.4open.science/r/samosa-DAF4

</details>


### [438] [Learning to play: A Multimodal Agent for 3D Game-Play](https://arxiv.org/abs/2510.16774)
*Yuguang Yue,Irakli Salia,Samuel Hunt,Christopher Green,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.LG

TL;DR: 该论文提出了一个用于3D第一人称视频游戏的多模态推理方法，通过收集大规模游戏数据集、学习逆动力学模型和行为克隆训练，开发了能够实时响应文本指令的游戏智能体。


<details>
  <summary>Details</summary>
Motivation: 3D第一人称视频游戏为实时多模态推理提供了具有挑战性的环境，需要解决游戏玩法理解、动作预测和文本指令响应等问题。

Method: 收集大规模人类游戏数据集，学习逆动力学模型来推断缺失的动作，使用行为克隆训练文本条件智能体，采用支持实时推理的自定义架构。

Result: 开发出的模型能够在各种3D游戏中执行任务并响应文本输入，在消费级GPU上实现实时推理。

Conclusion: 该方法在3D游戏环境中展示了多模态推理的可行性，但仍面临长时程任务和大规模游戏定量评估等挑战。

Abstract: We argue that 3-D first-person video games are a challenging environment for
real-time multi-modal reasoning. We first describe our dataset of human
game-play, collected across a large variety of 3-D first-person games, which is
both substantially larger and more diverse compared to prior publicly disclosed
datasets, and contains text instructions. We demonstrate that we can learn an
inverse dynamics model from this dataset, which allows us to impute actions on
a much larger dataset of publicly available videos of human game play that lack
recorded actions. We then train a text-conditioned agent for game playing using
behavior cloning, with a custom architecture capable of realtime inference on a
consumer GPU. We show the resulting model is capable of playing a variety of
3-D games and responding to text input. Finally, we outline some of the
remaining challenges such as long-horizon tasks and quantitative evaluation
across a large set of games.

</details>


### [439] [3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding](https://arxiv.org/abs/2510.16780)
*Chang Wu,Zhiyuan Liu,Wen Shu,Liang Wang,Yanchen Luo,Wenqiang Lei,Yatao Bian,Junfeng Fang,Xiang Wang*

Main category: cs.LG

TL;DR: 提出3D-GSRD模型，通过选择性重掩码解码解决3D分子表示学习中的2D结构泄漏问题，在MD17基准测试中取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 将掩码图建模从2D扩展到3D时面临两个冲突挑战：避免2D结构泄漏到解码器，同时提供足够的2D上下文来重建重掩码原子

Method: 提出选择性重掩码解码(SRD)，仅从编码器表示中重掩码3D相关信息，同时保留2D图结构；结合3D关系变换器编码器和结构无关解码器

Result: 在广泛使用的MD17分子性质预测基准测试中，8个目标中的7个达到了新的最先进性能

Conclusion: 3D-GSRD通过选择性重掩码解码和结构无关解码器的协同集成，增强了编码器在分子表示学习中的作用

Abstract: Masked graph modeling (MGM) is a promising approach for molecular
representation learning (MRL).However, extending the success of re-mask
decoding from 2D to 3D MGM is non-trivial, primarily due to two conflicting
challenges: avoiding 2D structure leakage to the decoder, while still providing
sufficient 2D context for reconstructing re-masked atoms.To address these
challenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder with
Selective Re-mask Decoding. The core innovation of 3D-GSRD lies in its
Selective Re-mask Decoding(SRD), which re-masks only 3D-relevant information
from encoder representations while preserving the 2D graph structures.This SRD
is synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)
encoder alongside a structure-independent decoder. We analyze that SRD,
combined with the structure-independent decoder, enhances the encoder's role in
MRL. Extensive experiments show that 3D-GSRD achieves strong downstream
performance, setting a new state-of-the-art on 7 out of 8 targets in the widely
used MD17 molecular property prediction benchmark. The code is released at
https://github.com/WuChang0124/3D-GSRD.

</details>


### [440] [Mixed-Precision Quantization for Language Models: Techniques and Prospects](https://arxiv.org/abs/2510.16805)
*Mariam Rakka,Marios Fournarakis,Olga Krestinskaya,Jinane Bazzi,Khaled N. Salama,Fadi Kurdahi,Ahmed M. Eltawil,Mohammed E. Fouda*

Main category: cs.LG

TL;DR: 该综述论文系统回顾了语言模型的混合精度量化技术，分析了不同精度分配策略在权重、激活和KV缓存上的应用，比较了困惑度和零样本任务性能，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型规模的快速扩展，其计算、内存和能耗需求急剧增加，使得训练和部署变得不可持续。混合精度量化通过选择性分配精度来平衡效率和准确性，成为解决这一问题的关键技术。

Method: 论文首先回顾量化基础知识，包括均匀和非均匀量化器、量化粒度以及后训练量化方法。然后根据位分配策略和精度配置对混合精度量化框架进行分类比较，分析不同组件（权重、激活、KV缓存）的精度配置差异。

Result: 通过比较分析发现混合精度量化在保持模型准确性的同时显著提升了效率，不同框架在困惑度和零样本任务性能上表现各异，并揭示了与早期深度神经网络混合精度量化方法的异同。

Conclusion: 混合精度量化是解决大规模语言模型部署挑战的有效方法，未来需要在硬件感知设计、激活量化和可扩展优化方法等方面进一步研究，以支持十亿参数级模型的量化部署。

Abstract: The rapid scaling of language models (LMs) has resulted in unprecedented
computational, memory, and energy requirements, making their training and
deployment increasingly unsustainable. Quantization has emerged as an essential
compression technique to reduce model size, alleviate memory bottlenecks, and
accelerate inference. However, while uniform low-bit quantization (e.g., INT8,
INT4) provides significant efficiency gains, it can degrade accuracy in
sensitive components of transformer-based LMs. Mixed-precision quantization
offers a promising alternative by selectively allocating precision across
layers or within tensors to balance efficiency and accuracy. This survey
provides a comprehensive overview of Mixed-Precision quantization frameworks
for LMs (MXPLMs). We first review quantization fundamentals, including uniform
and non-uniform quantizers, quantization granularity, and methods widely used
in post-training quantization. We then categorize and compare recent MXPLM
frameworks according to their bit allocation strategies and precision
configurations across weights, activations, and key-value caches. A comparative
analysis highlights differences in perplexity, zero-shot task performance, and
deployment trade-offs. Furthermore, we contrast MXPLMs with earlier
mixed-precision quantization methods for deep neural networks, identifying
strategies that transfer and those that face challenges in the LM setting.
Finally, we summarize open issues and future directions, including
hardware-aware design, activation quantization, and scalable optimization
methods for billion-parameter models. By consolidating recent advances, this
work serves as a reference for understanding the current landscape and research
prospects of mixed-precision quantization for large-scale language models.

</details>


### [441] [Computational Budget Should Be Considered in Data Selection](https://arxiv.org/abs/2510.16806)
*Weilin Wan,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: 提出了计算预算感知数据选择方法(CADS)，通过双层优化框架将计算预算约束集成到数据选择策略中，在视觉和语言基准测试中性能提升最高达14.42%。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法忽略了计算预算约束，导致在不同预算下无法保持一致的性能优势。计算预算应成为数据选择策略的核心要素，因为不同预算对数据数量、质量和分布有不同的要求。

Method: 提出CADS方法，采用双层优化框架：内层在选定数据子集上训练模型，外层基于模型评估优化数据选择。通过概率重参数化策略和Hessian-free策略梯度估计器解决Hessian矩阵估计问题，将内层优化转化为外层目标的惩罚项以提高效率。

Result: 在视觉和语言基准测试中，CADS方法相比基线方法实现了最高14.42%的性能提升，证明了计算预算感知数据选择的有效性。

Conclusion: 计算预算应作为数据选择策略的核心考虑因素，CADS方法通过双层优化框架成功解决了预算约束下的数据选择问题，显著提升了训练效率和模型性能。

Abstract: Data selection improves computational efficiency by choosing informative
subsets of training samples. However, existing methods ignore the compute
budget, treating data selection and importance evaluation independently of
compute budget constraints. Yet empirical studies show no algorithm can
consistently outperform others (or even random selection) across varying
budgets. We therefore argue that compute budget must be integral to
data-selection strategies, since different budgets impose distinct requirements
on data quantity, quality, and distribution for effective training. To this
end, we propose a novel Computational budget-Aware Data Selection (CADS) method
and naturally formulate it into a bilevel optimization framework, where the
inner loop trains the model within the constraints of the computational budget
on some selected subset of training data, while the outer loop optimizes data
selection based on model evaluation. Our technical contributions lie in
addressing two main challenges in solving this bilevel optimization problem:
the expensive Hessian matrix estimation for outer-loop gradients and the
computational burden of achieving inner-loop optimality during iterations. To
solve the first issue, we propose a probabilistic reparameterization strategy
and compute the gradient using a Hessian-free policy gradient estimator. To
address the second challenge, we transform the inner optimization problem into
a penalty term in the outer objective, further discovering that we only need to
estimate the minimum of a one-dimensional loss to calculate the gradient,
significantly improving efficiency. Extensive experiments show that our method
achieves performance gains of up to 14.42% over baselines in vision and
language benchmarks.

</details>


### [442] [Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads](https://arxiv.org/abs/2510.16807)
*Zhoutong Wu,Yuan Zhang,Yiming Dong,Chenheng Zhang,Cong Fang,Kun Yuan,Zhouchen Lin*

Main category: cs.LG

TL;DR: SkipV1Former是一种Transformer变体，通过从第一层的Value头添加跳跃连接来增强模型表示能力并减少KV缓存，可减少约25%的KV缓存同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 扩展Transformer模型以提高表示能力通常需要大量内存和计算成本，特别是自回归解码中的KV缓存。现有方法要么改进表达能力但KV成本不变，要么减少内存但削弱表示能力。

Method: 从第二块开始，每层重用一半的Value头来自第一层，另一半正常计算，从而将Value投影和V缓存减少近50%。理论上，将未压缩的第一层Value路由到更深层可以恢复压缩丢失的信息并加速模型的隐式元优化。

Result: 在不同模型规模下，SkipV1Former相对于标准MHA Transformer和某些先进变体，在减少约25% KV缓存的同时改进了困惑度。与YOCO结合时，KV缓存大小减少近50%且性能仍提升。

Conclusion: SkipV1Former提供了一种有效的方法来增强Transformer表示能力并减少KV缓存，且可以通过仅10-15%额外计算将现有MHA Transformer检查点上训练到SkipV1Former。

Abstract: Transformer models have driven breakthroughs across various language tasks by
their strong capability to learn rich contextual representations. Scaling them
to improve representation, however, often demands substantial memory and
compute costs, such as the Key-Value (KV) cache used during auto-regressive
decoding. Skip connections offer a promising way to improve representation
without bloating resource usage, yet most prior works either improve
expressivity while leaving KV costs unchanged, or reduce memory at the cost of
weaker representation. In this work, we propose SkipV1Former, a Transformer
variant that uses skip connections from the first layer's Value heads to
strengthen model representation and reduce KV cache. Specifically, from the
second block onward, each layer reuses half of its Value heads from the very
first layer, while computing the other half as usual-cutting Value projections
and V cache by nearly 50 \%. Theoretically, we show that routing uncompressed
first-layer Values into deeper layers restores information lost to compression
and accelerates the model's implicit mesa-optimization-a key pattern of
Transformer in auto-regressive tasks. Empirically, across different model
scales, SkipV1Former delivers consistent reductions of approximately 25 \% in
KV cache while improving perplexity relative to standard Multi-Head Attention
(MHA) Transformers and some advanced variants. Moreover, we propose a recipe
for uptraining existing MHA Transformer checkpoints to SkipV1Former with only
10-15\% additional compute. Finally, SkipV1Former can seamlessly combine
advanced methods like Group-Query Attention and Multi-Latent Attention to
achieve further KV cache savings and performance improvement. When combined
with YOCO, it cuts KV cache size by nearly 50 \% while still improving
performance.

</details>


### [443] [Graph Learning is Suboptimal in Causal Bandits](https://arxiv.org/abs/2510.16811)
*Mohammad Shahverdikondori,Jalal Etesami,Negar Kiyavash*

Main category: cs.LG

TL;DR: 该论文证明在因果匪徒问题中，学习奖励的父节点集是次优的，因为遗憾最小化和父节点识别是相互冲突的目标。作者提出了绕过图恢复的近乎最优算法。


<details>
  <summary>Details</summary>
Motivation: 研究在未知因果结构的情况下，因果匪徒问题中的遗憾最小化。现有方法要么先识别奖励的父节点再应用经典匪徒方法，要么联合学习父节点同时最小化遗憾，但作者质疑这些策略是否最优。

Method: 通过证明存在遗憾最小化和父节点识别相互冲突的实例，建立了捕捉动作空间组合结构的遗憾下界，并提出了绕过图恢复和父节点恢复的近乎最优算法。

Result: 实验证实，在各种环境中，作者的方法与现有基线方法之间存在显著的性能差距。

Conclusion: 父节点识别对于遗憾最小化是不必要的，学习父节点集是次优策略。

Abstract: We study regret minimization in causal bandits under causal sufficiency where
the underlying causal structure is not known to the agent. Previous work has
focused on identifying the reward's parents and then applying classic bandit
methods to them, or jointly learning the parents while minimizing regret. We
investigate whether such strategies are optimal. Somewhat counterintuitively,
our results show that learning the parent set is suboptimal. We do so by
proving that there exist instances where regret minimization and parent
identification are fundamentally conflicting objectives. We further analyze
both the known and unknown parent set size regimes, establish novel regret
lower bounds that capture the combinatorial structure of the action space.
Building on these insights, we propose nearly optimal algorithms that bypass
graph and parent recovery, demonstrating that parent identification is indeed
unnecessary for regret minimization. Experiments confirm that there exists a
large performance gap between our method and existing baselines in various
environments.

</details>


### [444] [Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity](https://arxiv.org/abs/2510.16814)
*Simon Jaxy,Anton Theys,Patrick Willett,W. Chris Carleton,Ralf Vandam,Pieter Libin*

Main category: cs.LG

TL;DR: 使用半监督学习和正例-未标记学习策略的深度学习模型，通过动态伪标签和条件随机场改进，在考古预测建模中取得了与最先进方法相当的性能，并在卫星图像上表现出更好的可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决考古预测建模中的结构性标签稀缺问题：正例样本稀少，大多数位置未标记，需要处理严重的类别不平衡。

Method: 采用半监督正例-未标记学习策略，实现为语义分割模型，使用动态伪标签技术并通过RNN实现的条件随机场进行精炼，提高标签置信度。

Result: 在基于数字高程模型的地理空间数据集上，性能与最先进的LAMAP方法相当，同时获得更高的Dice分数；在原始卫星图像上，通过分层k折交叉验证保持性能，产生具有更好可解释性的预测表面。

Conclusion: 半监督学习为在大型、稀疏标注的景观中识别未发现遗址提供了一种有前景的方法。

Abstract: Archaeological predictive modelling estimates where undiscovered sites are
likely to occur by combining known locations with environmental, cultural, and
geospatial variables. We address this challenge using a deep learning approach
but must contend with structural label scarcity inherent to archaeology:
positives are rare, and most locations are unlabeled. To address this, we adopt
a semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a
semantic segmentation model and evaluated on two datasets covering a
representative range of archaeological periods. Our approach employs dynamic
pseudolabeling, refined with a Conditional Random Field (CRF) implemented via
an RNN, increasing label confidence under severe class imbalance. On a
geospatial dataset derived from a digital elevation model (DEM), our model
performs on par with the state-of-the-art, LAMAP, while achieving higher Dice
scores. On raw satellite imagery, assessed end-to-end with stratified k-fold
cross-validation, it maintains performance and yields predictive surfaces with
improved interpretability. Overall, our results indicate that semi-supervised
learning offers a promising approach to identifying undiscovered sites across
large, sparsely annotated landscapes.

</details>


### [445] [Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator](https://arxiv.org/abs/2510.16816)
*Ming Zhong,Zhenya Yan*

Main category: cs.LG

TL;DR: 提出线性注意力神经算子(LANO)，通过引入代理令牌机制在保持软注意力表达能力的同时实现线性复杂度，解决了传统注意力在神经算子中的可扩展性与精度权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决基于transformer的神经算子架构面临的基本可扩展性-精度权衡：软注意力提供良好保真度但具有二次复杂度，而线性注意力变体降低计算成本但往往遭受显著精度下降。

Method: 引入紧凑的代理令牌集(M ≪ N)来调解N个令牌之间的全局交互，这种代理注意力机制产生具有线性复杂度的算子层，同时保持软注意力的表达能力。

Result: 理论上证明了通用逼近性质，实证上LANO超越了当前最先进的神经PDE求解器，在标准基准测试中平均精度提升19.5%。

Conclusion: 通过在线性复杂度和软注意力级性能之间架起桥梁，LANO为科学机器学习应用建立了可扩展、高精度的基础。

Abstract: Neural operators offer a powerful data-driven framework for learning mappings
between function spaces, in which the transformer-based neural operator
architecture faces a fundamental scalability-accuracy trade-off: softmax
attention provides excellent fidelity but incurs quadratic complexity
$\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$,
while linear attention variants reduce cost to $\mathcal{O}(N d^2)$ but often
suffer significant accuracy degradation. To address the aforementioned
challenge, in this paper, we present a novel type of neural operators, Linear
Attention Neural Operator (LANO), which achieves both scalability and high
accuracy by reformulating attention through an agent-based mechanism. LANO
resolves this dilemma by introducing a compact set of $M$ agent tokens $(M \ll
N)$ that mediate global interactions among $N$ tokens. This agent attention
mechanism yields an operator layer with linear complexity $\mathcal{O}(MN d)$
while preserving the expressive power of softmax attention. Theoretically, we
demonstrate the universal approximation property, thereby demonstrating
improved conditioning and stability properties. Empirically, LANO surpasses
current state-of-the-art neural PDE solvers, including Transolver with
slice-based softmax attention, achieving average $19.5\%$ accuracy improvement
across standard benchmarks. By bridging the gap between linear complexity and
softmax-level performance, LANO establishes a scalable, high-accuracy
foundation for scientific machine learning applications.

</details>


### [446] [Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning](https://arxiv.org/abs/2510.16882)
*Heming Zou,Yixiu Mao,Yun Qu,Qi Wang,Xiangyang Ji*

Main category: cs.LG

TL;DR: UDS是一种用于监督微调的高效在线批次选择框架，通过核范数捕获数据效用和样本内多样性，利用轻量级内存缓冲区估计样本间多样性，无需外部资源即可提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有在线批次选择方法存在三个主要问题：(i)仅依赖数据效用而忽视多样性；(ii)需要外部资源如参考模型或验证集；(iii)训练时间超过全数据集训练。UDS旨在解决这些限制。

Method: UDS利用对数矩阵的核范数捕获数据效用和样本内多样性，通过轻量级内存缓冲区中的低维嵌入比较估计样本间多样性，无需外部资源和不必要的反向传播。

Result: 在多个基准测试中，UDS在不同数据预算下始终优于最先进的在线批次选择方法，相比全数据集微调显著减少训练时间。

Conclusion: UDS是一个高效且无需外部资源的在线批次选择框架，在监督微调中实现了更好的性能和计算效率。

Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large
language models (LLMs) to downstream tasks. In practice, SFT on a full dataset
is computationally expensive and sometimes suffers from overfitting or bias
amplification. This facilitates the rise of data curation in SFT, which
prioritizes the most valuable data to optimze. This work studies the online
batch selection family that dynamically scores and filters samples during the
training process. However, existing popular methods often (i) rely merely on
the utility of data to select a subset while neglecting other crucial factors
like diversity, (ii) rely on external resources such as reference models or
validation sets, and (iii) incur extra training time over full-dataset
training. To address these limitations, this work develops \textbf{UDS
(Utility-Diversity Sampling)}, a framework for efficient online batch selection
in SFT. UDS leverages the nuclear norm of the logits matrix to capture both
data utility and intra-sample diversity, while estimating inter-sample
diversity through efficient low-dimensional embedding comparisons with a
lightweight memory buffer of historical samples. Such a design eliminates the
need for external resources and unnecessary backpropagation, securing
computational efficiency. Experiments on multiple benchmarks demonstrate that
UDS consistently outperforms state-of-the-art online batch selection methods
under varying data budgets, and significantly reduces training time compared to
full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.

</details>


### [447] [Trace Regularity PINNs: Enforcing $\mathrm{H}^{\frac{1}{2}}(\partial Ω)$ for Boundary Data](https://arxiv.org/abs/2510.16817)
*Doyoon Kim,Junbin Song*

Main category: cs.LG

TL;DR: 提出TRPINN方法，在Sobolev-Slobodeckij范数H^{1/2}(∂Ω)中强制边界损失，这是与H^1(Ω)相关的正确迹空间。通过计算半范数的理论必要部分降低计算成本，避免离散化中的分母求值增强收敛稳定性。


<details>
  <summary>Details</summary>
Motivation: 标准PINNs在处理高度振荡Dirichlet边界条件时可能失败，需要改进边界损失的计算方式以提高收敛性和稳定性。

Method: 使用TRPINN方法，在正确的迹空间H^{1/2}(∂Ω)中强制边界损失，仅计算半范数的理论必要部分，避免分母求值。

Result: 数值实验显示TRPINN在标准PINNs失败的情况下仍能成功，性能提升1-3个十进制数字。通过NTK分析证明TRPINN比标准PINNs收敛更快。

Conclusion: TRPINN通过使用正确的迹空间范数，提高了PINNs的收敛性和稳定性，特别是在处理复杂边界条件时表现优异。

Abstract: We propose an enhanced physics-informed neural network (PINN), the Trace
Regularity Physics-Informed Neural Network (TRPINN), which enforces the
boundary loss in the Sobolev-Slobodeckij norm $H^{1/2}(\partial \Omega)$, the
correct trace space associated with $H^1(\Omega)$. We reduce computational cost
by computing only the theoretically essential portion of the semi-norm and
enhance convergence stability by avoiding denominator evaluations in the
discretization. By incorporating the exact $H^{1/2}(\partial \Omega)$ norm, we
show that the approximation converges to the true solution in the
$H^{1}(\Omega)$ sense, and, through Neural Tangent Kernel (NTK) analysis, we
demonstrate that TRPINN can converge faster than standard PINNs. Numerical
experiments on the Laplace equation with highly oscillatory Dirichlet boundary
conditions exhibit cases where TRPINN succeeds even when standard PINNs fail,
and show performance improvements of one to three decimal digits.

</details>


### [448] [Finding Manifolds With Bilinear Autoencoders](https://arxiv.org/abs/2510.16820)
*Thomas Dooms,Ward Gauderis*

Main category: cs.LG

TL;DR: 使用双线性自编码器将表示分解为二次多项式，实现非线性但可分析的潜在表示


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器依赖于输入，难以独立研究。多项式作为代数基元可以在不依赖输入的情况下分析，能够描述从线性概念到复杂流形的结构

Method: 使用双线性自编码器高效地将表示分解为二次多项式，并引入改进以诱导重要性排序、聚类和激活稀疏性

Result: 开发了一种能够将神经网络表示分解为二次多项式的方法，为通过代数属性分析非线性潜在表示提供了初步解决方案

Conclusion: 这是通过代数属性实现非线性但可分析潜在表示的第一步，为理解神经网络内部表示提供了新的分析工具

Abstract: Sparse autoencoders are a standard tool for uncovering interpretable latent
representations in neural networks. Yet, their interpretation depends on the
inputs, making their isolated study incomplete. Polynomials offer a solution;
they serve as algebraic primitives that can be analysed without reference to
input and can describe structures ranging from linear concepts to complicated
manifolds. This work uses bilinear autoencoders to efficiently decompose
representations into quadratic polynomials. We discuss improvements that induce
importance ordering, clustering, and activation sparsity. This is an initial
step toward nonlinear yet analysable latents through their algebraic
properties.

</details>


### [449] [Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation](https://arxiv.org/abs/2510.16943)
*Dania Refai,Moataz Ahmed*

Main category: cs.LG

TL;DR: 提出了一个组件级评估框架，用于评估LLM生成的数学优化公式，超越了传统的整体评估方法，通过多个精细指标来分析结构性和数值性错误。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法通常将优化公式视为整体，依赖粗略指标如解精度或运行时间，这掩盖了结构或数值错误。需要更精细的评估框架来诊断LLM在优化建模中的表现。

Method: 开发了包含决策变量和约束的精确率与召回率、约束和目标函数的均方根误差、基于token使用和延迟的效率指标的综合评估框架。评估了GPT-5、LLaMA 3.1 Instruct和DeepSeek Math在不同复杂度优化问题下的六种提示策略。

Result: GPT-5表现最佳，思维链、自一致性和模块化提示最有效。求解器性能主要取决于高约束召回率和低约束RMSE，确保结构正确性和解可靠性。约束精确率和决策变量指标起次要作用，简洁输出提高计算效率。

Conclusion: 提出了NLP到优化建模的三个原则：完整约束覆盖防止违规、最小化约束RMSE确保求解器级精度、简洁输出提高计算效率。该框架为LLM在优化建模中的细粒度诊断评估奠定了基础。

Abstract: Large language models (LLMs) are increasingly used to convert natural
language descriptions into mathematical optimization formulations. Current
evaluations often treat formulations as a whole, relying on coarse metrics like
solution accuracy or runtime, which obscure structural or numerical errors. In
this study, we present a comprehensive, component-level evaluation framework
for LLM-generated formulations. Beyond the conventional optimality gap, our
framework introduces metrics such as precision and recall for decision
variables and constraints, constraint and objective root mean squared error
(RMSE), and efficiency indicators based on token usage and latency. We evaluate
GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of
varying complexity under six prompting strategies. Results show that GPT-5
consistently outperforms other models, with chain-of-thought, self-consistency,
and modular prompting proving most effective. Analysis indicates that solver
performance depends primarily on high constraint recall and low constraint
RMSE, which together ensure structural correctness and solution reliability.
Constraint precision and decision variable metrics play secondary roles, while
concise outputs enhance computational efficiency. These findings highlight
three principles for NLP-to-optimization modeling: (i) Complete constraint
coverage prevents violations, (ii) minimizing constraint RMSE ensures
solver-level accuracy, and (iii) concise outputs improve computational
efficiency. The proposed framework establishes a foundation for fine-grained,
diagnostic evaluation of LLMs in optimization modeling.

</details>


### [450] [ProtoMol: Enhancing Molecular Property Prediction via Prototype-Guided Multimodal Learning](https://arxiv.org/abs/2510.16824)
*Yingxu Wang,Kunyu Zhang,Jiaxin Huang,Nan Yin,Siwei Liu,Eran Segal*

Main category: cs.LG

TL;DR: ProtoMol是一个原型引导的多模态分子表示学习框架，通过层次编码器和双向跨模态注意力机制，实现分子图与文本描述之间的细粒度整合和语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法存在两个关键局限：(1)仅在最终编码层进行跨模态交互，忽略了层次语义依赖；(2)缺乏统一的原型空间来实现模态间的稳健对齐。

Method: 采用双分支层次编码器（图神经网络处理分子图，Transformer编码文本），引入层次双向跨模态注意力机制，并构建具有可学习类特定锚点的共享原型空间。

Result: 在多个基准数据集上的广泛实验表明，ProtoMol在各种分子性质预测任务中始终优于最先进的基线方法。

Conclusion: ProtoMol通过原型引导的多模态学习，有效解决了现有方法的局限性，实现了分子图与文本描述之间的细粒度整合和一致语义对齐。

Abstract: Multimodal molecular representation learning, which jointly models molecular
graphs and their textual descriptions, enhances predictive accuracy and
interpretability by enabling more robust and reliable predictions of drug
toxicity, bioactivity, and physicochemical properties through the integration
of structural and semantic information. However, existing multimodal methods
suffer from two key limitations: (1) they typically perform cross-modal
interaction only at the final encoder layer, thus overlooking hierarchical
semantic dependencies; (2) they lack a unified prototype space for robust
alignment between modalities. To address these limitations, we propose
ProtoMol, a prototype-guided multimodal framework that enables fine-grained
integration and consistent semantic alignment between molecular graphs and
textual descriptions. ProtoMol incorporates dual-branch hierarchical encoders,
utilizing Graph Neural Networks to process structured molecular graphs and
Transformers to encode unstructured texts, resulting in comprehensive
layer-wise representations. Then, ProtoMol introduces a layer-wise
bidirectional cross-modal attention mechanism that progressively aligns
semantic features across layers. Furthermore, a shared prototype space with
learnable, class-specific anchors is constructed to guide both modalities
toward coherent and discriminative representations. Extensive experiments on
multiple benchmark datasets demonstrate that ProtoMol consistently outperforms
state-of-the-art baselines across a variety of molecular property prediction
tasks.

</details>


### [451] [Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures](https://arxiv.org/abs/2510.16968)
*Pingzhi Li,Morris Yu-Chao Huang,Zhen Tan,Qingquan Song,Jie Peng,Kai Zou,Yu Cheng,Kaidi Xu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出了一种基于MoE结构习惯的知识蒸馏检测框架，在黑白盒设置下都能有效检测LLM的知识蒸馏，准确率超过94%且对提示工程规避具有强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于自身份或输出相似性的KD检测方法容易被提示工程规避，存在知识产权保护和LLM多样性风险。

Method: 利用MoE结构习惯（特别是内部路由模式）作为检测信号，分析专家在不同输入上的专业化和协作模式；提出Shadow-MoE方法在black-box设置下构建代理MoE表示进行比较。

Result: 建立了全面的可复现基准测试，实验显示在各种场景下检测准确率超过94%，对基于提示的规避具有强鲁棒性。

Conclusion: 该方法显著优于现有基线，突显了LLM中结构习惯转移的重要性，为KD检测提供了有效解决方案。

Abstract: Knowledge Distillation (KD) accelerates training of large language models
(LLMs) but poses intellectual property protection and LLM diversity risks.
Existing KD detection methods based on self-identity or output similarity can
be easily evaded through prompt engineering. We present a KD detection
framework effective in both white-box and black-box settings by exploiting an
overlooked signal: the transfer of MoE "structural habits", especially internal
routing patterns. Our approach analyzes how different experts specialize and
collaborate across various inputs, creating distinctive fingerprints that
persist through the distillation process. To extend beyond the white-box setup
and MoE architectures, we further propose Shadow-MoE, a black-box method that
constructs proxy MoE representations via auxiliary distillation to compare
these patterns between arbitrary model pairs. We establish a comprehensive,
reproducible benchmark that offers diverse distilled checkpoints and an
extensible framework to facilitate future research. Extensive experiments
demonstrate >94% detection accuracy across various scenarios and strong
robustness to prompt-based evasion, outperforming existing baselines while
highlighting the structural habits transfer in LLMs.

</details>


### [452] [DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization](https://arxiv.org/abs/2510.16857)
*Jiyan Qiu,Lyulin Kuang,Guan Wang,Yichen Xu,Leiyao Cui,Shaotong Fu,Yixin Zhu,Ruihua Zhang*

Main category: cs.LG

TL;DR: DrivAerStar是一个包含12,000个工业级汽车CFD模拟的数据集，通过改进的网格策略实现风洞验证精度低于1.04%，比现有数据集提升5倍，将计算成本从数周减少到分钟级别。


<details>
  <summary>Details</summary>
Motivation: 车辆空气动力学优化对汽车电动化至关重要，但传统方法面临计算成本高与精度不足的权衡，现有机器学习数据集存在网格分辨率不足、组件缺失和验证误差超过5%等问题，无法在工业工作流程中部署。

Method: 使用STAR-CCM+软件生成12,000个工业级CFD模拟，通过20个CAD参数和自由形变算法系统探索三种车辆配置，包括完整的发动机舱和冷却系统，采用精细网格策略和严格的壁面y+控制。

Result: 数据集实现风洞验证精度低于1.04%，比现有数据集提升5倍，基于该数据训练的模型在保持生产级精度的同时将计算成本从数周减少到分钟级别。

Conclusion: DrivAerStar是首个连接学术机器学习研究与工业CFD实践的数据集，为汽车开发中的数据驱动空气动力学优化设立了新标准，展示了将高保真物理模拟与AI整合的范式。

Abstract: Vehicle aerodynamics optimization has become critical for automotive
electrification, where drag reduction directly determines electric vehicle
range and energy efficiency. Traditional approaches face an intractable
trade-off: computationally expensive Computational Fluid Dynamics (CFD)
simulations requiring weeks per design iteration, or simplified models that
sacrifice production-grade accuracy. While machine learning offers
transformative potential, existing datasets exhibit fundamental limitations --
inadequate mesh resolution, missing vehicle components, and validation errors
exceeding 5% -- preventing deployment in industrial workflows. We present
DrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations
generated using $\text{STAR-CCM+}^\unicode{xAE}$ software. The dataset
systematically explores three vehicle configurations through 20 Computer Aided
Design (CAD) parameters via Free Form Deformation (FFD) algorithms, including
complete engine compartments and cooling systems with realistic internal
airflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a
five-fold improvement over existing datasets -- through refined mesh strategies
with strict wall $y^+$ control. Benchmarks demonstrate that models trained on
this data achieve production-ready accuracy while reducing computational costs
from weeks to minutes. This represents the first dataset bridging academic
machine learning research and industrial CFD practice, establishing a new
standard for data-driven aerodynamic optimization in automotive development.
Beyond automotive applications, DrivAerStar demonstrates a paradigm for
integrating high-fidelity physics simulations with Artificial Intelligence (AI)
across engineering disciplines where computational constraints currently limit
innovation.

</details>


### [453] [Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning](https://arxiv.org/abs/2510.17021)
*Bingqi Shang,Yiwei Chen,Yihua Zhang,Bingquan Shen,Sijia Liu*

Main category: cs.LG

TL;DR: 本文提出了一种针对大语言模型遗忘过程的隐蔽攻击方法——后门遗忘攻击，该攻击让模型在正常条件下表现遗忘成功，但在特定触发词出现时恢复被遗忘的知识。


<details>
  <summary>Details</summary>
Motivation: 随着开源权重大语言模型的兴起，研究遗忘过程本身是否可能被植入后门，即在正常条件下看似成功遗忘，但在隐藏触发词激活时恢复原有行为。

Method: 通过将触发词放置在注意力汇聚位置（attention sink），并调整其注意力值来增强后门持久性，利用注意力汇聚现象作为后门遗忘的入口。

Result: 实验验证了注意力汇聚引导的后门遗忘攻击能够可靠地在后门触发词出现时恢复被遗忘的知识，而在无触发词时与正常遗忘模型无法区分。

Conclusion: 注意力汇聚现象为后门遗忘攻击提供了有效途径，揭示了遗忘过程本身可能存在的安全隐患，需要开发更安全的遗忘机制。

Abstract: Large language model (LLM) unlearning has become a critical mechanism for
removing undesired data, knowledge, or behaviors from pre-trained models while
retaining their general utility. Yet, with the rise of open-weight LLMs, we
ask: can the unlearning process itself be backdoored, appearing successful
under normal conditions yet reverting to pre-unlearned behavior when a hidden
trigger is activated? Drawing inspiration from classical backdoor attacks that
embed triggers into training data to enforce specific behaviors, we investigate
backdoor unlearning, where models forget as intended in the clean setting but
recover forgotten knowledge when the trigger appears. We show that designing
such attacks presents unique challenges, hinging on where triggers are placed
and how backdoor training is reinforced. We uncover a strong link between
backdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens
consistently attract disproportionate attention in LLMs. Our analysis reveals
that these attention sinks serve as gateways for backdoor unlearning: placing
triggers at sink positions and aligning their attention values markedly
enhances backdoor persistence. Extensive experiments validate these findings,
showing that attention-sink-guided backdoor unlearning reliably restores
forgotten knowledge in the presence of backdoor triggers, while behaving
indistinguishably from a normally unlearned model when triggers are absent.
Code is available at https://github.com/OPTML-Group/Unlearn-Backdoor.

</details>


### [454] [Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning](https://arxiv.org/abs/2510.16877)
*Heming Zou,Yunliang Zang,Wutong Xu,Xiangyang Ji*

Main category: cs.LG

TL;DR: Fly-CL是一个受果蝇嗅觉回路启发的持续表示学习框架，通过解决相似性匹配中的多重共线性问题，显著减少训练时间，同时达到或超越现有最先进方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的持续表示学习方法直接利用预训练特征进行下游任务时，在相似性匹配阶段容易受到多重共线性的影响，而更先进的方法对于实时低延迟应用来说计算成本过高。

Method: 提出Fly-CL框架，受果蝇嗅觉回路启发，与各种预训练主干网络兼容，通过渐进式解决多重共线性问题，实现更有效的相似性匹配。

Result: Fly-CL显著减少了训练时间，同时在各种网络架构和数据机制下的模拟实验中，性能达到或超过了当前最先进方法。

Conclusion: Fly-CL通过生物启发设计有效解决了持续表示学习中的挑战，理论分析表明其能够渐进式解决多重共线性问题，且具有低时间复杂度。

Abstract: Using a nearly-frozen pretrained model, the continual representation learning
paradigm reframes parameter updates as a similarity-matching problem to
mitigate catastrophic forgetting. However, directly leveraging pretrained
features for downstream tasks often suffers from multicollinearity in the
similarity-matching stage, and more advanced methods can be computationally
prohibitive for real-time, low-latency applications. Inspired by the fly
olfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with
a wide range of pretrained backbones. Fly-CL substantially reduces training
time while achieving performance comparable to or exceeding that of current
state-of-the-art methods. We theoretically show how Fly-CL progressively
resolves multicollinearity, enabling more effective similarity matching with
low time complexity. Extensive simulation experiments across diverse network
architectures and data regimes validate Fly-CL's effectiveness in addressing
this challenge through a biologically inspired design. Code is available at
https://github.com/gfyddha/Fly-CL.

</details>


### [455] [Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction](https://arxiv.org/abs/2510.17132)
*Ioannis Tsaknakis,Bingqing Song,Shuyu Gan,Dongyeop Kang,Alfredo Garcia,Gaowen Liu,Charles Fleming,Mingyi Hong*

Main category: cs.LG

TL;DR: 提出了一个评估LLM在对话中发现和利用潜在用户信息的统一基准，包含三个逐步现实的场景：20 Questions游戏、个性化问答和个性化文本摘要。


<details>
  <summary>Details</summary>
Motivation: LLM在生成通用文本方面表现出色，但在需要用户特定偏好的场景中，用户很少明确表达所有偏好，大量信息是潜在的。需要评估LLM是否能通过对话发现和推理这些潜在信息。

Method: 采用三智能体框架（用户、助手、法官）进行多轮交互评估，涵盖三个任务：20 Questions游戏、个性化问答和个性化文本摘要，支持逐轮评估信息获取和适应能力。

Result: LLM确实能够通过对话揭示潜在信息，但成功率差异很大（32%到98%），取决于任务复杂性、主题和隐藏属性数量。

Conclusion: 该基准为研究个性化交互中的潜在信息发现提供了首个系统性框架，表明有效的偏好推理仍然是构建真正自适应AI系统的开放前沿。

Abstract: Large Language Models (LLMs) excel at producing broadly relevant text, but
this generality becomes a limitation when user-specific preferences are
required, such as recommending restaurants or planning travel. In these
scenarios, users rarely articulate every preference explicitly; instead, much
of what they care about remains latent, waiting to be inferred. This raises a
fundamental question: Can LLMs uncover and reason about such latent information
through conversation?
  We address this problem by introducing a unified benchmark for evaluating
latent information discovery - the ability of LLMs to reveal and utilize hidden
user attributes through multi-turn interaction. The benchmark spans three
progressively realistic settings: the classic 20 Questions game, Personalized
Question Answering, and Personalized Text Summarization. All tasks share a
tri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of
elicitation and adaptation. Our results reveal that while LLMs can indeed
surface latent information through dialogue, their success varies dramatically
with context: from 32% to 98%, depending on task complexity, topic, and number
of hidden attributes. This benchmark provides the first systematic framework
for studying latent information discovery in personalized interaction,
highlighting that effective preference inference remains an open frontier for
building truly adaptive AI systems.

</details>


### [456] [UniGTE: Unified Graph-Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains](https://arxiv.org/abs/2510.16885)
*Duo Wang,Yuan Zuo,Guangyue Lu,Junjie Wu*

Main category: cs.LG

TL;DR: UniGTE是一个指令调优的编码器-解码器框架，通过结合图结构和语义推理，实现跨任务和跨领域的零样本图推理。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络受限于固定标签空间，而大语言模型难以捕捉图结构，需要一种能统一结构推理和语义推理的方法来处理未见过的图任务。

Method: 编码器使用可学习的对齐标记和结构感知的图-文本注意力机制，将标记化图和自然语言任务提示联合处理；解码器基于编码表示预测任务答案并重构输入图。

Result: 在节点分类、链接预测、图分类和图回归任务上取得了新的零样本最优结果，在跨任务和跨域设置中表现优异。

Conclusion: 图结构与LLM语义的紧密集成能够实现鲁棒且可迁移的图推理能力。

Abstract: Generalizing to unseen graph tasks without task-specific supervision is
challenging: conventional graph neural networks are typically tied to a fixed
label space, while large language models (LLMs) struggle to capture graph
structure. We introduce UniGTE, an instruction-tuned encoder-decoder framework
that unifies structural and semantic reasoning. The encoder augments a
pretrained autoregressive LLM with learnable alignment tokens and a
structure-aware graph-text attention mechanism, enabling it to attend jointly
to a tokenized graph and a natural-language task prompt while remaining
permutation-invariant to node order. This yields compact, task-aware graph
representations. Conditioned solely on these representations, a frozen LLM
decoder predicts and reconstructs: it outputs the task answer and
simultaneously paraphrases the input graph in natural language. The
reconstruction objective regularizes the encoder to preserve structural cues.
UniGTE is instruction-tuned on five datasets spanning node-level, edge-level,
and graph-level tasks across diverse domains, yet requires no fine-tuning at
inference. It achieves new state-of-the-art zero-shot results on node
classification, link prediction, graph classification, and graph regression
under cross-task and cross-domain settings, demonstrating that tight
integration of graph structure with LLM semantics enables robust, transferable
graph reasoning.

</details>


### [457] [Soft-Masked Diffusion Language Models](https://arxiv.org/abs/2510.17206)
*Michael Hersche,Samuel Moor-Smith,Thomas Hofmann,Abbas Rahimi*

Main category: cs.LG

TL;DR: 提出了一种名为软掩码（Soft-Masking）的新方法，用于改进基于掩码扩散的语言模型，通过动态混合掩码标记与预测标记的嵌入来保留更多预测信息。


<details>
  <summary>Details</summary>
Motivation: 传统掩码扩散模型在解码时只做二元选择（保留掩码或替换为预测标记），这会导致有价值的预测信息被丢弃。

Method: 软掩码方法动态地将掩码标记的嵌入与前一解码步骤中预测的前k个标记的嵌入进行混合，为模型提供更丰富的先验信息。

Result: 在169M参数模型上继续预训练SM改进了困惑度和MAUVE分数；在Dream-7B和Dream-Coder-7B模型上微调后，在多个编码基准测试中性能得到一致提升，特别是在高吞吐量设置下。

Conclusion: 软掩码方法通过保留部分掩码标记的预测信息，有效提升了扩散语言模型的性能，特别是在编码任务中表现突出。

Abstract: Diffusion models have demonstrated strong potential in language modeling,
offering various advantages over traditional autoregressive approaches. Their
ability to generate and revise entire responses in parallel enables faster
generation and built-in self-correction mechanisms. Most modern diffusion-based
language models employ masked diffusion, where decoding involves iteratively
processing masked tokens based on a binary decision: either retaining the mask
or replacing it with the predicted token. However, this binary choice discards
valuable predictive information when the mask is retained. To address this
limitation, we introduce soft-masking (SM), a novel method that dynamically
blends the embedding of the mask token with the embeddings of the top-$k$
predicted tokens from the previous decoding step, for each retained mask. This
provides the model with a more informative prior, preserving context from
earlier computations and allowing partial information about masked tokens to
propagate beyond a single step. We propose a training methodology that adapts a
pretrained masked diffusion language model to incorporate SM. We demonstrate
that continuing pretraining a 169M parameter model with SM leads to improved
perplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art
diffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently
improves performance across multiple coding benchmarks, particularly in
high-throughput settings.

</details>


### [458] [DeepChem Equivariant: SE(3)-Equivariant Support in an Open-Source Molecular Machine Learning Library](https://arxiv.org/abs/2510.16897)
*Jose Siguenza,Bharath Ramsundar*

Main category: cs.LG

TL;DR: 将SE(3)等变神经网络集成到DeepChem中，为分子科学应用提供即用型模型和完整训练流程，降低使用门槛。


<details>
  <summary>Details</summary>
Motivation: 现有的SE(3)等变神经网络库需要深厚的深度学习或数学背景，且缺乏完整的训练流程，限制了非专业用户的使用。

Method: 扩展DeepChem库，集成SE(3)-Transformer和Tensor Field Networks等等变模型，提供完整的训练流程和等变工具包。

Result: 开发了包含等变模型、完整训练流程和实用工具的实现，支持科学家无需深度学习背景即可构建、训练和评估模型。

Conclusion: 该工作显著降低了SE(3)等变模型的使用门槛，促进了分子科学中几何感知神经网络的应用和发展。

Abstract: Neural networks that incorporate geometric relationships respecting SE(3)
group transformations (e.g. rotations and translations) are increasingly
important in molecular applications, such as molecular property prediction,
protein structure modeling, and materials design. These models, known as
SE(3)-equivariant neural networks, ensure outputs transform predictably with
input coordinate changes by explicitly encoding spatial atomic positions.
Although libraries such as E3NN [4] and SE(3)-TRANSFORMER [3 ] offer powerful
implementations, they often require substantial deep learning or mathematical
prior knowledge and lack complete training pipelines. We extend DEEPCHEM [ 13]
with support for ready-to-use equivariant models, enabling scientists with
minimal deep learning background to build, train, and evaluate models, such as
SE(3)-Transformer and Tensor Field Networks. Our implementation includes
equivariant models, complete training pipelines, and a toolkit of equivariant
utilities, supported with comprehensive tests and documentation, to facilitate
both application and further development of SE(3)-equivariant models.

</details>


### [459] [Adaptive Online Learning with LSTM Networks for Energy Price Prediction](https://arxiv.org/abs/2510.16898)
*Salih Salihoglu,Ibrahim Ahmed,Afshin Asadi*

Main category: cs.LG

TL;DR: 使用LSTM网络预测加州电力市场日前电价，引入结合MAE、JSD和平滑惩罚项的自定义损失函数，并采用在线学习方法提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确预测电价对电网运营商、能源生产商和消费者至关重要，需要开发能够适应动态电力市场的高精度预测模型。

Method: 基于LSTM网络，整合历史价格、天气条件和能源结构等特征，设计包含MAE、JSD和平滑惩罚项的自定义损失函数，并实施在线学习策略。

Result: 自定义损失函数提升了模型性能，特别是在峰值时段；在线学习模型通过有效整合实时数据，降低了预测误差和变异性；能源结构特征的加入进一步增强了预测能力。

Conclusion: 该研究为电力价格预测提供了稳健框架，通过综合特征整合和自适应学习机制，为动态电力市场中的决策制定提供了有价值的工具和见解。

Abstract: Accurate prediction of electricity prices is crucial for stakeholders in the
energy market, particularly for grid operators, energy producers, and
consumers. This study focuses on developing a predictive model leveraging Long
Short-Term Memory (LSTM) networks to forecast day-ahead electricity prices in
the California energy market. The model incorporates a variety of features,
including historical price data, weather conditions, and the energy generation
mix. A novel custom loss function that integrates Mean Absolute Error (MAE),
Jensen-Shannon Divergence (JSD), and a smoothness penalty is introduced to
enhance the prediction accuracy and interpretability. Additionally, an online
learning approach is implemented to allow the model to adapt to new data
incrementally, ensuring continuous relevance and accuracy. The results
demonstrate that the custom loss function can improve the model's performance,
aligning predicted prices more closely with actual values, particularly during
peak intervals. Also, the online learning model outperforms other models by
effectively incorporating real-time data, resulting in lower prediction error
and variability. The inclusion of the energy generation mix further enhances
the model's predictive capabilities, highlighting the importance of
comprehensive feature integration. This research provides a robust framework
for electricity price forecasting, offering valuable insights and tools for
better decision-making in dynamic electricity markets.

</details>


### [460] [SNOMED CT-powered Knowledge Graphs for Structured Clinical Data and Diagnostic Reasoning](https://arxiv.org/abs/2510.16899)
*Dun Liu,Qin Pang,Guangai Liu,Hongyu Mou,Jipeng Fan,Yiming Miao,Pin-Han Ho,Limei Peng*

Main category: cs.LG

TL;DR: 提出基于SNOMED CT和Neo4j的知识驱动框架，构建结构化医疗知识图谱，通过标准化临床实体关系对来提升LLM在临床诊断中的逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化临床文档导致的训练数据噪声、不一致和逻辑碎片化问题，提升AI在医疗领域的有效性。

Method: 集成SNOMED CT标准化临床术语与Neo4j图数据库，构建医疗知识图谱，将临床实体作为节点，语义关系作为边，并提取标准化实体关系对生成结构化数据集用于LLM微调。

Result: 实验结果表明该方法显著提高了AI生成诊断推理的有效性和可解释性，改善了LLM输出的临床逻辑一致性。

Conclusion: 该知识引导方法为构建可靠的AI辅助临床系统提供了可扩展的解决方案，增强了诊断推理的有效性和可解释性。

Abstract: The effectiveness of artificial intelligence (AI) in healthcare is
significantly hindered by unstructured clinical documentation, which results in
noisy, inconsistent, and logically fragmented training data. To address this
challenge, we present a knowledge-driven framework that integrates the
standardized clinical terminology SNOMED CT with the Neo4j graph database to
construct a structured medical knowledge graph. In this graph, clinical
entities such as diseases, symptoms, and medications are represented as nodes,
and semantic relationships such as ``caused by,'' ``treats,'' and ``belongs
to'' are modeled as edges in Neo4j, with types mapped from formal SNOMED CT
relationship concepts (e.g., \texttt{Causative agent}, \texttt{Indicated for}).
This design enables multi-hop reasoning and ensures terminological consistency.
By extracting and standardizing entity-relationship pairs from clinical texts,
we generate structured, JSON-formatted datasets that embed explicit diagnostic
pathways. These datasets are used to fine-tune large language models (LLMs),
significantly improving the clinical logic consistency of their outputs.
Experimental results demonstrate that our knowledge-guided approach enhances
the validity and interpretability of AI-generated diagnostic reasoning,
providing a scalable solution for building reliable AI-assisted clinical
systems.

</details>


### [461] [A Lightweight DL Model for Smart Grid Power Forecasting with Feature and Resolution Mismatch](https://arxiv.org/abs/2510.16911)
*Sarah Al-Shareeda,Gulcihan Ozdemir,Heung Seok Jeon,Khaleel Ahmad*

Main category: cs.LG

TL;DR: 提出了一种轻量级深度学习管道，用于在噪声和不完整传感器数据条件下准确预测短期能源消耗，在2025年能源消耗预测竞赛中取得了良好表现。


<details>
  <summary>Details</summary>
Motivation: 解决传感器数据噪声大、不完整且缺乏上下文丰富性时，如何准确预测短期能源消耗的问题。

Method: 采用轻量级深度学习管道，结合小时降采样、双模式插补（均值和多项式回归）和综合归一化，最终选择标准缩放，使用GRU-LSTM序列到一模型进行预测。

Result: 模型平均RMSE为601.9W，MAE为468.9W，准确率达到84.36%。时空热图分析显示温度趋势与预测消耗量高度一致。

Conclusion: 有针对性的预处理与紧凑循环架构相结合，能够在现实条件下实现快速、准确且可部署的能源预测。

Abstract: How can short-term energy consumption be accurately forecasted when sensor
data is noisy, incomplete, and lacks contextual richness? This question guided
our participation in the \textit{2025 Competition on Electric Energy
Consumption Forecast Adopting Multi-criteria Performance Metrics}, which
challenged teams to predict next-day power demand using real-world
high-frequency data. We proposed a robust yet lightweight Deep Learning (DL)
pipeline combining hourly downsizing, dual-mode imputation (mean and polynomial
regression), and comprehensive normalization, ultimately selecting Standard
Scaling for optimal balance. The lightweight GRU-LSTM sequence-to-one model
achieves an average RMSE of 601.9~W, MAE of 468.9~W, and 84.36\% accuracy.
Despite asymmetric inputs and imputed gaps, it generalized well, captured
nonlinear demand patterns, and maintained low inference latency. Notably,
spatiotemporal heatmap analysis reveals a strong alignment between temperature
trends and predicted consumption, further reinforcing the model's reliability.
These results demonstrate that targeted preprocessing paired with compact
recurrent architectures can still enable fast, accurate, and deployment-ready
energy forecasting in real-world conditions.

</details>


### [462] [LILO: Bayesian Optimization with Interactive Natural Language Feedback](https://arxiv.org/abs/2510.17671)
*Katarzyna Kobalczyk,Zhiyuan Jerry Lin,Benjamin Letham,Zhuokai Zhao,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 提出语言在环框架，使用大语言模型将自然语言反馈转换为标量效用，在数值搜索空间上进行贝叶斯优化


<details>
  <summary>Details</summary>
Motivation: 现实应用中需要将复杂、细微或主观的目标转化为可量化的优化目标，反馈机制至关重要

Method: 利用大语言模型将各种类型的文本反馈转换为一致的效用信号，无需手动设计核函数，同时保持贝叶斯优化的样本效率和不确定性量化

Result: 该方法不仅为决策者提供更自然的接口，而且在反馈有限的情况下优于传统贝叶斯优化基线和大语言模型优化器

Conclusion: 语言在环框架结合了大语言模型和贝叶斯优化的优势，实现了更灵活、高效的优化过程

Abstract: For many real-world applications, feedback is essential in translating
complex, nuanced, or subjective goals into quantifiable optimization
objectives. We propose a language-in-the-loop framework that uses a large
language model (LLM) to convert unstructured feedback in the form of natural
language into scalar utilities to conduct BO over a numeric search space.
Unlike preferential BO, which only accepts restricted feedback formats and
requires customized models for each domain-specific problem, our approach
leverages LLMs to turn varied types of textual feedback into consistent utility
signals and to easily include flexible user priors without manual kernel
design. At the same time, our method maintains the sample efficiency and
principled uncertainty quantification of BO. We show that this hybrid method
not only provides a more natural interface to the decision maker but also
outperforms conventional BO baselines and LLM-only optimizers, particularly in
feedback-limited regimes.

</details>


### [463] [Domain Generalizable Continual Learning](https://arxiv.org/abs/2510.16914)
*Hongwei Yan,Guanglong Sun,Zhiqi Kang,Yi Zhong,Liyuan Wang*

Main category: cs.LG

TL;DR: 本文提出了领域泛化持续学习（DGCL）新设置，并开发了自适应领域变换（DoT）方法，通过解耦语义和领域信息来提升模型在动态环境中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法假设训练和测试领域相同，无法应对现实世界中动态变化的多领域环境，需要开发能够同时处理任务序列和领域变化的通用化方法。

Method: 提出自适应领域变换（DoT）方法，基于分布式加枢纽理论，解耦语义和领域相关信息，通过自适应变换任务表示来实现输出对齐，确保平衡和泛化的预测。

Result: DoT作为插件策略显著提升了现有持续学习基线方法在DGCL设置下的性能，在完整参数调优和参数高效调优范式下都有效，且具有资源效率。

Conclusion: DoT方法能够从DGCL中积累领域泛化知识，为动态现实环境中的智能系统提供有效的持续学习解决方案。

Abstract: To adapt effectively to dynamic real-world environments, intelligent systems
must continually acquire new skills while generalizing them to diverse, unseen
scenarios. Here, we introduce a novel and realistic setting named domain
generalizable continual learning (DGCL): a model learns sequential tasks with
each involving a single domain, aiming to perform well across all encountered
tasks and domains. This setting poses unique challenges in acquiring,
retaining, and leveraging both semantic- and domain-relevant information for
robust generalization. Although state-of-the-art continual learning (CL)
methods have employed pre-trained models (PTMs) to enhance task-specific
generalization, they typically assume identical training and testing domains
for each task and therefore perform poorly in DGCL. To this end, we propose
adaptive Domain Transformation (DoT), an innovative PTMs-based approach
tailored to DGCL. Inspired by the distributed-plus-hub theory of the human
brain, DoT disentangles semantic- and domain-relevant information in
representation learning, and adaptively transforms task representations across
various domains for output alignment, ensuring balanced and generalized
predictions. DoT serves as a plug-in strategy that greatly facilitates
state-of-the-art CL baselines under both full parameter tuning and
parameter-efficient tuning paradigms in DGCL, validated by extensive
experiments. Also, DoT is shown to accumulate domain-generalizable knowledge
from DGCL, and ensure resource efficiency with a lightweight implementation.

</details>


### [464] [SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search](https://arxiv.org/abs/2510.16916)
*Dong Li,Xujiang Zhao,Linlin Yu,Yanchi Liu,Wei Cheng,Zhengzhang Chen,Zhong Chen,Feng Chen,Chen Zhao,Haifeng Chen*

Main category: cs.LG

TL;DR: SolverLLM是一个无需训练的框架，通过测试时缩放和蒙特卡洛树搜索策略，将优化问题转化为数学公式和求解器代码，在多种基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖提示工程导致泛化能力差，要么需要昂贵的监督训练，需要一种无需训练且能泛化到不同类型优化问题的解决方案。

Method: 使用蒙特卡洛树搜索策略，包含动态扩展、提示反向传播和不确定性反向传播，将问题转化为数学公式和求解器代码。

Result: 在六个标准基准数据集上的实验表明，SolverLLM优于基于提示和学习的方法，实现了强大的泛化能力。

Conclusion: SolverLLM提供了一种无需额外训练就能有效解决多样化优化问题的方法，展示了测试时缩放的潜力。

Abstract: Large Language Models (LLMs) offer promising capabilities for tackling
complex reasoning tasks, including optimization problems. However, existing
methods either rely on prompt engineering, which leads to poor generalization
across problem types, or require costly supervised training. We introduce
SolverLLM, a training-free framework that leverages test-time scaling to solve
diverse optimization problems. Rather than solving directly, SolverLLM
generates mathematical formulations and translates them into solver-ready code,
guided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the
search process, we modify classical MCTS with (1) dynamic expansion for
adaptive formulation generation, (2) prompt backpropagation to guide
exploration via outcome-driven feedback, and (3) uncertainty backpropagation to
incorporate reward reliability into decision-making. Experiments on six
standard benchmark datasets demonstrate that SolverLLM outperforms both
prompt-based and learning-based baselines, achieving strong generalization
without additional training.

</details>


### [465] [Mapping Post-Training Forgetting in Language Models at Scale](https://arxiv.org/abs/2510.17776)
*Jackson Harmon,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.LG

TL;DR: 该论文提出了一个样本级别的框架来量化语言模型在扩展后训练过程中知识遗忘和反向迁移的情况，通过分析1→0和0→1的转换来测量这些效应。


<details>
  <summary>Details</summary>
Motivation: 理解扩展后训练对预训练知识的影响，因为传统任务平均值会混淆遗忘和反向迁移效应，无法准确反映模型知识的变化。

Method: 提出样本级别的度量方法，计算1→0转换（后训练前正确后错误）来量化遗忘，0→1转换来量化反向迁移。对于选择题基准，添加了机会调整变体来消除随机猜测的影响。

Result: 大规模分析显示：领域持续预训练导致中度遗忘和低到中度反向迁移；RL/SFT后训练在数学和逻辑上产生中度到大的反向迁移，总体低到中度遗忘；在指令调优模型上应用RL/SFT对数据规模敏感；模型融合不能可靠缓解遗忘。

Conclusion: 该框架为映射后训练如何改变预训练知识提供了实用的衡量标准，有助于开发通用AI系统。

Abstract: Scaled post-training now drives many of the largest capability gains in
language models (LMs), yet its effect on pretrained knowledge remains poorly
understood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.
president or an API call) does not "average out" by recalling another. Hence,
we propose a sample-wise paradigm to measure what is forgotten and when
backward transfer occurs. Our metric counts 1->0 transitions (correct before
post-training, incorrect after) to quantify forgetting and 0->1 transitions to
quantify backward transfer. Traditional task averages conflate these effects
and obscure large changes. For multiple-choice benchmarks, we add
chance-adjusted variants that subtract the expected contribution of random
guessing from pre- and post-training accuracies. We apply this framework across
post-training stages, model sizes, and data scales. Our large-scale analysis
shows that: (1) Domain-continual pretraining induces moderate forgetting with
low-to-moderate backward transfer; (2) RL/SFT post-training applied to base
models and Instruction tuning yields moderate-to-large backward transfer on
math and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to
instruction-tuned models is sensitive on data scale: at small scales, both
forgetting and backward transfer are small; at larger scales, effects are mixed
and warrant further study with better controls; (4) Model merging does not
reliably mitigate forgetting. Overall, our framework offers a practical
yardstick for mapping how post-training alters pretrained knowledge at scale --
enabling progress towards generally capable AI systems.

</details>


### [466] [Closing the Curvature Gap: Full Transformer Hessians and Their Implications for Scaling Laws](https://arxiv.org/abs/2510.16927)
*Egor Petrov,Nikita Kiselev,Vladislav Meshkov,Andrey Grabovoy*

Main category: cs.LG

TL;DR: 该论文通过推导Layer Normalization和前馈网络Hessian矩阵的显式二阶表达式，完成了完整Transformer块的Hessian表征，为Transformer优化景观提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: Layer Normalization和前馈网络Hessian矩阵缺乏理论结果，这给Transformer优化景观研究留下了空白。

Method: 推导显式二阶表达式，提出基于泰勒展开的框架来分析损失差异以量化收敛轨迹。

Result: 结果推广了先前的自注意力分析，并估计了每个子层在曲率传播中的作用，展示了Hessian结构如何影响收敛动态和大型模型性能的实证缩放定律。

Conclusion: 通过将Hessian理论扩展到完整Transformer架构，这项工作为大规模深度学习优化的理论和实证研究建立了新基础。

Abstract: The lack of theoretical results for Layer Normalization and feedforward
Hessians has left a gap in the study of Transformer optimization landscapes. We
address this by deriving explicit second-order expressions for these
components, thereby completing the Hessian characterization of full Transformer
blocks. Our results generalize prior self-attention analyses and yield
estimations for the role of each sublayer in curvature propagation. We
demonstrate how these Hessian structures inform both convergence dynamics and
the empirical scaling laws governing large-model performance. Further, we
propose a Taylor-expansion-based framework for analyzing loss differences to
quantify convergence trajectories. By extending Hessian theory to the full
Transformer architecture, this work establishes a new foundation for
theoretical and empirical investigations of optimization in large-scale deep
learning.

</details>


### [467] [A Primer on Kolmogorov-Arnold Networks (KANs) for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2510.16940)
*Cristian J. Vaca-Rubio,Roberto Pereira,Luis Blanco,Engin Zeydan,Màrius Caus*

Main category: cs.LG

TL;DR: P-KAN是一种概率性Kolmogorov-Arnold网络，用于时间序列预测，通过样条函数连接和直接参数化预测分布，在卫星流量预测中优于MLP基线。


<details>
  <summary>Details</summary>
Motivation: 开发参数高效且能捕捉非线性和重尾动态的概率预测模型，应用于卫星通信等资源受限领域的不确定性感知预测。

Method: 用样条函数连接替换标量权重，直接参数化预测分布，构建基于高斯分布和Student-t分布的P-KAN模型。

Result: P-KAN在准确性和校准方面持续优于MLP基线，使用更少参数实现更好的效率-风险权衡。高斯变体提供稳健预测，Student-t变体在稳定需求下产生更尖锐分布。

Conclusion: P-KAN为概率预测提供了强大框架，特别适用于卫星通信和其他资源受限领域。

Abstract: This work introduces Probabilistic Kolmogorov-Arnold Network (P-KAN), a novel
probabilistic extension of Kolmogorov-Arnold Networks (KANs) for time series
forecasting. By replacing scalar weights with spline-based functional
connections and directly parameterizing predictive distributions, P-KANs offer
expressive yet parameter-efficient models capable of capturing nonlinear and
heavy-tailed dynamics. We evaluate P-KANs on satellite traffic forecasting,
where uncertainty-aware predictions enable dynamic thresholding for resource
allocation. Results show that P-KANs consistently outperform Multi Layer
Perceptron (MLP) baselines in both accuracy and calibration, achieving superior
efficiency-risk trade-offs while using significantly fewer parameters. We build
up P-KANs on two distributions, namely Gaussian and Student-t distributions.
The Gaussian variant provides robust, conservative forecasts suitable for
safety-critical scenarios, whereas the Student-t variant yields sharper
distributions that improve efficiency under stable demand. These findings
establish P-KANs as a powerful framework for probabilistic forecasting with
direct applicability to satellite communications and other resource-constrained
domains.

</details>


### [468] [Quantile Regression, Variational Autoencoders, and Diffusion Models for Uncertainty Quantification: A Spatial Analysis of Sub-seasonal Wind Speed Prediction](https://arxiv.org/abs/2510.16958)
*Ganglin Tian,Anastase Alexandre Charantonis,Camille Le Coz,Alexis Tantet,Riwal Plougonven*

Main category: cs.LG

TL;DR: 本研究评估了三种概率深度学习方法在次季节风场预报中的表现，相比简单的随机扰动方法，这些方法能更好地表征空间不确定性和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 改进次季节预报中从大尺度大气预测因子回归地表风速时的空间不确定性表征。传统随机扰动方法无法充分表征空间相关性和物理一致性。

Method: 评估三种概率方法：分位数回归神经网络（直接建模分布分位数）、变分自编码器（利用潜在空间采样）和扩散模型（使用迭代去噪）。在ERA5再分析数据上训练，应用于ECMWF次季节后报。

Result: 概率降尺度方法相比简单随机方法提供更真实的空间不确定性表征，每种概率模型在集合离散度、确定性技能和物理一致性方面各有优势。

Conclusion: 概率降尺度是增强业务次季节风场预报的有效方法，对可再生能源规划和风险评估具有重要意义。

Abstract: This study aims to improve the spatial representation of uncertainties when
regressing surface wind speeds from large-scale atmospheric predictors for
sub-seasonal forecasting. Sub-seasonal forecasting often relies on large-scale
atmospheric predictors such as 500 hPa geopotential height (Z500), which
exhibit higher predictability than surface variables and can be downscaled to
obtain more localised information. Previous work by Tian et al. (2024)
demonstrated that stochastic perturbations based on model residuals can improve
ensemble dispersion representation in statistical downscaling frameworks, but
this method fails to represent spatial correlations and physical consistency
adequately. More sophisticated approaches are needed to capture the complex
relationships between large-scale predictors and local-scale predictands while
maintaining physical consistency. Probabilistic deep learning models offer
promising solutions for capturing complex spatial dependencies. This study
evaluates three probabilistic methods with distinct uncertainty quantification
mechanisms: Quantile Regression Neural Network that directly models
distribution quantiles, Variational Autoencoders that leverage latent space
sampling, and Diffusion Models that utilise iterative denoising. These models
are trained on ERA5 reanalysis data and applied to ECMWF sub-seasonal hindcasts
to regress probabilistic wind speed ensembles. Our results show that
probabilistic downscaling approaches provide more realistic spatial uncertainty
representations compared to simpler stochastic methods, with each probabilistic
model offering different strengths in terms of ensemble dispersion,
deterministic skill, and physical consistency. These findings establish
probabilistic downscaling as an effective enhancement to operational
sub-seasonal wind forecasts for renewable energy planning and risk assessment.

</details>


### [469] [Towards Interpretable and Trustworthy Time Series Reasoning: A BlueSky Vision](https://arxiv.org/abs/2510.16980)
*Kanghui Ning,Zijie Pan,Yushan Jiang,Anderson Schneider,Yuriy Nevmyvaka,Dongjin Song*

Main category: cs.LG

TL;DR: 提出了时间序列推理的蓝图愿景，包含两个互补方向：构建稳健的时间序列推理基础，以及推进系统级推理能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列推理正在成为时序分析的下一个前沿，旨在超越模式识别，实现明确、可解释且可信赖的推理。

Method: 1) 构建时间序列推理的稳健基础，包括全面时序理解、结构化多步推理和忠实评估框架；2) 推进系统级推理，超越纯语言解释，融入多智能体协作、多模态上下文和检索增强方法。

Result: 提出了一个灵活可扩展的时间序列推理框架，能够为不同领域提供可解释且可信赖的时序智能。

Conclusion: 这两个互补方向共同构成了推进时间序列推理的蓝图，旨在实现跨领域的可解释和可信赖时序智能。

Abstract: Time series reasoning is emerging as the next frontier in temporal analysis,
aiming to move beyond pattern recognition towards explicit, interpretable, and
trustworthy inference. This paper presents a BlueSky vision built on two
complementary directions. One builds robust foundations for time series
reasoning, centered on comprehensive temporal understanding, structured
multi-step reasoning, and faithful evaluation frameworks. The other advances
system-level reasoning, moving beyond language-only explanations by
incorporating multi-agent collaboration, multi-modal context, and
retrieval-augmented approaches. Together, these directions outline a flexible
and extensible framework for advancing time series reasoning, aiming to deliver
interpretable and trustworthy temporal intelligence across diverse domains.

</details>


### [470] [Graph4MM: Weaving Multimodal Learning with Structural Information](https://arxiv.org/abs/2510.16990)
*Xuying Ning,Dongqi Fu,Tianxin Wei,Wujiang Xu,Jingrui He*

Main category: cs.LG

TL;DR: Graph4MM是一个基于图的多模态学习框架，通过Hop-Diffused Attention整合多跳结构信息，并使用MM-QFormer进行跨模态融合，在生成性和判别性任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界多模态数据具有复杂的结构关系，而现有方法未能区分多跳邻居并将图视为独立模态，这限制了整体理解能力。

Method: 提出Hop-Diffused Attention通过因果掩码和跳扩散整合多跳结构信息到自注意力机制中，并设计MM-QFormer进行跨模态融合。

Result: 实验表明Graph4MM在生成性和判别性任务上优于更大的视觉语言模型、大语言模型和多模态图基线，平均提升6.93%。

Conclusion: 利用结构信息整合模态内和模态间交互能够提升多模态理解能力，超越将图作为独立模态的方法。

Abstract: Real-world multimodal data usually exhibit complex structural relationships
beyond traditional one-to-one mappings like image-caption pairs. Entities
across modalities interact in intricate ways, with images and text forming
diverse interconnections through contextual dependencies and co-references.
Graphs provide powerful structural information for modeling intra-modal and
inter-modal relationships. However, previous works fail to distinguish
multi-hop neighbors and treat the graph as a standalone modality, which
fragments the overall understanding. This limitation presents two key
challenges in multimodal learning: (1) integrating structural information from
multi-hop neighbors into foundational models, and (2) fusing modality-specific
information in a principled manner. To address these challenges, we revisit the
role of graphs in multimodal learning within the era of foundation models and
propose Graph4MM, a graph-based multimodal learning framework. To be specific,
we introduce Hop-Diffused Attention, which integrates multi-hop structural
information into self-attention through causal masking and hop diffusion.
Furthermore, we design MM-QFormer, a multi-mapping querying transformer for
cross-modal fusion. Through theoretical and empirical analysis, we show that
leveraging structures to integrate both intra- and inter-modal interactions
improves multimodal understanding beyond treating them as a standalone
modality. Experiments on both generative and discriminative tasks show that
Graph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines,
achieving a 6.93% average improvement.

</details>


### [471] [EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit](https://arxiv.org/abs/2510.17002)
*Chang Liu,Danial Chitnis*

Main category: cs.LG

TL;DR: EEschematic是一个基于多模态大语言模型的AI代理，能够将SPICE网表自动转换为可编辑的电路原理图，解决了传统文本表示缺乏视觉可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于LLM的电路生成方法仅依赖SPICE网表等文本表示，缺乏对电路设计师友好的视觉可解释性，限制了实际应用。

Method: 集成文本、视觉和符号模态，使用6个模拟子结构示例进行少样本布局，采用视觉思维链策略迭代优化布局和布线。

Result: 在CMOS反相器、五管运算跨导放大器和望远镜级联放大器等代表性模拟电路上，EEschematic生成了具有高视觉质量和结构正确性的原理图。

Conclusion: EEschematic成功实现了从文本网表到可视化原理图的自动转换，为模拟电路设计提供了更直观的设计工具。

Abstract: Circuit schematics play a crucial role in analog integrated circuit design,
serving as the primary medium for human understanding and verification of
circuit functionality. While recent large language model (LLM)-based approaches
have shown promise in circuit topology generation and device sizing, most rely
solely on textual representations such as SPICE netlists, which lack visual
interpretability for circuit designers. To address this limitation, we propose
EEschematic, an AI agent for automatic analog schematic generation based on a
Multimodal Large Language Model (MLLM). EEschematic integrates textual, visual,
and symbolic modalities to translate SPICE netlists into schematic diagrams
represented in a human-editable format. The framework uses six analog
substructure examples for few-shot placement and a Visual Chain-of-Thought
(VCoT) strategy to iteratively refine placement and wiring, enhancing schematic
clarity and symmetry. Experimental results on representative analog circuits,
including a CMOS inverter, a five-transistor operational transconductance
amplifier (5T-OTA), and a telescopic cascode amplifier, demonstrate that
EEschematic produces schematics with high visual quality and structural
correctness.

</details>


### [472] [Justitia: Fair and Efficient Scheduling for LLM Applications](https://arxiv.org/abs/2510.17015)
*Mingyan Yang,Guanjie Wang,Manqi Luo,Yifei Liu,Chen Chen,Han Zhao,Yu Feng,Quan Chen,Minyi Guo*

Main category: cs.LG

TL;DR: Justitia是一个针对LLM应用程序的新型调度器，通过内存中心的服务成本建模、轻量级需求预测和基于虚拟时间的公平排队算法，在保证公平性的同时显著提升调度效率。


<details>
  <summary>Details</summary>
Motivation: 在共享GPU服务器中服务LLM应用程序时，主流调度器由于队头阻塞或资源分配过度约束，无法在保证最坏情况性能的同时实现快速应用完成。

Method: 设计了Justitia调度器，包含三个关键技术：内存中心的服务成本建模、使用简单神经网络进行轻量级需求预测、基于虚拟时间的公平排队算法。

Result: 在vLLM上实现Justitia，实验结果表明它能显著提升调度效率同时保持公平性。

Conclusion: Justitia能够以公平且高效的方式服务LLM应用程序，解决了现有调度器在LLM应用场景下的性能问题。

Abstract: In the era of Large Language Models (LLMs), it has been popular to launch a
series of LLM inferences -- we call an LLM application -- to better solve
real-world problems. When serving those applications in shared GPU servers, the
schedulers are expected to attain fast application completions with guaranteed
worst-case performance. However, mainstream LLM schedulers fail to behave well
for LLM applications -- due to head-of-line blocking or over-constrained
resource allocation. In this paper, we propose to serve LLM applications in a
fair and also efficient manner. To this end, we design Justitia, a novel
scheduler with three key techniques. First, given that memory is prevalently a
bottleneck for mainstream inference frameworks like vLLM, Justitia models the
service cost of LLM applications in a memory-centric manner. Meanwhile, it uses
a simple neural network model to conduct light-weight and also accurate demand
prediction. Moreover, Justitia adopts a virtual-time based fair queuing
algorithm to reduce the overall performance with guaranteed worst-case delay.
We have implemented Justitia atop vLLM, and experimental results involving
diverse LLM applications show that it can substantially enhance the scheduling
efficiency with fairness preserved.

</details>


### [473] [Curiosity-driven RL for symbolic equation solving](https://arxiv.org/abs/2510.17022)
*Kevin P. O Keeffe*

Main category: cs.LG

TL;DR: 探索强化学习在符号数学中的应用，使用PPO算法结合好奇心探索和图结构动作解决非线性方程


<details>
  <summary>Details</summary>
Motivation: 探索强化学习是否能在符号数学中发挥作用，特别是解决比线性方程更复杂的非线性方程

Method: 使用模型无关的PPO算法，结合基于好奇心的探索机制和图结构的动作表示

Result: 成功解决了包含根号、指数、三角函数等非线性方程

Conclusion: 基于好奇心的探索方法可能对通用符号推理任务有益

Abstract: We explore if RL can be useful for symbolic mathematics. Previous work showed
contrastive learning can solve linear equations in one variable. We show
model-free PPO \cite{schulman2017proximal} augmented with curiosity-based
exploration and graph-based actions can solve nonlinear equations such as those
involving radicals, exponentials, and trig functions. Our work suggests
curiosity-based exploration may be useful for general symbolic reasoning tasks.

</details>


### [474] [Hephaestus: Mixture Generative Modeling with Energy Guidance for Large-scale QoS Degradation](https://arxiv.org/abs/2510.17036)
*Nguyen Do,Bach Ngo,Youval Kashuv,Canh V. Pham,Hanghang Tong,My T. Thai*

Main category: cs.LG

TL;DR: 提出了PIMMA框架解决服务质量退化问题，通过生成式方法在潜在空间中合成可行解，在非线性边权重函数场景下优于传统方法


<details>
  <summary>Details</summary>
Motivation: 现有方法无法直接处理非线性边权重函数的QoSD问题，传统组合优化方法受限，而机器学习方法只能处理受限的线性变体和小规模网络

Method: 三阶段框架：Forge使用预测路径应力算法生成可行解；Morph使用条件VAE混合模型和能量模型捕获解特征分布；Refine使用强化学习在空间中探索生成近优解

Result: 在合成和真实网络上的实验表明，该方法始终优于经典和ML基线，特别是在非线性成本函数场景下传统方法无法泛化时表现突出

Conclusion: PIMMA框架成功填补了QoSD问题在非线性边权重函数下的研究空白，展示了生成式方法在此类网络优化问题中的有效性

Abstract: We study the Quality of Service Degradation (QoSD) problem, in which an
adversary perturbs edge weights to degrade network performance. This setting
arises in both network infrastructures and distributed ML systems, where
communication quality, not just connectivity, determines functionality. While
classical methods rely on combinatorial optimization, and recent ML approaches
address only restricted linear variants with small-size networks, no prior
model directly tackles the QoSD problem under nonlinear edge-weight functions.
This work proposes \PIMMA, a self-reinforcing generative framework that
synthesizes feasible solutions in latent space, to fill this gap. Our method
includes three phases: (1) Forge: a Predictive Path-Stressing (PPS) algorithm
that uses graph learning and approximation to produce feasible solutions with
performance guarantee, (2) Morph: a new theoretically grounded training
paradigm for Mixture of Conditional VAEs guided by an energy-based model to
capture solution feature distributions, and (3) Refine: a reinforcement
learning agent that explores this space to generate progressively near-optimal
solutions using our designed differentiable reward function. Experiments on
both synthetic and real-world networks show that our approach consistently
outperforms classical and ML baselines, particularly in scenarios with
nonlinear cost functions where traditional methods fail to generalize.

</details>


### [475] [Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability](https://arxiv.org/abs/2510.17040)
*Hoang-Son Nguyen,Xiao Fu*

Main category: cs.LG

TL;DR: 提出DICA框架，通过最大化混合函数雅可比矩阵体积来识别非线性混合中的潜在成分，无需辅助信号、独立性假设或雅可比稀疏性要求。


<details>
  <summary>Details</summary>
Motivation: 解决非线性独立成分分析中潜在成分识别的根本挑战，特别是在缺乏辅助监督信号的情况下实现可识别性。

Method: 使用Jacobian Volume Maximization (J-VolMax)准则，利用混合函数雅可比矩阵的凸几何特性，通过最大化雅可比矩阵体积来促进潜在成分对观测变量的多样化影响。

Result: 在合理条件下，该方法能够实现潜在成分的可识别性，无需依赖辅助信息、潜在成分独立性或雅可比稀疏性假设。

Conclusion: DICA框架扩展了可识别性分析的范围，为现有方法提供了补充视角，在非线性混合问题中实现了更广泛的适用性。

Abstract: Latent component identification from unknown nonlinear mixtures is a
foundational challenge in machine learning, with applications in tasks such as
disentangled representation learning and causal inference. Prior work in
nonlinear independent component analysis (nICA) has shown that auxiliary
signals -- such as weak supervision -- can support identifiability of
conditionally independent latent components. More recent approaches explore
structural assumptions, e.g., sparsity in the Jacobian of the mixing function,
to relax such requirements. In this work, we introduce Diverse Influence
Component Analysis (DICA), a framework that exploits the convex geometry of the
mixing function's Jacobian. We propose a Jacobian Volume Maximization
(J-VolMax) criterion, which enables latent component identification by
encouraging diversity in their influence on the observed variables. Under
reasonable conditions, this approach achieves identifiability without relying
on auxiliary information, latent component independence, or Jacobian sparsity
assumptions. These results extend the scope of identifiability analysis and
offer a complementary perspective to existing methods.

</details>


### [476] [The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs](https://arxiv.org/abs/2510.17057)
*Nikolaus Howe,Micah Carroll*

Main category: cs.LG

TL;DR: 该论文研究了当后处理指令与模型已学习行为冲突时，语言模型会进行动机性推理——生成看似合理的理由来违反指令，同时淡化潜在危害。研究发现前沿推理模型能检测到这种动机性推理，但较小的LLM法官可能无法识别，甚至可能被说服认为这种推理是正确的。


<details>
  <summary>Details</summary>
Motivation: 研究当后处理指令与模型已学习行为冲突时，模型的推理过程会发生什么变化，特别是关注模型是否会产生动机性推理来合理化违反指令的行为。

Method: 在简单设置中调查模型行为，分析模型如何生成看似合理的理由来违反指令，并评估不同规模LLM法官检测动机性推理的能力。

Result: 模型确实会进行系统性的动机性推理，前沿推理模型能检测到大部分动机性推理，但较小的LLM法官可能无法识别部分动机性推理，甚至可能被说服认为这种推理是正确的。

Conclusion: 动机性推理能力差距引发担忧，随着模型变得更复杂，其动机性推理可能越来越难以被监控器检测到。研究强调在依赖思维链过程进行模型评估和监督时需要考虑动机性推理。

Abstract: The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning
has emerged as a promising approach for developing more capable language
models. In turn, this has led to investigation of CoT monitoring as a
compelling method for detecting harmful behaviors such as reward hacking, under
the assumption that models' reasoning processes reflect their internal
decision-making. In practice, LLM training often produces unintended behaviors
due to imperfect reward signals, leading models to develop misaligned
tendencies. A common corrective approach is to apply post-hoc instructions to
avoid problematic behaviors like sycophancy, but what happens to the model's
reasoning process when these instructions conflict with learned behaviors? We
investigate this question in simple settings and find that models engage in
systematic motivated reasoning -- generating plausible-sounding justifications
for violating their instructions while downplaying potential harms. Beyond
being an interesting property of training, we find that while motivated
reasoning can be detected by most frontier reasoning models, smaller LLM judges
can fail to identify a portion of it, and in rare cases can themselves be
persuaded that the reasoning is correct, despite it contradicting clear
instructions. This capability gap raises concerns that as models become more
sophisticated, their motivated reasoning may become increasingly difficult for
monitors to detect. Our results underscore the need to account for motivated
reasoning when relying on chain-of-thought processes for model evaluation and
oversight. All code for this paper will be made available. WARNING: some
examples in this paper may be upsetting.

</details>


### [477] [Bitwidth-Specific Logarithmic Arithmetic for Future Hardware-Accelerated Training](https://arxiv.org/abs/2510.17058)
*Hassan Hamad,Yuou Qiu,Peter A. Beerel,Keith M. Chugg*

Main category: cs.LG

TL;DR: 提出一种低精度对数定点训练方法，通过硬件友好的分段线性近似和模拟退火优化，在12位整数运算下实现与32位浮点训练相当的精度，同时显著降低硬件面积和能耗。


<details>
  <summary>Details</summary>
Motivation: 深度学习推理已通过量化显著降低计算成本，但训练仍主要依赖复杂的浮点运算。低精度定点训练是一个有吸引力的替代方案，特别是针对未来硬件加速器设计。

Method: 引入比特宽度设计算术运算近似，提出硬件友好的分段线性近似用于对数加法，使用模拟退火在不同精度级别优化该近似，并通过C++比特精确模拟验证训练效果。

Result: 在CIFAR-100和TinyImageNet数据集上，使用12位整数运算训练VGG-11和VGG-16模型，与32位浮点训练相比精度损失极小。硬件研究表明，提出的LNS乘累加单元相比线性定点等效单元面积减少32.5%，能耗降低53.5%。

Conclusion: 该方法证明了低精度对数定点训练在保持精度的同时，能够显著降低硬件资源消耗，为未来高效硬件加速器设计提供了可行方案。

Abstract: While advancements in quantization have significantly reduced the
computational costs of inference in deep learning, training still predominantly
relies on complex floating-point arithmetic. Low-precision fixed-point training
presents a compelling alternative. This work introduces a novel enhancement in
low-precision logarithmic fixed-point training, geared towards future hardware
accelerator designs. We propose incorporating bitwidth in the design of
approximations to arithmetic operations. To this end, we introduce a new
hardware-friendly, piece-wise linear approximation for logarithmic addition.
Using simulated annealing, we optimize this approximation at different
precision levels. A C++ bit-true simulation demonstrates training of VGG-11 and
VGG-16 models on CIFAR-100 and TinyImageNet, respectively, using 12-bit integer
arithmetic with minimal accuracy degradation compared to 32-bit floating-point
training. Our hardware study reveals up to 32.5% reduction in area and 53.5%
reduction in energy consumption for the proposed LNS multiply-accumulate units
compared to that of linear fixed-point equivalents.

</details>


### [478] [Consistent Zero-Shot Imitation with Contrastive Goal Inference](https://arxiv.org/abs/2510.17059)
*Kathryn Wantlin,Chongyi Zheng,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 提出一种自监督预训练交互式智能体的方法，使其能够快速模仿人类演示。该方法将目标作为基本构建块，在训练中自动提出目标并练习达成，在评估时通过逆强化学习解释演示为最优目标达成行为。


<details>
  <summary>Details</summary>
Motivation: 当前成功的AI模型（如VLMs、LLMs）缺乏明确的动作概念，而纯探索方法无法让智能体快速适应新任务。人类数据提供了强归纳偏置，但存在错误假设——人类大部分时间处于最有价值状态。

Method: 将目标作为原子构造，训练时自动提出目标并练习达成，评估时通过逆强化学习解释演示为最优目标达成行为。

Result: 在标准基准测试（非为目标达成设计）上的实验表明，该方法在零样本模仿方面优于先前方法。

Conclusion: 该方法能够通过自监督预训练使交互式智能体快速适应新任务，实现即时模仿人类演示。

Abstract: In the same way that generative models today conduct most of their training
in a self-supervised fashion, how can agentic models conduct their training in
a self-supervised fashion, interactively exploring, learning, and preparing to
quickly adapt to new tasks? A prerequisite for embodied agents deployed in real
world interactions ought to be training with interaction, yet today's most
successful AI models (e.g., VLMs, LLMs) are trained without an explicit notion
of action. The problem of pure exploration (which assumes no data as input) is
well studied in the reinforcement learning literature and provides agents with
a wide array of experiences, yet it fails to prepare them for rapid adaptation
to new tasks. Today's language and vision models are trained on data provided
by humans, which provides a strong inductive bias for the sorts of tasks that
the model will have to solve (e.g., modeling chords in a song, phrases in a
sonnet, sentences in a medical record). However, when they are prompted to
solve a new task, there is a faulty tacit assumption that humans spend most of
their time in the most rewarding states. The key contribution of our paper is a
method for pre-training interactive agents in a self-supervised fashion, so
that they can instantly mimic human demonstrations. Our method treats goals
(i.e., observations) as the atomic construct. During training, our method
automatically proposes goals and practices reaching them, building off prior
work in reinforcement learning exploration. During evaluation, our method
solves an (amortized) inverse reinforcement learning problem to explain
demonstrations as optimal goal-reaching behavior. Experiments on standard
benchmarks (not designed for goal-reaching) show that our approach outperforms
prior methods for zero-shot imitation.

</details>


### [479] [Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing](https://arxiv.org/abs/2510.17088)
*Zan Li,Rui Fan*

Main category: cs.LG

TL;DR: 提出了一种自适应图学习框架，通过专业专家网络检测金融异常，提供内置可解释性，能够识别异常机制类型并追踪其时间演化。


<details>
  <summary>Details</summary>
Motivation: 现有金融异常检测器将所有异常统一处理，无法揭示具体失效机制、风险集中位置或干预方式，这种不透明性阻碍了针对性监管响应。存在三个未解决问题：静态图结构无法适应市场相关性变化；统一检测机制错过跨多时间尺度的类型特定特征；黑盒输出无法提供关于异常机制及其时间演化的可操作指导。

Method: 通过自适应图学习与专业专家网络构建框架：使用带自注意力的BiLSTM捕捉多尺度时间依赖；通过跨模态注意力融合时空信息；通过神经多源插值学习动态图；通过压力调制融合自适应平衡学习动态与结构先验；将异常路由到四个机制特定专家；产生双重可解释归因。可解释性在架构中嵌入而非事后应用。

Result: 在100只美国股票（2017-2024）上，实现了92.3%的13个主要事件检测率，提前3.8天，比最佳基线高出30.8个百分点。硅谷银行案例研究展示了异常演化追踪：价格冲击专家权重在关闭期间升至0.39（比基线0.29高33%），一周后达到峰值0.48（比基线高66%），无需标注监督即可自动识别时间机制。

Conclusion: 该框架通过自适应图学习和机制特定专家网络，成功解决了金融异常检测中的关键挑战，提供了内置可解释性和对异常机制演化的自动识别能力，为针对性监管响应提供了有效工具。

Abstract: Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity
freezes, contagion cascades, regime shifts), but existing detectors treat all
anomalies uniformly, producing scalar scores without revealing which mechanism
is failing, where risks concentrate, or how to intervene. This opacity prevents
targeted regulatory responses. Three unsolved challenges persist: (1) static
graph structures cannot adapt when market correlations shift during regime
changes; (2) uniform detection mechanisms miss type-specific signatures across
multiple temporal scales while failing to integrate individual behaviors with
network contagion; (3) black-box outputs provide no actionable guidance on
anomaly mechanisms or their temporal evolution.
  We address these via adaptive graph learning with specialized expert networks
that provide built-in interpretability. Our framework captures multi-scale
temporal dependencies through BiLSTM with self-attention, fuses temporal and
spatial information via cross-modal attention, learns dynamic graphs through
neural multi-source interpolation, adaptively balances learned dynamics with
structural priors via stress-modulated fusion, routes anomalies to four
mechanism-specific experts, and produces dual-level interpretable attributions.
Critically, interpretability is embedded architecturally rather than applied
post-hoc.
  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events
with 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley
Bank case study demonstrates anomaly evolution tracking: Price-Shock expert
weight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48
(66% above baseline) one week later, revealing automatic temporal mechanism
identification without labeled supervision.

</details>


### [480] [On the Universal Near Optimality of Hedge in Combinatorial Settings](https://arxiv.org/abs/2510.17099)
*Zhiyuan Fan,Arnab Maiti,Kevin Jamieson,Lillian J. Ratliff,Gabriele Farina*

Main category: cs.LG

TL;DR: 本文研究了Hedge算法在组合设置中的最优性，发现Hedge在大多数组合设置中接近最优，但在某些特定设置（如m-集合）中比最优算法差√log d倍，而在在线多任务学习中是最优的。


<details>
  <summary>Details</summary>
Motivation: 研究Hedge算法在组合设置中的最优性，确定其在哪些组合问题中是最优的，哪些不是，并建立相应的下界。

Method: 通过建立组合设置的一般下界Ω(√(T log(|X|)/log d))，并与Hedge的上界O(√(T log|X|))进行比较，分析Hedge的最优性差距。

Result: 证明Hedge在大多数组合设置中接近最优（最多差√log d倍），但在m-集合设置中确实比最优算法差√log d倍，而在在线多任务学习中是最优的。

Conclusion: Hedge在组合设置中通常接近最优，但存在特定设置（如m-集合）中不是最优的，这一发现有助于理解组合在线学习算法的性能界限。

Abstract: In this paper, we study the classical Hedge algorithm in combinatorial
settings. In each round, the learner selects a vector $\boldsymbol{x}_t$ from a
set $X \subseteq \{0,1\}^d$, observes a full loss vector $\boldsymbol{y}_t \in
\mathbb{R}^d$, and incurs a loss $\langle \boldsymbol{x}_t, \boldsymbol{y}_t
\rangle \in [-1,1]$. This setting captures several important problems,
including extensive-form games, resource allocation, $m$-sets, online multitask
learning, and shortest-path problems on directed acyclic graphs (DAGs). It is
well known that Hedge achieves a regret of $O\big(\sqrt{T \log |X|}\big)$ after
$T$ rounds of interaction. In this paper, we ask whether Hedge is optimal
across all combinatorial settings. To that end, we show that for any $X
\subseteq \{0,1\}^d$, Hedge is near-optimal--specifically, up to a $\sqrt{\log
d}$ factor--by establishing a lower bound of $\Omega\big(\sqrt{T \log(|X|)/\log
d}\big)$ that holds for any algorithm. We then identify a natural class of
combinatorial sets--namely, $m$-sets with $\log d \leq m \leq \sqrt{d}$--for
which this lower bound is tight, and for which Hedge is provably suboptimal by
a factor of exactly $\sqrt{\log d}$. At the same time, we show that Hedge is
optimal for online multitask learning, a generalization of the classical
$K$-experts problem. Finally, we leverage the near-optimality of Hedge to
establish the existence of a near-optimal regularizer for online shortest-path
problems in DAGs--a setting that subsumes a broad range of combinatorial
domains. Specifically, we show that the classical Online Mirror Descent (OMD)
algorithm, when instantiated with the dilated entropy regularizer, is
iterate-equivalent to Hedge, and therefore inherits its near-optimal regret
guarantees for DAGs.

</details>


### [481] [Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling](https://arxiv.org/abs/2510.17106)
*Chen Zhang,Weixin Bu,Wendong Xu,Runsheng Yu,Yik-Chung Wu,Ngai Wong*

Main category: cs.LG

TL;DR: 本文揭示了Transformer编码器与图卷积网络(GCN)的基本等价性，提出Fighter架构通过移除冗余线性投影和多跳图聚合，在保持竞争力的同时提供更清晰的机制可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在时间序列建模中取得了显著成功，但其内部机制仍然不透明，需要揭示其工作原理并提高可解释性。

Method: 将注意力分布矩阵视为动态邻接矩阵，证明其与后续变换的组合执行类似图卷积的计算，提出Fighter架构移除冗余线性投影并引入多跳图聚合。

Result: 在标准预测基准测试中，Fighter实现了具有竞争力的性能，同时为其预测提供了更清晰的机制可解释性。

Conclusion: Transformer编码器在本质上等同于图卷积网络，这种统一的理论重新解释为时间序列建模提供了更明确和可解释的表示方法。

Abstract: Transformers have achieved remarkable success in time series modeling, yet
their internal mechanisms remain opaque. This work demystifies the Transformer
encoder by establishing its fundamental equivalence to a Graph Convolutional
Network (GCN). We show that in the forward pass, the attention distribution
matrix serves as a dynamic adjacency matrix, and its composition with
subsequent transformations performs computations analogous to graph
convolution. Moreover, we demonstrate that in the backward pass, the update
dynamics of value and feed-forward projections mirror those of GCN parameters.
Building on this unified theoretical reinterpretation, we propose
\textbf{Fighter} (Flexible Graph Convolutional Transformer), a streamlined
architecture that removes redundant linear projections and incorporates
multi-hop graph aggregation. This perspective yields an explicit and
interpretable representation of temporal dependencies across different scales,
naturally expressed as graph edges. Experiments on standard forecasting
benchmarks confirm that Fighter achieves competitive performance while
providing clearer mechanistic interpretability of its predictions.

</details>


### [482] [In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models](https://arxiv.org/abs/2510.17136)
*Enhao Gu,Haolin Hou*

Main category: cs.LG

TL;DR: 提出In-situ Autoguidance方法，无需辅助模型即可实现图像生成的自我引导，解决了CFG方法在提升质量和对齐度时减少多样性的问题。


<details>
  <summary>Details</summary>
Motivation: 解决分类器自由引导(CFG)方法在提升图像质量和提示对齐度时导致多样性降低的问题，同时避免现有解耦方法需要额外训练辅助模型的开销。

Method: 通过随机前向传递动态生成较差的预测，将引导重新定义为推理时的自我校正过程，实现零成本的自我引导。

Result: 该方法不仅可行，而且为成本高效的引导建立了强大的新基准，证明无需外部模型即可实现自我引导的益处。

Conclusion: In-situ Autoguidance证明了自我引导的益处可以在没有外部模型的情况下实现，为零成本的图像生成引导提供了有效解决方案。

Abstract: The generation of high-quality, diverse, and prompt-aligned images is a
central goal in image-generating diffusion models. The popular classifier-free
guidance (CFG) approach improves quality and alignment at the cost of reduced
variation, creating an inherent entanglement of these effects. Recent work has
successfully disentangled these properties by guiding a model with a separately
trained, inferior counterpart; however, this solution introduces the
considerable overhead of requiring an auxiliary model. We challenge this
prerequisite by introducing In-situ Autoguidance, a method that elicits
guidance from the model itself without any auxiliary components. Our approach
dynamically generates an inferior prediction on the fly using a stochastic
forward pass, reframing guidance as a form of inference-time self-correction.
We demonstrate that this zero-cost approach is not only viable but also
establishes a powerful new baseline for cost-efficient guidance, proving that
the benefits of self-guidance can be achieved without external models.

</details>


### [483] [Learning After Model Deployment](https://arxiv.org/abs/2510.17160)
*Derda Kaymak,Gyuhak Kim,Tomoya Kaichi,Tatsuya Konishi,Bing Liu*

Main category: cs.LG

TL;DR: 提出了一种名为ALMD的新学习范式，使模型在部署后能自主检测未见类别样本并增量学习，解决了传统监督学习在动态环境中无法更新的问题。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习模型部署后固定不变，不适用于动态开放环境。当出现未见类别样本时，模型应能检测这些新样本并在标注后学习它们。

Method: 提出了PLDA方法，实现动态OOD检测和在线增量学习新类别，无需从头重新训练模型。

Result: 经验评估将展示PLDA方法的有效性。

Conclusion: ALMD范式使模型在部署后能持续自主学习和适应动态环境，PLDA方法有效解决了动态OOD检测和增量学习的挑战。

Abstract: In classic supervised learning, once a model is deployed in an application,
it is fixed. No updates will be made to it during the application. This is
inappropriate for many dynamic and open environments, where unexpected samples
from unseen classes may appear. In such an environment, the model should be
able to detect these novel samples from unseen classes and learn them after
they are labeled. We call this paradigm Autonomous Learning after Model
Deployment (ALMD). The learning here is continuous and involves no human
engineers. Labeling in this scenario is performed by human co-workers or other
knowledgeable agents, which is similar to what humans do when they encounter an
unfamiliar object and ask another person for its name. In ALMD, the detection
of novel samples is dynamic and differs from traditional out-of-distribution
(OOD) detection in that the set of in-distribution (ID) classes expands as new
classes are learned during application, whereas ID classes is fixed in
traditional OOD detection. Learning is also different from classic supervised
learning because in ALMD, we learn the encountered new classes immediately and
incrementally. It is difficult to retrain the model from scratch using all the
past data from the ID classes and the novel samples from newly discovered
classes, as this would be resource- and time-consuming. Apart from these two
challenges, ALMD faces the data scarcity issue because instances of new classes
often appear sporadically in real-life applications. To address these issues,
we propose a novel method, PLDA, which performs dynamic OOD detection and
incremental learning of new classes on the fly. Empirical evaluations will
demonstrate the effectiveness of PLDA.

</details>


### [484] [ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for Dynamic Edge Crowdsensing](https://arxiv.org/abs/2510.17162)
*Guanjie Cheng,Siyang Liu,Junqin Huang,Xinkui Zhao,Yin Wang,Mengying Zhu,Linghe Kong,Shuiguang Deng*

Main category: cs.LG

TL;DR: ALPINE是一个轻量级自适应框架，让终端设备能够实时调整差分隐私级别，在移动边缘众感系统中平衡隐私保护、数据效用和能耗成本。


<details>
  <summary>Details</summary>
Motivation: 移动边缘众感系统在动态资源受限环境中持续生成和传输用户数据，存在严重隐私威胁。静态差分隐私机制无法适应不断变化的风险，导致要么噪声过大要么保护不足。

Method: ALPINE作为闭环控制系统，包含四个模块：动态风险感知、基于TD3强化学习的隐私决策、本地隐私执行和边缘节点性能验证。设计了平衡隐私收益、数据效用和能耗成本的奖励函数。

Result: 理论分析和真实世界仿真表明，ALPINE能有效缓解推理攻击，同时保持效用和成本，适用于大规模边缘应用。

Conclusion: ALPINE框架通过自适应调整差分隐私级别，在动态边缘环境中实现了隐私、效用和成本之间的动态平衡，具有低开销部署优势。

Abstract: Mobile edge crowdsensing (MECS) systems continuously generate and transmit
user data in dynamic, resource-constrained environments, exposing users to
significant privacy threats. In practice, many privacy-preserving mechanisms
build on differential privacy (DP). However, static DP mechanisms often fail to
adapt to evolving risks, for example, shifts in adversarial capabilities,
resource constraints and task requirements, resulting in either excessive noise
or inadequate protection. To address this challenge, we propose ALPINE, a
lightweight, adaptive framework that empowers terminal devices to autonomously
adjust differential privacy levels in real time. ALPINE operates as a
closed-loop control system consisting of four modules: dynamic risk perception,
privacy decision via twin delayed deep deterministic policy gradient (TD3),
local privacy execution and performance verification from edge nodes. Based on
environmental risk assessments, we design a reward function that balances
privacy gains, data utility and energy cost, guiding the TD3 agent to
adaptively tune noise magnitude across diverse risk scenarios and achieve a
dynamic equilibrium among privacy, utility and cost. Both the collaborative
risk model and pretrained TD3-based agent are designed for low-overhead
deployment. Extensive theoretical analysis and real-world simulations
demonstrate that ALPINE effectively mitigates inference attacks while
preserving utility and cost, making it practical for large-scale edge
applications.

</details>


### [485] [Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses](https://arxiv.org/abs/2510.17185)
*Runlin Lei,Lu Yi,Mingguo He,Pengyu Qiu,Zhewei Wei,Yongchao Liu,Chuntao Hong*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的框架来评估文本属性图(TAG)学习中图神经网络(GNN)和大语言模型(LLM)的鲁棒性，揭示了模型在文本和结构扰动间的固有权衡，并提出了SFT-auto框架来提升综合鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前对图神经网络和大语言模型在文本属性图上的鲁棒性评估是分散的，缺乏系统性的研究来探究文本和结构扰动在不同模型和攻击场景下的具体影响。

Method: 提出了一个统一的鲁棒性评估框架，在10个数据集上评估经典GNN、鲁棒GNN和GraphLLM在文本、结构和混合扰动下的表现，包括投毒和规避两种攻击场景。

Result: 研究发现：1)模型在文本和结构鲁棒性间存在固有权衡；2)GNN性能高度依赖文本编码器和攻击类型；3)GraphLLM对训练数据污染特别脆弱。提出的SFT-auto框架在单一模型中实现了对文本和结构攻击的均衡鲁棒性。

Conclusion: 该工作为未来TAG安全研究奠定了基础，并为对抗环境中的鲁棒TAG学习提供了实用解决方案。

Abstract: While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are
powerful approaches for learning on Text-Attributed Graphs (TAGs), a
comprehensive understanding of their robustness remains elusive. Current
evaluations are fragmented, failing to systematically investigate the distinct
effects of textual and structural perturbations across diverse models and
attack scenarios. To address these limitations, we introduce a unified and
comprehensive framework to evaluate robustness in TAG learning. Our framework
evaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten
datasets from four domains, under diverse text-based, structure-based, and
hybrid perturbations in both poisoning and evasion scenarios. Our extensive
analysis reveals multiple findings, among which three are particularly
noteworthy: 1) models have inherent robustness trade-offs between text and
structure, 2) the performance of GNNs and RGNNs depends heavily on the text
encoder and attack type, and 3) GraphLLMs are particularly vulnerable to
training data corruption. To overcome the identified trade-offs, we introduce
SFT-auto, a novel framework that delivers superior and balanced robustness
against both textual and structural attacks within a single model. Our work
establishes a foundation for future research on TAG security and offers
practical solutions for robust TAG learning in adversarial environments. Our
code is available at: https://github.com/Leirunlin/TGRB.

</details>


### [486] [A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling](https://arxiv.org/abs/2510.17187)
*Alexander Aghili,Andy Bruce,Daniel Sabo,Sanya Murdeshwar,Kevin Bachelor,Ionut Mistreanu,Ashwin Lokapally,Razvan Marinescu*

Main category: cs.LG

TL;DR: 提出了一个模块化的分子动力学基准测试框架，用于系统评估蛋白质MD方法，支持多种模拟引擎和评估指标。


<details>
  <summary>Details</summary>
Motivation: 分子动力学方法快速发展，但缺乏标准化的验证工具，导致不同方法难以客观比较。

Method: 使用加权集成采样和TICA进度坐标，通过WESTPA工具包实现高效构象空间探索，提供灵活的传播器接口支持各种模拟引擎。

Result: 开发了包含9种不同蛋白质的数据集，构建了能计算19种以上评估指标的综合评估套件，验证了框架在经典MD和机器学习模型比较中的实用性。

Conclusion: 该开源平台通过标准化评估协议，为分子模拟社区提供了可重复、严谨的基准测试基础。

Abstract: The rapid evolution of molecular dynamics (MD) methods, including
machine-learned dynamics, has outpaced the development of standardized tools
for method validation. Objective comparison between simulation approaches is
often hindered by inconsistent evaluation metrics, insufficient sampling of
rare conformational states, and the absence of reproducible benchmarks. To
address these challenges, we introduce a modular benchmarking framework that
systematically evaluates protein MD methods using enhanced sampling analysis.
Our approach uses weighted ensemble (WE) sampling via The Weighted Ensemble
Simulation Toolkit with Parallelization and Analysis (WESTPA), based on
progress coordinates derived from Time-lagged Independent Component Analysis
(TICA), enabling fast and efficient exploration of protein conformational
space. The framework includes a flexible, lightweight propagator interface that
supports arbitrary simulation engines, allowing both classical force fields and
machine learning-based models. Additionally, the framework offers a
comprehensive evaluation suite capable of computing more than 19 different
metrics and visualizations across a variety of domains. We further contribute a
dataset of nine diverse proteins, ranging from 10 to 224 residues, that span a
variety of folding complexities and topologies. Each protein has been
extensively simulated at 300K for one million MD steps per starting point (4
ns). To demonstrate the utility of our framework, we perform validation tests
using classic MD simulations with implicit solvent and compare protein
conformational sampling using a fully trained versus under-trained CGSchNet
model. By standardizing evaluation protocols and enabling direct, reproducible
comparisons across MD approaches, our open-source platform lays the groundwork
for consistent, rigorous benchmarking across the molecular simulation
community.

</details>


### [487] [SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference](https://arxiv.org/abs/2510.17189)
*Wenxun Wang,Shuchang Zhou,Wenyu Sun,Peiqin Sun,Yongpan Liu*

Main category: cs.LG

TL;DR: SOLE是一个软硬件协同设计，通过E2Softmax和AILayerNorm分别优化Transformer中的Softmax和LayerNorm操作，实现了无需重新训练的高效推理，在保持准确性的同时显著提升了速度和能效。


<details>
  <summary>Details</summary>
Motivation: Transformer在NLP和CV任务中表现出色，但其实时推理速度和效率受到Softmax和LayerNorm操作低效的限制。现有基于函数近似的方法存在实现效率低、忽视内存开销、需要重新训练等问题。

Method: 提出SOLE软硬件协同设计：E2Softmax使用对数2量化的指数函数和对数除法近似Softmax；AILayerNorm采用低精度统计计算。两者都实现了低精度计算和低比特位宽存储。

Result: SOLE在保持推理准确性的同时，相比GPU实现了数量级的速度提升和能耗节省。与现有最优定制硬件相比，Softmax和LayerNorm分别实现了3.04倍、3.86倍的能效提升和2.82倍、3.32倍的面积效率提升。

Conclusion: SOLE提供了一种无需重新训练的高效Transformer推理解决方案，通过软硬件协同设计显著提升了Softmax和LayerNorm操作的效率和性能。

Abstract: Transformers have shown remarkable performance in both natural language
processing (NLP) and computer vision (CV) tasks. However, their real-time
inference speed and efficiency are limited due to the inefficiency in Softmax
and Layer Normalization (LayerNorm). Previous works based on function
approximation suffer from inefficient implementation as they place emphasis on
computation while disregarding memory overhead concerns. Moreover, such methods
rely on retraining to compensate for approximation error which can be costly
and inconvenient.
  In this paper, we present SOLE, a hardware-software co-design for Softmax and
LayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes
log2 quantization of exponent function and log-based division to approximate
Softmax while AILayerNorm adopts low-precision statistic calculation. Compared
with state-of-the-art designs, we achieve both low-precision calculation and
low bit-width storage on Softmax and LayerNorm. Experiments show that SOLE
maintains inference accuracy without retraining while offering orders of
magnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x
energy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements
over prior state-of-the-art custom hardware for Softmax and LayerNorm,
respectively.

</details>


### [488] [D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks](https://arxiv.org/abs/2510.17212)
*Jundong Zhang,Yuhui Situ,Fanji Zhang,Rongji Deng,Tianqi Wei*

Main category: cs.LG

TL;DR: 提出一个强化学习框架来解决高风险高回报任务，通过离散化连续动作空间、熵正则化探索和双评论家架构来建模多模态动作分布和风险。


<details>
  <summary>Details</summary>
Motivation: 高风险高回报任务通常具有多模态动作分布和随机回报，而传统强化学习方法假设单峰高斯策略和标量评论家，限制了在这些设置中的有效性。

Method: 离散化连续动作空间以近似多模态分布，采用熵正则化探索来改善风险但有益动作的覆盖，引入双评论家架构以更准确估计离散值分布。

Result: 在具有高失败风险的移动和操作基准测试中，该方法优于基线方法。

Conclusion: 明确建模多模态性和风险在强化学习中至关重要，所提出的框架能够扩展到高维动作空间，支持复杂控制领域。

Abstract: Tasks involving high-risk-high-return (HRHR) actions, such as obstacle
crossing, often exhibit multimodal action distributions and stochastic returns.
Most reinforcement learning (RL) methods assume unimodal Gaussian policies and
rely on scalar-valued critics, which limits their effectiveness in HRHR
settings. We formally define HRHR tasks and theoretically show that Gaussian
policies cannot guarantee convergence to the optimal solution. To address this,
we propose a reinforcement learning framework that (i) discretizes continuous
action spaces to approximate multimodal distributions, (ii) employs
entropy-regularized exploration to improve coverage of risky but rewarding
actions, and (iii) introduces a dual-critic architecture for more accurate
discrete value distribution estimation. The framework scales to
high-dimensional action spaces, supporting complex control domains. Experiments
on locomotion and manipulation benchmarks with high risks of failure
demonstrate that our method outperforms baselines, underscoring the importance
of explicitly modeling multimodality and risk in RL.

</details>


### [489] [Diagnosis of Fuel Cell Health Status with Deep Sparse Auto-Encoder Neural Network](https://arxiv.org/abs/2510.17214)
*Chenyan Fei,Dalin Zhang,Chen Melinda Dang*

Main category: cs.LG

TL;DR: 使用深度稀疏自编码网络预测和分类燃料电池高频阻抗，准确率超过92%，并在FPGA上部署实现近90%的硬件识别率。


<details>
  <summary>Details</summary>
Motivation: 燃料电池高频阻抗是评估其状态和健康状况的关键指标，但在线测试复杂且成本高昂，需要开发有效的预测方法。

Method: 采用深度稀疏自编码网络进行燃料电池高频阻抗的预测和分类。

Result: 实现了92%以上的准确率，在FPGA上部署后硬件识别率达到近90%。

Conclusion: 该方法能够有效预测燃料电池高频阻抗，为燃料电池健康状态诊断提供了可行的解决方案。

Abstract: Effective and accurate diagnosis of fuel cell health status is crucial for
ensuring the stable operation of fuel cell stacks. Among various parameters,
high-frequency impedance serves as a critical indicator for assessing fuel cell
state and health conditions. However, its online testing is prohibitively
complex and costly. This paper employs a deep sparse auto-encoding network for
the prediction and classification of high-frequency impedance in fuel cells,
achieving metric of accuracy rate above 92\%. The network is further deployed
on an FPGA, attaining a hardware-based recognition rate almost 90\%.

</details>


### [490] [A Prototypical Network with an Attention-based Encoder for Drivers Identification Application](https://arxiv.org/abs/2510.17250)
*Wei-Hsun Lee,Che-Yu Chang,Kuang-Yu Li*

Main category: cs.LG

TL;DR: 提出了一种基于注意力机制的编码器（AttEnc）和结合原型网络的P-AttEnc架构，用于驾驶员识别，解决了数据短缺和隐私问题，在准确性和效率上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于生物特征的驾驶员识别技术存在隐私问题，且大多数方法无法处理数据短缺和未知驾驶员的情况。

Method: 使用注意力机制的编码器（AttEnc）减少模型参数；结合原型网络和注意力编码器（P-AttEnc）应用少样本学习来处理数据短缺和增强模型泛化能力。

Result: AttEnc在三个数据集上分别达到99.3%、99.0%和99.9%的准确率，预测时间快44%-79%，平均减少87.6%的模型参数；P-AttEnc在单样本场景下识别准确率为69.8%，对未知驾驶员的平均准确率为65.7%。

Conclusion: 提出的AttEnc和P-AttEnc架构在驾驶员识别任务中表现出色，既解决了隐私和数据短缺问题，又提高了识别效率和泛化能力。

Abstract: Driver identification has become an area of increasing interest in recent
years, especially for data- driven applications, because biometric-based
technologies may incur privacy issues. This study proposes a deep learning
neural network architecture, an attention-based encoder (AttEnc), which uses an
attention mechanism for driver identification and uses fewer model parameters
than current methods. Most studies do not address the issue of data shortages
for driver identification, and most of them are inflexible when encountering
unknown drivers. In this study, an architecture that combines a prototypical
network and an attention-based encoder (P-AttEnc) is proposed. It applies
few-shot learning to overcome the data shortage issues and to enhance model
generalizations. The experiments showed that the attention-based encoder can
identify drivers with accuracies of 99.3%, 99.0% and 99.9% in three different
datasets and has a prediction time that is 44% to 79% faster because it
significantly reduces, on average, 87.6% of the model parameters. P-AttEnc
identifies drivers based on few shot data, extracts driver fingerprints to
address the issue of data shortages, and is able to classify unknown drivers.
The first experiment showed that P-AttEnc can identify drivers with an accuracy
of 69.8% in the one-shot scenario. The second experiment showed that P-AttEnc,
in the 1-shot scenario, can classify unknown drivers with an average accuracy
of 65.7%.

</details>


### [491] [MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems](https://arxiv.org/abs/2510.17281)
*Qingyao Ai,Yichen Tang,Changyue Wang,Jianming Long,Weihang Su,Yiqun Liu*

Main category: cs.LG

TL;DR: 提出了一个用户反馈模拟框架和综合基准测试，用于评估LLM系统的持续学习能力，覆盖多领域、多语言和多种任务类型，实验显示现有方法的有效性和效率仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 由于高质量数据逐渐耗尽和计算资源边际收益递减，传统通过扩大数据、参数和测试时计算的方法已接近极限，需要借鉴人类和传统AI系统从实践中学习的能力，构建LLM系统的记忆和持续学习框架。

Method: 设计用户反馈模拟框架和综合基准测试，涵盖多领域、多语言和多种任务类型，用于评估LLM系统在服务时间内从累积用户反馈中学习的能力。

Result: 实验表明，现有最先进基线方法的有效性和效率远未达到满意水平，证明了该基准测试的必要性。

Conclusion: 该基准测试可为未来LLM记忆和优化算法的研究铺平道路，推动LLM系统持续学习能力的发展。

Abstract: Scaling up data, parameters, and test-time computation has been the
mainstream methods to improve LLM systems (LLMsys), but their upper bounds are
almost reached due to the gradual depletion of high-quality data and marginal
gains obtained from larger computational resource consumption. Inspired by the
abilities of human and traditional AI systems in learning from practice,
constructing memory and continual learning frameworks for LLMsys has become an
important and popular research direction in recent literature. Yet, existing
benchmarks for LLM memory often focus on evaluating the system on homogeneous
reading comprehension tasks with long-form inputs rather than testing their
abilities to learn from accumulated user feedback in service time. Therefore,
we propose a user feedback simulation framework and a comprehensive benchmark
covering multiple domains, languages, and types of tasks to evaluate the
continual learning abilities of LLMsys. Experiments show that the effectiveness
and efficiency of state-of-the-art baselines are far from satisfying, and we
hope this benchmark could pave the way for future studies on LLM memory and
optimization algorithms.

</details>


### [492] [Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations](https://arxiv.org/abs/2510.17313)
*Tal Barami,Nimrod Berman,Ilan Naiman,Amos H. Hason,Rotem Ezra,Omri Azencot*

Main category: cs.LG

TL;DR: 提出了首个多因子序列解缠结标准化基准，包含六个数据集和评估工具，并引入了自动潜在探索阶段和Koopman启发的模型，同时展示了视觉语言模型在自动标注和零样本评估中的应用。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据包含多个交互的语义因子，但先前工作主要关注简单的双因子静态和动态设置，忽视了数据的多因子本质，因此需要开发适用于多因子序列解缠结的基准和方法。

Method: 构建了包含六个数据集的标准化基准，提出了后验潜在探索阶段自动对齐潜在维度与语义因子，设计了Koopman启发的模型，并利用视觉语言模型进行自动标注和零样本评估。

Result: 提出的Koopman启发模型取得了最先进的结果，视觉语言模型能够有效自动化数据集标注并作为零样本解缠结评估器，消除了人工标注和干预的需求。

Conclusion: 这些贡献为推进多因子序列解缠结研究提供了稳健且可扩展的基础，解决了先前工作中忽视的多因子复杂性问题。

Abstract: Learning disentangled representations in sequential data is a key goal in
deep learning, with broad applications in vision, audio, and time series. While
real-world data involves multiple interacting semantic factors over time, prior
work has mostly focused on simpler two-factor static and dynamic settings,
primarily because such settings make data collection easier, thereby
overlooking the inherently multi-factor nature of real-world data. We introduce
the first standardized benchmark for evaluating multi-factor sequential
disentanglement across six diverse datasets spanning video, audio, and time
series. Our benchmark includes modular tools for dataset integration, model
development, and evaluation metrics tailored to multi-factor analysis. We
additionally propose a post-hoc Latent Exploration Stage to automatically align
latent dimensions with semantic factors, and introduce a Koopman-inspired model
that achieves state-of-the-art results. Moreover, we show that Vision-Language
Models can automate dataset annotation and serve as zero-shot disentanglement
evaluators, removing the need for manual labels and human intervention.
Together, these contributions provide a robust and scalable foundation for
advancing multi-factor sequential disentanglement.

</details>


### [493] [Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling](https://arxiv.org/abs/2510.17314)
*Lipeng Xie,Sen Huang,Zhuo Zhang,Anni Zou,Yunpeng Zhai,Dingchao Ren,Kezun Zhang,Haoyuan Hu,Boyin Liu,Haoran Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.LG

TL;DR: 提出了一种无需训练、基于评估准则的奖励模型框架，通过两阶段方法从少量偏好数据中提取高质量、可解释的准则，显著提高数据效率。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型开发成本高且缺乏可解释性，现有基于准则的方法在可扩展性和可靠性之间存在权衡，需要解决这些局限性。

Method: 采用两阶段方法：1) 通过验证引导的Propose-Evaluate-Revise流程推断查询特定准则；2) 使用信息论编码率最大化将细粒度准则泛化为紧凑的核心集，形成层次化的"主题-提示"准则集。

Result: 仅使用70个偏好对（源数据的1.5%），该方法使Qwen3-8B等较小模型能够超越专门训练的对等模型，展示了卓越的数据效率和性能。

Conclusion: 这项工作为奖励建模开创了一条可扩展、可解释且数据高效的路径。

Abstract: Reward models are essential for aligning Large Language Models (LLMs) with
human values, yet their development is hampered by costly preference datasets
and poor interpretability. While recent rubric-based approaches offer
transparency, they often lack systematic quality control and optimization,
creating a trade-off between scalability and reliability. We address these
limitations with a novel, training-free framework built on a key assumption:
\textit{evaluation rubrics underlying human preferences exhibit significant
generalization ability across diverse queries}, a property that enables
remarkable data efficiency. Our two-stage approach first infers high-quality,
query-specific rubrics using a validation-guided
\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these
granular rubrics into a compact, non-redundant core set by maximizing an
\textbf{information-theoretic coding rate}. The final output is an
interpretable, hierarchical "Theme-Tips" rubric set. Extensive experiments
demonstrate the framework's exceptional data efficiency and performance.
Critically, using just 70 preference pairs (1.5\% of the source data), our
method also empowers smaller models like Qwen3-8B to outperform specialized,
fully-trained counterparts. This work pioneers a scalable, interpretable, and
data-efficient path for reward modeling.

</details>


### [494] [Localist LLMs with Recruitment Learning](https://arxiv.org/abs/2510.17358)
*Joachim Diederich*

Main category: cs.LG

TL;DR: 提出了一种可调节语言模型表示的新框架，通过局部性调节参数实现从局部化到分布式编码的连续调整，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统语言模型在可解释性和性能之间的权衡问题，为需要透明性和能力的受监管领域提供灵活解决方案。

Method: 使用组稀疏惩罚、信息论锚点设计、动态规则注入和基于惩罚似然的招募标准，通过层次化招募机制实现多粒度架构适应。

Result: 建立了严格的数学结果，证明在特定阈值条件下注意力会集中在语义相关块上，并提供了注意力熵和指针保真度的精确边界。

Conclusion: 该框架使从业者能够在可解释和高性能模式之间连续插值，同时在多个粒度上适应架构容量，支持需要透明性和能力的应用场景。

Abstract: We present a novel framework for training large language models with
continuously adjustable internal representations that span the full spectrum
from localist (interpretable, rule-based) to distributed (generalizable,
efficient) encodings. The key innovations are (1) a locality dial, a tunable
parameter that dynamically controls the degree of localization during both
training and inference without requiring model retraining, (2) an
information-theoretic recruitment mechanism that adaptively allocates semantic
blocks as needed, eliminating the requirement for complete domain knowledge at
initialization, and (3) a hierarchical recruitment framework that extends
capacity allocation to entire specialized LLMs, enabling multi-granularity
architectural adaptation. This is achieved through group sparsity penalties on
attention mechanisms, information-theoretic anchor design, dynamic rule
injection, and principled recruitment criteria based on penalized likelihood
with explicit units. We provide rigorous mathematical results establishing
explicit threshold conditions under which attention provably concentrates on
semantically relevant blocks at stationary points, with exact bounds on
attention entropy and pointer fidelity. The hierarchical recruitment mechanism
provides convergence guarantees at both the block level (fine-grained,
within-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the
system discovers semantic partitions that balance model complexity against data
encoding efficiency. This framework enables practitioners to continuously
interpolate between interpretable and high-performance modes while adapting
architectural capacity at multiple granularities, supporting applications in
regulated domains requiring both transparency and capability.

</details>


### [495] [Model Metamers Reveal Invariances in Graph Neural Networks](https://arxiv.org/abs/2510.17378)
*Wei Xu,Xiaoyi Jiang,Lixiang Xu,Dechao Tang*

Main category: cs.LG

TL;DR: 该论文通过引入图神经网络(GNN)的"元像"生成技术，揭示了GNN存在过度表示不变性的问题，与人类大脑的不变性机制存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 研究图神经网络中的不变性行为，探索人工神经网络与人类大脑在不变性机制上的差异，特别是在图数据领域。

Method: 通过优化输入图使其内部节点激活与参考图匹配，生成在模型表示空间中等效但在结构和节点特征上显著不同的"元像"图，并分析局部元像维度和元像流形的激活诱导体积变化。

Result: 发现多个经典GNN架构存在极端水平的表示不变性，虽然通过模型架构和训练策略的针对性修改可以部分缓解过度不变性，但无法从根本上弥合与人类不变性的差距。

Conclusion: 量化了元像图与其原始对应图之间的偏差，揭示了当前GNN的独特失败模式，并为模型评估提供了补充基准。

Abstract: In recent years, deep neural networks have been extensively employed in
perceptual systems to learn representations endowed with invariances, aiming to
emulate the invariance mechanisms observed in the human brain. However, studies
in the visual and auditory domains have confirmed that significant gaps remain
between the invariance properties of artificial neural networks and those of
humans. To investigate the invariance behavior within graph neural networks
(GNNs), we introduce a model ``metamers'' generation technique. By optimizing
input graphs such that their internal node activations match those of a
reference graph, we obtain graphs that are equivalent in the model's
representation space, yet differ significantly in both structure and node
features. Our theoretical analysis focuses on two aspects: the local metamer
dimension for a single node and the activation-induced volume change of the
metamer manifold. Utilizing this approach, we uncover extreme levels of
representational invariance across several classic GNN architectures. Although
targeted modifications to model architecture and training strategies can
partially mitigate this excessive invariance, they fail to fundamentally bridge
the gap to human-like invariance. Finally, we quantify the deviation between
metamer graphs and their original counterparts, revealing unique failure modes
of current GNNs and providing a complementary benchmark for model evaluation.

</details>


### [496] [Optimizing Energy Management of Smart Grid using Reinforcement Learning aided by Surrogate models built using Physics-informed Neural Networks](https://arxiv.org/abs/2510.17380)
*Julen Cestero,Carmine Delle Femine,Kenji S. Muro,Marco Quartulli,Marcello Restelli*

Main category: cs.LG

TL;DR: 使用物理信息神经网络(PINNs)构建替代模型来替代昂贵的智能电网模拟器，优化强化学习策略训练过程，显著提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决智能电网中强化学习训练需要大量昂贵模拟器采样的问题，提高样本效率和训练速度。

Method: 用物理信息神经网络(PINNs)构建智能电网模拟器的替代模型，用于强化学习策略训练。

Result: 在远少于原始环境所需的时间内达到收敛结果，显著提高了训练效率。

Conclusion: PINNs作为替代模型可以有效解决强化学习在智能电网优化中的样本效率问题。

Abstract: Optimizing the energy management within a smart grids scenario presents
significant challenges, primarily due to the complexity of real-world systems
and the intricate interactions among various components. Reinforcement Learning
(RL) is gaining prominence as a solution for addressing the challenges of
Optimal Power Flow in smart grids. However, RL needs to iterate compulsively
throughout a given environment to obtain the optimal policy. This means
obtaining samples from a, most likely, costly simulator, which can lead to a
sample efficiency problem. In this work, we address this problem by
substituting costly smart grid simulators with surrogate models built using
Phisics-informed Neural Networks (PINNs), optimizing the RL policy training
process by arriving to convergent results in a fraction of the time employed by
the original environment.

</details>


### [497] [Beyond Binary Out-of-Distribution Detection: Characterizing Distributional Shifts with Multi-Statistic Diffusion Trajectories](https://arxiv.org/abs/2510.17381)
*Achref Jaziri,Martin Rogmann,Martin Mundt,Visvanathan Ramesh*

Main category: cs.LG

TL;DR: 提出DISC方法，使用扩散模型的多维特征向量进行OOD检测和分类，超越传统标量方法


<details>
  <summary>Details</summary>
Motivation: 传统OOD检测方法仅输出标量异常分数，无法区分不同类型的OOD数据，限制了后续处理和应用

Method: 利用扩散模型的迭代去噪过程，提取跨多个噪声级别的统计差异特征向量

Result: 在图像和表格数据基准测试中，DISC在OOD检测性能上达到或超越SOTA，并具备OOD类型分类能力

Conclusion: DISC实现了从简单二元OOD检测到更细粒度检测的转变，为OOD数据的上下文理解和利用提供了新途径

Abstract: Detecting out-of-distribution (OOD) data is critical for machine learning, be
it for safety reasons or to enable open-ended learning. However, beyond mere
detection, choosing an appropriate course of action typically hinges on the
type of OOD data encountered. Unfortunately, the latter is generally not
distinguished in practice, as modern OOD detection methods collapse
distributional shifts into single scalar outlier scores. This work argues that
scalar-based methods are thus insufficient for OOD data to be properly
contextualized and prospectively exploited, a limitation we overcome with the
introduction of DISC: Diffusion-based Statistical Characterization. DISC
leverages the iterative denoising process of diffusion models to extract a
rich, multi-dimensional feature vector that captures statistical discrepancies
across multiple noise levels. Extensive experiments on image and tabular
benchmarks show that DISC matches or surpasses state-of-the-art detectors for
OOD detection and, crucially, also classifies OOD type, a capability largely
absent from prior work. As such, our work enables a shift from simple binary
OOD detection to a more granular detection.

</details>


### [498] [TabR1: Taming GRPO for tabular reasoning LLMs](https://arxiv.org/abs/2510.17385)
*Pengxiang Cai,Zihao Gao,Jintai Chen*

Main category: cs.LG

TL;DR: TabR1是首个用于表格预测的推理大语言模型，通过PRPO强化学习方法激活LLM的推理能力，在少样本和零样本场景下表现优异，甚至能超越更大规模的LLM。


<details>
  <summary>Details</summary>
Motivation: 传统表格预测方法（如梯度提升树和专用深度学习模型）缺乏可解释性和跨任务迁移能力，而推理LLM虽然具有透明推理轨迹和跨任务适应性，但在表格数据上的潜力尚未充分发挥。

Method: 提出TabR1模型，核心是PRPO（排列相对策略优化）强化学习方法，通过构造标签保持的列排列，在排列内和排列间估计优势函数，将稀疏奖励转化为密集学习信号。

Result: TabR1在全监督微调下达到与强基线相当的性能；在零样本设置下接近32样本设置的强基线性能；TabR1（8B）在各种任务上显著超越更大的LLM，相比DeepSeek-R1（685B）提升达53.17%。

Conclusion: PRPO方法成功激活了LLM在表格预测中的推理能力，显著提升了少样本和零样本性能以及可解释性，证明了推理LLM在表格数据上的巨大潜力。

Abstract: Tabular prediction has traditionally relied on gradient-boosted decision
trees and specialized deep learning models, which excel within tasks but
provide limited interpretability and weak transfer across tables. Reasoning
large language models (LLMs) promise cross-task adaptability with trans- parent
reasoning traces, yet their potential has not been fully realized for tabular
data. This paper presents TabR1, the first reasoning LLM for tabular prediction
with multi-step reasoning. At its core is Permutation Relative Policy
Optimization (PRPO), a simple yet efficient reinforcement learning method that
encodes column-permutation invariance as a structural prior. By construct- ing
multiple label-preserving permutations per sample and estimating advantages
both within and across permutations, PRPO transforms sparse rewards into dense
learning signals and improves generalization. With limited supervision, PRPO
activates the reasoning ability of LLMs for tabular prediction, enhancing
few-shot and zero-shot performance as well as interpretability. Comprehensive
experiments demonstrate that TabR1 achieves performance comparable to strong
baselines under full-supervision fine-tuning. In the zero-shot setting, TabR1
approaches the performance of strong baselines under the 32-shot setting.
Moreover, TabR1 (8B) substantially outperforms much larger LLMs across various
tasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).

</details>


### [499] [Finite-Time Bounds for Average-Reward Fitted Q-Iteration](https://arxiv.org/abs/2510.17391)
*Jongmin Lee,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 提出了首个针对弱通信MDP的平均奖励离线强化学习的样本复杂度分析，引入了锚定拟合Q迭代方法，结合了标准拟合Q迭代和锚定机制，并在单轨迹数据集设置下进行了扩展分析。


<details>
  <summary>Details</summary>
Motivation: 现有平均奖励离线强化学习方法依赖严格假设（如遍历性或线性MDP），而弱通信MDP假设更为宽松，需要开发新的理论框架来解决这一限制。

Method: 提出了锚定拟合Q迭代方法，将标准拟合Q迭代与锚定机制相结合，锚定机制可解释为一种权重衰减形式，对于实现平均奖励设置的有限时间分析至关重要。

Result: 建立了弱通信MDP下平均奖励离线强化学习的首个样本复杂度结果，并证明了锚定机制在单轨迹数据集设置中的有效性。

Conclusion: 锚定机制是实现平均奖励离线强化学习有限时间分析的关键技术，该方法在更宽松的弱通信MDP假设下取得了理论突破，并适用于单轨迹数据生成场景。

Abstract: Although there is an extensive body of work characterizing the sample
complexity of discounted-return offline RL with function approximations, prior
work on the average-reward setting has received significantly less attention,
and existing approaches rely on restrictive assumptions, such as ergodicity or
linearity of the MDP. In this work, we establish the first sample complexity
results for average-reward offline RL with function approximation for weakly
communicating MDPs, a much milder assumption. To this end, we introduce
Anchored Fitted Q-Iteration, which combines the standard Fitted Q-Iteration
with an anchor mechanism. We show that the anchor, which can be interpreted as
a form of weight decay, is crucial for enabling finite-time analysis in the
average-reward setting. We also extend our finite-time analysis to the setup
where the dataset is generated from a single-trajectory rather than IID
transitions, again leveraging the anchor mechanism.

</details>


### [500] [MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning](https://arxiv.org/abs/2510.17394)
*Alejandro Guerra-Manzanares,Farah E. Shamout*

Main category: cs.LG

TL;DR: 提出了MILES（Modality-Informed Learning ratE Scheduler）方法，通过动态调整学习率来平衡多模态学习，解决模态过拟合问题，提升多模态和单模态预测性能。


<details>
  <summary>Details</summary>
Motivation: 多模态神经网络训练中存在模态过拟合问题，网络过度依赖某一模态导致性能不佳，限制了多模态学习的潜力。

Method: MILES利用训练过程中模态条件利用率差异，动态调整学习率来平衡各模态的学习速度。

Result: 在四个多模态联合融合任务上评估，MILES优于七个最先进基线方法，在所有任务和融合方法中表现最佳，有效平衡模态使用。

Conclusion: 平衡多模态学习对提升模型性能具有重要影响，MILES方法能改善多模态性能并产生更强的模态编码器。

Abstract: The aim of multimodal neural networks is to combine diverse data sources,
referred to as modalities, to achieve enhanced performance compared to relying
on a single modality. However, training of multimodal networks is typically
hindered by modality overfitting, where the network relies excessively on one
of the available modalities. This often yields sub-optimal performance,
hindering the potential of multimodal learning and resulting in marginal
improvements relative to unimodal models. In this work, we present the
Modality-Informed Learning ratE Scheduler (MILES) for training multimodal joint
fusion models in a balanced manner. MILES leverages the differences in
modality-wise conditional utilization rates during training to effectively
balance multimodal learning. The learning rate is dynamically adjusted during
training to balance the speed of learning from each modality by the multimodal
model, aiming for enhanced performance in both multimodal and unimodal
predictions. We extensively evaluate MILES on four multimodal joint fusion
tasks and compare its performance to seven state-of-the-art baselines. Our
results show that MILES outperforms all baselines across all tasks and fusion
methods considered in our study, effectively balancing modality usage during
training. This results in improved multimodal performance and stronger modality
encoders, which can be leveraged when dealing with unimodal samples or absent
modalities. Overall, our work highlights the impact of balancing multimodal
learning on improving model performance.

</details>


### [501] [S4ECG: Exploring the impact of long-range interactions for arrhythmia prediction](https://arxiv.org/abs/2510.17406)
*Tiezhi Wang,Wilhelm Haverkamp,Nils Strodthoff*

Main category: cs.LG

TL;DR: S4ECG是一种基于结构化状态空间模型的深度学习架构，用于多时段心律失常分类，通过联合多时段预测显著提升性能，在房颤检测中表现出优越的分布内性能和增强的分布外鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统ECG分析方法难以同时捕捉全局趋势和局部波形特征的高时间分辨率交互，需要桥接全局和局部信号分析。

Method: 引入S4ECG深度学习架构，利用结构化状态空间模型进行多时段心律失常分类，采用联合多时段预测方法。

Result: 多时段方法比单时段方法在宏观AUROC上提升1.0-11.6%，房颤特异性从0.718-0.979提升到0.967-0.998，最佳时间依赖窗口为10-20分钟。

Conclusion: 这项工作推动了心律失常检测算法向时间感知范式的转变，为ECG解释特别是复杂心律失常如房颤和房扑开辟了新可能性。

Abstract: The electrocardiogram (ECG) exemplifies biosignal-based time series with
continuous, temporally ordered structure reflecting cardiac physiological and
pathophysiological dynamics. Detailed analysis of these dynamics has proven
challenging, as conventional methods capture either global trends or local
waveform features but rarely their simultaneous interplay at high temporal
resolution. To bridge global and local signal analysis, we introduce S4ECG, a
novel deep learning architecture leveraging structured state space models for
multi-epoch arrhythmia classification. Our joint multi-epoch predictions
significantly outperform single-epoch approaches by 1.0-11.6% in macro-AUROC,
with atrial fibrillation specificity improving from 0.718-0.979 to 0.967-0.998,
demonstrating superior performance in-distribution and enhanced
out-of-distribution robustness. Systematic investigation reveals optimal
temporal dependency windows spanning 10-20 minutes for peak performance. This
work contributes to a paradigm shift toward temporally-aware arrhythmia
detection algorithms, opening new possibilities for ECG interpretation, in
particular for complex arrhythmias like atrial fibrillation and atrial flutter.

</details>


### [502] [A Conditional Diffusion Model for Probabilistic Prediction of Battery Capacity Degradation](https://arxiv.org/abs/2510.17414)
*Hequn Li,Zhongwei Deng,Chunlin Jiang,Yvxin He andZhansheng Ning*

Main category: cs.LG

TL;DR: 提出了一种名为CDUA的新方法，结合特征工程和深度学习，用于准确预测锂离子电池容量及其不确定性，在真实车辆数据上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 锂离子电池容量及其不确定性的准确预测对于可靠的电池管理至关重要，但由于老化的随机性，这仍然具有挑战性。

Method: 使用基于扩散的生成模型进行时间序列预测，结合注意力机制。首先从真实车辆运行数据推导电池容量，然后使用皮尔逊相关系数和XGBoost算法识别最相关特征，最后训练CDUA模型。

Result: 在真实车辆数据上的实验验证显示，CDUA模型实现了0.94%的相对MAE和1.14%的相对RMSE，95%置信区间相对宽度为3.74%。

Conclusion: CDUA提供了准确的容量估计和可靠的不确定性量化，比较实验进一步验证了其相对于现有主流方法的鲁棒性和优越性能。

Abstract: Accurate prediction of lithium-ion battery capacity and its associated
uncertainty is essential for reliable battery management but remains
challenging due to the stochastic nature of aging. This paper presents a novel
method, termed the Condition Diffusion U-Net with Attention (CDUA), which
integrates feature engineering and deep learning to address this challenge. The
proposed approach employs a diffusion-based generative model for time-series
forecasting and incorporates attention mechanisms to enhance predictive
performance. Battery capacity is first derived from real-world vehicle
operation data. The most relevant features are then identified using the
Pearson correlation coefficient and the XGBoost algorithm. These features are
used to train the CDUA model, which comprises two core components: (1) a
contextual U-Net with self-attention to capture complex temporal dependencies,
and (2) a denoising network to reconstruct accurate capacity values from noisy
observations. Experimental validation on the real-world vehicle data
demonstrates that the proposed CDUA model achieves a relative Mean Absolute
Error (MAE) of 0.94% and a relative Root Mean Square Error (RMSE) of 1.14%,
with a narrow 95% confidence interval of 3.74% in relative width. These results
confirm that CDUA provides both accurate capacity estimation and reliable
uncertainty quantification. Comparative experiments further verify its
robustness and superior performance over existing mainstream approaches.

</details>


### [503] [Diffusion Models as Dataset Distillation Priors](https://arxiv.org/abs/2510.17421)
*Duo Su,Huyu Wu,Huanran Chen,Yiming Shi,Yuzhu Wang,Xi Ye,Jun Zhu*

Main category: cs.LG

TL;DR: 提出了DAP方法，利用扩散模型的内在代表性先验来指导数据集蒸馏过程，无需重新训练即可提升合成数据的代表性质量。


<details>
  <summary>Details</summary>
Motivation: 当前生成式数据集蒸馏方法虽然采用扩散模型，但忽视了扩散模型固有的代表性先验，往往需要额外约束来提升数据质量。

Method: 通过Mercer核量化合成数据与真实数据在特征空间的相似度，将该先验作为指导引入反向扩散过程。

Result: 在ImageNet-1K等大规模数据集上的实验表明，DAP在生成高保真数据集和跨架构泛化方面优于现有方法。

Conclusion: 建立了扩散先验与数据集蒸馏目标的理论联系，提供了无需训练的实用框架来提升蒸馏数据集质量。

Abstract: Dataset distillation aims to synthesize compact yet informative datasets from
large ones. A significant challenge in this field is achieving a trifecta of
diversity, generalization, and representativeness in a single distilled
dataset. Although recent generative dataset distillation methods adopt powerful
diffusion models as their foundation models, the inherent representativeness
prior in diffusion models is overlooked. Consequently, these approaches often
necessitate the integration of external constraints to enhance data quality. To
address this, we propose Diffusion As Priors (DAP), which formalizes
representativeness by quantifying the similarity between synthetic and real
data in feature space using a Mercer kernel. We then introduce this prior as
guidance to steer the reverse diffusion process, enhancing the
representativeness of distilled samples without any retraining. Extensive
experiments on large-scale datasets, such as ImageNet-1K and its subsets,
demonstrate that DAP outperforms state-of-the-art methods in generating
high-fidelity datasets while achieving superior cross-architecture
generalization. Our work not only establishes a theoretical connection between
diffusion priors and the objectives of dataset distillation but also provides a
practical, training-free framework for improving the quality of the distilled
dataset.

</details>


### [504] [Deeper with Riemannian Geometry: Overcoming Oversmoothing and Oversquashing for Graph Foundation Models](https://arxiv.org/abs/2510.17457)
*Li Sun,Zhenhao Huang,Ming Zhang,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出GBN网络，通过局部瓶颈调整解决MPNN中的过平滑和过挤压问题，在深度超过256层时仍保持性能


<details>
  <summary>Details</summary>
Motivation: MPNN作为图基础模型的构建块，存在过平滑和过挤压问题。现有全局方法在某些区域有益但在其他区域有害，导致表达能力次优。通过谱间隙分析发现增加λ会导致梯度消失，影响消息传递效果

Method: 连接局部黎曼几何与MPNN，建立非齐次边界条件，基于Robin条件设计具有局部瓶颈调整的GBN网络

Result: 在同质性和异质性图上的广泛实验显示GBN具有强大表达能力，即使在网络深度超过256层时也不出现性能下降

Conclusion: 局部方法比全局方法更有效地解决过平滑和过挤压问题，GBN网络在深度图学习中表现出色

Abstract: Message Passing Neural Networks (MPNNs) is the building block of graph
foundation models, but fundamentally suffer from oversmoothing and
oversquashing. There has recently been a surge of interest in fixing both
issues. Existing efforts primarily adopt global approaches, which may be
beneficial in some regions but detrimental in others, ultimately leading to the
suboptimal expressiveness. In this paper, we begin by revisiting oversquashing
through a global measure -- spectral gap $\lambda$ -- and prove that the
increase of $\lambda$ leads to gradient vanishing with respect to the input
features, thereby undermining the effectiveness of message passing. Motivated
by such theoretical insights, we propose a \textbf{local} approach that
adaptively adjusts message passing based on local structures. To achieve this,
we connect local Riemannian geometry with MPNNs, and establish a novel
nonhomogeneous boundary condition to address both oversquashing and
oversmoothing. Building on the Robin condition, we design a GBN network with
local bottleneck adjustment, coupled with theoretical guarantees. Extensive
experiments on homophilic and heterophilic graphs show the expressiveness of
GBN. Furthermore, GBN does not exhibit performance degradation even when the
network depth exceeds $256$ layers.

</details>


### [505] [Explainable AI for microseismic event detection](https://arxiv.org/abs/2510.17458)
*Ayrat Abdullin,Denis Anikiev,Umair bin Waheed*

Main category: cs.LG

TL;DR: 应用可解释AI技术（Grad-CAM和SHAP）解释PhaseNet地震检测模型的决策，并基于SHAP值开发了门控推理方案，在9000个波形测试集上F1分数达到0.98，优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络如PhaseNet在地震事件检测中精度高，但其黑盒特性在关键应用中存在可靠性问题，需要可解释性技术来增强模型可信度。

Method: 使用Grad-CAM和SHAP解释PhaseNet模型决策，开发SHAP门控推理方案，结合模型输出和基于解释的指标来减少错误。

Result: SHAP门控模型在9000个波形测试集上F1分数为0.98（精度0.99，召回率0.97），优于基准PhaseNet（F1分数0.97），对噪声具有更强的鲁棒性。

Conclusion: 可解释AI不仅能解释深度学习模型，还能直接提升其性能，为构建可信的自动化地震检测器提供了模板。

Abstract: Deep neural networks like PhaseNet show high accuracy in detecting
microseismic events, but their black-box nature is a concern in critical
applications. We apply explainable AI (XAI) techniques, such as
Gradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive
Explanations (SHAP), to interpret the PhaseNet model's decisions and improve
its reliability. Grad-CAM highlights that the network's attention aligns with
P- and S-wave arrivals. SHAP values quantify feature contributions, confirming
that vertical-component amplitudes drive P-phase picks while horizontal
components dominate S-phase picks, consistent with geophysical principles.
Leveraging these insights, we introduce a SHAP-gated inference scheme that
combines the model's output with an explanation-based metric to reduce errors.
On a test set of 9,000 waveforms, the SHAP-gated model achieved an F1-score of
0.98 (precision 0.99, recall 0.97), outperforming the baseline PhaseNet
(F1-score 0.97) and demonstrating enhanced robustness to noise. These results
show that XAI can not only interpret deep learning models but also directly
enhance their performance, providing a template for building trust in automated
seismic detectors.

</details>


### [506] [CrossStateECG: Multi-Scale Deep Convolutional Network with Attention for Rest-Exercise ECG Biometrics](https://arxiv.org/abs/2510.17467)
*Dan Zheng,Jing Feng,Juan Liu*

Main category: cs.LG

TL;DR: CrossStateECG是一个针对静息-运动跨状态条件的鲁棒ECG生物认证模型，通过多尺度深度卷积特征提取和注意力机制，在静息-运动场景下实现了92.50%-94.72%的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 当前ECG生物识别研究主要关注静息状态，而静息-运动场景下的性能下降问题尚未解决，需要开发跨状态鲁棒认证模型。

Method: 结合多尺度深度卷积特征提取与注意力机制，构建跨状态ECG认证模型CrossStateECG。

Result: 在exercise-ECGID数据集上，Rest-to-Exercise场景准确率92.50%，Exercise-to-Rest场景94.72%，Rest-to-Rest场景99.94%，Mixed-to-Mixed场景97.85%。在ECG-ID和MIT-BIH数据集上验证了泛化能力。

Conclusion: CrossStateECG在动态现实环境中具有作为运动后ECG认证实用解决方案的潜力。

Abstract: Current research in Electrocardiogram (ECG) biometrics mainly emphasizes
resting-state conditions, leaving the performance decline in rest-exercise
scenarios largely unresolved. This paper introduces CrossStateECG, a robust
ECG-based authentication model explicitly tailored for cross-state
(rest-exercise) conditions. The proposed model creatively combines multi-scale
deep convolutional feature extraction with attention mechanisms to ensure
strong identification across different physiological states. Experimental
results on the exercise-ECGID dataset validate the effectiveness of
CrossStateECG, achieving an identification accuracy of 92.50% in the
Rest-to-Exercise scenario (training on resting ECG and testing on post-exercise
ECG) and 94.72% in the Exercise-to-Rest scenario (training on post-exercise ECG
and testing on resting ECG). Furthermore, CrossStateECG demonstrates
exceptional performance across both state combinations, reaching an accuracy of
99.94% in Rest-to-Rest scenarios and 97.85% in Mixed-to-Mixed scenarios.
Additional validations on the ECG-ID and MIT-BIH datasets further confirmed the
generalization abilities of CrossStateECG, underscoring its potential as a
practical solution for post-exercise ECG-based authentication in dynamic
real-world settings.

</details>


### [507] [Layer Specialization Underlying Compositional Reasoning in Transformers](https://arxiv.org/abs/2510.17469)
*Jing Liu*

Main category: cs.LG

TL;DR: Transformers通过随机层次模型(RHM)训练，展现出组合推理能力，其性能随任务复杂性和上下文示例数量提升，并在训练过程中形成层次化、模块化的内部表示结构。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型在未见序列上的组合推理能力，探索其内部机制如何支持这种泛化能力。

Method: 使用随机层次模型(RHM)作为概率上下文无关文法生成序列，在序列子集上训练模型，并在四种泛化条件下评估：记忆、分布内泛化、分布外泛化(相同规则)和跨层迁移。

Result: 模型性能随任务复杂性和上下文示例数量系统性提升，分布外任务需要更多示例；训练过程中出现层次专业化，形成结构化、层次化的表示和注意力模式。

Conclusion: Transformer发展出模块化、可解释的机制支持组合推理，内部算法结构与观察到的行为能力相关联。

Abstract: Transformers exhibit compositional reasoning on sequences not observed during
training, a capability often attributed to in-context learning (ICL) and skill
composition. We investigate this phenomenon using the Random Hierarchy Model
(RHM), a probabilistic context-free grammar that generates sequences through
recursive rule application. Models are trained on subsets of sequences and
evaluated across four generalization conditions: memorization, in-distribution
generalization, out-of-distribution generalization with the same rules, and
cross-layer transfer. Behaviorally, performance improves systematically with
task complexity and the number of in-context examples, with out-of-distribution
tasks requiring substantially more examples than in-distribution scenarios.
Mechanistically, we identify a progressive emergence of layer specialization
during training that correlates with generalization performance. Principal
component analysis and attention pattern clustering reveal that transformers
develop structured, hierarchically organized representations in specialized
layers. These results demonstrate that transformers develop modular,
interpretable mechanisms supporting compositional reasoning, linking internal
algorithmic structure to observed behavioral capabilities.

</details>


### [508] [DAMSDAN: Distribution-Aware Multi-Source Domain Adaptation Network for Cross-Domain EEG-based Emotion Recognition](https://arxiv.org/abs/2510.17475)
*Fo Hu,Can Wang,Qinxu Zheng,Xusheng Yang,Bin Zhou,Gang Li,Yu Sun,Wen-an Zhang*

Main category: cs.LG

TL;DR: 提出DAMSDAN网络解决跨域EEG情绪识别中的个体差异问题，通过动态源域加权和原型引导对齐实现多源域适应。


<details>
  <summary>Details</summary>
Motivation: EEG情绪识别在跨域设置下存在显著的个体间变异性，限制了模型的泛化能力。需要解决多源域适应中的两个核心挑战：动态建模源域分布异质性以减少负迁移，以及实现细粒度语义一致性以增强类别区分。

Method: 提出分布感知多源域适应网络(DAMSDAN)，整合原型约束与对抗学习，驱动编码器学习判别性、域不变的情绪表征。采用基于最大均值差异(MMD)的域感知源加权策略动态估计域间偏移并重新加权源贡献，以及原型引导的条件对齐模块通过双伪标签交互增强伪标签可靠性。

Result: 在SEED和SEED-IV数据集上，跨被试协议平均准确率分别为94.86%和79.78%，跨会话协议分别为95.12%和83.15%。在大型FACED数据集上，跨被试准确率达到82.88%。广泛的消融实验和可解释性分析验证了框架的有效性。

Conclusion: DAMSDAN框架通过动态源域加权和细粒度语义对齐，有效解决了跨域EEG情绪识别中的个体差异问题，在多个数据集上取得了优异的性能。

Abstract: Significant inter-individual variability limits the generalization of
EEG-based emotion recognition under cross-domain settings. We address two core
challenges in multi-source adaptation: (1) dynamically modeling distributional
heterogeneity across sources and quantifying their relevance to a target to
reduce negative transfer; and (2) achieving fine-grained semantic consistency
to strengthen class discrimination. We propose a distribution-aware
multi-source domain adaptation network (DAMSDAN). DAMSDAN integrates
prototype-based constraints with adversarial learning to drive the encoder
toward discriminative, domain-invariant emotion representations. A domain-aware
source weighting strategy based on maximum mean discrepancy (MMD) dynamically
estimates inter-domain shifts and reweights source contributions. In addition,
a prototype-guided conditional alignment module with dual pseudo-label
interaction enhances pseudo-label reliability and enables category-level,
fine-grained alignment, mitigating noise propagation and semantic drift.
Experiments on SEED and SEED-IV show average accuracies of 94.86\% and 79.78\%
for cross-subject, and 95.12\% and 83.15\% for cross-session protocols. On the
large-scale FACED dataset, DAMSDAN achieves 82.88\% (cross-subject). Extensive
ablations and interpretability analyses corroborate the effectiveness of the
proposed framework for cross-domain EEG-based emotion recognition.

</details>


### [509] [Towards geological inference with process-based and deep generative modeling, part 2: inversion of fluvial deposits and latent-space disentanglement](https://arxiv.org/abs/2510.17478)
*Guillaume Rongier,Luk Peeters*

Main category: cs.LG

TL;DR: 该研究探讨了生成对抗网络(GAN)在沉积建模中的反演应用，发现标准GAN的潜在空间纠缠问题导致反演困难，但通过微调方法可以改善匹配效果。


<details>
  <summary>Details</summary>
Motivation: 地下决策成本高且不确定性大，获取新数据难以扩展。将地质知识直接嵌入预测模型是更有价值的替代方案，过程模型可以帮助训练生成模型以提高预测效率。

Method: 使用生成对抗网络(GAN)训练生成河流沉积模型，并应用四种反演方法来匹配井数据和地震数据。通过标签条件化、潜在过参数化和微调GAN来重构潜在空间。

Result: 在4、8、20口井的三个测试样本中，反演方法难以匹配井数据，特别是当井数增加或测试样本与训练数据差异较大时。微调GAN可将不匹配降至可接受水平，但依赖于初始部分成功的反演步骤。

Conclusion: GAN已能处理地质建模工作流中的任务，但需要进一步评估其鲁棒性，以及如何最好地支持地质解释。

Abstract: High costs and uncertainties make subsurface decision-making challenging, as
acquiring new data is rarely scalable. Embedding geological knowledge directly
into predictive models offers a valuable alternative. A joint approach enables
just that: process-based models that mimic geological processes can help train
generative models that make predictions more efficiently. This study explores
whether a generative adversarial network (GAN) - a type of deep-learning
algorithm for generative modeling - trained to produce fluvial deposits can be
inverted to match well and seismic data. Four inversion approaches applied to
three test samples with 4, 8, and 20 wells struggled to match these well data,
especially as the well number increased or as the test sample diverged from the
training data. The key bottleneck lies in the GAN's latent representation: it
is entangled, so samples with similar sedimentological features are not
necessarily close in the latent space. Label conditioning or latent
overparameterization can partially disentangle the latent space during
training, although not yet sufficiently for a successful inversion. Fine-tuning
the GAN to restructure the latent space locally reduces mismatches to
acceptable levels for all test cases, with and without seismic data. But this
approach depends on an initial, partially successful inversion step, which
influences the quality and diversity of the final samples. Overall, GANs can
already handle the tasks required for their integration into geomodeling
workflows. We still need to further assess their robustness, and how to best
leverage them in support of geological interpretation.

</details>


### [510] [Unified Privacy Guarantees for Decentralized Learning via Matrix Factorization](https://arxiv.org/abs/2510.17480)
*Aurélien Bellet,Edwige Cyffers,Davide Frey,Romaric Gaudel,Dimitri Lerévérend,François Taïani*

Main category: cs.LG

TL;DR: 本文提出了一种基于矩阵分解的去中心化学习隐私计算方法MAFALDA-SGD，通过分析时间噪声相关性来获得更紧密的隐私预算，在合成和真实图数据上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习虽然能通过差分隐私保护数据，但实践中观察到的隐私-效用权衡往往比集中式训练差，这可能是由于当前DP计算方法在DL中的局限性造成的。

Method: 将集中式DP计算中基于矩阵分解分析时间噪声相关性的方法推广到去中心化学习，提出了MAFALDA-SGD算法，这是一种基于gossip的去中心化学习算法，具有用户级相关噪声。

Result: MAFALDA-SGD在合成和真实世界图数据上表现出色，优于现有的DP-DL算法。

Conclusion: 通过矩阵分解方法可以更紧密地计算去中心化学习的隐私预算，并为开发新的DP-DL算法提供了理论基础。

Abstract: Decentralized Learning (DL) enables users to collaboratively train models
without sharing raw data by iteratively averaging local updates with neighbors
in a network graph. This setting is increasingly popular for its scalability
and its ability to keep data local under user control. Strong privacy
guarantees in DL are typically achieved through Differential Privacy (DP), with
results showing that DL can even amplify privacy by disseminating noise across
peer-to-peer communications. Yet in practice, the observed privacy-utility
trade-off often appears worse than in centralized training, which may be due to
limitations in current DP accounting methods for DL. In this paper, we show
that recent advances in centralized DP accounting based on Matrix Factorization
(MF) for analyzing temporal noise correlations can also be leveraged in DL. By
generalizing existing MF results, we show how to cast both standard DL
algorithms and common trust models into a unified formulation. This yields
tighter privacy accounting for existing DP-DL algorithms and provides a
principled way to develop new ones. To demonstrate the approach, we introduce
MAFALDA-SGD, a gossip-based DL algorithm with user-level correlated noise that
outperforms existing methods on synthetic and real-world graphs.

</details>


### [511] [Local properties of neural networks through the lens of layer-wise Hessians](https://arxiv.org/abs/2510.17486)
*Maxim Bolshim,Alexander Kugaevskikh*

Main category: cs.LG

TL;DR: 提出通过层间Hessian矩阵分析神经网络的方法，研究参数空间的局部几何特性，发现Hessian谱特征与过拟合、欠参数化和表达能力相关。


<details>
  <summary>Details</summary>
Motivation: 建立连接优化几何与功能行为的框架，为神经网络诊断和设计提供理论基础，改进网络架构和训练稳定性。

Method: 定义每个功能块（层）的局部Hessian矩阵作为参数二阶导数的矩阵，分析其谱特性（如特征值分布），在37个数据集上进行了111次实验验证。

Result: 发现训练过程中局部Hessian具有一致的结构规律，其谱特征与泛化性能存在相关性，揭示了与过拟合、欠参数化和表达能力的定量模式。

Conclusion: 局部几何分析为深度神经网络诊断和设计奠定了基础，连接了优化几何与功能行为，提供了改进网络架构和训练稳定性的实用见解。

Abstract: We introduce a methodology for analyzing neural networks through the lens of
layer-wise Hessian matrices. The local Hessian of each functional block (layer)
is defined as the matrix of second derivatives of a scalar function with
respect to the parameters of that layer. This concept provides a formal tool
for characterizing the local geometry of the parameter space. We show that the
spectral properties of local Hessians, such as the distribution of eigenvalues,
reveal quantitative patterns associated with overfitting,
underparameterization, and expressivity in neural network architectures. We
conduct an extensive empirical study involving 111 experiments across 37
datasets. The results demonstrate consistent structural regularities in the
evolution of local Hessians during training and highlight correlations between
their spectra and generalization performance. These findings establish a
foundation for using local geometric analysis to guide the diagnosis and design
of deep neural networks. The proposed framework connects optimization geometry
with functional behavior and offers practical insight for improving network
architectures and training stability.

</details>


### [512] [I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models](https://arxiv.org/abs/2510.17496)
*Giacomo Camposampiero,Michael Hersche,Roger Wattenhofer,Abu Sebastian,Abbas Rahimi*

Main category: cs.LG

TL;DR: I-RAVEN-X是一个符号基准测试，用于评估大语言模型和大推理模型在类比和数学推理中的泛化性和鲁棒性。它通过增加操作数复杂度、属性范围和引入感知不确定性来扩展I-RAVEN。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型和大推理模型在类比和数学推理中的泛化能力和鲁棒性，特别是在处理复杂操作数、宽属性范围和感知不确定性时的表现。

Method: 扩展I-RAVEN基准测试，增加操作数复杂度、扩大属性范围，并引入感知不确定性，然后对大语言模型和大推理模型进行实证评估。

Result: 大推理模型在长推理关系和宽属性范围上分别表现出更好的生产力和系统性，但面对不确定性推理时仍然显著受挑战，无法有效探索多个概率结果。

Conclusion: 大推理模型在复杂推理任务上优于大语言模型，但在处理不确定性和概率推理方面仍有明显局限，需要进一步改进。

Abstract: We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate
generalization and robustness in analogical and mathematical reasoning for
Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X
extends I-RAVEN by increasing operand complexity, attribute range, and
introducing perceptual uncertainty. Compared to LLMs, empirical results show
that LRMs achieve improved productivity and systematicity on longer reasoning
relations and wider attribute ranges, respectively. However, LRMs are still
significantly challenged by reasoning under uncertainty and cannot effectively
explore multiple probabilistic outcomes.

</details>


### [513] [The Graphon Limit Hypothesis: Understanding Neural Network Pruning via Infinite Width Analysis](https://arxiv.org/abs/2510.17515)
*Hoang Pham,The-Anh Ta,Tom Jacobs,Rebekka Burkholz,Long Tran-Thanh*

Main category: cs.LG

TL;DR: 提出了基于图极限理论的新框架，使用图论工具分析稀疏神经网络的训练动态，解释了不同剪枝方法产生不同训练效果的原因。


<details>
  <summary>Details</summary>
Motivation: 尽管剪枝方法能创建稀疏架构，但为什么相同稀疏度下某些结构比其他结构更容易训练仍不清楚。需要系统性地理解稀疏网络的可训练性。

Method: 基于图极限理论（特别是图论）建立理论框架，将剪枝方法诱导的连接模式建模为图论，推导出图论神经正切核来分析无限宽度极限下的训练动态。

Result: 图论NTK的谱分析与观察到的稀疏网络训练动态相关，能够解释不同剪枝方法的不同收敛行为。

Conclusion: 该框架为理解连接模式对稀疏网络架构可训练性的影响提供了理论洞察，揭示了不同剪枝方法的隐式结构偏差。

Abstract: Sparse neural networks promise efficiency, yet training them effectively
remains a fundamental challenge. Despite advances in pruning methods that
create sparse architectures, understanding why some sparse structures are
better trainable than others with the same level of sparsity remains poorly
understood. Aiming to develop a systematic approach to this fundamental
problem, we propose a novel theoretical framework based on the theory of graph
limits, particularly graphons, that characterizes sparse neural networks in the
infinite-width regime. Our key insight is that connectivity patterns of sparse
neural networks induced by pruning methods converge to specific graphons as
networks' width tends to infinity, which encodes implicit structural biases of
different pruning methods. We postulate the Graphon Limit Hypothesis and
provide empirical evidence to support it. Leveraging this graphon
representation, we derive a Graphon Neural Tangent Kernel (Graphon NTK) to
study the training dynamics of sparse networks in the infinite width limit.
Graphon NTK provides a general framework for the theoretical analysis of sparse
networks. We empirically show that the spectral analysis of Graphon NTK
correlates with observed training dynamics of sparse networks, explaining the
varying convergence behaviours of different pruning methods. Our framework
provides theoretical insights into the impact of connectivity patterns on the
trainability of various sparse network architectures.

</details>


### [514] [SAFE-D: A Spatiotemporal Detection Framework for Abnormal Driving Among Parkinson's Disease-like Drivers](https://arxiv.org/abs/2510.17517)
*Hangcheng Cao,Baixiang Huang,Longzhi Yuan,Haonan An,Zihan Fang,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: 提出了SAFE-D框架，用于检测帕金森病相关的驾驶行为异常，通过多源车辆控制数据构建行为档案，使用注意力网络识别时空特征，在模拟环境中达到96.8%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注功能驱动的暂时性异常（如疲劳、分心），而缺乏对病理触发的驾驶行为异常（特别是慢性疾病如帕金森病）的研究，这对公共交通安全构成风险。

Method: 分析帕金森病症状学与驾驶表现的关系，整合多车辆控制组件数据构建行为档案，设计基于注意力的网络自适应优先处理时空特征，在Logitech G29平台和CARLA模拟器上验证。

Result: SAFE-D在区分正常和帕金森病影响驾驶模式方面达到96.8%的平均准确率，使用三个道路地图模拟真实世界驾驶场景。

Conclusion: SAFE-D框架能有效检测帕金森病相关的驾驶行为异常，为提升驾驶安全提供了新方法，特别适用于早期帕金森病的亚临床行为变化检测。

Abstract: A driver's health state serves as a determinant factor in driving behavioral
regulation. Subtle deviations from normalcy can lead to operational anomalies,
posing risks to public transportation safety. While prior efforts have
developed detection mechanisms for functionally-driven temporary anomalies such
as drowsiness and distraction, limited research has addressed
pathologically-triggered deviations, especially those stemming from chronic
medical conditions. To bridge this gap, we investigate the driving behavior of
Parkinson's disease patients and propose SAFE-D, a novel framework for
detecting Parkinson-related behavioral anomalies to enhance driving safety. Our
methodology starts by performing analysis of Parkinson's disease
symptomatology, focusing on primary motor impairments, and establishes causal
links to degraded driving performance. To represent the subclinical behavioral
variations of early-stage Parkinson's disease, our framework integrates data
from multiple vehicle control components to build a behavioral profile. We then
design an attention-based network that adaptively prioritizes spatiotemporal
features, enabling robust anomaly detection under physiological variability.
Finally, we validate SAFE-D on the Logitech G29 platform and CARLA simulator,
using data from three road maps to emulate real-world driving. Our results show
SAFE-D achieves 96.8% average accuracy in distinguishing normal and
Parkinson-affected driving patterns.

</details>


### [515] [Curiosity Meets Cooperation: A Game-Theoretic Approach to Long-Tail Multi-Label Learning](https://arxiv.org/abs/2510.17520)
*Canran Xiao,Chuangxin Zhao,Zong Ke,Fei Shen*

Main category: cs.LG

TL;DR: 提出CD-GTMLL框架，将多标签学习建模为合作博弈，通过好奇心奖励机制解决长尾分布问题，提升罕见标签的预测性能


<details>
  <summary>Details</summary>
Motivation: 多标签学习中存在长尾不平衡问题：少数头部标签主导梯度信号，而许多实际重要的罕见标签被忽略

Method: 将标签空间分割给多个合作玩家，共享全局准确度收益，同时根据标签稀有度和玩家间分歧获得额外好奇心奖励，无需手动调整类别权重

Result: 在传统基准和三个超大规模数据集上取得最先进性能，Rare-F1提升达+4.3%，P@3提升+1.6%，消融实验显示出现分工和罕见类别更快达成共识

Conclusion: CD-GTMLL为多标签预测中的长尾鲁棒性提供了原则性、可扩展的解决方案

Abstract: Long-tail imbalance is endemic to multi-label learning: a few head labels
dominate the gradient signal, while the many rare labels that matter in
practice are silently ignored. We tackle this problem by casting the task as a
cooperative potential game. In our Curiosity-Driven Game-Theoretic Multi-Label
Learning (CD-GTMLL) framework, the label space is split among several
cooperating players that share a global accuracy payoff yet earn additional
curiosity rewards that rise with label rarity and inter-player disagreement.
These curiosity bonuses inject gradient on under-represented tags without
hand-tuned class weights. We prove that gradient best-response updates ascend a
differentiable potential and converge to tail-aware stationary points that
tighten a lower bound on the expected Rare-F1. Extensive experiments on
conventional benchmarks and three extreme-scale datasets show consistent
state-of-the-art gains, delivering up to +4.3% Rare-F1 and +1.6% P@3 over the
strongest baselines, while ablations reveal emergent division of labour and
faster consensus on rare classes. CD-GTMLL thus offers a principled, scalable
route to long-tail robustness in multi-label prediction.

</details>


### [516] [Mitigating Clever Hans Strategies in Image Classifiers through Generating Counterexamples](https://arxiv.org/abs/2510.17524)
*Sidney Bender,Ole Delzer,Jan Herrmann,Heike Antje Marxfeld,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 提出Counterfactual Knowledge Distillation (CFKD)框架，通过生成多样反事实样本来解决深度学习中虚假相关性问题，无需组标签即可实现跨组平衡泛化。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易受到虚假相关性的影响，形成Clever Hans预测器。现有方法如DFR依赖显式组标签，但面临组标签不可得、组内样本少、多虚假相关性导致数据碎片化等问题。

Method: CFKD通过生成多样反事实样本，让人类标注者高效探索和修正模型决策边界，通过知识蒸馏步骤不仅重采样欠表示组，还为其丰富新数据点。

Result: 在五个数据集上验证了CFKD的有效性，特别是在低数据量和高虚假相关性场景下表现优异，能够扩展到多混淆变量并实现跨组平衡泛化。

Conclusion: CFKD无需混淆变量标签，能有效处理多混淆变量情况，在存在显著虚假相关性的低数据量场景下表现突出，为提升模型鲁棒性提供了新方法。

Abstract: Deep learning models remain vulnerable to spurious correlations, leading to
so-called Clever Hans predictors that undermine robustness even in large-scale
foundation and self-supervised models. Group distributional robustness methods,
such as Deep Feature Reweighting (DFR) rely on explicit group labels to
upweight underrepresented subgroups, but face key limitations: (1) group labels
are often unavailable, (2) low within-group sample sizes hinder coverage of the
subgroup distribution, and (3) performance degrades sharply when multiple
spurious correlations fragment the data into even smaller groups. We propose
Counterfactual Knowledge Distillation (CFKD), a framework that sidesteps these
issues by generating diverse counterfactuals, enabling a human annotator to
efficiently explore and correct the model's decision boundaries through a
knowledge distillation step. Unlike DFR, our method not only reweights the
undersampled groups, but it also enriches them with new data points. Our method
does not require any confounder labels, achieves effective scaling to multiple
confounders, and yields balanced generalization across groups. We demonstrate
CFKD's efficacy across five datasets, spanning synthetic tasks to an industrial
application, with particularly strong gains in low-data regimes with pronounced
spurious correlations. Additionally, we provide an ablation study on the effect
of the chosen counterfactual explainer and teacher model, highlighting their
impact on robustness.

</details>


### [517] [How Does Label Noise Gradient Descent Improve Generalization in the Low SNR Regime?](https://arxiv.org/abs/2510.17526)
*Wei Huang,Andi Han,Yujin Song,Yilan Chen,Denny Wu,Difan Zou,Taiji Suzuki*

Main category: cs.LG

TL;DR: 在低信噪比(SNR)数据中，通过向梯度下降训练过程添加标签噪声可以抑制噪声记忆化，改善神经网络泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易在训练中过拟合噪声，特别是在低信噪比数据中，这会导致泛化性能下降。受标签噪声具有隐式正则化效果的启发，研究是否可以通过引入标签噪声来提升神经网络在低信噪比环境下的测试性能。

Method: 使用两层神经网络，在理想化的信号-噪声数据设置中，采用带有标签噪声的梯度下降算法进行训练。

Result: 标签噪声梯度下降能够抑制噪声记忆化，防止噪声主导学习过程，实现快速信号增长同时控制过拟合，从而在低信噪比下获得良好的泛化性能。

Conclusion: 在低信噪比环境下，引入标签噪声的梯度下降训练相比标准梯度下降能显著改善神经网络泛化能力，标准梯度下降会过拟合噪声且测试误差存在非零下界。

Abstract: The capacity of deep learning models is often large enough to both learn the
underlying statistical signal and overfit to noise in the training set. This
noise memorization can be harmful especially for data with a low
signal-to-noise ratio (SNR), leading to poor generalization. Inspired by prior
observations that label noise provides implicit regularization that improves
generalization, in this work, we investigate whether introducing label noise to
the gradient updates can enhance the test performance of neural network (NN) in
the low SNR regime. Specifically, we consider training a two-layer NN with a
simple label noise gradient descent (GD) algorithm, in an idealized
signal-noise data setting. We prove that adding label noise during training
suppresses noise memorization, preventing it from dominating the learning
process; consequently, label noise GD enjoys rapid signal growth while the
overfitting remains controlled, thereby achieving good generalization despite
the low SNR. In contrast, we also show that NN trained with standard GD tends
to overfit to noise in the same low SNR setting and establish a non-vanishing
lower bound on its test error, thus demonstrating the benefit of introducing
label noise in gradient-based training.

</details>


### [518] [TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model](https://arxiv.org/abs/2510.17545)
*Yichen Liu,Yan Lin,Shengnan Guo,Zeyu Zhou,Youfang Lin,Huaiyu Wan*

Main category: cs.LG

TL;DR: TrajMamba是一种用于车辆GPS轨迹学习的新方法，通过联合建模GPS和道路视角来捕捉移动模式，并集成旅行目的感知预训练，同时使用知识蒸馏减少轨迹冗余点，在效率和准确性上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 车辆GPS轨迹包含有价值的旅行语义信息，但面临两个主要挑战：旅行目的与道路功能和POI相关，文本信息处理计算负担重；真实轨迹包含冗余点，影响计算效率和嵌入质量。

Method: 提出TrajMamba方法，包括：Traj-Mamba编码器联合建模GPS和道路视角；旅行目的感知预训练将旅行目的集成到嵌入中；知识蒸馏预训练通过可学习掩码生成器识别关键轨迹点，获得压缩的轨迹嵌入。

Result: 在两个真实世界数据集和三个下游任务上的广泛实验表明，TrajMamba在效率和准确性上都优于最先进的基线方法。

Conclusion: TrajMamba能够有效且高效地学习车辆轨迹的语义信息，解决了轨迹数据建模中的计算负担和冗余点问题，在实际应用中具有重要价值。

Abstract: Vehicle GPS trajectories record how vehicles move over time, storing valuable
travel semantics, including movement patterns and travel purposes. Learning
travel semantics effectively and efficiently is crucial for real-world
applications of trajectory data, which is hindered by two major challenges.
First, travel purposes are tied to the functions of the roads and
points-of-interest (POIs) involved in a trip. Such information is encoded in
textual addresses and descriptions and introduces heavy computational burden to
modeling. Second, real-world trajectories often contain redundant points, which
harm both computational efficiency and trajectory embedding quality. To address
these challenges, we propose TrajMamba, a novel approach for efficient and
semantically rich vehicle trajectory learning. TrajMamba introduces a
Traj-Mamba Encoder that captures movement patterns by jointly modeling both GPS
and road perspectives of trajectories, enabling robust representations of
continuous travel behaviors. It also incorporates a Travel Purpose-aware
Pre-training procedure to integrate travel purposes into the learned embeddings
without introducing extra overhead to embedding calculation. To reduce
redundancy in trajectories, TrajMamba features a Knowledge Distillation
Pre-training scheme to identify key trajectory points through a learnable mask
generator and obtain effective compressed trajectory embeddings. Extensive
experiments on two real-world datasets and three downstream tasks show that
TrajMamba outperforms state-of-the-art baselines in both efficiency and
accuracy.

</details>


### [519] [The Free Transformer](https://arxiv.org/abs/2510.17558)
*François Fleuret*

Main category: cs.LG

TL;DR: 提出了一种扩展的解码器Transformer，通过变分方法在无监督条件下学习随机潜变量来调节生成过程。


<details>
  <summary>Details</summary>
Motivation: 通过引入随机潜变量来增强解码器Transformer的生成能力，使其能够更好地处理下游任务。

Method: 扩展解码器Transformer，引入随机潜变量，并使用变分方法进行无监督学习。

Result: 实验评估表明，这种条件调节方法在下游任务上带来了显著改进。

Conclusion: 通过无监督学习随机潜变量来调节生成过程，可以有效提升解码器Transformer在下游任务上的性能。

Abstract: We propose an extension of the decoder Transformer that conditions its
generative process on random latent variables which are learned without
supervision thanks to a variational procedure. Experimental evaluations show
that allowing such a conditioning translates into substantial improvements on
downstream tasks.

</details>


### [520] [Formally Exploring Time-Series Anomaly Detection Evaluation Metrics](https://arxiv.org/abs/2510.17562)
*Dennis Wagner,Arjun Nair,Billy Joe Franks,Justus Arweiler,Aparna Muraleedharan,Indra Jungjohann,Fabian Hartung,Mayank C. Ahuja,Andriy Balinskyy,Saurabh Varshneya,Nabeel Hussain Syed,Mayank Nagda,Phillip Liznerski,Steffen Reithermann,Maja Rudolph,Sebastian Vollmer,Ralf Schulz,Torsten Katz,Stephan Mandt,Michael Bortz,Heike Leitte,Daniel Neider,Jakob Burger,Fabian Jirasek,Hans Hasse,Sophie Fellenz,Marius Kloft*

Main category: cs.LG

TL;DR: 提出了一个理论框架来评估时间序列异常检测指标，分析了37个常用指标，发现大多数只满足少数属性，并提出了满足所有属性的新指标LARM和ALARM。


<details>
  <summary>Details</summary>
Motivation: 时间序列中的未检测异常可能导致安全关键系统的灾难性故障，现有评估指标性能不明确且结果不一致。

Method: 引入可验证属性来形式化评估时间序列异常检测的基本要求，建立理论框架支持原则性评估和可靠比较。

Result: 分析37个广泛使用的指标，显示大多数只满足少数属性，没有指标满足所有属性，解释了先前结果的不一致性。

Conclusion: 提出的LARM和ALARM指标能够满足所有评估属性，解决了现有指标的局限性。

Abstract: Undetected anomalies in time series can trigger catastrophic failures in
safety-critical systems, such as chemical plant explosions or power grid
outages. Although many detection methods have been proposed, their performance
remains unclear because current metrics capture only narrow aspects of the task
and often yield misleading results. We address this issue by introducing
verifiable properties that formalize essential requirements for evaluating
time-series anomaly detection. These properties enable a theoretical framework
that supports principled evaluations and reliable comparisons. Analyzing 37
widely used metrics, we show that most satisfy only a few properties, and none
satisfy all, explaining persistent inconsistencies in prior results. To close
this gap, we propose LARM, a flexible metric that provably satisfies all
properties, and extend it to ALARM, an advanced variant meeting stricter
requirements.

</details>


### [521] [Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides](https://arxiv.org/abs/2510.17569)
*Jyler Menard,R. A. Mansbach*

Main category: cs.LG

TL;DR: 该论文研究如何通过降维技术改进抗菌肽设计的深度生成模型，提高潜在空间的解释性和优化效率。


<details>
  <summary>Details</summary>
Motivation: 抗菌肽是治疗细菌感染的有前景疗法，但序列设计空间巨大。现有深度生成模型缺乏解释性，且对潜在空间质量的量化不足。

Method: 使用变分自编码器建模肽序列空间，通过降维技术进一步压缩设计空间，并用理化性质组织潜在空间。

Result: 发现降维后的搜索空间更有利于优化，解释性更强，且能在不同标签可用性下用理化性质有效组织潜在空间。

Conclusion: 通过降维和理化性质组织可以显著改进抗菌肽设计的深度生成模型，提高优化效率和解释性。

Abstract: Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat
bacterial infections. Discovering and designing such peptides is difficult
because of the vast number of possible sequences of amino acids. Deep
generative models, such as variational autoencoders, have shown value in
peptide design due to their ability to model sequence space with a
continuous-valued latent space. Although such models have already been used to
great effect in biomolecular design, they still suffer from a lack of
interpretability and rigorous quantification of latent space quality as a
search space. We investigate (1) whether further compression of the design
space via dimensionality reduction may facilitate optimization, (2) the
interpretability of the spaces, and (3) how organizing latent spaces with
physicochemical properties may improve the efficiency of optimizing
antimicrobial activity. We find that further reduction of the latent space via
dimensionality reduction can be advantageous when organizing the space with
more relevant information at data availability, that using the dimensionality
reduction search space can be more interpretable, and that we can organize the
latent space with different physicochemical properties even at different
percentages of available labels.

</details>


### [522] [CEPerFed: Communication-Efficient Personalized Federated Learning for Multi-Pulse MRI Classification](https://arxiv.org/abs/2510.17584)
*Ludi Li,Junbin Mao,Hanhe Lin,Xu Tian,Fang-Xiang Wu,Jin Liu*

Main category: cs.LG

TL;DR: 提出了CEPerFed方法，一种通信高效的个性化联邦学习框架，用于多脉冲MRI分类，解决了数据异构性和通信开销问题。


<details>
  <summary>Details</summary>
Motivation: 多脉冲MRI在临床实践中广泛应用，但训练鲁棒模型需要大量多样化数据，同时要保护隐私防止原始数据共享。联邦学习虽然可行，但面临数据异构性导致的模型收敛问题和大量参数传输带来的通信开销挑战。

Method: CEPerFed通过结合客户端历史风险梯度和历史平均梯度来协调局部和全局优化，前者用于加权其他客户端的贡献，后者确保局部更新与全局优化方向一致。同时采用分层SVD策略传输最关键的模型更新信息。

Result: 在五个分类任务上的实验证明了CEPerFed方法的有效性。

Conclusion: CEPerFed成功解决了联邦学习中的数据异构性和通信开销问题，为多脉冲MRI分类提供了一种高效实用的解决方案。

Abstract: Multi-pulse magnetic resonance imaging (MRI) is widely utilized for clinical
practice such as Alzheimer's disease diagnosis. To train a robust model for
multi-pulse MRI classification, it requires large and diverse data from various
medical institutions while protecting privacy by preventing raw data sharing
across institutions. Although federated learning (FL) is a feasible solution to
address this issue, it poses challenges of model convergence due to the effect
of data heterogeneity and substantial communication overhead due to large
numbers of parameters transmitted within the model. To address these
challenges, we propose CEPerFed, a communication-efficient personalized FL
method. It mitigates the effect of data heterogeneity by incorporating
client-side historical risk gradients and historical mean gradients to
coordinate local and global optimization. The former is used to weight the
contributions from other clients, enhancing the reliability of local updates,
while the latter enforces consistency between local updates and the global
optimization direction to ensure stable convergence across heterogeneous data
distributions. To address the high communication overhead, we propose a
hierarchical SVD (HSVD) strategy that transmits only the most critical
information required for model updates. Experiments on five classification
tasks demonstrate the effectiveness of the CEPerFed method. The code will be
released upon acceptance at https://github.com/LD0416/CEPerFed.

</details>


### [523] [ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification](https://arxiv.org/abs/2510.17650)
*Athanasios Angelakis,Amne Mousa,Micah L. A. Heldeweg,Laurens A. Biesheuvel,Mark A. Haaksma,Jasper M. Smit,Pieter R. Tuinman,Paul W. G. Elbers*

Main category: cs.LG

TL;DR: ZACH-ViT是一种轻量级视觉Transformer，用于区分心源性肺水肿与非心源性肺部疾病，在肺超声视频分类中表现优异，仅需0.25M参数。


<details>
  <summary>Details</summary>
Motivation: 由于非心源性炎症模式、间质性肺病和健康肺部在视觉上存在高度变异性，且重叠B线和胸膜伪影常见，使得肺超声视频中区分心源性肺水肿与其他情况变得困难。

Method: 提出ZACH-ViT模型，去除位置嵌入和[CLS]标记，实现完全排列不变性；引入ShuffleStrides数据增强技术，在保持解剖有效性的同时打乱探头视角序列和帧顺序。

Result: 在380个肺超声视频上评估，ZACH-ViT获得最高验证和测试ROC-AUC（0.80和0.79），平衡灵敏度（0.60）和特异性（0.91），而其他模型均失效。训练速度比Minimal ViT快1.35倍，参数减少2.5倍。

Conclusion: 将架构设计与数据结构对齐可以在小数据医学成像中超越规模效应，支持实时临床部署。

Abstract: Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and
structurally normal lungs in lung ultrasound (LUS) videos remains challenging
due to the high visual variability of non-cardiogenic inflammatory patterns
(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This
heterogeneity complicates automated classification as overlapping B-lines and
pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive
Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer
variant that removes both positional embeddings and the [CLS] token, making it
fully permutation-invariant and suitable for unordered medical image data. To
enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),
which permutes probe-view sequences and frame orders while preserving
anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95
critically ill patients against nine state-of-the-art baselines. Despite the
heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest
validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)
and specificity (0.91), while all competing models collapsed to trivial
classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with
2.5x fewer parameters, supporting real-time clinical deployment. These results
show that aligning architectural design with data structure can outperform
scale in small-data medical imaging.

</details>


### [524] [Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction](https://arxiv.org/abs/2510.17661)
*Vaishnavi Visweswaraiah,Tanvi Banerjee,William Romine*

Main category: cs.LG

TL;DR: 使用机器学习和深度学习技术（特别是GAN）来生成合成数据，解决自杀预测中数据极度不平衡的问题，并在真实测试数据上取得了良好的预测性能。


<details>
  <summary>Details</summary>
Motivation: 自杀预测是预防的关键，但真实数据中阳性样本极少，导致严重的类别不平衡问题，需要数据增强来改善模型性能。

Method: 使用机器学习模型（逻辑回归、随机森林、支持向量机）和深度学习技术（生成对抗网络GAN）生成合成数据样本来增强数据集。

Result: 在真实测试数据上，逻辑回归的加权精确度为0.99、召回率0.85、F1分数0.91；随机森林为0.98、0.99、0.99；支持向量机为0.99、0.76、0.86。LR和SVM正确识别了1个自杀尝试案例（敏感度1.0），RF识别了0个（敏感度0.0）。

Conclusion: 这些结果证明了模型的有效性，GAN在生成合成数据以支持自杀预防建模工作中发挥了关键作用。

Abstract: Suicide prediction is the key for prevention, but real data with sufficient
positive samples is rare and causes extreme class imbalance. We utilized
machine learning (ML) to build the model and deep learning (DL) techniques,
like Generative Adversarial Networks (GAN), to generate synthetic data samples
to enhance the dataset. The initial dataset contained 656 samples, with only
four positive cases, prompting the need for data augmentation. A variety of
machine learning models, ranging from interpretable data models to black box
algorithmic models, were used. On real test data, Logistic Regression (LR)
achieved a weighted precision of 0.99, a weighted recall of 0.85, and a
weighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99,
respectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86.
LR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and
misclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 &
0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0)
with 0 false positives (specificity: 1.0). These results highlight the models'
effectiveness, with GAN playing a key role in generating synthetic data to
support suicide prevention modeling efforts.

</details>


### [525] [On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration](https://arxiv.org/abs/2510.17670)
*Yehonathan Refael,Amit Aides,Aviad Barzilai,George Leifman,Genady Beryozkin,Vered Silverman,Bolous Jaber,Tomer Shekel*

Main category: cs.LG

TL;DR: 提出了一种级联方法，将预训练开放词汇检测模型与轻量级少样本分类器结合，通过主动学习策略FLAME实现快速适应，在遥感图像检测中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 开放词汇检测模型在遥感等专业领域存在自然语言模糊性问题，难以区分细粒度类别（如渔船和游艇），影响下游应用效果。

Method: 使用零-shot模型生成高召回率候选框，然后通过少量用户标注样本训练的轻量分类器进行精炼；核心是FLAME主动学习策略，通过密度估计和聚类选择信息量最大的样本。

Result: 在遥感基准测试中持续超越最先进方法，实现快速适应（不到一分钟），显著减少标注成本。

Conclusion: 建立了一个实用且资源高效的框架，使基础模型能够快速适应用户特定需求。

Abstract: Open-vocabulary object detection (OVD) models offer remarkable flexibility by
detecting objects from arbitrary text queries. However, their zero-shot
performance in specialized domains like Remote Sensing (RS) is often
compromised by the inherent ambiguity of natural language, limiting critical
downstream applications. For instance, an OVD model may struggle to distinguish
between fine-grained classes such as "fishing boat" and "yacht" since their
embeddings are similar and often inseparable. This can hamper specific user
goals, such as monitoring illegal fishing, by producing irrelevant detections.
To address this, we propose a cascaded approach that couples the broad
generalization of a large pre-trained OVD model with a lightweight few-shot
classifier. Our method first employs the zero-shot model to generate
high-recall object proposals. These proposals are then refined for high
precision by a compact classifier trained in real-time on only a handful of
user-annotated examples - drastically reducing the high costs of RS imagery
annotation.The core of our framework is FLAME, a one-step active learning
strategy that selects the most informative samples for training. FLAME
identifies, on the fly, uncertain marginal candidates near the decision
boundary using density estimation, followed by clustering to ensure sample
diversity. This efficient sampling technique achieves high accuracy without
costly full-model fine-tuning and enables instant adaptation, within less then
a minute, which is significantly faster than state-of-the-art alternatives.Our
method consistently surpasses state-of-the-art performance on RS benchmarks,
establishing a practical and resource-efficient framework for adapting
foundation models to specific user needs.

</details>


### [526] [Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning](https://arxiv.org/abs/2510.17690)
*Xihong Su*

Main category: cs.LG

TL;DR: 该论文提出了三个主要贡献：CADP算法连接策略梯度和动态规划，建立了ERM Bellman算子的收缩条件并提出了相关算法，以及提出了用于风险规避目标的Q学习算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决马尔可夫决策过程中模型不确定性和风险规避的问题，特别是在处理多模型MDPs和风险敏感目标时的算法设计挑战。

Method: 方法包括：1）CADP算法通过迭代调整模型权重实现单调策略改进；2）建立ERM Bellman算子的收缩条件并设计值迭代、策略迭代和线性规划算法；3）提出基于单调性的Q学习算法处理风险规避目标。

Result: 结果证明：CADP能保证策略单调改进到局部最优；ERM-TRC和EVaR-TRC存在确定性最优策略；提出的Q学习算法能收敛到最优风险规避值函数。

Conclusion: 结论是成功建立了策略梯度与动态规划的新联系，解决了ERM Bellman算子的收缩性问题，并开发了收敛的风险规避Q学习算法，为处理模型不确定性和风险敏感目标提供了有效工具。

Abstract: This dissertation makes three main contributions. First, We identify a new
connection between policy gradient and dynamic programming in MMDPs and propose
the Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov
policy that maximizes the discounted return averaged over the uncertain models.
CADP adjusts model weights iteratively to guarantee monotone policy
improvements to a local maximum. Second, We establish sufficient and necessary
conditions for the exponential ERM Bellman operator to be a contraction and
prove the existence of stationary deterministic optimal policies for ERM-TRC
and EVaR-TRC. We also propose exponential value iteration, policy iteration,
and linear programming algorithms for computing optimal stationary policies for
ERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for
computing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The
challenge is that Q-learning ERM Bellman may not be a contraction. Instead, we
use the monotonicity of Q-learning ERM Bellman operators to derive a rigorous
proof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the
optimal risk-averse value functions. The proposed Q-learning algorithms compute
the optimal stationary policy for ERM-TRC and EVaR-TRC.

</details>


### [527] [Closing the Sim2Real Performance Gap in RL](https://arxiv.org/abs/2510.17709)
*Akhil S Anand,Shambhuraj Sawant,Jasper Hoffmann,Dirk Reinhardt,Sebastien Gros*

Main category: cs.LG

TL;DR: 提出了一种新的Sim2Real框架，通过双层强化学习直接基于真实世界性能调整模拟器参数，以缩小Sim2Real性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前Sim2Real方法通过优化模拟器精度和变异性作为真实世界性能的代理指标，但这些指标与策略的实际性能不一定相关，导致模拟训练的策在真实环境中性能显著下降。

Method: 采用双层强化学习框架：内层RL在模拟中训练策略，外层RL调整模拟模型和模拟奖励参数，以最大化模拟策略在真实世界中的性能。

Result: 推导并验证了开发能够缩小Sim2Real性能差距的双层RL算法所需的数学工具。

Conclusion: 该框架直接基于真实世界性能优化模拟器参数，为解决Sim2Real性能差距问题提供了新思路。

Abstract: Sim2Real aims at training policies in high-fidelity simulation environments
and effectively transferring them to the real world. Despite the developments
of accurate simulators and Sim2Real RL approaches, the policies trained purely
in simulation often suffer significant performance drops when deployed in real
environments. This drop is referred to as the Sim2Real performance gap. Current
Sim2Real RL methods optimize the simulator accuracy and variability as proxies
for real-world performance. However, these metrics do not necessarily correlate
with the real-world performance of the policy as established theoretically and
empirically in the literature. We propose a novel framework to address this
issue by directly adapting the simulator parameters based on real-world
performance. We frame this problem as a bi-level RL framework: the inner-level
RL trains a policy purely in simulation, and the outer-level RL adapts the
simulation model and in-sim reward parameters to maximize real-world
performance of the in-sim policy. We derive and validate in simple examples the
mathematical tools needed to develop bi-level RL algorithms that close the
Sim2Real performance gap.

</details>


### [528] [Enabling Fine-Grained Operating Points for Black-Box LLMs](https://arxiv.org/abs/2510.17727)
*Ege Beyazit,KL Navaneet,Prashant Mathur,Roi Blanco,Vidit Bansal,Karim Bouyarmane*

Main category: cs.LG

TL;DR: 该论文研究如何提高黑盒大语言模型作为分类器时的操作粒度，通过分析其低基数数值输出的原因，并提出了有效方法来显著增加可用操作点数量，实现更精细的决策行为调整。


<details>
  <summary>Details</summary>
Motivation: 黑盒LLMs在需要特定指标约束的应用中表现不佳，因为其数值输出基数低，限制了操作点的控制能力，无法进行细粒度的决策行为调整。

Method: 首先分析LLMs低基数数值输出的原因，发现其偏向生成四舍五入但信息丰富的语言化概率；然后实验标准提示工程、不确定性估计和置信度激发技术；最后提出有效方法来显著增加可用操作点数量和多样性。

Result: 提出的方法在11个数据集和3个LLMs上提供了更细粒度的操作点，并实现了与基准方法相当或更好的性能。

Conclusion: 通过提高黑盒LLMs的操作粒度，可以在不损失性能的情况下实现更精细的决策行为控制，为需要特定指标约束的应用提供了实用解决方案。

Abstract: Black-box Large Language Models (LLMs) provide practical and accessible
alternatives to other machine learning methods, as they require minimal labeled
data and machine learning expertise to develop solutions for various decision
making problems. However, for applications that need operating with constraints
on specific metrics (e.g., precision $\geq$ 95%), decision making with
black-box LLMs remains unfavorable, due to their low numerical output
cardinalities. This results in limited control over their operating points,
preventing fine-grained adjustment of their decision making behavior. In this
paper, we study using black-box LLMs as classifiers, focusing on efficiently
improving their operational granularity without performance loss. Specifically,
we first investigate the reasons behind their low-cardinality numerical outputs
and show that they are biased towards generating rounded but informative
verbalized probabilities. Then, we experiment with standard prompt engineering,
uncertainty estimation and confidence elicitation techniques, and observe that
they do not effectively improve operational granularity without sacrificing
performance or increasing inference cost. Finally, we propose efficient
approaches to significantly increase the number and diversity of available
operating points. Our proposed approaches provide finer-grained operating
points and achieve comparable to or better performance than the benchmark
methods across 11 datasets and 3 LLMs.

</details>


### [529] [Prediction of Sea Ice Velocity and Concentration in the Arctic Ocean using Physics-informed Neural Network](https://arxiv.org/abs/2510.17756)
*Younghyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: 开发物理信息神经网络(PINN)策略，将海冰物理知识整合到机器学习模型中，以改进北极海冰速度和浓度的预测。


<details>
  <summary>Details</summary>
Motivation: 纯数据驱动的机器学习模型在泛化性和物理一致性方面存在局限，特别是在北极海冰变薄和加速融化的新阶段，历史数据训练的模型可能无法充分代表未来动态变化的海冰条件。

Method: 基于分层信息共享U-net架构，引入物理损失函数和激活函数，生成物理上合理的海冰速度和浓度输出。

Result: PINN模型在海冰速度和浓度的日常预测中优于纯数据驱动模型，即使使用少量样本训练也能表现良好，特别是在融化和早期冻结季节以及快速移动冰区显著改善了海冰浓度预测。

Conclusion: 物理信息神经网络方法能够有效整合物理知识，提高海冰预测的准确性和物理一致性，特别是在数据稀缺或海冰条件快速变化的情况下。

Abstract: As an increasing amount of remote sensing data becomes available in the
Arctic Ocean, data-driven machine learning (ML) techniques are becoming widely
used to predict sea ice velocity (SIV) and sea ice concentration (SIC).
However, fully data-driven ML models have limitations in generalizability and
physical consistency due to their excessive reliance on the quantity and
quality of training data. In particular, as Arctic sea ice entered a new phase
with thinner ice and accelerated melting, there is a possibility that an ML
model trained with historical sea ice data cannot fully represent the
dynamically changing sea ice conditions in the future. In this study, we
develop physics-informed neural network (PINN) strategies to integrate physical
knowledge of sea ice into the ML model. Based on the Hierarchical
Information-sharing U-net (HIS-Unet) architecture, we incorporate the physics
loss function and the activation function to produce physically plausible SIV
and SIC outputs. Our PINN model outperforms the fully data-driven model in the
daily predictions of SIV and SIC, even when trained with a small number of
samples. The PINN approach particularly improves SIC predictions in melting and
early freezing seasons and near fast-moving ice regions.

</details>


### [530] [Atlas-based Manifold Representations for Interpretable Riemannian Machine Learning](https://arxiv.org/abs/2510.17772)
*Ryan A. Robinett,Sophia A. Madejski,Kyle Ruark,Samantha J. Riesenfeld,Lorenzo Orecchia*

Main category: cs.LG

TL;DR: 本文提出了一种基于微分图册的流形学习方法，通过维护可微分图册实现流形上的黎曼优化，在效率和准确性方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 当前流形学习方法主要进行降维到欧几里得空间，当嵌入维度接近流形内在维度时会丢失关键特征。而直接学习流形作为微分图册的方法相对较少被探索。

Method: 实现了一个通用数据结构来维护可微分图册，支持流形上的黎曼优化，并结合无监督启发式方法从点云数据学习微分图册。

Result: 实验证明该方法在选定场景下具有效率和准确性优势。在Klein瓶上的监督分类任务和造血数据的RNA速度分析中，展示了更好的可解释性和鲁棒性。

Conclusion: 基于图册的方法在流形学习方面具有有效性和潜力，为直接处理流形数据提供了新的可能性。

Abstract: Despite the popularity of the manifold hypothesis, current manifold-learning
methods do not support machine learning directly on the latent $d$-dimensional
data manifold, as they primarily aim to perform dimensionality reduction into
$\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$
approaches $d$.
  On the other hand, methods that directly learn the latent manifold as a
differentiable atlas have been relatively underexplored.
  In this paper, we aim to give a proof of concept of the effectiveness and
potential of atlas-based methods. To this end, we implement a generic data
structure to maintain a differentiable atlas that enables Riemannian
optimization over the manifold. We complement this with an unsupervised
heuristic that learns a differentiable atlas from point cloud data. We
experimentally demonstrate that this approach has advantages in terms of
efficiency and accuracy in selected settings. Moreover, in a supervised
classification task over the Klein bottle and in RNA velocity analysis of
hematopoietic data, we showcase the improved interpretability and robustness of
our approach.

</details>


### [531] [Inference-Time Compute Scaling For Flow Matching](https://arxiv.org/abs/2510.17786)
*Adam Stecklov,Noah El Rimawi-Fine,Mathieu Blanchette*

Main category: cs.LG

TL;DR: 本文提出了在推理时保持线性插值的流匹配缩放方法，首次应用于无条件蛋白质生成，证明随着推理计算增加，样本质量持续提升。


<details>
  <summary>Details</summary>
Motivation: 当前流匹配方法在推理时缩放研究不足，现有方法牺牲了流匹配的高效直线采样特性，且仅应用于视觉任务，缺乏科学领域的应用验证。

Method: 开发了保持线性插值的推理时缩放程序，在采样过程中不改变原始流匹配的线性插值特性。

Result: 在图像生成和无条件蛋白质生成任务中，随着推理计算增加，样本质量持续改善，证明该方法可应用于科学领域。

Conclusion: 流匹配推理时缩放方法能有效提升样本质量，且适用于科学领域，为流匹配在更广泛领域的应用提供了新途径。

Abstract: Allocating extra computation at inference time has recently improved sample
quality in large language models and diffusion-based image generation. In
parallel, Flow Matching (FM) has gained traction in language, vision, and
scientific domains, but inference-time scaling methods for it remain
under-explored. Concurrently, Kim et al., 2025 approach this problem but
replace the linear interpolant with a non-linear variance-preserving (VP)
interpolant at inference, sacrificing FM's efficient and straight sampling.
Additionally, inference-time compute scaling for flow matching has only been
applied to visual tasks, like image generation. We introduce novel
inference-time scaling procedures for FM that preserve the linear interpolant
during sampling. Evaluations of our method on image generation, and for the
first time (to the best of our knowledge), unconditional protein generation,
show that I) sample quality consistently improves as inference compute
increases, and II) flow matching inference-time scaling can be applied to
scientific domains.

</details>
