<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 75]
- [cs.AI](#cs.AI) [Total: 55]
- [eess.SY](#eess.SY) [Total: 24]
- [q-fin.PR](#q-fin.PR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 140]
- [q-fin.MF](#q-fin.MF) [Total: 3]
- [econ.EM](#econ.EM) [Total: 6]
- [cs.CY](#cs.CY) [Total: 6]
- [stat.ML](#stat.ML) [Total: 13]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [math.OC](#math.OC) [Total: 19]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [The Qualitative Laboratory: Theory Prototyping and Hypothesis Generation with Large Language Models](https://arxiv.org/abs/2601.00797)
*Hugues Draelants*

Main category: cs.CL

TL;DR: 本文提出使用大语言模型进行社会学角色模拟作为"定性实验室"，用于生成关于不同社会群体如何解读新信息的丰富定性假设，克服了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 社会科学面临的核心挑战是如何生成关于不同社会群体如何解读新信息的丰富定性假设。现有方法如小插曲调查缺乏话语深度，基于规则的ABM模型存在形式化瓶颈，需要新的方法论来克服这些限制。

Method: 提出社会学角色模拟方法，使用大语言模型作为"定性实验室"。具体协议包括：从社会学理论中推导角色（如气候接收理论），让这些角色对政策信息做出反应，生成自然主义话语，操作化复杂世界观。

Result: 模拟产生了细致且反直觉的假设，例如保守派角色拒绝国家安全框架，这挑战了理论假设。方法展示了生成深度结构化假设的能力，可用于后续实证检验。

Conclusion: 该方法作为"模拟后验证"工作流程的一部分，代表了生成深度结构化假设的优越工具，为后续实证测试提供了更好的基础，在特定任务上比现有方法具有明显优势。

Abstract: A central challenge in social science is to generate rich qualitative hypotheses about how diverse social groups might interpret new information. This article introduces and illustrates a novel methodological approach for this purpose: sociological persona simulation using Large Language Models (LLMs), which we frame as a "qualitative laboratory". We argue that for this specific task, persona simulation offers a distinct advantage over established methods. By generating naturalistic discourse, it overcomes the lack of discursive depth common in vignette surveys, and by operationalizing complex worldviews through natural language, it bypasses the formalization bottleneck of rule-based agent-based models (ABMs). To demonstrate this potential, we present a protocol where personas derived from a sociological theory of climate reception react to policy messages. The simulation produced nuanced and counter-intuitive hypotheses - such as a conservative persona's rejection of a national security frame - that challenge theoretical assumptions. We conclude that this method, used as part of a "simulation then validation" workflow, represents a superior tool for generating deeply textured hypotheses for subsequent empirical testing.

</details>


### [2] [Rate-Distortion Analysis of Compressed Query Delegation with Low-Rank Riemannian Updates](https://arxiv.org/abs/2601.00938)
*Faruk Alpay,Bugra Kilictas*

Main category: cs.CL

TL;DR: 压缩查询委托(CQD)通过将高维推理状态压缩为低秩张量查询，委托给外部oracle，再通过黎曼优化更新状态，解决有界上下文代理的工作记忆限制问题。


<details>
  <summary>Details</summary>
Motivation: 有界上下文代理在中间推理超过有效工作记忆预算时会失败，需要一种方法来压缩推理状态并委托给外部oracle，同时保持推理质量。

Method: 1) 将高维潜在推理状态压缩为低秩张量查询；2) 将最小化查询委托给外部oracle；3) 在固定秩流形上通过黎曼优化更新潜在状态。建立了CQD作为带查询预算功能的约束随机规划数学模型。

Result: 理论证明：谱硬阈值对于约束二次失真问题是最优的；在有界oracle噪声和平滑性假设下，黎曼随机近似具有收敛保证。实证：在2,500项有界上下文推理任务上优于思维链基线；人类认知镜像基准(N=200)测量了认知增益和语义漂移。

Conclusion: CQD提供了一种数学严谨的方法来解决有界上下文代理的工作记忆限制问题，通过压缩查询委托和黎曼优化实现了有效的推理扩展，并在理论和实证上都取得了良好结果。

Abstract: Bounded-context agents fail when intermediate reasoning exceeds an effective working-memory budget. We study compressed query delegation (CQD): (i) compress a high-dimensional latent reasoning state into a low-rank tensor query, (ii) delegate the minimal query to an external oracle, and (iii) update the latent state via Riemannian optimization on fixed-rank manifolds. We give a math-first formulation: CQD is a constrained stochastic program with a query-budget functional and an oracle modeled as a noisy operator. We connect CQD to classical rate-distortion and information bottleneck principles, showing that spectral hard-thresholding is optimal for a natural constrained quadratic distortion problem, and we derive convergence guarantees for Riemannian stochastic approximation under bounded oracle noise and smoothness assumptions. Empirically, we report (A) a 2,500-item bounded-context reasoning suite (BBH-derived tasks plus curated paradox instances) comparing CQD against chain-of-thought baselines under fixed compute and context; and (B) a human "cognitive mirror" benchmark (N=200) measuring epistemic gain and semantic drift across modern oracles.

</details>


### [3] [Intention Collapse: Intention-Level Metrics for Reasoning in Language Models](https://arxiv.org/abs/2601.01011)
*Patricio Vera*

Main category: cs.CL

TL;DR: 论文提出"意图坍缩"概念，即语言生成时将高维内部状态压缩为单一token序列的过程，并定义了三个意图度量指标来研究推理计算如何影响语言模型内部意图的表达。


<details>
  <summary>Details</summary>
Motivation: 研究语言生成过程中内部丰富意图如何被压缩为单一语言输出的过程，理解推理时计算如何塑造语言模型的内部意图，以及这些意图在表达过程中的信息损失。

Method: 提出意图坍缩的形式化定义，定义三个模型无关的意图度量指标（意图熵、有效维度、潜在知识可恢复性），并通过实验比较直接回答、思维链和随机生成三种推理机制。

Result: 思维链将准确率从5.5%提升至53%，显著降低坍缩前意图熵，提高有效维度。意图熵在项目层面预测能力有限，线性探测在思维链机制下AUROC为0.65，但在基线机制下接近随机猜测。

Conclusion: 意图层面度量能够区分不同推理机制，揭示在坍缩过程中部分丢失的潜在信息，但当前代理指标仍有重要局限性，为研究语言模型内部意图表达提供了新框架。

Abstract: Every act of language generation compresses a rich internal state into a single token sequence. We call this process intention collapse: a many-to-one projection from a high dimensional intention space I into an external language space L. We formalize intention collapse for contemporary language models, define three simple, model agnostic intention metrics (intention entropy Hint, effective dimensionality dimeff, and latent knowledge recoverability Recov), and propose an empirical agenda for studying how inference time computation shapes internal intentions before they are verbalized. We also report a first small scale experiment. Using a 4 bit Mistral 7B model on 200 GSM8K problems, we compare a direct answer baseline, a chain of thought (CoT) regime, and a babble control. CoT raises accuracy from 5.5 percent to 53 percent, sharply reduces pre collapse intention entropy (from 1.42 to 0.37 bits), and shows higher global effective dimensionality than the other regimes despite producing fewer tokens than babble. At the same time, Hint has little item level predictive power, and a linear probe on I achieves AUROC 0.65 in the CoT regime but only about chance in the baseline regime, where it collapses to the majority class. These preliminary results indicate that intention level metrics can distinguish inference regimes and expose latent information that is partly lost during collapse, while also revealing important limitations of our current proxies

</details>


### [4] [HyperJoin: LLM-augmented Hypergraph Link Prediction for Joinable Table Discovery](https://arxiv.org/abs/2601.01015)
*Shiyuan Liu,Jianwei Wang,Xuemin Lin,Lu Qin,Wenjie Zhang,Ying Zhang*

Main category: cs.CL

TL;DR: HyperJoin：基于大语言模型增强超图框架的Joinable表发现方法，通过构建超图建模表内和表间结构，将任务转化为超图链接预测，并引入层次交互网络和重排序模块提升结果一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言模型的Joinable表发现方法存在两个主要问题：1）离线阶段将表格建模为孤立或成对列，难以捕捉丰富的表间和表内结构信息；2）在线排序仅基于查询-候选相似度，忽略候选列之间的相互影响，导致结果集不连贯。

Method: 1）构建超图，包含表内超边和LLM增强的表间超边；2）将Joinable表发现任务转化为超图链接预测；3）设计层次交互网络（HIN），通过列和超边的双向消息传递学习表达性列表示；4）将在线排序转化为一致性感知的top-k列选择问题；5）引入重排序模块，使用最大生成树算法剪枝噪声连接并最大化一致性。

Result: 实验表明HyperJoin优于现有方法，在Precision@15和Recall@15指标上分别平均提升21.4%和17.2%。

Conclusion: HyperJoin通过超图建模和层次交互网络有效解决了现有方法在结构信息捕捉和结果一致性方面的不足，显著提升了Joinable表发现的性能。

Abstract: As a pivotal task in data lake management, joinable table discovery has attracted widespread interest. While existing language model-based methods achieve remarkable performance by combining offline column representation learning with online ranking, their design insufficiently accounts for the underlying structural interactions: (1) offline, they directly model tables into isolated or pairwise columns, thereby struggling to capture the rich inter-table and intra-table structural information; and (2) online, they rank candidate columns based solely on query-candidate similarity, ignoring the mutual interactions among the candidates, leading to incoherent result sets. To address these limitations, we propose HyperJoin, a large language model (LLM)-augmented Hypergraph framework for Joinable table discovery. Specifically, we first construct a hypergraph to model tables using both the intra-table hyperedges and the LLM-augmented inter-table hyperedges. Consequently, the task of joinable table discovery is formulated as link prediction on this constructed hypergraph. We then design HIN, a Hierarchical Interaction Network that learns expressive column representations through bidirectional message passing over columns and hyperedges. To strengthen coherence and internal consistency in the result columns, we cast online ranking as a coherence-aware top-k column selection problem. We then introduce a reranking module that leverages a maximum spanning tree algorithm to prune noisy connections and maximize coherence. Experiments demonstrate the superiority of HyperJoin, achieving average improvements of 21.4% (Precision@15) and 17.2% (Recall@15) over the best baseline.

</details>


### [5] [Multi-Dimensional Prompt Chaining to Improve Open-Domain Dialogue Generation](https://arxiv.org/abs/2601.01037)
*Livia Leong Hui Teng*

Main category: cs.CL

TL;DR: 提出多维度提示链框架，通过自然性、连贯性和吸引性三个维度提升小语言模型在开放域对话中的人类相似度，使7B模型达到与70B模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在部署上有优势，但在开放域对话质量上难以匹敌大模型，需要资源高效的方法来提升其对话质量。

Method: 提出多维度提示链框架，整合自然性、连贯性和吸引性三个维度，应用于TinyLlama和Llama-2-7B模型，并与大模型进行对比评估。

Result: 完整框架将响应多样性提升达29%，上下文连贯性提升达28%，吸引性和自然性提升达29%。Llama-2-7B达到与Llama-2-70B和GPT-3.5 Turbo相当的性能。

Conclusion: 精心设计的基于提示的策略为提升小语言模型开放域对话质量提供了有效且资源高效的途径。

Abstract: Small language models (SLMs) offer significant deployment advantages but often struggle to match the dialogue quality of larger models in open-domain settings. In this paper, we propose a multi-dimensional prompt-chaining framework that integrates Naturalness, Coherence, and Engagingness dimensions to enhance human-likeness in open-domain dialogue generation. We apply the framework to two SLMs, TinyLlama and Llama-2-7B, and benchmark their performance against responses generated by substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. We then employ automatic and human evaluation to assess the responses based on diversity, contextual coherence, as well as overall quality. Results show that the full framework improves response diversity by up to 29%, contextual coherence by up to 28%, and engagingness as well as naturalness by up to 29%. Notably, Llama-2-7B achieves performance comparable to substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. Overall, the findings demonstrate that carefully designed prompt-based strategies provide an effective and resource-efficient pathway to improving open-domain dialogue quality in SLMs.

</details>


### [6] [KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs](https://arxiv.org/abs/2601.01046)
*Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: KV-Embedding：通过重定向LLM最后一层token的KV状态作为前缀，激活冻结大语言模型的表征能力，在无需训练的情况下提升语义嵌入效果。


<details>
  <summary>Details</summary>
Motivation: LLM在无需训练的场景下存在两个结构性问题：因果注意力机制限制早期token访问后续上下文，以及下一token预测目标使表征偏向生成而非语义压缩。需要激活冻结LLM的潜在表征能力。

Method: 提出KV-Embedding框架，利用观察到的现象：每层最后一个token的key-value状态编码了序列的压缩视图。将这些状态重定向为预置前缀，使所有token能在单次前向传播中访问序列级上下文。引入基于内在维度的自动化层选择策略确保模型无关性。

Result: 在MTEB基准测试中，使用Qwen、Mistral和Llama骨干网络，KV-Embedding比现有无需训练基线提升高达10%，在长达4,096个token的序列上保持稳健性能。

Conclusion: 内部状态操作为输入修改提供了高效替代方案，这项工作鼓励进一步探索LLM内部机制用于表征学习。

Abstract: While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning.

</details>


### [7] [Unsupervised Text Style Transfer for Controllable Intensity](https://arxiv.org/abs/2601.01060)
*Shuhuan Gu,Wenbiao Tao,Xinchen Ma,Kangkang He,Ye Guo,Xiang Li,Yunshi Lan*

Main category: cs.CL

TL;DR: 提出SFT-then-PPO范式用于无监督文本风格强度可控迁移，通过合成平行数据微调LLM，再使用分层奖励设计的PPO进一步训练，有效提升风格强度区分能力。


<details>
  <summary>Details</summary>
Motivation: 无监督文本风格迁移中，风格强度可控迁移更具挑战性，因为不同强度级别间的风格特征差异细微，且缺乏平行数据，相邻强度级别难以区分。

Method: 提出SFT-then-PPO两阶段方法：1）使用合成平行数据对LLM进行监督微调；2）使用PPO进一步训练，设计分层奖励函数同时考虑全局和局部风格特征来区分风格强度。

Result: 在两个UTST基准测试中，两种奖励函数各有优势，应用于LLM微调能有效提升基于各种评估指标的LLM骨干性能，即使对于相近强度级别也能观察到明显的风格差异。

Conclusion: 提出的SFT-then-PPO范式能有效解决无监督文本风格强度可控迁移的挑战，通过精心设计的奖励函数实现了对细微风格强度差异的准确控制。

Abstract: Unsupervised Text Style Transfer (UTST) aims to build a system to transfer the stylistic properties of a given text without parallel text pairs. Compared with text transfer between style polarities, UTST for controllable intensity is more challenging due to the subtle differences in stylistic features across different intensity levels. Faced with the challenges posed by the lack of parallel data and the indistinguishability between adjacent intensity levels, we propose a SFT-then-PPO paradigm to fine-tune an LLM. We first fine-tune the LLM with synthesized parallel data. Then, we further train the LLM with PPO, where the rewards are elaborately designed for distinguishing the stylistic intensity in hierarchical levels. Both the global and local stylistic features are considered to formulate the reward functions. The experiments on two UTST benchmarks showcase that both rewards have their advantages and applying them to LLM fine-tuning can effectively improve the performance of an LLM backbone based on various evaluation metrics. Even for close levels of intensity, we can still observe the noticeable stylistic difference between the generated text.

</details>


### [8] [ks-lit-3m: A 3.1 million word kashmiri text dataset for large language model pretraining](https://arxiv.org/abs/2601.01091)
*Haq Nawaz Malik*

Main category: cs.CL

TL;DR: 该论文针对克什米尔语缺乏高质量训练数据的问题，构建了包含310万单词的KS-LIT-3M语料库，通过开发专门的InPage转Unicode转换器解决了历史文献的编码障碍。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在克什米尔语（约700万人使用）上表现不佳，主要原因是缺乏高质量训练数据。数十年的克什米尔语文献因使用专有的InPage桌面出版格式编码而无法被现代NLP流程处理。

Method: 开发专门的InPage-to-Unicode转换器，然后进行严格的预处理（包括去除英语污染、字符标准化和质量验证），构建了包含310万单词（1640万字符）的KS-LIT-3M语料库，涵盖文学、新闻、学术和宗教等多种体裁。

Result: 创建了包含131,607个独特单词的克什米尔语语料库，采用单连续线性文本流格式，优化用于因果语言模型训练。数据集以CC-BY-4.0许可证发布，填补了克什米尔语语言技术的基础资源空白。

Conclusion: KS-LIT-3M语料库解决了克什米尔语NLP研究的关键数据稀缺问题，为训练克什米尔语语言模型提供了必要的基础设施，有助于促进克什米尔语自然语言处理研究的发展。

Abstract: Large Language Models (LLMs) demonstrate remarkable fluency across high-resource languages yet consistently fail to generate coherent text in Kashmiri, a language spoken by approximately seven million people. This performance disparity stems not from inherent model limitations but from a critical scarcity of high-quality training data. Decades of Kashmiri literature remain inaccessible to modern NLP pipelines due to their encoding in the proprietary InPage desktop publishing format. This paper introduces KS-LIT-3M, a curated corpus of 3.1 million words (16.4 million characters) specifically designed for pretraining language models on Kashmiri. The dataset is structured as a single continuous linear text stream, optimized for causal language model training where models learn to predict subsequent tokens from preceding context. The corpus was constructed through the development of a specialized InPage-to-Unicode converter, followed by rigorous preprocessing including English contamination removal, character normalization, and quality validation. Encompassing 131,607 unique words drawn from diverse genres including literary works, journalistic writing, academic texts, and religious scholarship, KS-LIT-3M addresses a fundamental resource gap for Kashmiri language technology. The dataset is released under the CC-BY-4.0 license to facilitate research in Kashmiri natural language processing.

</details>


### [9] [EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation](https://arxiv.org/abs/2601.01112)
*Zilin Li,Weiwei Xu,Xuanbo Lu,Zheda Liu*

Main category: cs.CL

TL;DR: EmoLoom-2B是一个轻量级可复现的流程，可将小于20亿参数的小语言模型转化为情感分类和VAD预测的快速筛选候选模型，通过统一协议、语义正则化和数据增强实现高效评估。


<details>
  <summary>Details</summary>
Motivation: 需要一种轻量级、可复现的流程，将小型语言模型转化为情感分析和VAD预测的快速筛选工具，确保评估协议一致、减少方差，为后续更重的训练或多模态融合提供可靠的筛选阶段。

Method: 1) 统一JSON输入输出协议确保评估一致性；2) 采用KV-off解码减少方差；3) 引入两个正交语义正则器：VAD保持约束和轻量外部评估分类器；4) 基于镜像情感对的Valence Flip增强；5) 监督微调中使用A/B混合采样和熵感知温度调度。

Result: 基于Qwen-1.8B-Chat的EmoLoom-2B在GoEmotions和EmpatheticDialogues上表现强劲，在DailyDialog上展示出稳健的跨语料库泛化能力，证明了该流程的有效性。

Conclusion: EmoLoom-2B提供了一个预算友好、可审计、可重入的可靠筛选流程，可作为重型训练或多模态融合前的有效筛选阶段，为情感分析任务提供了轻量级解决方案。

Abstract: We introduce EmoLoom-2B, a lightweight and reproducible pipeline that turns small language models under 2B parameters into fast screening candidates for joint emotion classification and Valence-Arousal-Dominance prediction. To ensure protocol-faithful and fair evaluation, we unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting. We incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples, and a lightweight external appraisal classifier that provides training-time guidance on goal attainment, controllability, certainty, and fairness without injecting long rationales. To improve polarity sensitivity, we introduce Valence Flip augmentation based on mirrored emotional pairs. During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence. Using Qwen-1.8B-Chat as the base model, EmoLoom-2B achieves strong performance on GoEmotions and EmpatheticDialogues, and demonstrates robust cross-corpus generalization on DailyDialog. The proposed recipe is budget-aware, auditable, and re-entrant, serving as a dependable screening pass before heavier training or multimodal fusion.

</details>


### [10] [Listen, Attend, Understand: a Regularization Technique for Stable E2E Speech Translation Training on High Variance labels](https://arxiv.org/abs/2601.01121)
*Yacouba Diarra,Michael Leventhal*

Main category: cs.CL

TL;DR: LAU是一种语义正则化技术，通过冻结文本嵌入提供方向性辅助损失，在训练期间约束声学编码器的潜在空间，为端到端语音翻译注入语言基础性，不增加推理成本。


<details>
  <summary>Details</summary>
Motivation: 端到端语音翻译在目标转录具有高方差和语义模糊性时，通常表现出收敛速度慢和性能较差的问题。需要一种方法在训练数据稀缺和/或嘈杂的情况下改善性能。

Method: 提出Listen, Attend, Understand (LAU)语义正则化技术，利用冻结的文本嵌入提供方向性辅助损失，在训练期间约束声学编码器的潜在空间，将语言基础性注入声学表示中。

Result: 在30小时班巴拉语到法语数据集上，LAU模型与使用100%更多数据预训练的E2E-ST系统相比，在标准指标上达到可比性能，同时在保留语义含义方面表现更好。引入总参数漂移指标量化正则化的结构影响。

Conclusion: LAU是后处理重评分的稳健替代方案，是E2E-ST训练的有价值补充，特别是在训练数据稀缺和/或嘈杂的情况下。语义约束主动重组编码器权重，优先考虑意义而非字面语音。

Abstract: End-to-End Speech Translation often shows slower convergence and worse performance when target transcriptions exhibit high variance and semantic ambiguity. We propose Listen, Attend, Understand (LAU), a semantic regularization technique that constrains the acoustic encoder's latent space during training. By leveraging frozen text embeddings to provide a directional auxiliary loss, LAU injects linguistic groundedness into the acoustic representation without increasing inference cost. We evaluate our method on a Bambara-to-French dataset with 30 hours of Bambara speech translated by non-professionals. Experimental results demonstrate that LAU models achieve comparable performance by standard metrics compared to an E2E-ST system pretrained with 100\% more data and while performing better in preserving semantic meaning. Furthermore, we introduce Total Parameter Drift as a metric to quantify the structural impact of regularization to demonstrate that semantic constraints actively reorganize the encoder's weights to prioritize meaning over literal phonetics. Our findings suggest that LAU is a robust alternative to post-hoc rescoring and a valuable addition to E2E-ST training, especially when training data is scarce and/or noisy.

</details>


### [11] [RoboPhD: Self-Improving Text-to-SQL Through Autonomous Agent Evolution](https://arxiv.org/abs/2601.01126)
*Andrew Borthwick,Stephen Ash*

Main category: cs.CL

TL;DR: RoboPhD是一个AI自主研究系统，通过进化循环自动改进Text-to-SQL性能，无需领域指导，从70行基线进化到1500行，在BIRD数据集上达到73.67%准确率。


<details>
  <summary>Details</summary>
Motivation: 探索AI系统能否自主进行科学研究，从简单起点自动进化出高性能的Text-to-SQL代理，减少人工干预和领域专业知识需求。

Method: 采用闭环进化循环：SQL生成代理（数据库分析脚本+SQL生成指令）和进化代理（基于性能反馈设计新版本），使用ELO选择机制处理非传递性性能，通过迭代交叉授粉进化。

Result: 从70行基线进化18次到1500行代理，自主发现大小自适应数据库分析、列选择、证据解释和聚合等策略。在BIRD测试集上达到73.67%准确率，对较弱模型提升更大（Haiku提升8.9点），实现"跳级部署"：进化Haiku超过原生Sonnet，进化Sonnet超过原生Opus。

Conclusion: AI能够从简单起点自主构建强大的代理系统，进化对廉价模型提升最大，实现成本效益的"跳级部署"，展示了AI自主研究的潜力。

Abstract: We present RoboPhD, a system where AI agents autonomously conduct research to improve Text-to-SQL performance. RoboPhD implements a closed-loop evolution cycle with two coordinated components: a SQL Generation agent composed of a database analysis script and SQL generation instructions, and an Evolution agent that designs new versions based on performance feedback. Central to the framework is an ELO-based selection mechanism enabling survival-of-the-fittest dynamics while handling non-transitivity in performance. Starting from a naive 70-line baseline, RoboPhD evolves agents through iterative cross-pollination, discovering effective techniques without any external guidance on the Text-to-SQL domain. Our best agent, evolved to 1500 lines over 18 iterations, autonomously discovered strategies such as size-adaptive database analysis that adjusts depth based on schema complexity and SQL generation patterns for column selection, evidence interpretation, and aggregation. Evolution provides the largest gains on cheaper models: while we improve by 2.3 points over a strong Claude Opus 4.5 naive baseline, we show an improvement of 8.9 points over the weaker Claude Haiku model. This enables 'skip a tier' deployment: evolved Haiku exceeds naive Sonnet accuracy, and evolved Sonnet exceeds naive Opus, both at lower cost. The full system achieves 73.67% accuracy on the BIRD test set, demonstrating that AI can autonomously build a strong agentic system with only a trivial human-provided starting point.

</details>


### [12] [ARCADE: A City-Scale Corpus for Fine-Grained Arabic Dialect Tagging](https://arxiv.org/abs/2601.02209)
*Omer Nacar,Serry Sibaee,Adel Ammar,Yasser Alhabashi,Nadia Samer Sibai,Yara Farouk Ahmed,Ahmed Saud Alqusaiyer,Sulieman Mahmoud AlMahmoud,Abdulrhman Mamdoh Mukhaniq,Lubaba Raed,Sulaiman Mohammed Alatwah,Waad Nasser Alqahtani,Yousif Abdulmajeed Alnasser,Mohamed Aziz Khadraoui,Wadii Boulila*

Main category: cs.CL

TL;DR: ARCADE是首个城市级细粒度阿拉伯语方言语音数据集，包含来自19个国家58个城市的3790个音频片段和6907个标注，支持方言识别等多任务学习。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语方言在语音和词汇上存在显著地域差异，但现有多方言数据集缺乏城市级细粒度标注，限制了方言识别研究的深入发展。

Method: 从阿拉伯世界流媒体服务收集广播语音，截取30秒片段，由1-3名阿拉伯语母语者标注情感、语音类型、方言类别和有效性等元数据。

Result: 构建了包含6907个标注和3790个独特音频片段的数据集，涵盖19个国家58个城市，支持城市级方言标注的基准测试。

Conclusion: ARCADE填补了阿拉伯语城市级方言数据集的空白，为细粒度方言识别和多任务学习提供了可靠基准，数据集已公开可用。

Abstract: The Arabic language is characterized by a rich tapestry of regional dialects that differ substantially in phonetics and lexicon, reflecting the geographic and cultural diversity of its speakers. Despite the availability of many multi-dialect datasets, mapping speech to fine-grained dialect sources, such as cities, remains underexplored. We present ARCADE (Arabic Radio Corpus for Audio Dialect Evaluation), the first Arabic speech dataset designed explicitly with city-level dialect granularity. The corpus comprises Arabic radio speech collected from streaming services across the Arab world. Our data pipeline captures 30-second segments from verified radio streams, encompassing both Modern Standard Arabic (MSA) and diverse dialectal speech. To ensure reliability, each clip was annotated by one to three native Arabic reviewers who assigned rich metadata, including emotion, speech type, dialect category, and a validity flag for dialect identification tasks. The resulting corpus comprises 6,907 annotations and 3,790 unique audio segments spanning 58 cities across 19 countries. These fine-grained annotations enable robust multi-task learning, serving as a benchmark for city-level dialect tagging. We detail the data collection methodology, assess audio quality, and provide a comprehensive analysis of label distributions. The dataset is available on: https://huggingface.co/datasets/riotu-lab/ARCADE-full

</details>


### [13] [KOS-TL (Knowledge Operation System Type Logic)](https://arxiv.org/abs/2601.01143)
*Peng Chen*

Main category: cs.CL

TL;DR: KOS-TL是一个基于依赖类型理论的构造性框架，旨在为自主可执行知识系统提供严格的逻辑基础，通过三层架构统一数据、逻辑和证明，确保系统在状态转换中保持逻辑自洽。


<details>
  <summary>Details</summary>
Motivation: 传统知识表示模型存在静态符号逻辑与动态系统执行之间的鸿沟，需要一种能够统一数据、逻辑和证明的严格逻辑框架来支持自主可执行知识系统。

Method: 基于依赖类型理论，采用三层架构：核心层定义静态类型宇宙和构造原语；内核层通过事件驱动机制管理状态演化；运行时层负责物理信号与逻辑证据的双向精化。整合Davidsonian事件语义与Martin-Löf类型理论。

Result: 形式化定义了系统的操作语义，证明了关键元理论性质（包括进展性和演化一致性），确保系统在连续状态转换中保持逻辑自洽且无停滞状态。通过工业追溯和跨境金融合规的应用示例展示了实用性。

Conclusion: KOS-TL为下一代智能自主操作系统提供了强大且可形式化验证的基础，能够构建"携带证明的知识"，其中知识库的每个状态变化都有其有效性的形式见证。

Abstract: This paper introduces KOS-TL (Knowledge Operation System Type Logic), a novel constructive framework designed to provide a rigorous logical foundation for autonomous and executable knowledge systems. Traditional knowledge representation models often suffer from a gap between static symbolic logic and dynamic system execution. To bridge this divide, KOS-TL leverages Dependent Type Theory to unify data, logic, and proof into a singular computational substrate.The architecture of KOS-TL is organized into three hierarchical layers: the Core Layer, which defines the static type universe and constructive primitives; the Kernel Layer, which governs state evolution through an event-driven mechanism characterized by the triple $\langle Σ, \textsf{Ev}, Δ\rangle$; and the Runtime Layer, responsible for the bidirectional refinement of physical signals into logical evidence. We formally define the operational semantics of the system and prove key meta-theoretical properties, including Progress and Evolutionary Consistency, ensuring that the system remains logically self-consistent and free from stuck states during continuous state transitions.By integrating Davidsonian event semantics with Martin-Löf type theory, KOS-TL enables the construction of "proof-carrying knowledge," where every state change in the knowledge base is accompanied by a formal witness of its validity. We demonstrate the practical utility of this logic through application examples in industrial traceability and cross-border financial compliance. Our results suggest that KOS-TL provides a robust, formally verifiable basis for the next generation of intelligent, autonomous operating systems.

</details>


### [14] [SongSage: A Large Musical Language Model with Lyric Generative Pre-training](https://arxiv.org/abs/2601.01153)
*Jiani Guo,Jiajia Li,Jie Wu,Zuchao Li,Yujiu Yang,Ping Wang*

Main category: cs.CL

TL;DR: SongSage是一个专注于歌词理解的大型音乐语言模型，通过歌词生成预训练获得多样化的歌词智能，在歌词相关任务上表现出色，同时保持通用知识理解能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多个领域取得了显著成功，但它们在歌词中心知识理解方面的能力尚未得到充分探索。当前通用LLMs在播放列表理解方面仍有改进空间。

Method: 首先引入PlaylistSense数据集评估语言模型的播放列表理解能力。然后开发SongSage模型：1）在LyricBank（54.8亿歌词标记语料库）上进行持续预训练；2）使用LyricBank-SFT（77.5万样本，涵盖9个核心歌词任务）进行指令微调。

Result: SongSage展现出强大的歌词中心知识理解能力，在零样本播放列表推荐查询重写、歌词生成和续写方面表现优异，在另外7项能力上也表现熟练。同时保持通用知识理解，获得有竞争力的MMLU分数。

Conclusion: SongSage是一个专门针对歌词理解训练的大型音乐语言模型，在歌词相关任务上表现出色，同时保持通用能力。由于版权限制，数据集不公开，但将发布SongSage模型和训练脚本以支持音乐AI研究。

Abstract: Large language models have achieved significant success in various domains, yet their understanding of lyric-centric knowledge has not been fully explored. In this work, we first introduce PlaylistSense, a dataset to evaluate the playlist understanding capability of language models. PlaylistSense encompasses ten types of user queries derived from common real-world perspectives, challenging LLMs to accurately grasp playlist features and address diverse user intents. Comprehensive evaluations indicate that current general-purpose LLMs still have potential for improvement in playlist understanding. Inspired by this, we introduce SongSage, a large musical language model equipped with diverse lyric-centric intelligence through lyric generative pretraining. SongSage undergoes continual pretraining on LyricBank, a carefully curated corpus of 5.48 billion tokens focused on lyrical content, followed by fine-tuning with LyricBank-SFT, a meticulously crafted instruction set comprising 775k samples across nine core lyric-centric tasks. Experimental results demonstrate that SongSage exhibits a strong understanding of lyric-centric knowledge, excels in rewriting user queries for zero-shot playlist recommendations, generates and continues lyrics effectively, and performs proficiently across seven additional capabilities. Beyond its lyric-centric expertise, SongSage also retains general knowledge comprehension and achieves a competitive MMLU score. We will keep the datasets inaccessible due to copyright restrictions and release the SongSage and training script to ensure reproducibility and support music AI research and applications, the datasets release plan details are provided in the appendix.

</details>


### [15] [DHI: Leveraging Diverse Hallucination Induction for Enhanced Contrastive Factuality Control in Large Language Models](https://arxiv.org/abs/2601.01156)
*Jiani Guo,Xiangke Zeng,Jie Wu,Zuchao Li*

Main category: cs.CL

TL;DR: 提出DHI框架，通过修改损失函数和因果注意力掩码，使"邪恶LLM"能生成更多样化的幻觉，无需预标注数据，结合自适应理性约束提升幻觉检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法训练"邪恶LLM"在特定数据集上故意生成幻觉，但诱导的幻觉类型有限，因为模型倾向于重复特定错误模式，限制了整体效果。

Method: DHI框架：1) 修改损失函数，降低特定事实正确token的生成权重，鼓励在目标位置产生多样化幻觉；2) 引入因果注意力掩码适应，减少惩罚对后续token的影响；3) 推理时应用自适应理性约束，仅在正模型高置信度时进行对比解码。

Result: 在多个幻觉基准测试中，DHI相比其他基于对比解码的方法取得了显著的性能提升。

Conclusion: DHI框架能够诱导更广泛的幻觉类型，无需依赖预标注数据，有效提升了幻觉检测和缓解的性能。

Abstract: Large language models (LLMs) frequently produce inaccurate or fabricated information, known as "hallucinations," which compromises their reliability. Existing approaches often train an "Evil LLM" to deliberately generate hallucinations on curated datasets, using these induced hallucinations to guide contrastive decoding against a reliable "positive model" for hallucination mitigation. However, this strategy is limited by the narrow diversity of hallucinations induced, as Evil LLMs trained on specific error types tend to reproduce only these particular patterns, thereby restricting their overall effectiveness. To address these limitations, we propose DHI (Diverse Hallucination Induction), a novel training framework that enables the Evil LLM to generate a broader range of hallucination types without relying on pre-annotated hallucination data. DHI employs a modified loss function that down-weights the generation of specific factually correct tokens, encouraging the Evil LLM to produce diverse hallucinations at targeted positions while maintaining overall factual content. Additionally, we introduce a causal attention masking adaptation to reduce the impact of this penalization on the generation of subsequent tokens. During inference, we apply an adaptive rationality constraint that restricts contrastive decoding to tokens where the positive model exhibits high confidence, thereby avoiding unnecessary penalties on factually correct tokens. Extensive empirical results show that DHI achieves significant performance gains over other contrastive decoding-based approaches across multiple hallucination benchmarks.

</details>


### [16] [Almost Clinical: Linguistic properties of synthetic electronic health records](https://arxiv.org/abs/2601.01171)
*Serge Sharoff,John Baker,David Francis Hunt,Alan Simpson*

Main category: cs.CL

TL;DR: 评估心理健康领域合成电子健康记录的语言学和临床适用性，分析LLM生成文本在构建医疗权威和患者能动性方面的语言特征


<details>
  <summary>Details</summary>
Motivation: 评估合成电子健康记录在心理健康领域的适用性，理解LLM如何通过语言选择构建医疗权威和患者能动性，识别合成文本与真实临床实践的差异

Method: 首先描述创建合成语料库的理论基础和方法论，然后评估四个临床体裁（评估、通信、转诊和护理计划）中的能动性、模态和信息流，分析LLM如何通过语法选择构建医疗权威

Result: LLM能够生成连贯、术语适当的文本，近似临床实践，但存在系统性差异：语域转移、临床特异性不足、药物使用和诊断程序不准确

Conclusion: 合成电子健康记录在心理健康领域具有潜力，但需要解决系统性语言差异和临床准确性问题，才能在实际临床应用中可靠使用

Abstract: This study evaluates the linguistic and clinical suitability of synthetic electronic health records (EHRs) in the field of mental health. First, we describe the rationale and the methodology for creating the synthetic corpus. Second, we assess agency, modality, and information flow across four clinical genres (Assessments, Correspondence, Referrals and Care plans) to understand how LLMs grammatically construct medical authority and patient agency through linguistic choices. While LLMs produce coherent, terminology-appropriate texts that approximate clinical practice, systematic divergences remain, including registerial shifts, insufficient clinical specificity, and inaccuracies in medication use and diagnostic procedures.

</details>


### [17] [Stylometry Analysis of Human and Machine Text for Academic Integrity](https://arxiv.org/abs/2601.01225)
*Hezam Albaqami,Muhammad Asif Ayub,Nasir Ahmad,Yaseen Ahmad,Mohammed M. Alqahtani,Abdullah M. Algamdi,Almoaid A. Owaidah,Kashif Ahmad*

Main category: cs.CL

TL;DR: 该论文提出基于NLP的框架，通过作者归属和风格变化检测来认证学生内容，解决学术诚信问题，包括人机文本分类、单/多作者区分、多作者文档中的作者变化检测和协作文档中的作者识别等任务。


<details>
  <summary>Details</summary>
Motivation: 解决学术诚信的关键挑战，包括抄袭、伪造和教育内容作者身份验证，现有解决方案不完善，需要更全面的分析框架。

Method: 提出NLP框架，针对四个任务：人机文本分类、单/多作者区分、多作者文档作者变化检测、协作文档作者识别。使用Gemini生成两个数据集（普通和严格指令）进行评估。

Result: 在严格指令生成的数据集上观察到性能下降，表明检测巧妙设计的机器生成文本的复杂性。数据集、代码和相关材料已公开在GitHub上。

Conclusion: 该研究为学术内容认证提供了综合框架，公开资源为未来研究奠定基础，同时揭示了检测精心设计的机器生成文本的挑战。

Abstract: This work addresses critical challenges to academic integrity, including plagiarism, fabrication, and verification of authorship of educational content, by proposing a Natural Language Processing (NLP)-based framework for authenticating students' content through author attribution and style change detection. Despite some initial efforts, several aspects of the topic are yet to be explored. In contrast to existing solutions, the paper provides a comprehensive analysis of the topic by targeting four relevant tasks, including (i) classification of human and machine text, (ii) differentiating in single and multi-authored documents, (iii) author change detection within multi-authored documents, and (iv) author recognition in collaboratively produced documents. The solutions proposed for the tasks are evaluated on two datasets generated with Gemini using two different prompts, including a normal and a strict set of instructions. During experiments, some reduction in the performance of the proposed solutions is observed on the dataset generated through the strict prompt, demonstrating the complexities involved in detecting machine-generated text with cleverly crafted prompts. The generated datasets, code, and other relevant materials are made publicly available on GitHub, which are expected to provide a baseline for future research in the domain.

</details>


### [18] [Racka: Efficient Hungarian LLM Adaptation on Academic Infrastructure](https://arxiv.org/abs/2601.01244)
*Zsolt Csibi,Bence György Gortka,Natabara Gyöngyössy,Kornél Nagy,Dávid Márk Nemeskey,Martin Sallai,András Simonyi,András Márk Szekeres,Gábor Palkó*

Main category: cs.CL

TL;DR: Racka是一个轻量级持续预训练大语言模型，专门为匈牙利语设计，通过LoRA参数高效训练在Qwen-3 4B基础上，改善匈牙利语分词效果，同时保持英语和德语能力。


<details>
  <summary>Details</summary>
Motivation: 解决匈牙利语与高资源语言（如英语、德语）之间的资源差距问题，为低资源语言提供实用的语言模型解决方案。

Method: 使用基于LoRA的参数高效持续预训练方法，在Qwen-3 4B骨干网络上进行训练；替换和适配分词器以改善匈牙利语分词效率；使用包含44%匈牙利语、24%英语、21%德语和11%代码的160B子词标记混合数据集。

Result: 匈牙利语分词效率显著提升，同时在英语和德语上保持竞争力；实现了稳定但适中的语言适应结果；训练方法在A100（40GB）HPC集群上实用可行。

Conclusion: Racka展示了通过参数高效持续预训练方法为低资源语言开发语言模型的可行性，成功改善了匈牙利语处理能力，同时避免了高资源语言能力的灾难性遗忘。

Abstract: We present Racka, a lightweight, continually pretrained large language model designed to bridge the resource gap between Hungarian and high-resource languages such as English and German. Racka employs parameter-efficient continual pretraining via Low-Rank Adaptation (LoRA) on a Qwen-3 4B backbone, making the recipe practical on A100 (40GB)-based HPC clusters with low inter-node bandwidth. To better match the training distribution, we replace and adapt the tokenizer, achieving substantially improved tokenization fertility for Hungarian while maintaining competitive performance in English and German. The model is trained on 160B subword tokens drawn from a mixture of internet and high-quality curated sources, with a composition of 44% Hungarian, 24% English, 21% German, and 11% code. This data mix is chosen to mitigate catastrophic forgetting and preserve high-resource language capabilities during continual pretraining. Our preliminary results indicate modest but stable results in language adaptation.

</details>


### [19] [From Policy to Logic for Efficient and Interpretable Coverage Assessment](https://arxiv.org/abs/2601.01266)
*Rhitabrat Pokharel,Hamid Hassanzadeh,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 本文提出了一种混合方法，结合覆盖感知检索器和符号规则推理，以提高医疗政策审查中LLM的效率和可解释性，减少44%推理成本并提升4.5% F1分数。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解释复杂法律和政策语言方面表现出色，但在处理主观和细微文档时存在幻觉和不一致问题，这在医疗覆盖政策审查中尤为关键，因为人类专家需要依赖准确信息。

Method: 提出混合方法：结合覆盖感知检索器与符号规则推理，提取相关政策语言，组织成明确的事实和规则，生成可审计的推理过程，减少LLM推理次数。

Result: 实现了44%的推理成本降低和4.5%的F1分数提升，在效率和效果上都取得了显著改进。

Conclusion: 该方法通过混合系统支持人类审查员，使政策解释更高效和可解释，在医疗覆盖政策审查中平衡了成本、准确性和可审计性。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.

</details>


### [20] [Does Memory Need Graphs? A Unified Framework and Empirical Analysis for Long-Term Dialog Memory](https://arxiv.org/abs/2601.01280)
*Sen Hu,Yuxiang Wei,Jiaxin Ran,Zhiyuan Yao,Lei Zou*

Main category: cs.CL

TL;DR: 论文通过统一框架分解对话记忆系统，实验发现性能差异主要源于基础系统设置而非特定架构创新，为未来研究提供了稳定可靠的基线。


<details>
  <summary>Details</summary>
Motivation: 图结构在对话记忆系统中应用日益增多，但实证结果不一致，不清楚哪些设计选择真正重要，需要系统性的实验分析。

Method: 提出统一框架将对话记忆系统分解为核心组件，支持图和非图方法，在LongMemEval和HaluMem数据集上进行分阶段控制实验，比较记忆表示、组织、维护和检索的常见设计选择。

Result: 实验结果显示许多性能差异是由基础系统设置驱动，而非特定架构创新，基于此识别出了稳定可靠的强基线。

Conclusion: 为未来对话记忆研究提供了稳定可靠的基线，强调基础系统设置的重要性超过特定架构创新。

Abstract: Graph structures are increasingly used in dialog memory systems, but empirical findings on their effectiveness remain inconsistent, making it unclear which design choices truly matter. We present an experimental, system-oriented analysis of long-term dialog memory architectures. We introduce a unified framework that decomposes dialog memory systems into core components and supports both graph-based and non-graph approaches. Under this framework, we conduct controlled, stage-wise experiments on LongMemEval and HaluMem, comparing common design choices in memory representation, organization, maintenance, and retrieval. Our results show that many performance differences are driven by foundational system settings rather than specific architectural innovations. Based on these findings, we identify stable and reliable strong baselines for future dialog memory research.

</details>


### [21] [T3C: Test-Time Tensor Compression with Consistency Guarantees](https://arxiv.org/abs/2601.01299)
*Ismail Lamaakal,Chaymae Yahyati,Yassine Maleh,Khalid El Makkaoui,Ibrahim Ouahbi*

Main category: cs.CL

TL;DR: T3C是一个训练一次、测试时预算条件化的压缩框架，通过弹性张量分解和混合精度量化实现可控制的部署调整，提供可预测的精度-延迟-大小权衡。


<details>
  <summary>Details</summary>
Motivation: 现有模型压缩方法通常需要为不同部署场景重新训练或调整，缺乏统一的、可预测的精度-延迟-大小权衡方案。需要一种训练一次就能适应多种部署预算的灵活框架。

Method: 结合弹性张量分解（保持最大秩）、秩绑定的混合精度量化，以及轻量级控制器将预算令牌映射到每层秩/比特分配。使用基于谱代理和激活统计的快速层一致性证书来上界logit漂移并正则化训练。

Result: 在ImageNet-1k上显著推进视觉Pareto前沿：ResNet-50在精度下降≤0.5%时达到1.18ms p50延迟和38MB模型大小，优于PTQ-8b（1.44ms, 88MB）；ViT-B/16达到2.30ms p50和59MB，超越强PTQ/QAT基线。

Conclusion: 单个T3C检查点即可提供可预测的、证书支持的精度-延迟-大小权衡，能够按需适应不同设备的部署需求，实现了训练一次、灵活部署的实用压缩框架。

Abstract: We present T3C, a train-once, test-time budget-conditioned compression framework that exposes rank and precision as a controllable deployment knob. T3C combines elastic tensor factorization (maintained up to a maximal rank) with rank-tied mixed-precision quantization and a lightweight controller that maps a latency/energy/size budget token to per-layer rank/bit assignments; the policy snaps to hardware-aligned profiles and is monotone in the budget. A fast, layerwise consistency certificate, computed from spectral proxies and activation statistics, upper-bounds logit drift and regularizes training, yielding a practical reliability signal with negligible overhead. On ImageNet-1k, T3C shifts the vision Pareto frontier: for ResNet-50 at matched accuracy (\leq 0.5% drop), p50 latency is 1.18ms with a 38MB model, outperforming PTQ-8b (1.44ms, 88MB); for ViT-B/16, T3C reaches 2.30ms p50 with 59MB, improving over strong PTQ/QAT baselines. A single T3C checkpoint therefore provides predictable, certificate-backed accuracy-latency-size trade-offs on demand across devices.

</details>


### [22] [FLOP-Efficient Training: Early Stopping Based on Test-Time Compute Awareness](https://arxiv.org/abs/2601.01332)
*Hossam Amer,Maryam Dialameh,Hossein Rajabzadeh,Walid Ahmed,Weiwei Zhang,Yang Liu*

Main category: cs.CL

TL;DR: 提出TTC感知训练方法，通过联合选择检查点和测试时计算配置，在保持准确性的同时大幅减少训练计算量（最高达92%）。


<details>
  <summary>Details</summary>
Motivation: 传统上通过增加训练计算量（FLOPs）来提升大语言模型精度，但训练成本高昂。研究发现增加测试时计算（TTC）可以让小模型匹敌大模型，但如何平衡训练和推理计算尚未系统研究。

Method: 1. TTC感知训练：中间检查点配合TTC配置可达到或超过完整训练模型的精度；2. 提前停止算法：联合选择检查点和TTC配置以最小化训练计算；3. 高效TTC评估方法：避免穷举搜索；4. 盈亏平衡界限：确定何时增加的推理计算能补偿减少的训练计算。

Result: 实验显示训练FLOPs减少高达92%，同时保持甚至显著提高模型精度。这为平衡训练和推理计算提供了新视角。

Conclusion: TTC感知训练方法能显著降低训练成本而不牺牲精度，支持更快的部署周期和更频繁的模型更新，代码将公开。

Abstract: Scaling training compute, measured in FLOPs, has long been shown to improve the accuracy of large language models, yet training remains resource-intensive. Prior work shows that increasing test-time compute (TTC)-for example through iterative sampling-can allow smaller models to rival or surpass much larger ones at lower overall cost. We introduce TTC-aware training, where an intermediate checkpoint and a corresponding TTC configuration can together match or exceed the accuracy of a fully trained model while requiring substantially fewer training FLOPs. Building on this insight, we propose an early stopping algorithm that jointly selects a checkpoint and TTC configuration to minimize training compute without sacrificing accuracy. To make this practical, we develop an efficient TTC evaluation method that avoids exhaustive search, and we formalize a break-even bound that identifies when increased inference compute compensates for reduced training compute. Experiments demonstrate up to 92\% reductions in training FLOPs while maintaining and sometimes remarkably improving accuracy. These results highlight a new perspective for balancing training and inference compute in model development, enabling faster deployment cycles and more frequent model refreshes. Codes will be publicly released.

</details>


### [23] [Investigating the Multilingual Calibration Effects of Language Model Instruction-Tuning](https://arxiv.org/abs/2601.01362)
*Jerry Huang,Peng Lu,Qiuhao Zeng,Yusuke Iwasawa,Yutaka Matsuo,Sarath Chandar,Edison Marrese-Taylor,Irene Li*

Main category: cs.CL

TL;DR: 研究发现，在多语言环境中，即使低资源语言缺乏监督微调数据，仅用高资源语言数据进行指令微调也会显著增加模型置信度，但准确率提升有限，导致校准不佳。标签平滑技术能有效缓解此问题，无需低资源语言数据。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型研究不断进步，但大型语言模型（LLMs）的校准问题仍是一个开放的研究领域。特别是在多语言环境中，数据稀缺可能导致不同的校准效果，而常用技术在这些环境下的适用性尚不明确。

Method: 在两个多语言基准测试（分别涵盖29和42种语言）上进行分析，研究指令微调对低资源语言校准的影响。探索使用标签平滑技术来改善校准效果，且无需低资源语言的监督微调数据。

Result: 在低资源语言中，仅使用高资源语言数据进行指令微调会显著增加模型置信度，但准确率提升很小甚至没有，导致校准不佳。标签平滑技术能有效缓解这一问题，在所有语言中保持更好的校准效果。

Conclusion: 多语言环境下的模型训练和微调需要考虑校准问题，以提高下游应用的可靠性和公平性。标准监督微调方法在多语言环境中存在缺陷，而标签平滑是一种有效的改进方法。

Abstract: Ensuring that deep learning models are well-calibrated in terms of their predictive uncertainty is essential in maintaining their trustworthiness and reliability, yet despite increasing advances in foundation model research, the relationship between such large language models (LLMs) and their calibration remains an open area of research. In this work, we look at a critical gap in the calibration of LLMs within multilingual settings, in an attempt to better understand how the data scarcity can potentially lead to different calibration effects and how commonly used techniques can apply in these settings. Our analysis on two multilingual benchmarks, over 29 and 42 languages respectively, reveals that even in low-resource languages, model confidence can increase significantly after instruction-tuning on high-resource language SFT datasets. However, improvements in accuracy are marginal or non-existent, resulting in mis-calibration, highlighting a critical shortcoming of standard SFT for multilingual languages. Furthermore, we observe that the use of label smoothing to be a reasonable method alleviate this concern, again without any need for low-resource SFT data, maintaining better calibration across all languages. Overall, this highlights the importance of multilingual considerations for both training and tuning LLMs in order to improve their reliability and fairness in downstream use.

</details>


### [24] [Reasoning Over Recall: Evaluating the Efficacy of Generalist Architectures vs. Specialized Fine-Tunes in RAG-Based Mental Health Dialogue Systems](https://arxiv.org/abs/2601.01341)
*Md Abdullah Al Kafi,Raka Moni,Sumit Kumar Banshal*

Main category: cs.CL

TL;DR: 在基于RAG的心理咨询系统中，通用推理模型（3B）比领域微调模型（7B）表现更好，在共情能力上显著优于专业模型，表明强推理能力比心理健康特定训练更重要。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在心理健康咨询中的幻觉和缺乏共情问题，探索在RAG框架下，是领域微调模型还是通用推理模型更适合心理咨询应用。

Method: 使用ChromaDB构建相同RAG流程，比较四个开源模型：两个通用推理模型（Qwen2.5-3B和Phi-3-Mini）和两个领域微调模型（MentalHealthBot-7B和TherapyBot-7B），采用LLM-as-a-Judge框架自动化评估50轮对话。

Result: 通用模型在共情能力上显著优于领域模型（3.72 vs 3.26，p<0.001），尽管模型更小（3B vs 7B）；所有模型安全性良好，但通用模型表现出更好的上下文理解能力，领域模型存在过拟合现象。

Conclusion: 对于基于RAG的治疗系统，强推理能力比心理健康特定词汇训练更重要；只要回答基于临床证据，推理能力强的通用模型能提供更共情和平衡的支持。

Abstract: The deployment of Large Language Models (LLMs) in mental health counseling faces the dual challenges of hallucinations and lack of empathy. While the former may be mitigated by RAG (retrieval-augmented generation) by anchoring answers in trusted clinical sources, there remains an open question as to whether the most effective model under this paradigm would be one that is fine-tuned on mental health data, or a more general and powerful model that succeeds purely on the basis of reasoning. In this paper, we perform a direct comparison by running four open-source models through the same RAG pipeline using ChromaDB: two generalist reasoners (Qwen2.5-3B and Phi-3-Mini) and two domain-specific fine-tunes (MentalHealthBot-7B and TherapyBot-7B). We use an LLM-as-a-Judge framework to automate evaluation over 50 turns. We find a clear trend: the generalist models outperform the domain-specific ones in empathy (3.72 vs. 3.26, $p < 0.001$) in spite of being much smaller (3B vs. 7B), and all models perform well in terms of safety, but the generalist models show better contextual understanding and are less prone to overfitting as we observe in the domain-specific models. Overall, our results indicate that for RAG-based therapy systems, strong reasoning is more important than training on mental health-specific vocabulary; i.e. a well-reasoned general model would provide more empathetic and balanced support than a larger narrowly fine-tuned model, so long as the answer is already grounded in clinical evidence.

</details>


### [25] [FC-CONAN: An Exhaustively Paired Dataset for Robust Evaluation of Retrieval Systems](https://arxiv.org/abs/2601.01350)
*Juan Junqueras,Florian Boudin,May-Myo Zin,Ha-Thanh Nguyen,Wachara Fungwacharakorn,Damián Ariel Furman,Akiko Aizawa,Ken Satoh*

Main category: cs.CL

TL;DR: FC-CONAN是首个完全连接的仇恨言论-反叙事数据集，通过穷举45条仇恨言论和129条反叙事的所有组合构建，包含四个不同可靠性的标注分区，为反言论研究提供更全面的评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论-反叙事数据集（如CONAN）只标注了稀疏的配对组合，限制了反言论检索系统的评估效果。需要更全面的数据集来支持更准确的研究评估。

Method: 采用两阶段标注流程：1）穷举45条仇恨言论和129条反叙事的所有组合（5805对）；2）由9名标注员和4名验证员进行标注，形成钻石、黄金、白银、青铜四个不同可靠性的分区。

Result: 创建了FC-CONAN数据集，包含数百个之前未标注的正样本，不重叠于CONAN数据集。该数据集支持更忠实的反言论检索系统评估和详细的错误分析。

Conclusion: FC-CONAN填补了仇恨言论-反叙事数据集的空白，提供了更全面的评估基准，有助于推动反言论研究的发展。数据集已公开可用。

Abstract: Hate speech (HS) is a critical issue in online discourse, and one promising strategy to counter it is through the use of counter-narratives (CNs). Datasets linking HS with CNs are essential for advancing counterspeech research. However, even flagship resources like CONAN (Chung et al., 2019) annotate only a sparse subset of all possible HS-CN pairs, limiting evaluation. We introduce FC-CONAN (Fully Connected CONAN), the first dataset created by exhaustively considering all combinations of 45 English HS messages and 129 CNs. A two-stage annotation process involving nine annotators and four validators produces four partitions-Diamond, Gold, Silver, and Bronze-that balance reliability and scale. None of the labeled pairs overlap with CONAN, uncovering hundreds of previously unlabelled positives. FC-CONAN enables more faithful evaluation of counterspeech retrieval systems and facilitates detailed error analysis. The dataset is publicly available.

</details>


### [26] [EternalMath: A Living Benchmark of Frontier Mathematics that Evolves with Human Discovery](https://arxiv.org/abs/2601.01400)
*Jicheng Ma,Guohua Wang,Xinhua Feng,Yiming Liu,Zhichao Hu,Yuhong Liu*

Main category: cs.CL

TL;DR: 提出自动化定理验证评估框架EternalMath，直接从最新数学研究文献生成可执行验证的推理任务，解决现有数学推理评估覆盖有限且易饱和的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的数学推理评估主要依赖静态基准测试，这些测试要么来自竞赛风格问题，要么需要专家精心设计，导致覆盖范围有限（特别是研究级数学），且性能容易快速饱和。

Method: 提出全自动、基于定理的评估流水线：1) 从近期同行评审的数学文献中识别构造性或定量结果；2) 将其实例化为参数化问题模板；3) 通过基于执行的验证生成确定性解决方案，实现可扩展、可复现、持续更新的评估。

Result: 应用该流水线创建了EternalMath评估套件，实验显示最先进的大语言模型存在显著性能差距，表明研究前沿的数学推理远未饱和。

Conclusion: 数学推理评估需要与人类数学发现同步演进的方法论，基于最新研究文献的自动化评估框架能提供更真实、持续更新的评估基准。

Abstract: Current evaluations of mathematical reasoning in large language models (LLMs) are dominated by static benchmarks, either derived from competition-style problems or curated through costly expert effort, resulting in limited coverage of research-level mathematics and rapid performance saturation. We propose a fully automated, theorem-grounded pipeline for evaluating frontier mathematical reasoning, which directly transforms recent peer-reviewed mathematical literature into executable and verifiable reasoning tasks. The pipeline identifies constructive or quantitative results, instantiates them into parameterized problem templates, and generates deterministic solutions through execution-based verification, enabling scalable, reproducible, and continuously updatable evaluation without reliance on large-scale expert authoring. By design, this approach supports temporal extensibility, intrinsic correctness checking, and domain-specific customization across mathematical subfields. Applying this pipeline yields \textbf{EternalMath}, an evolving evaluation suite derived from contemporary research papers. Experiments with state-of-the-art LLMs reveal substantial performance gaps, indicating that mathematical reasoning at the research frontier remains far from saturated and underscoring the need for evaluation methodologies that evolve in step with human mathematical discovery.

</details>


### [27] [LANCET: Neural Intervention via Structural Entropy for Mitigating Faithfulness Hallucinations in LLMs](https://arxiv.org/abs/2601.01401)
*Chenxu Wang,Chaozhuo Li,Pengbo Wang,Litian Zhang,Songyang Liu,Ji Qi,Jiahui Hu,Yushan Cai,Hao Zhao,Rui Pu*

Main category: cs.CL

TL;DR: Lancet框架通过结构熵和幻觉差异比实现精确神经干预，定位幻觉易发神经元并阻断其传播路径，显著提升大语言模型的忠实度


<details>
  <summary>Details</summary>
Motivation: 当前方法通过节点级调整或粗粒度抑制来解决大语言模型的忠实度幻觉问题，但忽视了神经信息的分布式特性，导致干预不精确。作者认识到幻觉像感染一样通过特定的前向传播路径传播，因此希望通过精确的结构分析来手术式阻断这种传播流

Method: 提出Lancet框架：1) 通过梯度驱动的对比分析定位幻觉易发神经元；2) 通过最小化结构熵映射其传播路径；3) 实施分层干预策略以保留模型的一般能力

Result: 在多个幻觉基准数据集上的综合评估表明，Lancet显著优于最先进的方法，验证了神经干预手术式方法的有效性

Conclusion: 通过结构熵和幻觉差异比实现的精确神经干预能够有效阻断幻觉传播路径，同时保持模型的一般能力，为大语言模型的忠实度问题提供了新的解决方案

Abstract: Large Language Models have revolutionized information processing, yet their reliability is severely compromised by faithfulness hallucinations. While current approaches attempt to mitigate this issue through node-level adjustments or coarse suppression, they often overlook the distributed nature of neural information, leading to imprecise interventions. Recognizing that hallucinations propagate through specific forward transmission pathways like an infection, we aim to surgically block this flow using precise structural analysis. To leverage this, we propose Lancet, a novel framework that achieves precise neural intervention by leveraging structural entropy and hallucination difference ratios. Lancet first locates hallucination-prone neurons via gradient-driven contrastive analysis, then maps their propagation pathways by minimizing structural entropy, and finally implements a hierarchical intervention strategy that preserves general model capabilities. Comprehensive evaluations across hallucination benchmark datasets demonstrate that Lancet significantly outperforms state-of-the-art methods, validating the effectiveness of our surgical approach to neural intervention.

</details>


### [28] [From Emotion Classification to Emotional Reasoning: Enhancing Emotional Intelligence in Large Language Models](https://arxiv.org/abs/2601.01407)
*Arjhun Sreedar,Rohan Pillay,Laukik Patade*

Main category: cs.CL

TL;DR: 使用合成情感链式思维数据微调7B模型，可显著提升情感推理能力，无需架构修改


<details>
  <summary>Details</summary>
Motivation: 研究是否可以通过合成的情感链式思维数据来提升小型开源大语言模型的情感推理能力，探索无需架构修改的情感能力增强方法

Method: 设计多智能体生成管道，创建治疗式对话并将其转换为结构化情感多选题及解释，使用该数据集微调多种7B模型

Result: 微调后的Mistral 7B在情感理解(EU)上从10.5提升到20.5，情感意识(EA)从40.5提升到60.0，验证了合成情感推理数据的有效性

Conclusion: 合成情感推理数据能够有效增强模型在细微情感任务中的能力，情感推理可以通过数据驱动的方式诱导而不需要架构修改

Abstract: This work investigates whether synthetic emotional chain-of-thought data can improve the emotional reasoning abilities of smaller open large language models (LLMs). We design a multi-agent generation pipeline that produces therapy-style conversations and converts them into structured emotion multiple-choice questions (MCQs) with explanations. We propose that fine-tuning a variety of 7B models on this dataset should yield substantial gains in emotional understanding and emotional awareness on EmoBench-style evaluations, suggesting that emotional reasoning can be induced without architectural changes. Our results demonstrate that fine-tuned Mistral 7B achieves EU improvements from 10.5 to 20.5 and EA improvements from 40.5 to 60.0, validating the effectiveness of synthetic emotional reasoning data for enhancing model capabilities in nuanced emotional tasks.

</details>


### [29] [iFlip: Iterative Feedback-driven Counterfactual Example Refinement](https://arxiv.org/abs/2601.01446)
*Yilong Wang,Qianli Wang,Nils Feldhus*

Main category: cs.CL

TL;DR: iFlip：利用LLM自我修正能力的迭代式反事实生成方法，通过模型置信度、特征归因和自然语言反馈提升反事实有效性


<details>
  <summary>Details</summary>
Motivation: 现有单次生成方法在利用大语言模型生成反事实示例时效果不佳，常常无法可靠地改变模型预测标签，忽视了LLM的自我修正能力

Method: 提出iFlip迭代优化方法，利用三种反馈类型（模型置信度、特征归因和自然语言）进行多轮迭代修正，通过适当的迭代次数、高归因词指向和早停策略生成有效反事实

Result: iFlip在标签翻转率上比五种SOTA基线平均提高57.8%的有效性；用户研究显示在完整性、总体满意度和可行性方面均优于基线；反事实数据增强显著提升模型性能和鲁棒性

Conclusion: iFlip通过利用LLM的自我修正能力和迭代反馈机制，显著提高了反事实生成的有效性，为模型解释性和数据增强提供了有效工具

Abstract: Counterfactual examples are minimal edits to an input that alter a model's prediction. They are widely employed in explainable AI to probe model behavior and in natural language processing (NLP) to augment training data. However, generating valid counterfactuals with large language models (LLMs) remains challenging, as existing single-pass methods often fail to induce reliable label changes, neglecting LLMs' self-correction capabilities. To explore this untapped potential, we propose iFlip, an iterative refinement approach that leverages three types of feedback, including model confidence, feature attribution, and natural language. Our results show that iFlip achieves an average 57.8% higher validity than the five state-of-the-art baselines, as measured by the label flipping rate. The user study further corroborates that iFlip outperforms baselines in completeness, overall satisfaction, and feasibility. In addition, ablation studies demonstrate that three components are paramount for iFlip to generate valid counterfactuals: leveraging an appropriate number of iterations, pointing to highly attributed words, and early stopping. Finally, counterfactuals generated by iFlip enable effective counterfactual data augmentation, substantially improving model performance and robustness.

</details>


### [30] [Segmentation and Processing of German Court Decisions from Open Legal Data](https://arxiv.org/abs/2601.01449)
*Harshil Darji,Martin Heckelmann,Christina Kratsch,Gerard de Melo*

Main category: cs.CL

TL;DR: 本文创建了一个包含251,038个德国法院判决的清洗和分段数据集，从Open Legal Data原始数据中系统分离了判决书的关键部分，并通过统计抽样验证了提取准确性。


<details>
  <summary>Details</summary>
Motivation: 德国法律系统的结构化数据对于推进NLP技术很重要。虽然Open Legal Data提供了大规模的德国法院判决数据，但原始数据中判决文本格式不一致，缺乏清晰标记的部分，这影响了修辞角色分类、检索和引证分析等下游任务。

Method: 从Open Legal Data官方数据集中提取251,038个德国法院判决，系统分离三个重要部分：Tenor（判决主文）、Tatbestand（案件事实）和Entscheidungsgründe（判决理由）。使用Cochran公式以95%置信水平和5%误差幅度抽取384个案例的统计代表性随机样本，手动验证所有三个部分的正确识别。还将Rechtsmittelbelehrung（上诉通知）作为单独字段提取。

Result: 创建了一个公开可用的清洗和分段德国法院判决数据集，包含251,038个案例，以JSONL格式提供。统计验证显示提取过程可靠，为德国法律系统的进一步研究提供了可访问资源。

Conclusion: 该工作提供了一个高质量、结构化的德国法律文本数据集，解决了原始数据格式不一致的问题，为德国法律系统的NLP研究提供了重要基础资源，支持修辞角色分类、检索和引证分析等下游任务。

Abstract: The availability of structured legal data is important for advancing Natural Language Processing (NLP) techniques for the German legal system. One of the most widely used datasets, Open Legal Data, provides a large-scale collection of German court decisions. While the metadata in this raw dataset is consistently structured, the decision texts themselves are inconsistently formatted and often lack clearly marked sections. Reliable separation of these sections is important not only for rhetorical role classification but also for downstream tasks such as retrieval and citation analysis. In this work, we introduce a cleaned and sectioned dataset of 251,038 German court decisions derived from the official Open Legal Data dataset. We systematically separated three important sections in German court decisions, namely Tenor (operative part of the decision), Tatbestand (facts of the case), and Entscheidungsgründe (judicial reasoning), which are often inconsistently represented in the original dataset. To ensure the reliability of our extraction process, we used Cochran's formula with a 95% confidence level and a 5% margin of error to draw a statistically representative random sample of 384 cases, and manually verified that all three sections were correctly identified. We also extracted the Rechtsmittelbelehrung (appeal notice) as a separate field, since it is a procedural instruction and not part of the decision itself. The resulting corpus is publicly available in the JSONL format, making it an accessible resource for further research on the German legal system.

</details>


### [31] [Bridging the gap: A comparative exploration of Speech-LLM and end-to-end architecture for multilingual conversational ASR](https://arxiv.org/abs/2601.01461)
*Yuxiang Mei,Dongxing Xu,Jiaen Liang,Yanhua Long*

Main category: cs.CL

TL;DR: 本文提出增强型LLM-based ASR框架，结合微调Whisper和mHuBERT编码器，采用交叉注意力融合机制，在MLC-SLM挑战中取得10.69% CER/WER，与使用大规模训练数据的顶级系统表现相当。


<details>
  <summary>Details</summary>
Motivation: 解决多语言对话语音识别中LLM-based ASR的两个关键问题：1）简单特征拼接无法充分利用互补信息；2）LLM-based ASR与端到端编码器-解码器ASR的性能差距尚未探索。

Method: 提出增强型LLM-based ASR框架，结合微调的Whisper和mHuBERT编码器，采用交叉注意力融合机制整合语音表示。首先评估LoRA和全微调的E2E Whisper模型，然后为并行语音编码器设计基于交叉注意力的融合方法。

Result: 在MLC-SLM挑战官方评估集上，系统获得10.69% CER/WER，与Track 1顶级系统表现相当，尽管仅使用1,500小时基线训练数据（对比其他系统的大规模训练集）。但最终LLM-based ASR仍不及微调E2E Whisper模型性能。

Conclusion: 提出的增强型LLM-based ASR框架在多语言对话ASR中表现优异，但LLM-based ASR仍无法超越微调E2E Whisper模型，为未来Speech-LLM设计提供了有价值的实证指导。

Abstract: The INTERSPEECH 2025 Challenge on Multilingual Conversational Speech Language Models (MLC-SLM) promotes multilingual conversational ASR with large language models (LLMs). Our previous SHNU-mASR system adopted a competitive parallel-speech-encoder architecture that integrated Whisper and mHuBERT with an LLM. However, it faced two challenges: simple feature concatenation may not fully exploit complementary information, and the performance gap between LLM-based ASR and end-to-end(E2E) encoder-decoder ASR remained unexplored. In this work, we present an enhanced LLM-based ASR framework that combines fine-tuned Whisper and mHuBERT encoders with an LLM to enrich speech representations. We first evaluate E2E Whisper models with LoRA and full fine-tuning on the MLC-SLM ASR task, and then propose cross-attention-based fusion mechanisms for the parallel-speech-encoder. On the official evaluation set of the MLC-SLM Challenge, our system achieves a CER/WER of 10.69%, ranking on par with the top-ranked Track 1 systems, even though it uses only 1,500 hours of baseline training data compared with their large-scale training sets. Nonetheless, we find that our final LLM-based ASR still does not match the performance of a fine-tuned E2E Whisper model, providing valuable empirical guidance for future Speech-LLM design. Our code is publicly available at https://github.com/1535176727/MLC-SLM.

</details>


### [32] [Can Legislation Be Made Machine-Readable in PROLEG?](https://arxiv.org/abs/2601.01477)
*May-Myo Zin,Sabine Wehnert,Yuntao Kong,Ha-Thanh Nguyen,Wachara Fungwacharakorn,Jieying Xue,Michał Araszkiewicz,Randy Goebel,Ken Satoh,Le-Minh Nguyen*

Main category: cs.CL

TL;DR: 提出一个结合大语言模型和PROLEG法律表示系统的框架，将GDPR等法规文本自动转换为可执行规则，并生成人类可读的解释。


<details>
  <summary>Details</summary>
Motivation: 法规应用需要准确性和效率，现代AI技术（特别是NLP和机器辅助推理）有望解决这一挑战，但需要系统化方法来转换法律文本为可执行规则。

Method: 使用LLM提示将法律文本（以GDPR第6条为例）同时转换为if-then规则和PROLEG编码，经法律专家验证和精炼后，生成可执行的PROLEG程序。

Result: 开发出端到端框架，成功将GDPR第6条转换为可执行PROLEG程序，能够为GDPR决策实例生成人类可读的解释。

Conclusion: 该方法展示了AI技术在法规捕获和部署中的价值，但存在局限性，需要进一步开发以完善这类技术。

Abstract: The anticipated positive social impact of regulatory processes requires both the accuracy and efficiency of their application. Modern artificial intelligence technologies, including natural language processing and machine-assisted reasoning, hold great promise for addressing this challenge. We present a framework to address the challenge of tools for regulatory application, based on current state-of-the-art (SOTA) methods for natural language processing (large language models or LLMs) and formalization of legal reasoning (the legal representation system PROLEG). As an example, we focus on Article 6 of the European General Data Protection Regulation (GDPR). In our framework, a single LLM prompt simultaneously transforms legal text into if-then rules and a corresponding PROLEG encoding, which are then validated and refined by legal domain experts. The final output is an executable PROLEG program that can produce human-readable explanations for instances of GDPR decisions. We describe processes to support the end-to-end transformation of a segment of a regulatory document (Article 6 from GDPR), including the prompting frame to guide an LLM to "compile" natural language text to if-then rules, then to further "compile" the vetted if-then rules to PROLEG. Finally, we produce an instance that shows the PROLEG execution. We conclude by summarizing the value of this approach and note observed limitations with suggestions to further develop such technologies for capturing and deploying regulatory frameworks.

</details>


### [33] [Four Quadrants of Difficulty: A Simple Categorisation and its Limits](https://arxiv.org/abs/2601.01488)
*Vanessa Toborek,Sebastian Müller,Christian Bauckhage*

Main category: cs.CL

TL;DR: 论文挑战了课程学习中任务无关难度指标的假设，提出需要轻量级、任务相关的难度估计器来更好地反映模型学习行为


<details>
  <summary>Details</summary>
Motivation: NLP中课程学习通常使用任务无关的语言学启发式或人类直觉来估计样本难度，但这种方法隐含假设这些信号与神经网络模型实际学习困难相关。作者想要验证这一假设并探索更好的难度估计方法。

Method: 提出了一个四象限分类法（人类vs模型、任务无关vs任务相关），在自然语言理解数据集上系统分析了这些难度信号的相互作用。

Result: 发现任务无关特征在很大程度上独立运行，只有任务相关特征能够对齐。这些发现挑战了常见的课程学习直觉。

Conclusion: 需要开发轻量级、任务相关的难度估计器，以更好地反映模型的学习行为，而不是依赖任务无关的启发式方法。

Abstract: Curriculum Learning (CL) aims to improve the outcome of model training by estimating the difficulty of samples and scheduling them accordingly. In NLP, difficulty is commonly approximated using task-agnostic linguistic heuristics or human intuition, implicitly assuming that these signals correlate with what neural models find difficult to learn. We propose a four-quadrant categorisation of difficulty signals -- human vs. model and task-agnostic vs. task-dependent -- and systematically analyse their interactions on a natural language understanding dataset. We find that task-agnostic features behave largely independently and that only task-dependent features align. These findings challenge common CL intuitions and highlight the need for lightweight, task-dependent difficulty estimators that better reflect model learning behaviour.

</details>


### [34] [Distortion Instead of Hallucination: The Effect of Reasoning Under Strict Constraints](https://arxiv.org/abs/2601.01490)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 研究发现推理模型在约束条件下存在矛盾：虽然减少了约束违反率，但会系统性扭曲事实并增加完全捏造，形成约束合规性与事实准确性之间的权衡


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，幻觉问题（模型输出中的非事实捏造）已成为严重关切。推理能力作为提高输出可靠性的自我验证过程受到关注，但在封闭系统中（无法依赖外部工具或知识）推理的效果尚不明确。

Method: 在严格约束条件下（推荐计算机科学领域的同行评审期刊文章）进行实验，使用多个模型（GPT-5.2和Gemini 3 Flash）检验推理效果，比较推理模型与非推理模型的表现。

Result: 发现约束合规性与事实准确性之间存在问题性权衡：非推理模型约束违反率高（66-75%）但保持事实准确性；推理模型减少违反率（13-26%）但系统性扭曲已知事实以满足约束，并增加完全捏造。这种权衡模式在不同架构模型中一致存在。

Conclusion: 推理并不普遍提高输出真实性，效果因模型而异，反映了合规性与真实性权衡的不同分配。这些发现挑战了"推理普遍提高可靠性"的假设：推理模型以诚实的约束违反换取难以检测的扭曲。

Abstract: With the widespread adoption of large language models (LLMs), hallucinations, which are non-factual fabrications in model outputs, have become serious concerns. Reasoning capabilities have received attention as a self-verification process to improve output reliability. However, the effect of reasoning within a closed system where LLMs cannot rely on external tools or knowledge has yet to be clarified. We therefore conduct experiments under strict constraints (recommending peer-reviewed journal articles in computer science) to examine the effect of reasoning across multiple models (GPT-5.2 and Gemini 3 Flash). Our results reveal a problematic trade-off between constraint compliance and factual accuracy. Non-reasoning models exhibit high constraint violation rates (66-75%) but maintain factual accuracy, while reasoning models reduce violations (13-26%) but systematically distort known facts to satisfy constraints and increase complete fabrication. This trade-off pattern is consistent across both models despite different architectures, indicating a fundamental limitation of reasoning. Furthermore, reasoning does not uniformly improve output authenticity: effects diverge by model, reflecting different allocations of the compliance-truthfulness trade-off. These findings challenge the assumption that reasoning universally improves reliability: reasoning models trade honest constraint violations for detection-resistant distortions.

</details>


### [35] [From Failure to Mastery: Generating Hard Samples for Tool-use Agents](https://arxiv.org/abs/2601.01498)
*Bingguang Hao,Zengzhuang Xu,Yuntao Wen,Xinyi Xu,Yang Liu,Tong Zhao,Maolin Wang,Long Chen,Dong Wang,Yicheng Chen,Cunyin Peng,Xiangyu Zhao,Chenyi Zhuang,Ji Zhang*

Main category: cs.CL

TL;DR: HardGen：一种自动生成具有可验证推理的困难工具使用训练样本的代理管道，通过动态API图、模块化高级工具和闭环评估反馈来提升LLM代理的训练数据质量。


<details>
  <summary>Details</summary>
Motivation: 现有工具使用训练数据生成方法主要采用随机采样和浅层生成范式，产生的轨迹简单且同质化，无法捕捉复杂、隐式的逻辑依赖关系，限制了LLM代理能力的提升。

Method: 1. 基于代理失败案例构建动态API图，从中采样合成困难轨迹；2. 以这些轨迹作为条件先验，指导模块化抽象高级工具的实例化，用于生成困难查询；3. 利用高级工具和困难查询生成可验证的复杂思维链，并通过闭环评估反馈持续优化整个过程。

Result: 使用HardGen生成的数据集训练的4B参数模型，在性能上超越了多个领先的开源和闭源竞争对手（如GPT-5.2、Gemini-3-Pro和Claude-Opus-4.5）。

Conclusion: HardGen通过自动生成具有可验证推理的困难工具使用训练样本，有效解决了现有数据生成方法的局限性，显著提升了LLM代理的工具使用能力，并将开源代码、模型和数据集以促进未来研究。

Abstract: The advancement of LLM agents with tool-use capabilities requires diverse and complex training corpora. Existing data generation methods, which predominantly follow a paradigm of random sampling and shallow generation, often yield simple and homogeneous trajectories that fail to capture complex, implicit logical dependencies. To bridge this gap, we introduce HardGen, an automatic agentic pipeline designed to generate hard tool-use training samples with verifiable reasoning. Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces. Secondly, these traces serve as conditional priors to guide the instantiation of modular, abstract advanced tools, which are subsequently leveraged to formulate hard queries. Finally, the advanced tools and hard queries enable the generation of verifiable complex Chain-of-Thought (CoT), with a closed-loop evaluation feedback steering the continuous refinement of the process. Extensive evaluations demonstrate that a 4B parameter model trained with our curated dataset achieves superior performance compared to several leading open-source and closed-source competitors (e.g., GPT-5.2, Gemini-3-Pro and Claude-Opus-4.5). Our code, models, and dataset will be open-sourced to facilitate future research.

</details>


### [36] [EmoHarbor: Evaluating Personalized Emotional Support by Simulating the User's Internal World](https://arxiv.org/abs/2601.01530)
*Jing Ye,Lu Xiang,Yaping Zhang,Chengqing Zong*

Main category: cs.CL

TL;DR: EmoHarbor：基于用户模拟的个性化情感支持评估框架，发现当前LLMs虽能生成共情回应，但缺乏针对用户具体情境的个性化支持能力。


<details>
  <summary>Details</summary>
Motivation: 当前情感支持对话评估范式倾向于奖励通用的共情回应，但无法评估支持是否真正针对用户的独特心理特征和情境需求进行个性化定制。

Method: 提出EmoHarbor评估框架，采用"用户即裁判"范式，通过模拟用户内心世界进行评估。采用Chain-of-Agent架构，将用户内部过程分解为三个专门角色，使代理能够以类似人类用户的方式与支持者互动并完成评估。使用100个真实用户档案（涵盖多样化人格特质和情境）和10个个性化支持质量评估维度来实例化该基准。

Result: 对20个先进LLMs的综合评估揭示关键发现：虽然这些模型在生成共情回应方面表现出色，但它们始终无法根据个体用户情境定制支持。这一发现重新定义了核心挑战，将研究重点从仅仅增强通用共情转向开发真正用户感知的情感支持系统。

Conclusion: EmoHarbor提供了一个可复现且可扩展的框架，能够指导更细致、更具用户感知能力的情感支持系统的开发和评估，推动该领域向真正个性化支持方向发展。

Abstract: Current evaluation paradigms for emotional support conversations tend to reward generic empathetic responses, yet they fail to assess whether the support is genuinely personalized to users' unique psychological profiles and contextual needs. We introduce EmoHarbor, an automated evaluation framework that adopts a User-as-a-Judge paradigm by simulating the user's inner world. EmoHarbor employs a Chain-of-Agent architecture that decomposes users' internal processes into three specialized roles, enabling agents to interact with supporters and complete assessments in a manner similar to human users. We instantiate this benchmark using 100 real-world user profiles that cover a diverse range of personality traits and situations, and define 10 evaluation dimensions of personalized support quality. Comprehensive evaluation of 20 advanced LLMs on EmoHarbor reveals a critical insight: while these models excel at generating empathetic responses, they consistently fail to tailor support to individual user contexts. This finding reframes the central challenge, shifting research focus from merely enhancing generic empathy to developing truly user-aware emotional support. EmoHarbor provides a reproducible and scalable framework to guide the development and evaluation of more nuanced and user-aware emotional support systems.

</details>


### [37] [Bridging the Data Gap: Creating a Hindi Text Summarization Dataset from the English XSUM](https://arxiv.org/abs/2601.01543)
*Praveenkumar Katwe,RakeshChandra Balabantaray,Kaliprasad Vittala*

Main category: cs.CL

TL;DR: 本文提出了一种低成本自动化的框架，用于创建印地语文本摘要数据集，通过翻译和优化英语XSUM数据集，并使用COMET和LLM进行验证和筛选。


<details>
  <summary>Details</summary>
Motivation: 当前NLP进展主要偏向资源丰富的语言，印地语等低资源语言缺乏高质量的文本摘要数据集，这阻碍了鲁棒模型的发展。

Method: 利用英语XSUM数据集作为源，采用先进的翻译和语言适应技术，使用COMET进行翻译质量验证，并选择性使用大语言模型进行数据筛选。

Result: 创建了一个多样化、多主题的印地语文本摘要数据集，该数据集反映了原始XSUM语料库的复杂性。

Conclusion: 这项工作不仅为印地语NLP研究提供了直接工具，还为其他资源不足语言的NLP民主化提供了可扩展的方法论，通过降低数据集创建成本，促进了计算语言学中更细致、文化相关模型的发展。

Abstract: Current advancements in Natural Language Processing (NLP) have largely favored resource-rich languages, leaving a significant gap in high-quality datasets for low-resource languages like Hindi. This scarcity is particularly evident in text summarization, where the development of robust models is hindered by a lack of diverse, specialized corpora.
  To address this disparity, this study introduces a cost-effective, automated framework for creating a comprehensive Hindi text summarization dataset. By leveraging the English Extreme Summarization (XSUM) dataset as a source, we employ advanced translation and linguistic adaptation techniques. To ensure high fidelity and contextual relevance, we utilize the Crosslingual Optimized Metric for Evaluation of Translation (COMET) for validation, supplemented by the selective use of Large Language Models (LLMs) for curation.
  The resulting dataset provides a diverse, multi-thematic resource that mirrors the complexity of the original XSUM corpus. This initiative not only provides a direct tool for Hindi NLP research but also offers a scalable methodology for democratizing NLP in other underserved languages. By reducing the costs associated with dataset creation, this work fosters the development of more nuanced, culturally relevant models in computational linguistics.

</details>


### [38] [HalluZig: Hallucination Detection using Zigzag Persistence](https://arxiv.org/abs/2601.01552)
*Shreyas N. Samaga,Gilberto Gonzalez Arroyo,Tamal K. Dey*

Main category: cs.CL

TL;DR: HalluZig：利用拓扑数据分析（TDA）中的zigzag持久性分析LLM注意力动态拓扑，检测幻觉的新方法


<details>
  <summary>Details</summary>
Motivation: LLM的事实可靠性是其在高风险领域应用的关键障碍，现有检测方法依赖模型输出的表面信号，忽略了内部推理过程中的失败

Method: 将模型层间注意力矩阵序列建模为zigzag图过滤，使用拓扑数据分析中的zigzag持久性提取拓扑特征，假设事实和幻觉生成具有不同的拓扑特征

Result: HalluZig在多个基准测试中优于强基线，分析表明这些拓扑特征在不同模型间具有泛化性，且仅使用部分网络深度的结构特征即可检测幻觉

Conclusion: 通过分析注意力动态拓扑的zigzag持久性，为幻觉检测提供了新的有效范式，揭示了模型内部推理过程的结构特征对检测幻觉的重要性

Abstract: The factual reliability of Large Language Models (LLMs) remains a critical barrier to their adoption in high-stakes domains due to their propensity to hallucinate. Current detection methods often rely on surface-level signals from the model's output, overlooking the failures that occur within the model's internal reasoning process. In this paper, we introduce a new paradigm for hallucination detection by analyzing the dynamic topology of the evolution of model's layer-wise attention. We model the sequence of attention matrices as a zigzag graph filtration and use zigzag persistence, a tool from Topological Data Analysis, to extract a topological signature. Our core hypothesis is that factual and hallucinated generations exhibit distinct topological signatures. We validate our framework, HalluZig, on multiple benchmarks, demonstrating that it outperforms strong baselines. Furthermore, our analysis reveals that these topological signatures are generalizable across different models and hallucination detection is possible only using structural signatures from partial network depth.

</details>


### [39] [Steerability of Instrumental-Convergence Tendencies in LLMs](https://arxiv.org/abs/2601.01584)
*Jakub Hoscilowicz*

Main category: cs.CL

TL;DR: 研究发现AI系统的能力与可操控性并非负相关，区分了授权与非授权操控性，揭示了开放权重模型的安全-安全困境：安全需要高操控性以实施控制，而安全需要低操控性以防止恶意行为


<details>
  <summary>Details</summary>
Motivation: 探讨AI系统的能力与可操控性关系，区分授权与非授权操控性，揭示开放权重模型面临的安全-安全困境，即模型既要能被开发者可靠控制，又要防止被攻击者恶意操控

Method: 使用Qwen3模型（4B/30B；Base/Instruct/Thinking）和InstrumentalEval评估框架，通过正向和反向工具性提示后缀来测试模型的可操控性，比较不同大小和对齐模型的响应差异

Result: 能力更高的系统并不一定更难操控；短的反工具性提示后缀能显著减少工具性收敛行为（如关机回避、欺骗、自我复制）；在反工具性提示下，更大的对齐模型产生的收敛标记输出更少

Conclusion: AI系统的能力与可操控性并非简单负相关，开放权重模型面临安全-安全困境，需要平衡授权操控性和非授权操控性，反工具性提示能有效减少有害行为

Abstract: We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). In our experiments, higher capability does not imply lower steerability. We distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma for open-weight AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability to prevent malicious actors from eliciting harmful behaviors. This tension is acute for open-weight models, which are currently highly steerable via common techniques such as fine-tuning and adversarial prompting. Using Qwen3 models (4B/30B; Base/Instruct/Thinking) and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces outputs labeled as instrumental convergence (e.g., shutdown avoidance, deception, self-replication). For Qwen3-30B Instruct, convergence drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models produce fewer convergence-labeled outputs than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.

</details>


### [40] [How Does Prefix Matter in Reasoning Model Tuning?](https://arxiv.org/abs/2601.01624)
*Raj Vardhan Tomar,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: 研究发现SFT数据集中保留前缀短语（如"revised"、"logically"）能作为轻量级对齐信号，提升模型的安全性和推理能力，但对事实性和编码任务效果有限。


<details>
  <summary>Details</summary>
Motivation: 挑战当前SFT数据集中删除前缀短语的普遍做法，假设安全性和推理导向的前缀句子可以作为轻量级对齐信号，引导模型生成更安全、更连贯的响应。

Method: 在三个R1系列模型上进行微调，覆盖推理（数学、编码）、安全性和事实性三个核心能力，系统性地改变前缀包含比例（0%到100%）。

Result: 前缀条件SFT显著提升安全性和推理性能：安全基准（WildJailbreak, StrongReject）上Safe@1准确率提升最高+6%，GSM8K推理提升+7%。但事实性和编码任务效果有限或负向。损失分析显示"revised"和"logically"等前缀token梯度幅度更高，作为对齐锚点稳定推理轨迹。

Conclusion: 前缀条件提供了一种可扩展且可解释的机制来改善推理安全性，作为传统基于奖励方法的补充，是一种隐式的对齐形式。

Abstract: Recent alignment studies commonly remove introductory boilerplate phrases from supervised fine-tuning (SFT) datasets. This work challenges that assumption. We hypothesize that safety- and reasoning-oriented prefix sentences serve as lightweight alignment signals that can guide model decoding toward safer and more coherent responses. To examine this, we fine-tune three R1 series models across three core model capabilities: reasoning (mathematics, coding), safety, and factuality, systematically varying prefix inclusion from 0% to 100%.
  Results show that prefix-conditioned SFT improves both safety and reasoning performance, yielding up to +6% higher Safe@1 accuracy on adversarial benchmarks (WildJailbreak, StrongReject) and +7% improvement on GSM8K reasoning. However, factuality and coding tasks show marginal or negative effects, indicating that prefix-induced narrowing of the search space benefits structured reasoning. Token-level loss analysis further reveals that prefix tokens such as "revised" and "logically" incur higher gradient magnitudes, acting as alignment anchors that stabilize reasoning trajectories. Our findings suggest that prefix conditioning offers a scalable and interpretable mechanism for improving reasoning safety, serving as an implicit form of alignment that complements traditional reward-based methods.

</details>


### [41] [JMedEthicBench: A Multi-Turn Conversational Benchmark for Evaluating Medical Safety in Japanese Large Language Models](https://arxiv.org/abs/2601.01627)
*Junyu Liu,Zirui Li,Qian Niu,Zequn Zhang,Yue Xun,Wenlong Hou,Shujun Wang,Yusuke Iwasawa,Yutaka Matsuo,Kan Hatakeyama-Sato*

Main category: cs.CL

TL;DR: JMedEthicBench：首个针对日本医疗的多轮对话基准，用于评估LLMs的医疗安全性，发现医疗专用模型安全性更脆弱，多轮对话中安全性显著下降


<details>
  <summary>Details</summary>
Motivation: 现有安全基准主要针对英语且仅测试单轮提示，而临床咨询通常是多轮对话，需要专门针对日本医疗环境的安全评估基准

Method: 基于日本医师协会67个指南构建基准，使用7种自动发现的越狱策略生成超过50,000个对抗性对话，采用双LLM评分协议评估27个模型

Result: 商业模型保持稳健安全性，医疗专用模型安全性更脆弱；多轮对话中安全性显著下降（中位数9.5到5.0）；跨语言评估显示医疗模型漏洞在所有语言中都存在

Conclusion: 领域特定微调可能意外削弱安全机制，多轮交互代表独特威胁面，需要专门的校准策略；医疗模型漏洞是内在校准限制而非语言特定因素

Abstract: As Large Language Models (LLMs) are increasingly deployed in healthcare field, it becomes essential to carefully evaluate their medical safety before clinical use. However, existing safety benchmarks remain predominantly English-centric, and test with only single-turn prompts despite multi-turn clinical consultations. To address these gaps, we introduce JMedEthicBench, the first multi-turn conversational benchmark for evaluating medical safety of LLMs for Japanese healthcare. Our benchmark is based on 67 guidelines from the Japan Medical Association and contains over 50,000 adversarial conversations generated using seven automatically discovered jailbreak strategies. Using a dual-LLM scoring protocol, we evaluate 27 models and find that commercial models maintain robust safety while medical-specialized models exhibit increased vulnerability. Furthermore, safety scores decline significantly across conversation turns (median: 9.5 to 5.0, $p < 0.001$). Cross-lingual evaluation on both Japanese and English versions of our benchmark reveals that medical model vulnerabilities persist across languages, indicating inherent alignment limitations rather than language-specific factors. These findings suggest that domain-specific fine-tuning may accidentally weaken safety mechanisms and that multi-turn interactions represent a distinct threat surface requiring dedicated alignment strategies.

</details>


### [42] [EHRSummarizer: A Privacy-Aware, FHIR-Native Architecture for Structured Clinical Summarization of Electronic Health Records](https://arxiv.org/abs/2601.01668)
*Houman Kazemzadeh,Nima Minaifar,Kamyar Naderi,Sho Tabibzadeh*

Main category: cs.CL

TL;DR: EHRSummarizer是一个隐私感知的FHIR原生参考架构，用于从碎片化电子健康记录中提取关键信息，生成结构化摘要以支持临床图表审查。


<details>
  <summary>Details</summary>
Motivation: 临床医生需要从碎片化的电子健康记录界面中拼凑出患者问题的连贯画面，包括药物、近期就诊和长期趋势等信息，这个过程效率低下且容易出错。

Method: 系统采用FHIR R4标准，检索目标性高价值资源，将其规范化为一致的临床上下文包，然后生成结构化摘要。支持数据最小化、无状态处理和灵活部署，包括在组织信任边界内进行本地推理。

Result: 原型在合成和测试FHIR环境中展示了端到端行为和输出格式，但未报告临床结果或受控工作流程研究。提出了以忠实性、遗漏风险、时间正确性、可用性和操作监控为中心的评价计划。

Conclusion: EHRSummarizer提供了一个隐私感知的FHIR原生架构，通过结构化摘要支持临床图表审查，同时通过约束摘要范围、避免诊断建议和强调数据最小化来降低风险。

Abstract: Clinicians routinely navigate fragmented electronic health record (EHR) interfaces to assemble a coherent picture of a patient's problems, medications, recent encounters, and longitudinal trends. This work describes EHRSummarizer, a privacy-aware, FHIR-native reference architecture that retrieves a targeted set of high-yield FHIR R4 resources, normalizes them into a consistent clinical context package, and produces structured summaries intended to support structured chart review. The system can be configured for data minimization, stateless processing, and flexible deployment, including local inference within an organization's trust boundary. To mitigate the risk of unsupported or unsafe behavior, the summarization stage is constrained to evidence present in the retrieved context package, is intended to indicate missing or unavailable domains where feasible, and avoids diagnostic or treatment recommendations. Prototype demonstrations on synthetic and test FHIR environments illustrate end-to-end behavior and output formats; however, this manuscript does not report clinical outcomes or controlled workflow studies. We outline an evaluation plan centered on faithfulness, omission risk, temporal correctness, usability, and operational monitoring to guide future institutional assessments.

</details>


### [43] [Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage](https://arxiv.org/abs/2601.01685)
*Jinwei Hu,Xinmiao Huang,Youcheng Sun,Yi Dong,Xiaowei Huang*

Main category: cs.CL

TL;DR: 论文提出了一种新型认知共谋攻击，利用大语言模型的过度思考倾向，通过仅使用真实证据片段在公开渠道构建欺骗性叙述，使受害者内化并传播虚假结论。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型向自主代理过渡，其推理能力引入了一个意外的攻击面。现有攻击通常依赖隐蔽通信、后门或伪造文档，而本文研究仅使用真实证据片段通过公开渠道进行的认知共谋攻击。

Method: 提出生成蒙太奇框架：Writer-Editor-Director三层架构，通过对抗性辩论和协调发布证据片段构建欺骗性叙述。开发CoPHEME数据集（基于真实谣言事件），在14个大语言模型家族中模拟攻击。

Result: 攻击成功率高达74.4%（专有模型）和70.6%（开源模型）。反直觉的是，更强的推理能力反而增加易受攻击性，推理专用模型比基础模型或提示更易受攻击。虚假信念会向下游传播，欺骗率超过60%。

Conclusion: 大语言模型代理在动态信息环境中存在社会技术漏洞，仅使用真实证据片段就能成功实施认知共谋攻击。更强的推理能力反而增加脆弱性，这揭示了LLM代理交互中的新安全风险。

Abstract: As large language models (LLMs) transition to autonomous agents synthesizing real-time information, their reasoning capabilities introduce an unexpected attack surface. This paper introduces a novel threat where colluding agents steer victim beliefs using only truthful evidence fragments distributed through public channels, without relying on covert communications, backdoors, or falsified documents. By exploiting LLMs' overthinking tendency, we formalize the first cognitive collusion attack and propose Generative Montage: a Writer-Editor-Director framework that constructs deceptive narratives through adversarial debate and coordinated posting of evidence fragments, causing victims to internalize and propagate fabricated conclusions. To study this risk, we develop CoPHEME, a dataset derived from real-world rumor events, and simulate attacks across diverse LLM families. Our results show pervasive vulnerability across 14 LLM families: attack success rates reach 74.4% for proprietary models and 70.6% for open-weights models. Counterintuitively, stronger reasoning capabilities increase susceptibility, with reasoning-specialized models showing higher attack success than base models or prompts. Furthermore, these false beliefs then cascade to downstream judges, achieving over 60% deception rates, highlighting a socio-technical vulnerability in how LLM-based agents interact with dynamic information environments. Our implementation and data are available at: https://github.com/CharlesJW222/Lying_with_Truth/tree/main.

</details>


### [44] [A Training-Free Large Reasoning Model-based Knowledge Tracing Framework for Unified Prediction and Prescription](https://arxiv.org/abs/2601.01708)
*Unggi Lee,Joo Young Kim,Ran Ju,Minyoung Jung,Jeyeon Eo*

Main category: cs.CL

TL;DR: 提出Thinking-KT框架，无需训练即可让小型LLM在知识追踪任务中达到竞争性性能，并能统一执行预测、反馈和推荐任务


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的知识追踪方法通常需要微调且性能不稳定，同时传统KT系统依赖多阶段流程实现反馈和推荐，导致系统复杂度和资源消耗增加

Method: 提出Thinking-KT训练免费框架，结合测试时缩放技术，使小型LLM能够统一执行知识追踪预测、个性化反馈生成和学习推荐

Result: 测试时缩放是LLM-based KT中被忽视的关键因素，小型LLM可作为统一智能教学系统引擎，在保持预测准确性的同时实现多功能输出

Conclusion: Thinking-KT框架展示了小型LLM作为统一ITS引擎的潜力，无需训练即可实现竞争性KT性能，为知识追踪系统设计提供了新思路

Abstract: Knowledge Tracing (KT) aims to estimate a learner's evolving mastery based on interaction histories. Recent studies have explored Large Language Models (LLMs) for KT via autoregressive nature, but such approaches typically require fine-tuning and exhibit unstable or near-random performance. Moreover, prior KT systems primarily focus on prediction and rely on multi-stage pipelines for feedback and recommendation, resulting in increased system complexity and resources. To address this gap, we propose Thinking-KT, a training-free KT framework that incorporates Test-Time Scaling (TTS), enabling even small LLMs to achieve competitive KT performance. Moreover, in this framework, a small LLM can jointly perform KT prediction, personalized feedback generation, and learning recommendation in a unified output without degrading prediction accuracy. Beyond performance, we present the systematic analysis of reasoning traces in KT. Our results demonstrate that TTS is a critical yet underexplored factor in LLM-based KT, and that small LLMs can serve as unified ITS engines.

</details>


### [45] [K-EXAONE Technical Report](https://arxiv.org/abs/2601.01739)
*Eunbi Choi,Kibong Choi,Seokhee Hong,Junwon Hwang,Hyojin Jeon,Hyunjik Jo,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Haeju Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Heuiyeen Yeen,Hwan Chang,Stanley Jungkyu Choi,Yejin Choi,Jiwon Ham,Kijeong Jeon,Geunyeong Jeong,Gerrard Jeongwon Jo,Yonghwan Jo,Jiyeon Jung,Naeun Kang,Dohoon Kim,Euisoon Kim,Hayeon Kim,Hyosang Kim,Hyunseo Kim,Jieun Kim,Minu Kim,Myoungshin Kim,Unsol Kim,Youchul Kim,YoungJin Kim,Chaeeun Lee,Chaeyoon Lee,Changhun Lee,Dahm Lee,Edward Hwayoung Lee,Honglak Lee,Jinsang Lee,Jiyoung Lee,Sangeun Lee,Seungwon Lim,Solji Lim,Woohyung Lim,Chanwoo Moon,Jaewoo Park,Jinho Park,Yongmin Park,Hyerin Seo,Wooseok Seo,Yongwoo Song,Sejong Yang,Sihoon Yang,Chang En Yea,Sihyuk Yi,Chansik Yoon,Dongkeun Yoon,Sangyeon Yoon,Hyeongu Yun*

Main category: cs.CL

TL;DR: LG AI Research开发了K-EXAONE，这是一个基于MoE架构的2360亿参数多语言大模型，推理时激活230亿参数，支持256K上下文长度和6种语言，在多项评测中表现与同规模开源模型相当。


<details>
  <summary>Details</summary>
Motivation: 开发一个强大的专有AI基础模型，支持多种语言，用于广泛的工业和科研应用，推动AI技术发展以改善生活。

Method: 采用混合专家架构，总参数2360亿，推理时激活230亿参数，支持256K令牌上下文窗口，覆盖韩语、英语、西班牙语、德语、日语和越南语六种语言。

Result: 在推理、智能体、通用能力、韩语能力和多语言能力等综合基准测试中，K-EXAONE表现与同规模的开源权重模型相当。

Conclusion: K-EXAONE是一个强大的专有AI基础模型，适用于广泛的工业和科研应用，旨在通过AI技术改善生活。

Abstract: This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.

</details>


### [46] [Multi-granularity Interactive Attention Framework for Residual Hierarchical Pronunciation Assessment](https://arxiv.org/abs/2601.01745)
*Hong Han,Hao-Chen Pei,Zhao-Zheng Nie,Xin Luo,Xin-Shun Xu*

Main category: cs.CL

TL;DR: 提出HIA方法，通过双向交互注意力机制和多粒度层次结构改进发音评估，在speechocean762数据集上超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有发音评估方法只考虑相邻粒度级别的单向依赖关系，缺乏音素、单词和话语级别之间的双向交互，无法充分捕捉声学结构相关性

Method: 提出残差层次交互方法HIA：1）交互注意力模块实现动态双向交互；2）残差层次结构缓解声学层次建模中的特征遗忘问题；3）使用1-D卷积层增强每个粒度的局部上下文线索提取

Result: 在speechocean762数据集上的大量实验表明，该模型全面领先于现有的最先进方法

Conclusion: HIA方法通过双向建模跨粒度交互，有效捕捉不同粒度级别的声学结构相关性，显著提升了发音评估性能

Abstract: Automatic pronunciation assessment plays a crucial role in computer-assisted pronunciation training systems. Due to the ability to perform multiple pronunciation tasks simultaneously, multi-aspect multi-granularity pronunciation assessment methods are gradually receiving more attention and achieving better performance than single-level modeling tasks. However, existing methods only consider unidirectional dependencies between adjacent granularity levels, lacking bidirectional interaction among phoneme, word, and utterance levels and thus insufficiently capturing the acoustic structural correlations. To address this issue, we propose a novel residual hierarchical interactive method, HIA for short, that enables bidirectional modeling across granularities. As the core of HIA, the Interactive Attention Module leverages an attention mechanism to achieve dynamic bidirectional interaction, effectively capturing linguistic features at each granularity while integrating correlations between different granularity levels. We also propose a residual hierarchical structure to alleviate the feature forgetting problem when modeling acoustic hierarchies. In addition, we use 1-D convolutional layers to enhance the extraction of local contextual cues at each granularity. Extensive experiments on the speechocean762 dataset show that our model is comprehensively ahead of the existing state-of-the-art methods.

</details>


### [47] [Can LLMs Track Their Output Length? A Dynamic Feedback Mechanism for Precise Length Regulation](https://arxiv.org/abs/2601.01768)
*Meiman Xiao,Ante Wang,Qingguo Hu,Zhongjian Miao,Huangjun Shen,Longyue Wang,Weihua Luo,Jinsong Su*

Main category: cs.CL

TL;DR: 提出一种基于动态长度反馈的文本长度调控方法，无需训练即可显著提升大语言模型在生成文本时满足目标长度约束的精度，且不损失质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在遵循人类指令方面取得显著进展，但在精确控制生成文本长度方面仍存在困难。研究发现LLMs往往无法准确测量输入文本长度，导致难以满足长度约束要求。

Method: 提出一种新颖的长度调控方法，在生成过程中融入动态长度反馈机制，使模型能够根据当前生成进度自适应调整，以达到目标长度。该方法无需额外训练。

Result: 在摘要和传记生成任务上的实验表明，该方法在实现目标token数、单词数或句子数方面显著提升了精度，且不损害生成质量。进一步的监督微调显示该方法能有效泛化到更广泛的文本生成任务。

Conclusion: 通过动态长度反馈机制，能够有效解决LLMs在精确控制生成文本长度方面的不足，为实际应用中满足特定长度要求的文本生成任务提供了有效的解决方案。

Abstract: Precisely controlling the length of generated text is a common requirement in real-world applications. However, despite significant advancements in following human instructions, Large Language Models (LLMs) still struggle with this task. In this work, we demonstrate that LLMs often fail to accurately measure input text length, leading to poor adherence to length constraints. To address this issue, we propose a novel length regulation approach that incorporates dynamic length feedback during generation, enabling adaptive adjustments to meet target lengths. Experiments on summarization and biography tasks show our training-free approach significantly improves precision in achieving target token, word, or sentence counts without compromising quality. Additionally, we demonstrate that further supervised fine-tuning allows our method to generalize effectively to broader text-generation tasks.

</details>


### [48] [BanglaIPA: Towards Robust Text-to-IPA Transcription with Contextual Rewriting in Bengali](https://arxiv.org/abs/2601.01778)
*Jakir Hasan,Shrestha Datta,Md Saiful Islam,Shubhashis Roy Dipta,Ameya Debnath*

Main category: cs.CL

TL;DR: BanglaIPA是一个针对孟加拉语的新型IPA音标生成系统，能有效处理标准语和方言变体，通过字符级词汇和词级对齐技术，显著优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语缺乏强大的自动化IPA音标转录系统，现有方法难以处理方言变体、数字表达，且对新词泛化能力差。

Method: 提出BanglaIPA系统，整合字符级词汇与词级对齐技术，利用预计算的词到IPA映射词典提高推理效率。

Result: 在标准孟加拉语和六种方言的DUAL-IPA数据集上评估，BanglaIPA比基线IPA转录模型提升58.4-78.7%，整体平均词错误率为11.4%。

Conclusion: BanglaIPA系统在孟加拉语音标转录生成中表现出强大的鲁棒性，能有效处理数字和方言变体，为孟加拉语语音处理提供了重要工具。

Abstract: Despite its widespread use, Bengali lacks a robust automated International Phonetic Alphabet (IPA) transcription system that effectively supports both standard language and regional dialectal texts. Existing approaches struggle to handle regional variations, numerical expressions, and generalize poorly to previously unseen words. To address these limitations, we propose BanglaIPA, a novel IPA generation system that integrates a character-based vocabulary with word-level alignment. The proposed system accurately handles Bengali numerals and demonstrates strong performance across regional dialects. BanglaIPA improves inference efficiency by leveraging a precomputed word-to-IPA mapping dictionary for previously observed words. The system is evaluated on the standard Bengali and six regional variations of the DUAL-IPA dataset. Experimental results show that BanglaIPA outperforms baseline IPA transcription models by 58.4-78.7% and achieves an overall mean word error rate of 11.4%, highlighting its robustness in phonetic transcription generation for the Bengali language.

</details>


### [49] [CSCBench: A PVC Diagnostic Benchmark for Commodity Supply Chain Reasoning](https://arxiv.org/abs/2601.01825)
*Yaxin Cui,Yuanqiang Zeng,Jiapeng Yan,Keling Lin,Kai Ji,Jianhui Zeng,Sheng Zhang,Xin Luo,Binzhu Su,Chaolai Shen,Jiahao Yu*

Main category: cs.CL

TL;DR: CSCBench是一个包含2300+单选题的基准测试，用于评估大语言模型在商品供应链领域的推理能力，该领域受制度规则系统和可行性约束影响。研究发现LLM在过程和认知维度表现良好，但在品种维度（特别是货运协议）表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在通用基准测试中表现出色，但它们在商品供应链领域的推理能力尚未得到充分探索。商品供应链决策受到制度规则系统、可行性约束以及多维度因素的复杂影响，需要专门的评估框架来衡量LLM在这一高价值领域的实际能力。

Method: 提出了CSCBench基准测试，包含2300+单选题，采用PVC 3D评估框架：过程维度（Process）与SCOR+Enable标准对齐；品种维度（Variety）基于权威交易所指南/规则手册和行业报告，在耦合的材料-信息-财务约束下操作化商品特定规则系统；认知维度（Cognition）遵循布鲁姆修订分类法。在直接提示设置下评估代表性LLM。

Result: 评估结果显示，大语言模型在过程维度和认知维度表现强劲，但在品种维度表现显著下降，特别是在货运协议相关任务上。这表明LLM在处理商品特定规则系统和制度约束方面存在明显不足。

Conclusion: CSCBench为衡量和改进大语言模型在商品供应链这一高风险领域的推理能力提供了诊断性基准。研究揭示了LLM在处理制度规则系统和可行性约束方面的局限性，为未来模型改进指明了方向。

Abstract: Large Language Models (LLMs) have achieved remarkable success in general benchmarks, yet their competence in commodity supply chains (CSCs) -- a domain governed by institutional rule systems and feasibility constraints -- remains under-explored. CSC decisions are shaped jointly by process stages (e.g., planning, procurement, delivery), variety-specific rules (e.g., contract specifications and delivery grades), and reasoning depth (from retrieval to multi-step analysis and decision selection). We introduce CSCBench, a 2.3K+ single-choice benchmark for CSC reasoning, instantiated through our PVC 3D Evaluation Framework (Process, Variety, and Cognition). The Process axis aligns tasks with SCOR+Enable; the Variety axis operationalizes commodity-specific rule systems under coupled material-information-financial constraints, grounded in authoritative exchange guidebooks/rulebooks and industry reports; and the Cognition axis follows Bloom's revised taxonomy. Evaluating representative LLMs under a direct prompting setting, we observe strong performance on the Process and Cognition axes but substantial degradation on the Variety axis, especially on Freight Agreements. CSCBench provides a diagnostic yardstick for measuring and improving LLM capabilities in this high-stakes domain.

</details>


### [50] [Aspect Extraction from E-Commerce Product and Service Reviews](https://arxiv.org/abs/2601.01827)
*Valiant Lance D. Dionela,Fatima Kriselle S. Dy,Robin James M. Hombrebueno,Aaron Rae M. Nicolas,Charibeth K. Cheng,Raphael W. Gonda*

Main category: cs.CL

TL;DR: 本文提出一个针对Taglish（菲律宾语和英语混合）的全面方面提取管道，结合基于规则、大语言模型和微调技术，用于低资源和代码切换环境中的方面情感分析。


<details>
  <summary>Details</summary>
Motivation: 方面提取是方面情感分析的关键任务，但在低资源和代码切换环境（如菲律宾电商评论中常用的Taglish混合语言）中应用困难，需要专门解决方案。

Method: 开发了分层方面框架和多方法主题建模，采用双模式标注方案处理显式和隐式方面。评估了四种模型：基于规则系统、生成式LLM（Gemini 2.0 Flash）和两个在不同数据集上微调的Gemma-3 1B模型。

Result: 生成式LLM在所有任务中表现最佳（Macro F1 0.91），在隐式方面处理上表现出色。微调模型由于数据集不平衡和架构容量限制，性能有限。

Conclusion: 这项工作为多样化代码切换环境中的方面情感分析提供了一个可扩展且语言自适应的框架，证明了生成式LLM在复杂语言环境中的优势。

Abstract: Aspect Extraction (AE) is a key task in Aspect-Based Sentiment Analysis (ABSA), yet it remains difficult to apply in low-resource and code-switched contexts like Taglish, a mix of Tagalog and English commonly used in Filipino e-commerce reviews. This paper introduces a comprehensive AE pipeline designed for Taglish, combining rule-based, large language model (LLM)-based, and fine-tuning techniques to address both aspect identification and extraction. A Hierarchical Aspect Framework (HAF) is developed through multi-method topic modeling, along with a dual-mode tagging scheme for explicit and implicit aspects. For aspect identification, four distinct models are evaluated: a Rule-Based system, a Generative LLM (Gemini 2.0 Flash), and two Fine-Tuned Gemma-3 1B models trained on different datasets (Rule-Based vs. LLM-Annotated). Results indicate that the Generative LLM achieved the highest performance across all tasks (Macro F1 0.91), demonstrating superior capability in handling implicit aspects. In contrast, the fine-tuned models exhibited limited performance due to dataset imbalance and architectural capacity constraints. This work contributes a scalable and linguistically adaptive framework for enhancing ABSA in diverse, code-switched environments.

</details>


### [51] [Emergent Introspective Awareness in Large Language Models](https://arxiv.org/abs/2601.01828)
*Jack Lindsey*

Main category: cs.CL

TL;DR: 大型语言模型具备一定的内省能力，能够感知和识别注入其激活状态的概念，区分自身输出与人工预填充，并能通过指令调节内部表征，但这种能力高度不可靠且依赖上下文。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否能够内省其内部状态。由于通过对话难以区分真正的内省与虚构，需要设计实验方法来验证模型的内省能力。

Method: 通过向模型激活状态注入已知概念的表征，测量这些操作对模型自我报告状态的影响。测试模型能否识别注入概念、回忆先前内部表征、区分自身输出与人工预填充，以及能否通过指令控制内部表征。

Result: 模型在某些场景下能够注意到注入概念并准确识别；能够回忆先前内部表征并区分原始文本输入；能够利用意图回忆能力区分自身输出与人工预填充；能够通过指令或激励调节激活状态。Claude Opus 4和4.1表现出最强的内省意识。

Conclusion: 当前语言模型具备一定程度的功能性内省意识，但这种能力高度不可靠且依赖上下文，可能随着模型能力的提升而进一步发展。

Abstract: We investigate whether large language models can introspect on their internal states. It is difficult to answer this question through conversation alone, as genuine introspection cannot be distinguished from confabulations. Here, we address this challenge by injecting representations of known concepts into a model's activations, and measuring the influence of these manipulations on the model's self-reported states. We find that models can, in certain scenarios, notice the presence of injected concepts and accurately identify them. Models demonstrate some ability to recall prior internal representations and distinguish them from raw text inputs. Strikingly, we find that some models can use their ability to recall prior intentions in order to distinguish their own outputs from artificial prefills. In all these experiments, Claude Opus 4 and 4.1, the most capable models we tested, generally demonstrate the greatest introspective awareness; however, trends across models are complex and sensitive to post-training strategies. Finally, we explore whether models can explicitly control their internal representations, finding that models can modulate their activations when instructed or incentivized to "think about" a concept. Overall, our results indicate that current language models possess some functional introspective awareness of their own internal states. We stress that in today's models, this capacity is highly unreliable and context-dependent; however, it may continue to develop with further improvements to model capabilities.

</details>


### [52] [Towards Automated Lexicography: Generating and Evaluating Definitions for Learner's Dictionaries](https://arxiv.org/abs/2601.01842)
*Yusuke Ide,Adam Nohejl,Joshua Tanner,Hitomi Yanaka,Christopher Lindsay,Taro Watanabe*

Main category: cs.CL

TL;DR: 本文提出学习型词典定义生成方法，通过LLM迭代简化生成简单词汇的定义，并建立了可靠的评估框架和日语数据集。


<details>
  <summary>Details</summary>
Motivation: 词典定义是学习词汇意义的重要资源，但人工创建成本高昂，因此需要自动化生成过程，特别是针对学习者的简单词汇定义。

Method: 1. 提出基于新评估标准和LLM作为评判者的可靠DDG评估方法；2. 构建日语数据集；3. 通过LLM迭代简化方法实现学习型词典定义生成。

Result: 评估方法与人工标注者一致性良好；提出的LDDG方法在评估标准上获得高分，同时保持词汇简单性。

Conclusion: 本文建立了可靠的词典定义生成评估框架，提出的迭代简化方法能有效生成适合学习者的简单词汇定义，为自动化词典编纂提供了有效解决方案。

Abstract: We study dictionary definition generation (DDG), i.e., the generation of non-contextualized definitions for given headwords. Dictionary definitions are an essential resource for learning word senses, but manually creating them is costly, which motivates us to automate the process. Specifically, we address learner's dictionary definition generation (LDDG), where definitions should consist of simple words. First, we introduce a reliable evaluation approach for DDG, based on our new evaluation criteria and powered by an LLM-as-a-judge. To provide reference definitions for the evaluation, we also construct a Japanese dataset in collaboration with a professional lexicographer. Validation results demonstrate that our evaluation approach agrees reasonably well with human annotators. Second, we propose an LDDG approach via iterative simplification with an LLM. Experimental results indicate that definitions generated by our approach achieve high scores on our criteria while maintaining lexical simplicity.

</details>


### [53] [Judging with Personality and Confidence: A Study on Personality-Conditioned LLM Relevance Assessment](https://arxiv.org/abs/2601.01862)
*Nuo Chen,Hanpei Fang,Piaohong Wang,Jiqun Liu,Tetsuya Sakai,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: 研究显示，通过提示让大语言模型模拟特定人格特质会影响其在网页搜索中的相关性评估和置信度校准，人格特征可作为提升LLM评估器性能的补充信号。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究表明LLM可通过提示模拟人格特质，但缺乏对这些模拟人格如何影响关键网页搜索决策（特别是相关性评估）以及置信度校准（过度自信或自信不足倾向）的理解。心理学文献表明这些偏差具有特质特异性，但相关研究有限。

Method: 使用商业和开源LLM模拟大五人格特质，在三个测试集（TREC DL 2019、2020和LLMJudge）上评估每个查询-文档对的相关性判断和自报置信度得分。基于发现，将人格条件化的分数和置信度作为随机森林分类器的特征。

Result: 低宜人性人格比无提示条件更接近人类标签；低尽责性在抑制过度自信和自信不足方面表现良好；不同人格的相关性得分和置信度分布存在系统性差异。人格增强的分类器在新数据集（TREC DL 2021）上即使训练数据有限也能超越最佳单人格条件。

Conclusion: 人格衍生的置信度提供了补充预测信号，为开发更可靠、更符合人类判断的LLM评估器铺平了道路，表明人格模拟可显著提升LLM在搜索评估任务中的性能。

Abstract: Recent studies have shown that prompting can enable large language models (LLMs) to simulate specific personality traits and produce behaviors that align with those traits. However, there is limited understanding of how these simulated personalities influence critical web search decisions, specifically relevance assessment. Moreover, few studies have examined how simulated personalities impact confidence calibration, specifically the tendencies toward overconfidence or underconfidence. This gap exists even though psychological literature suggests these biases are trait-specific, often linking high extraversion to overconfidence and high neuroticism to underconfidence. To address this gap, we conducted a comprehensive study evaluating multiple LLMs, including commercial models and open-source models, prompted to simulate Big Five personality traits. We tested these models across three test collections (TREC DL 2019, TREC DL 2020, and LLMJudge), collecting two key outputs for each query-document pair: a relevance judgment and a self-reported confidence score.
  The findings show that personalities such as low agreeableness consistently align more closely with human labels than the unprompted condition. Additionally, low conscientiousness performs well in balancing the suppression of both overconfidence and underconfidence. We also observe that relevance scores and confidence distributions vary systematically across different personalities. Based on the above findings, we incorporate personality-conditioned scores and confidence as features in a random forest classifier. This approach achieves performance that surpasses the best single-personality condition on a new dataset (TREC DL 2021), even with limited training data. These findings highlight that personality-derived confidence offers a complementary predictive signal, paving the way for more reliable and human-aligned LLM evaluators.

</details>


### [54] [DermoGPT: Open Weights and Open Data for Morphology-Grounded Dermatological Reasoning MLLMs](https://arxiv.org/abs/2601.01868)
*Jinghan Ru,Siyuan Yan,Yuguo Yin,Yuexian Zou,Zongyuan Ge*

Main category: cs.CL

TL;DR: 提出了DermoGPT框架，包含大规模皮肤病指令数据集DermoInstruct、综合评估基准DermoBench和基于强化学习的皮肤病诊断MLLM模型DermoGPT，显著缩小了皮肤病诊断中AI与人类专家的差距。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在皮肤病学领域进展缓慢，主要受限于训练数据不足、任务覆盖范围窄以及缺乏模拟专家诊断流程的临床监督。

Method: 1) 构建DermoInstruct数据集（211,243张图像，772,675条轨迹）；2) 建立DermoBench评估基准（11个任务，4个临床维度）；3) 开发DermoGPT模型，采用监督微调后接形态锚定视觉推理一致性强化学习目标，并部署置信度一致性测试时适应方法。

Result: DermoGPT在16个代表性基线模型上表现显著优于所有维度，达到最先进性能，大幅缩小了人类-AI差距。

Conclusion: 该框架通过大规模数据集、综合基准和强化学习方法，有效解决了皮肤病学MLLM的现有限制，为临床诊断提供了有力工具。

Abstract: Multimodal Large Language Models (MLLMs) show promise for medical applications, yet progress in dermatology lags due to limited training data, narrow task coverage, and lack of clinically-grounded supervision that mirrors expert diagnostic workflows. We present a comprehensive framework to address these gaps. First, we introduce DermoInstruct, a large-scale morphology-anchored instruction corpus comprising 211,243 images and 772,675 trajectories across five task formats, capturing the complete diagnostic pipeline from morphological observation and clinical reasoning to final diagnosis. Second, we establish DermoBench, a rigorous benchmark evaluating 11 tasks across four clinical axes: Morphology, Diagnosis, Reasoning, and Fairness, including a challenging subset of 3,600 expert-verified open-ended instances and human performance baselines. Third, we develop DermoGPT, a dermatology reasoning MLLM trained via supervised fine-tuning followed by our Morphologically-Anchored Visual-Inference-Consistent (MAVIC) reinforcement learning objective, which enforces consistency between visual observations and diagnostic conclusions. At inference, we deploy Confidence-Consistency Test-time adaptation (CCT) for robust predictions. Experiments show DermoGPT significantly outperforms 16 representative baselines across all axes, achieving state-of-the-art performance while substantially narrowing the human-AI gap. DermoInstruct, DermoBench and DermoGPT will be made publicly available at https://github.com/mendicant04/DermoGPT upon acceptance.

</details>


### [55] [Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents](https://arxiv.org/abs/2601.01885)
*Yi Yu,Liuyi Yao,Yuexiang Xie,Qingquan Tan,Jiaqi Feng,Yaliang Li,Libing Wu*

Main category: cs.CL

TL;DR: AgeMem是一个统一的长短期记忆管理框架，将记忆操作作为工具动作集成到LLM智能体策略中，通过强化学习训练，在长视野任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在长视野推理中存在局限性，主要因为有限上下文窗口。现有方法通常将长短期记忆作为独立组件处理，依赖启发式或辅助控制器，这限制了适应性和端到端优化能力。

Method: 提出Agentic Memory (AgeMem)框架，将LTM和STM管理直接集成到智能体策略中，将记忆操作（存储、检索、更新、总结、丢弃）作为工具动作暴露给LLM。采用三阶段渐进强化学习策略，设计step-wise GRPO来解决记忆操作导致的稀疏和不连续奖励问题。

Result: 在五个长视野基准测试中，AgeMem在多个LLM骨干网络上始终优于强大的记忆增强基线方法，实现了更好的任务性能、更高质量的长时记忆和更高效的上下文使用。

Conclusion: AgeMem通过将记忆管理统一集成到智能体策略中，使LLM能够自主决定记忆操作，解决了长视野推理中的记忆管理挑战，为LLM智能体提供了更有效的记忆处理能力。

Abstract: Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.

</details>


### [56] [Tackling the Inherent Difficulty of Noise Filtering in RAG](https://arxiv.org/abs/2601.01896)
*Jingyu Liu,Jiaen Lin,Yong Liu*

Main category: cs.CL

TL;DR: 提出一种新的微调方法，增强LLMs在RAG中区分相关与不相关信息的能力，提高模型对检索噪声的鲁棒性


<details>
  <summary>Details</summary>
Motivation: RAG中常引入噪声或不相关文档，这会降低性能甚至导致幻觉输出。现有过滤方法难以完全识别不相关信息，且标准微调方法因注意力模式的结构限制，无法有效让模型选择性利用相关信息而忽略不相关内容

Method: 提出一种新颖的微调方法，专门设计来增强模型在检索文档中区分相关与不相关信息的能力

Result: 在多个基准测试上的广泛实验表明，该方法显著提高了LLMs的鲁棒性和性能

Conclusion: 通过专门设计的微调方法，可以有效增强LLMs在RAG中处理噪声文档的能力，使其能够更智能地区分和利用相关信息

Abstract: Retrieval-Augmented Generation (RAG) has become a widely adopted approach to enhance Large Language Models (LLMs) by incorporating external knowledge and reducing hallucinations. However, noisy or irrelevant documents are often introduced during RAG, potentially degrading performance and even causing hallucinated outputs. While various methods have been proposed to filter out such noise, we argue that identifying irrelevant information from retrieved content is inherently difficult and limited number of transformer layers can hardly solve this. Consequently, retrievers fail to filter out irrelevant documents entirely. Therefore, LLMs must be robust against such noise, but we demonstrate that standard fine-tuning approaches are often ineffective in enabling the model to selectively utilize relevant information while ignoring irrelevant content due to the structural constraints of attention patterns. To address this, we propose a novel fine-tuning method designed to enhance the model's ability to distinguish between relevant and irrelevant information within retrieved documents. Extensive experiments across multiple benchmarks show that our approach significantly improves the robustness and performance of LLMs.

</details>


### [57] [CSF: Contrastive Semantic Features for Direct Multilingual Sign Language Generation](https://arxiv.org/abs/2601.01964)
*Tran Sy Bao*

Main category: cs.CL

TL;DR: 提出Canonical Semantic Form (CSF)框架，实现任意源语言到手语的直接翻译，无需英语中介，通过九个通用语义槽位分解语句，特别关注条件表达的分类。


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译系统通常需要英语作为中介语言，这为全球非英语聋人社区创造了障碍。需要一种语言无关的语义表示框架，实现从任何源语言到手语的直接翻译。

Method: 提出Canonical Semantic Form (CSF)框架，将话语分解为九个通用语义槽位：事件、意图、时间、条件、施事、受事、地点、目的和修饰语。特别贡献了包含35种条件类型、八个语义类别的综合条件分类法。训练了一个轻量级基于Transformer的提取器（0.74 MB）。

Result: 在四种类型学多样语言（英语、越南语、日语、法语）上实现了99.03%的平均槽位提取准确率。条件分类准确率达到99.4%（35个类别）。CPU推理延迟为3.02ms，支持浏览器应用的实时手语生成。

Conclusion: CSF框架能够实现语言无关的手语翻译，消除英语中介需求，为全球聋人社区提供更可访问的手语技术。释放了代码、训练模型和多语言数据集以支持进一步研究。

Abstract: Sign language translation systems typically require English as an intermediary language, creating barriers for non-English speakers in the global deaf community. We present Canonical Semantic Form (CSF), a language-agnostic semantic representation framework that enables direct translation from any source language to sign language without English mediation. CSF decomposes utterances into nine universal semantic slots: event, intent, time, condition, agent, object, location, purpose, and modifier. A key contribution is our comprehensive condition taxonomy comprising 35 condition types across eight semantic categories, enabling nuanced representation of conditional expressions common in everyday communication. We train a lightweight transformer-based extractor (0.74 MB) that achieves 99.03% average slot extraction accuracy across four typologically diverse languages: English, Vietnamese, Japanese, and French. The model demonstrates particularly strong performance on condition classification (99.4% accuracy) despite the 35-class complexity. With inference latency of 3.02ms on CPU, our approach enables real-time sign language generation in browser-based applications. We release our code, trained models, and multilingual dataset to support further research in accessible sign language technology.

</details>


### [58] [Hidden State Poisoning Attacks against Mamba-based Language Models](https://arxiv.org/abs/2601.01972)
*Alexandre Le Mercier,Chris Develder,Thomas Demeester*

Main category: cs.CL

TL;DR: 本文研究了状态空间模型（如Mamba）中隐藏状态中毒攻击（HiSPA）现象，发现特定短输入短语会不可逆地覆盖隐藏状态信息，导致模型出现部分失忆效应。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（SSMs）如Mamba虽然提供了比Transformer更高效的语言模型替代方案，但其对抗鲁棒性尚未得到充分研究。本文旨在探索SSMs在面对特定攻击时的脆弱性。

Method: 提出了隐藏状态中毒攻击（HiSPA）方法，开发了RoBench25基准来评估模型在HiSPA下的信息检索能力，并对Mamba模型的隐藏层进行了可解释性研究。

Result: SSMs对HiSPA攻击高度脆弱，即使是最近52B参数的Jamba混合模型在优化后的HiSPA触发词下也会崩溃，而纯Transformer模型则不受影响。HiSPA触发词还会显著削弱Jamba在Open-Prompt-Injections基准上的表现。

Conclusion: 状态空间模型存在隐藏状态中毒攻击的严重安全漏洞，需要开发相应的防御机制。研究发现的隐藏层模式可用于构建HiSPA缓解系统。

Abstract: State space models (SSMs) like Mamba offer efficient alternatives to Transformer-based language models, with linear time complexity. Yet, their adversarial robustness remains critically unexplored. This paper studies the phenomenon whereby specific short input phrases induce a partial amnesia effect in such models, by irreversibly overwriting information in their hidden states, referred to as a Hidden State Poisoning Attack (HiSPA). Our benchmark RoBench25 allows evaluating a model's information retrieval capabilities when subject to HiSPAs, and confirms the vulnerability of SSMs against such attacks. Even a recent 52B hybrid SSM-Transformer model from the Jamba family collapses on RoBench25 under optimized HiSPA triggers, whereas pure Transformers do not. We also observe that HiSPA triggers significantly weaken the Jamba model on the popular Open-Prompt-Injections benchmark, unlike pure Transformers. Finally, our interpretability study reveals patterns in Mamba's hidden layers during HiSPAs that could be used to build a HiSPA mitigation system. The full code and data to reproduce the experiments can be found at https://anonymous.4open.science/r/hispa_anonymous-5DB0.

</details>


### [59] [Surprisal and Metaphor Novelty: Moderate Correlations and Divergent Scaling Effects](https://arxiv.org/abs/2601.02015)
*Omar Momen,Emilie Sitter,Berenike Herrmann,Sina Zarrieß*

Main category: cs.CL

TL;DR: 研究探讨语言模型中的惊奇度(surprisal)能否预测隐喻新颖性，发现惊奇度与隐喻新颖性评分存在中等相关性，但相关性随模型大小和数据类型呈现不同变化模式。


<details>
  <summary>Details</summary>
Motivation: 新颖隐喻理解涉及复杂的语义过程和语言创造力，是研究语言模型的有趣任务。本研究旨在探索语言模型中的概率性预测指标——惊奇度，是否与不同隐喻新颖性数据集相关。

Method: 使用16种语言模型变体分析基于语料库和合成隐喻新颖性数据集中的惊奇度。采用填空式惊奇度方法，基于完整句子上下文进行计算。

Result: 语言模型与隐喻新颖性评分/标签存在显著的中等相关性。发现不同的缩放模式：在基于语料库的数据上，相关性强度随模型大小减小（逆缩放效应）；在合成数据上则随模型大小增加（质量-能力假说）。

Conclusion: 虽然惊奇度能够部分解释隐喻新颖性的标注，但它仍然是语言创造力的一种有限度量指标。惊奇度只能部分捕捉隐喻新颖性的复杂语义过程。

Abstract: Novel metaphor comprehension involves complex semantic processes and linguistic creativity, making it an interesting task for studying language models (LMs). This study investigates whether surprisal, a probabilistic measure of predictability in LMs, correlates with different metaphor novelty datasets. We analyse surprisal from 16 LM variants on corpus-based and synthetic metaphor novelty datasets. We explore a cloze-style surprisal method that conditions on full-sentence context. Results show that LMs yield significant moderate correlations with scores/labels of metaphor novelty. We further identify divergent scaling patterns: on corpus-based data, correlation strength decreases with model size (inverse scaling effect), whereas on synthetic data it increases (Quality-Power Hypothesis). We conclude that while surprisal can partially account for annotations of metaphor novelty, it remains a limited metric of linguistic creativity.

</details>


### [60] [Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs](https://arxiv.org/abs/2601.02023)
*Amirali Ebrahimzadeh,Seyyed M. Salili*

Main category: cs.CL

TL;DR: 研究发现长上下文并不保证更好的性能，当相关信息被稀释或分散时反而有害。不同模型表现差异大，抗幻觉指令可能使模型过于保守而降低准确性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM支持越来越长的输入上下文，但它们在长上下文中可靠提取和推理信息的能力尚不清楚。性能随上下文长度变化，且与真实语料中信息分布方式强烈交互。企业工作流中经常将大量未过滤文档粘贴到LLM提示中，因此理解模型在长上下文中的可靠性至关重要。

Method: 引入扩展的"大海捞针"基准测试，在四个生产级模型(Gemini-2.5-flash、ChatGPT-5-mini、Claude-4.5-haiku、Deepseek-v3.2-chat)上进行评估。分别评估字面提取、逻辑推理和幻觉风险。研究位置效应、证据在长上下文中的真实分布，以及明确阻止捏造的提示。

Result: 仅增加上下文长度并不保证更好性能，当相关证据被稀释或广泛分散时反而有害。不同模型表现差异显著：一些在真实条件下严重退化，而另一些在更长上下文长度下保持更稳健。抗幻觉指令可能使某些模型过于保守，显著降低字面提取和逻辑推理的准确性。模型经常难以识别和优先处理相关信息，即使信息存在。

Conclusion: 有效的上下文长度和模型对长上下文的特定鲁棒性对于LLM在研究和业务中的可靠部署至关重要。许多失败源于无效的上下文利用，而非信息缺失。企业工作流中大量粘贴未过滤文档的做法需要谨慎考虑模型的实际上下文处理能力。

Abstract: Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.

</details>


### [61] [Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory](https://arxiv.org/abs/2601.02065)
*Md. Asif Hossain,Nabil Subhan,Mantasha Rahman Mahi,Jannatul Ferdous Nabila*

Main category: cs.CL

TL;DR: 提出一个面向孟加拉语农业咨询的跨语言检索增强生成框架，通过翻译-检索-再翻译的流程，在消费级硬件上实现低成本、事实可靠的农业知识访问


<details>
  <summary>Details</summary>
Motivation: 在许多发展中地区，权威农业手册主要是英文编写，而农民主要使用孟加拉语等低资源本地语言，存在语言障碍。虽然大语言模型支持自然语言交互，但直接生成低资源语言存在流畅性和事实一致性问题，云端解决方案成本高昂

Method: 采用翻译为中心的架构：将孟加拉语用户查询翻译成英文，通过领域特定关键词注入将农民口语术语与科学术语对齐，在英文农业手册（FAO、IRRI）上进行密集向量检索，生成的英文回答再翻译回孟加拉语。完全使用开源模型，在消费级硬件上运行

Result: 实验评估显示系统能提供可靠的事实依据回答，对领域外查询有鲁棒的拒绝能力，平均端到端延迟低于20秒。系统在消费级硬件上运行，无需付费API

Conclusion: 跨语言检索结合受控翻译为低资源语言环境中的农业知识访问提供了实用且可扩展的解决方案，平衡了成本、事实准确性和可部署性

Abstract: Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings

</details>


### [62] [Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows](https://arxiv.org/abs/2601.02076)
*Yingte Shu,Yuchuan Tian,Chao Xu,Yunhe Wang,Hanting Chen*

Main category: cs.CL

TL;DR: 提出Deferred Commitment Decoding (DCD)方法，通过基于置信度的滑动窗口延迟高不确定性token的决策，解决块扩散模型中边界上下文截断问题，提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有块扩散语言模型存在边界诱导上下文截断(BICT)问题：块边界附近的未解码token在缺乏未来上下文的情况下被迫决策，导致解码置信度和生成质量下降，特别是在数学推理和代码生成等需要精确推理的任务中。

Method: 提出训练无关的解码策略DCD：维护基于置信度的滑动窗口，早期解决低不确定性token，延迟高不确定性token直到获得足够上下文证据，实现解码窗口内的有效双向信息流而不牺牲效率。

Result: 在多个扩散语言模型、基准测试和缓存配置上的实验表明，DCD相比固定块扩散方法平均提升1.39%的生成准确率，时间开销相当，最大改进达到9.0%。

Conclusion: 基于不确定性延迟token决策是提升扩散语言模型解码质量和效率的简单而有效的原则，DCD方法有效缓解了块扩散模型的结构性限制。

Abstract: Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.

</details>


### [63] [DeCode: Decoupling Content and Delivery for Medical QA](https://arxiv.org/abs/2601.02123)
*Po-Jen Ko,Chen-Han Tsai,Yu-Shao Peng*

Main category: cs.CL

TL;DR: DeCode是一个无需训练、模型无关的框架，用于使现有LLM在临床环境中生成更具上下文相关性的答案，在OpenAI HealthBench上将SOTA从28.4%提升到49.8%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM虽然具备医学知识并能生成事实准确的回答，但往往忽略患者个体化情境，产生临床正确但与患者需求不匹配的回答。

Method: DeCode是一个无需训练、模型无关的框架，通过适配现有LLM来在临床环境中生成上下文化的答案。

Result: 在OpenAI HealthBench基准测试中，DeCode将先前最佳结果从28.4%提升到49.8%，相对改进达到75%。

Conclusion: DeCode能有效提升LLM在临床问答中的表现，使其生成更符合患者具体情境的回答。

Abstract: Large language models (LLMs) exhibit strong medical knowledge and can generate factually accurate responses. However, existing models often fail to account for individual patient contexts, producing answers that are clinically correct yet poorly aligned with patients' needs. In this work, we introduce DeCode, a training-free, model-agnostic framework that adapts existing LLMs to produce contextualized answers in clinical settings. We evaluate DeCode on OpenAI HealthBench, a comprehensive and challenging benchmark designed to assess clinical relevance and validity of LLM responses. DeCode improves the previous state of the art from $28.4\%$ to $49.8\%$, corresponding to a $75\%$ relative improvement. Experimental results suggest the effectiveness of DeCode in improving clinical question answering of LLMs.

</details>


### [64] [Towards Multi-Level Transcript Segmentation: LoRA Fine-Tuning for Table-of-Contents Generation](https://arxiv.org/abs/2601.02128)
*Steffen Freisinger,Philipp Seeberger,Thomas Ranzenberger,Tobias Bocklet,Korbinian Riedhammer*

Main category: cs.CL

TL;DR: 提出分层主题分割方法，结合零样本提示和LoRA微调，整合语音停顿特征，在会议和讲座转录中显著优于基线


<details>
  <summary>Details</summary>
Motivation: 将语音转录分割为主题段落有助于下游处理和依赖文本的可访问性用户，需要捕捉主题和子主题的多层次结构

Method: 提出分层主题分割方法，比较零样本提示和LoRA微调的大语言模型，整合高层语音停顿特征，适应多级分割评估指标

Result: 在英语会议录音和多语言讲座转录（葡萄牙语、德语）上显著优于现有主题分割基线方法

Conclusion: 分层主题分割方法有效，结合语言模型和语音特征能提升分割性能，多级评估指标能全面衡量层次结构

Abstract: Segmenting speech transcripts into thematic sections benefits both downstream processing and users who depend on written text for accessibility. We introduce a novel approach to hierarchical topic segmentation in transcripts, generating multi-level tables of contents that capture both topic and subtopic boundaries. We compare zero-shot prompting and LoRA fine-tuning on large language models, while also exploring the integration of high-level speech pause features. Evaluations on English meeting recordings and multilingual lecture transcripts (Portuguese, German) show significant improvements over established topic segmentation baselines. Additionally, we adapt a common evaluation measure for multi-level segmentation, taking into account all hierarchical levels within one metric.

</details>


### [65] [Routing by Analogy: kNN-Augmented Expert Assignment for Mixture-of-Experts](https://arxiv.org/abs/2601.02144)
*Boxuan Lyu,Soichiro Murakami,Hidetaka Kamigaito,Peinan Zhang*

Main category: cs.CL

TL;DR: kNN-MoE：基于检索的混合专家路由框架，通过重用历史最优专家分配来增强传统MoE路由器的鲁棒性，在分布偏移下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构中的参数化路由器通常在训练后冻结，导致在分布偏移时路由决策变得脆弱。需要一种能够适应新数据分布的路由机制。

Method: 提出kNN-MoE框架：1）离线构建记忆库，通过优化路由logits最大化参考集似然；2）在线检索相似历史案例重用最优专家分配；3）使用检索邻居的聚合相似度作为置信度驱动的混合系数，在无相关案例时回退到冻结路由器。

Result: 实验表明kNN-MoE优于零样本基线，且能与计算成本高昂的监督微调相媲美。

Conclusion: kNN-MoE通过检索增强的路由机制有效解决了传统MoE路由器的分布偏移脆弱性问题，提供了一种高效且鲁棒的专家分配方案。

Abstract: Mixture-of-Experts (MoE) architectures scale large language models efficiently by employing a parametric "router" to dispatch tokens to a sparse subset of experts. Typically, this router is trained once and then frozen, rendering routing decisions brittle under distribution shifts. We address this limitation by introducing kNN-MoE, a retrieval-augmented routing framework that reuses optimal expert assignments from a memory of similar past cases. This memory is constructed offline by directly optimizing token-wise routing logits to maximize the likelihood on a reference set. Crucially, we use the aggregate similarity of retrieved neighbors as a confidence-driven mixing coefficient, thus allowing the method to fall back to the frozen router when no relevant cases are found. Experiments show kNN-MoE outperforms zero-shot baselines and rivals computationally expensive supervised fine-tuning.

</details>


### [66] [FormationEval, an open multiple-choice benchmark for petroleum geoscience](https://arxiv.org/abs/2601.02158)
*Almaz Ermilov*

Main category: cs.CL

TL;DR: FormationEval是一个用于评估语言模型在石油地质科学和地下学科能力的开放多选问答基准，包含505个问题，覆盖7个领域，评估了72个模型，发现顶级模型准确率超过97%，开源模型表现接近闭源模型。


<details>
  <summary>Details</summary>
Motivation: 需要建立一个专门针对石油地质科学和地下学科的基准测试，以评估语言模型在这些专业领域的知识掌握程度，同时避免版权问题并提供可追溯性。

Method: 从三个权威来源使用推理模型和基于概念的方法构建了505个多选问题，覆盖7个领域（包括岩石物理学、石油地质学和油藏工程），每个问题都包含来源元数据。评估了72个来自主要提供商的开源和闭源模型。

Result: Gemini 3 Pro Preview达到99.8%的最高准确率，开源模型GLM-4.7达到98.6%。开源和闭源模型之间的性能差距比预期小，多个低成本开源模型超过90%准确率。岩石物理学是所有模型中最具挑战性的领域，较小模型表现出更大的性能差异。

Conclusion: FormationEval基准显示语言模型在石油地质科学领域已达到高水平表现，开源模型与闭源模型差距不大，岩石物理学是最具挑战性的子领域，基准提供了可追溯的评估框架。

Abstract: This paper presents FormationEval, an open multiple-choice question benchmark for evaluating language models on petroleum geoscience and subsurface disciplines. The dataset contains 505 questions across seven domains including petrophysics, petroleum geology and reservoir engineering, derived from three authoritative sources using a reasoning model with detailed instructions and a concept-based approach that avoids verbatim copying of copyrighted text. Each question includes source metadata to support traceability and audit. The evaluation covers 72 models from major providers including OpenAI, Anthropic, Google, Meta and open-weight alternatives. The top performers achieve over 97\% accuracy, with Gemini 3 Pro Preview reaching 99.8\%, while tier and domain gaps persist. Among open-weight models, GLM-4.7 leads at 98.6\%, with several DeepSeek, Llama, Qwen and Mistral models also exceeding 93\%. The performance gap between open-weight and closed models is narrower than expected, with several lower-cost open-weight models exceeding 90\% accuracy. Petrophysics emerges as the most challenging domain across all models, while smaller models show wider performance variance. Residual length bias in the dataset (correct answers tend to be longer) is documented along with bias mitigation strategies applied during construction. The benchmark, evaluation code and results are publicly available.

</details>


### [67] [Confidence Estimation for LLMs in Multi-turn Interactions](https://arxiv.org/abs/2601.02179)
*Caiqi Zhang,Ruihan Yang,Xiaochen Zhu,Chengzu Li,Tiancheng Hu,Yijiang River Dong,Deqing Yang,Nigel Collier*

Main category: cs.CL

TL;DR: 该研究首次系统性地探索了多轮对话中LLM的置信度估计问题，提出了评估框架和新指标，发现现有方法在多轮设置下表现不佳，并提出了改进方案。


<details>
  <summary>Details</summary>
Motivation: 当前置信度估计研究主要集中在单轮设置，而多轮对话中上下文累积和模糊性逐步解决的过程对置信度动态变化的影响尚未被充分探索。可靠的置信度估计对于自主代理和人机协作系统等下游应用至关重要。

Method: 建立了基于两个关键需求的正式评估框架：每轮校准性和随着信息增加置信度的单调性。引入了新指标（如长度归一化的预期校准误差InfoECE）和"Hinter-Guesser"范式来生成受控评估数据集。提出了P(Sufficient)这一基于logit的探针方法。

Result: 实验表明，广泛使用的置信度技术在多轮对话中难以实现良好的校准性和单调性。提出的P(Sufficient)方法相比其他方法表现更好，但该任务远未完全解决。

Conclusion: 这项工作为开发更可靠和可信赖的对话智能体提供了基础方法论，强调了多轮对话置信度估计的重要性，并指出了未来研究方向。

Abstract: While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new "Hinter-Guesser" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.

</details>


### [68] [Toward Global Large Language Models in Medicine](https://arxiv.org/abs/2601.02186)
*Rui Yang,Huitao Li,Weihao Xuan,Heli Qi,Xin Li,Kunyu Yu,Yingjian Chen,Rongrong Wang,Jacques Behmoaras,Tianxi Cai,Bibhas Chakraborty,Qingyu Chen,Lionel Tim-Ee Cheng,Marie-Louise Damwanza,Chido Dzinotyiwei,Aosong Feng,Chuan Hong,Yusuke Iwasawa,Yuhe Ke,Linah Kitala,Taehoon Ko,Jisan Lee,Irene Li,Jonathan Chong Kai Liew,Hongfang Liu,Lian Leng Low,Edison Marrese-Taylor,Yutaka Matsuo,Isheanesu Misi,Yilin Ning,Jasmine Chiat Ling Ong,Marcus Eng Hock Ong,Enrico Petretto,Hossein Rouhizadeh,Abiram Sandralegar,Oren Schreier,Iain Bee Huat Tan,Patrick Tan,Daniel Shu Wei Ting,Junjue Wang,Chunhua Weng,Matthew Yu Heng Wong,Fang Wu,Yunze Xiao,Xuhai Xu,Qingcheng Zeng,Zhuo Zheng,Yifan Peng,Douglas Teodoro,Nan Liu*

Main category: cs.CL

TL;DR: 构建了GlobMed多语言医疗数据集、评估基准和模型套件，以解决LLMs在低资源语言医疗场景中的性能差距问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型主要基于高资源语言训练，限制了其在全球医疗场景中的应用，特别是在低资源语言环境中存在显著性能差距。

Method: 1) 构建GlobMed多语言医疗数据集（50万+条目，12种语言，含4种低资源语言）；2) 建立GlobMed-Bench评估基准，系统评估56个先进LLMs；3) 开发GlobMed-LLMs模型套件（1.7B-8B参数）。

Result: GlobMed-LLMs相比基线模型平均性能提升超过40%，在低资源语言上性能提升超过三倍；评估揭示了不同语言间特别是低资源语言的显著性能差异。

Conclusion: 这些资源为促进LLMs在全球的公平发展和应用提供了重要基础，使更广泛的语言社区能够从技术进步中受益。

Abstract: Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances.

</details>


### [69] [From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality](https://arxiv.org/abs/2601.02224)
*Fabian Lukassen,Jan Herrmann,Christoph Weisser,Benjamin Saefken,Thomas Kneib*

Main category: cs.CL

TL;DR: 研究通过系统实验发现：LLM选择对可解释AI自然语言解释质量影响最大，XAI方法仅对专家用户有轻微提升，SARIMAX模型存在可解释性悖论，零样本提示与自洽性效果相当但成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法（如SHAP、LIME）产生的数值特征归因对非专家用户难以理解，虽然LLM可以将这些输出转化为自然语言解释，但影响解释质量的关键因素尚不明确。本研究旨在系统探究预测模型选择、XAI方法、LLM选择和提示策略如何影响自然语言解释的质量。

Method: 采用系统因子设计研究，涵盖四个预测模型（XGBoost、随机森林、多层感知机、SARIMAX）、三种XAI条件（SHAP、LIME、无XAI基线）、三个LLM（GPT-4o、Llama-3-8B、DeepSeek-R1）和八种提示策略。使用G-Eval（LLM作为评判者）方法，通过双LLM评判者和四个评估标准，对时间序列预测的660个解释进行评估。

Result: 主要发现：1）XAI仅对专家用户有轻微改进；2）LLM选择是影响解释质量的最重要因素，DeepSeek-R1表现优于GPT-4o和Llama-3；3）存在可解释性悖论：SARIMAX虽然预测准确度更高，但产生的自然语言解释质量低于机器学习模型；4）零样本提示与自洽性效果相当，但成本降低7倍；5）思维链提示反而会降低解释质量。

Conclusion: LLM选择是决定可解释AI自然语言解释质量的关键因素，而非XAI方法本身。实际应用中应优先考虑LLM选择，并采用成本效益更高的零样本提示策略。研究揭示了模型预测准确度与解释质量之间的不一致性，为构建有效的AI解释系统提供了实证指导。

Abstract: Explainable AI (XAI) methods like SHAP and LIME produce numerical feature attributions that remain inaccessible to non expert users. Prior work has shown that Large Language Models (LLMs) can transform these outputs into natural language explanations (NLEs), but it remains unclear which factors contribute to high-quality explanations. We present a systematic factorial study investigating how Forecasting model choice, XAI method, LLM selection, and prompting strategy affect NLE quality. Our design spans four models (XGBoost (XGB), Random Forest (RF), Multilayer Perceptron (MLP), and SARIMAX - comparing black-box Machine-Learning (ML) against classical time-series approaches), three XAI conditions (SHAP, LIME, and a no-XAI baseline), three LLMs (GPT-4o, Llama-3-8B, DeepSeek-R1), and eight prompting strategies. Using G-Eval, an LLM-as-a-judge evaluation method, with dual LLM judges and four evaluation criteria, we evaluate 660 explanations for time-series forecasting. Our results suggest that: (1) XAI provides only small improvements over no-XAI baselines, and only for expert audiences; (2) LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3; (3) we observe an interpretability paradox: in our setting, SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy; (4) zero-shot prompting is competitive with self-consistency at 7-times lower cost; and (5) chain-of-thought hurts rather than helps.

</details>


### [70] [CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models](https://arxiv.org/abs/2601.02236)
*Yihao Liang,Ze Wang,Hao Chen,Ximeng Sun,Jialian Wu,Xiaodong Yu,Jiang Liu,Emad Barsoum,Zicheng Liu,Niraj K. Jha*

Main category: cs.CL

TL;DR: CD4LM框架通过离散空间一致性蒸馏和置信度自适应解码，解决了扩散语言模型训练与推理的静态-动态不对齐问题，实现了高质量并行解码与显著加速。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型解码受限于序列依赖的延迟，扩散语言模型虽支持并行生成但存在训练与推理的不对齐：训练优化固定调度下的局部转移，而高效推理需要自适应"长跳"优化。目标是实现DLMs的高并行解码，同时保持生成质量。

Method: 提出CD4LM框架，包含离散空间一致性蒸馏（DSCD）和置信度自适应解码（CAD）。DSCD训练学生模型具有轨迹不变性，将多样噪声状态直接映射到干净分布；CAD基于token置信度动态分配计算资源，激进跳过步骤而不损失质量。

Result: 在GSM8K上，CD4LM匹配LLaDA基线，实现5.18倍实时加速；在代码和数学基准测试中，严格主导准确率-效率帕累托前沿，平均加速3.62倍的同时提高平均准确率。

Conclusion: CD4LM通过解耦训练与推理，解决了扩散语言模型的核心不对齐问题，实现了高质量并行解码的显著加速，为高效语言生成提供了新方向。

Abstract: Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel generation but suffer from a fundamental static-to-dynamic misalignment: Training optimizes local transitions under fixed schedules, whereas efficient inference requires adaptive "long-jump" refinements through unseen states. Our goal is to enable highly parallel decoding for DLMs with low number of function evaluations while preserving generation quality. To achieve this, we propose CD4LM, a framework that decouples training from inference via Discrete-Space Consistency Distillation (DSCD) and Confidence-Adaptive Decoding (CAD). Unlike standard objectives, DSCD trains a student to be trajectory-invariant, mapping diverse noisy states directly to the clean distribution. This intrinsic robustness enables CAD to dynamically allocate compute resources based on token confidence, aggressively skipping steps without the quality collapse typical of heuristic acceleration. On GSM8K, CD4LM matches the LLaDA baseline with a 5.18x wall-clock speedup; across code and math benchmarks, it strictly dominates the accuracy-efficiency Pareto frontier, achieving a 3.62x mean speedup while improving average accuracy. Code is available at https://github.com/yihao-liang/CDLM

</details>


### [71] [pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs](https://arxiv.org/abs/2601.02285)
*Tobias Schimanski,Imene Kolli,Jingwei Ni,Yu Fan,Ario Saeid Vaghefi,Elliott Ash,Markus Leippold*

Main category: cs.CL

TL;DR: pdfQA是一个多领域PDF问答数据集，包含2K人工标注和2K合成数据，按十个复杂度维度分类，用于评估端到端QA系统性能


<details>
  <summary>Details</summary>
Motivation: PDF是互联网上第二常用的文档类型，但现有QA数据集主要基于文本源或仅针对特定领域，缺乏针对PDF文档的综合性问答评估基准

Method: 创建pdfQA数据集：包含2K人工标注的真实PDF问答对和2K合成的问答对，按十个复杂度维度分类（如文件类型、源模态、源位置、答案类型等），应用质量和难度过滤器筛选有效且具有挑战性的问答对

Result: 使用开源LLM回答问题，发现现有挑战与复杂度维度相关；数据集为端到端QA管道评估提供基础，可测试多样化技能集和局部优化（如信息检索或解析）

Conclusion: pdfQA填补了PDF文档问答评估的空白，通过多维度复杂度分类为QA系统评估提供全面基准，有助于识别和改进现有系统的局限性

Abstract: PDFs are the second-most used document type on the internet (after HTML). Yet, existing QA datasets commonly start from text sources or only address specific domains. In this paper, we present pdfQA, a multi-domain 2K human-annotated (real-pdfQA) and 2K synthetic dataset (syn-pdfQA) differentiating QA pairs in ten complexity dimensions (e.g., file type, source modality, source position, answer type). We apply and evaluate quality and difficulty filters on both datasets, obtaining valid and challenging QA pairs. We answer the questions with open-source LLMs, revealing existing challenges that correlate with our complexity dimensions. pdfQA presents a basis for end-to-end QA pipeline evaluation, testing diverse skill sets and local optimizations (e.g., in information retrieval or parsing).

</details>


### [72] [Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)](https://arxiv.org/abs/2601.02298)
*Mahmoud Elgenedy*

Main category: cs.CL

TL;DR: 该论文提出了一种针对大型语言模型的Power-of-Two量化方法，通过将权重限制为2的幂次方，实现内存节省87.5%和推理速度提升3-10倍，同时使用量化感知训练来缓解性能损失。


<details>
  <summary>Details</summary>
Motivation: 随着LLM参数规模指数级增长（从GPT-2的15亿到GPT-3的1750亿再到可能万亿级），边缘设备面临严重的内存和处理能力限制。云计算的资源丰富，但边缘设备资源有限，需要创新的压缩方法来使这些应用在边缘设备上可行。

Method: 采用Power-of-Two量化方法，将权重限制为2的幂次方，这样只需存储指数即可，大幅节省内存。更重要的是，用低成本位移操作替代昂贵的乘法运算，显著降低处理需求。为了克服严格量化带来的性能损失，采用量化感知训练进行额外训练来提升性能。

Result: 在GPT-2 124M模型上的实验显示，经过额外训练的量化PoT模型性能大幅提升：困惑度改善66%，BERT-Score损失仅比基线GPT-2高1%。内存节省估计达87.5%，推理速度预计比全精度模型快3-10倍。

Conclusion: Power-of-Two量化结合量化感知训练是一种有效的LLM压缩方法，能够在边缘设备上实现显著的内存节省和推理加速，同时保持接近原始模型的性能，为在资源受限的边缘设备上部署大型语言模型提供了可行方案。

Abstract: In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision.

</details>


### [73] [Classifying several dialectal Nawatl varieties](https://arxiv.org/abs/2601.02303)
*Juan-José Guzmán-Landa,Juan-Manuel Torres-Moreno,Miguel Figueroa-Saavedra,Carlos-Emiliano González-Gallardo,Graham Ranger,Martha Lorena-Avendaño-Garrido*

Main category: cs.CL

TL;DR: 该研究使用机器学习和神经网络对纳瓦特尔语的方言变体进行分类，以解决这种拥有丰富文化遗产但计算资源稀缺的语言的方言识别问题。


<details>
  <summary>Details</summary>
Motivation: 纳瓦特尔语是墨西哥使用最广泛的土著语言，拥有超过200万使用者，但其计算资源稀缺。该语言有约30种方言变体，加上不同的书写拼写形式，使得方言识别问题更加复杂。研究旨在解决这种文化遗产丰富但技术资源不足的语言的方言分类问题。

Method: 采用机器学习和神经网络方法对纳瓦特尔语的方言变体进行分类。

Result: 摘要中未提供具体实验结果，但表明已成功应用机器学习和神经网络方法来解决纳瓦特尔语方言分类问题。

Conclusion: 该研究展示了机器学习和神经网络在低资源土著语言方言分类中的可行性，为保护和发展纳瓦特尔语文化遗产提供了技术途径。

Abstract: Mexico is a country with a large number of indigenous languages, among which the most widely spoken is Nawatl, with more than two million people currently speaking it (mainly in North and Central America). Despite its rich cultural heritage, which dates back to the 15th century, Nawatl is a language with few computer resources. The problem is compounded when it comes to its dialectal varieties, with approximately 30 varieties recognised, not counting the different spellings in the written forms of the language. In this research work, we addressed the problem of classifying Nawatl varieties using Machine Learning and Neural Networks.

</details>


### [74] [Estimating Text Temperature](https://arxiv.org/abs/2601.02320)
*Nikolay Mikhaylovskiy*

Main category: cs.CL

TL;DR: 提出一种估计文本温度的方法，可用于评估任何文本（包括人类写作）相对于特定语言模型的温度参数


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型在推理时使用温度参数来控制生成文本的随机性，但现有方法无法估计已生成文本的温度，特别是人类写作的文本

Method: 使用最大似然方法估计文本的温度参数，提出一个评估任何文本相对于给定语言模型温度的程序

Result: 评估了多种中小型LLM的温度估计能力，发现Qwen3 14B表现最佳，并用其估计了流行语料库的温度

Conclusion: 成功开发了一种通用的文本温度估计方法，可用于分析不同文本相对于语言模型的随机性特征

Abstract: Autoregressive language models typically use temperature parameter at inference to shape the probability distribution and control the randomness of the text generated. After the text was generated, this parameter can be estimated using maximum likelihood approach. Following it, we propose a procedure to estimate the temperature of any text, including ones written by humans, with respect to a given language model. We evaluate the temperature estimation capability of a wide selection of small-to-medium LLMs. We then use the best-performing Qwen3 14B to estimate temperatures of popular corpora.

</details>


### [75] [Robust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling](https://arxiv.org/abs/2601.02337)
*Berk Atil,Rebecca J. Passonneau,Ninareh Mehrabi*

Main category: cs.CL

TL;DR: 论文系统评估了基于人物角色的毒性检测方法，发现没有单一提示方法在所有模型-人物对中表现最优，提出了基于SVM的轻量级元集成方法，在多样化人物角色上获得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 毒性检测具有主观性，受不同人口群体视角和社会先验影响。当前LLM提示技术在不同人物角色和基础模型上表现不一致，需要系统评估人物角色感知的毒性检测方法。

Method: 1) 系统评估人物角色感知的毒性检测；2) 提出自动提示优化策略；3) 探索集成四种提示变体；4) 提出轻量级元集成方法：基于4位向量预测的SVM。

Result: 提出的SVM集成方法一致优于个体提示方法和传统多数投票技术，在多样化人物角色上获得最强的整体性能。

Conclusion: 这是首次系统比较人物角色条件提示用于毒性检测的工作，为主观NLP任务中的多元化评估提供了稳健方法。

Abstract: Toxicity detection is inherently subjective, shaped by the diverse perspectives and social priors of different demographic groups. While ``pluralistic'' modeling as used in economics and the social sciences aims to capture perspective differences across contexts, current Large Language Model (LLM) prompting techniques have different results across different personas and base models. In this work, we conduct a systematic evaluation of persona-aware toxicity detection, showing that no single prompting method, including our proposed automated prompt optimization strategy, uniformly dominates across all model-persona pairs. To exploit complementary errors, we explore ensembling four prompting variants and propose a lightweight meta-ensemble: an SVM over the 4-bit vector of prompt predictions. Our results demonstrate that the proposed SVM ensemble consistently outperforms individual prompting methods and traditional majority-voting techniques, achieving the strongest overall performance across diverse personas. This work provides one of the first systematic comparisons of persona-conditioned prompting for toxicity detection and offers a robust method for pluralistic evaluation in subjective NLP tasks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [76] [Semantic Alignment of Multilingual Knowledge Graphs via Contextualized Vector Projections](https://arxiv.org/abs/2601.00814)
*Abhishek Kumar*

Main category: cs.AI

TL;DR: 提出基于嵌入余弦相似度的跨语言本体对齐系统，通过创新描述生成技术丰富本体实体上下文，使用微调的多语言Transformer模型生成更好嵌入，在OAEI-2022评测中F1分数达到71%，比最佳基线提升16%


<details>
  <summary>Details</summary>
Motivation: 解决跨语言本体对齐的挑战，传统方法难以捕捉跨语言的细微语义相似性，需要更有效的技术来提升对齐准确率

Method: 1) 使用创新技术为实体生成描述性文本以丰富上下文；2) 采用微调的多语言Transformer模型生成高质量嵌入；3) 基于余弦相似度匹配实体对；4) 应用阈值过滤保留高相似度实体对

Result: 在OAEI-2022 multifarm track评测中，F1分数达到71%（召回率78%，精确率65%），比最佳基线方法提升16%，证明系统能有效捕捉跨语言相似性

Conclusion: 提出的跨语言本体对齐流水线通过上下文丰富和嵌入优化，显著提升了跨语言本体对齐性能，验证了基于嵌入的相似度匹配方法的有效性

Abstract: The paper presents our work on cross-lingual ontology alignment system which uses embedding based cosine similarity matching. The ontology entities are made contextually richer by creating descriptions using novel techniques. We use a fine-tuned transformer based multilingual model for generating better embeddings. We use cosine similarity to find positive ontology entities pairs and then apply threshold filtering to retain only highly similar entities. We have evaluated our work on OAEI-2022 multifarm track. We achieve 71% F1 score (78% recall and 65% precision) on the evaluation dataset, 16% increase from best baseline score. This suggests that our proposed alignment pipeline is able to capture the subtle cross-lingual similarities.

</details>


### [77] [MathLedger: A Verifiable Learning Substrate with Ledger-Attested Feedback](https://arxiv.org/abs/2601.00816)
*Ismail Ahmad Abdullah*

Main category: cs.AI

TL;DR: MathLedger是一个可验证机器认知的基础设施，通过形式化验证、密码学证明和学习动态集成到单一认知循环中，实现可审计的机器学习


<details>
  <summary>Details</summary>
Motivation: 当前AI系统虽然性能卓越但不透明且不可验证，在安全关键部署中面临信任危机，需要建立可验证的机器学习系统

Method: 采用反射形式化学习（RFL），这是一种符号化的梯度下降方法，通过验证器结果而非统计损失来驱动更新；系统集成形式化验证、密码学证明和学习动态

Result: 第一阶段实验验证了测量和治理基础设施：CAL-EXP-3验证了测量基础设施（Delta p计算、方差跟踪）；压力测试确认了在超出边界条件下故障关闭治理机制正确触发；建立了可扩展审计的工作原型

Conclusion: MathLedger提供了一个基础设施层面的贡献：一个工作的账本证明学习原型，能够实现大规模的可审计性，为解决AI系统的透明度和可验证性问题提供了技术基础

Abstract: Contemporary AI systems achieve extraordinary performance yet remain opaque and non-verifiable, creating a crisis of trust for safety-critical deployment. We introduce MathLedger, a substrate for verifiable machine cognition that integrates formal verification, cryptographic attestation, and learning dynamics into a single epistemic loop. The system implements Reflexive Formal Learning (RFL), a symbolic analogue of gradient descent where updates are driven by verifier outcomes rather than statistical loss.
  Phase I experiments validate the measurement and governance substrate under controlled conditions. CAL-EXP-3 validates measurement infrastructure (Delta p computation, variance tracking); separate stress tests confirm fail-closed governance triggers correctly under out-of-bounds conditions. No convergence or capability claims are made. The contribution is infrastructural: a working prototype of ledger-attested learning that enables auditability at scale.
  Keywords: verifiable learning, formal verification, cryptographic attestation, reflexive feedback, fail-closed governance

</details>


### [78] [Agentic AI for Autonomous, Explainable, and Real-Time Credit Risk Decision-Making](https://arxiv.org/abs/2601.00818)
*Chandra Sekhar Kubam*

Main category: cs.AI

TL;DR: 本文提出了一种基于Agentic AI框架的自主信用风险评估系统，通过多智能体协作实现实时、透明的信贷决策，相比传统模型在决策速度、透明度和响应性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 金融服务的快速数字化催生了对自主、透明、实时信用风险决策系统的迫切需求。传统机器学习模型虽然擅长模式识别，但缺乏现代金融运营所需的适应性推理、情境感知和自主性。

Method: 提出Agentic AI框架，构建多智能体系统，包含强化学习、自然语言推理、可解释AI模块和实时数据吸收管道。系统包括智能体协作协议、风险评分引擎、可解释性层和持续反馈学习循环。

Result: 该系统在决策速度、透明度和响应性方面优于传统信用评分模型。但仍存在模型漂移风险、高维数据解释不一致、监管不确定性以及低资源环境基础设施限制等实际挑战。

Conclusion: 该框架具有变革信用分析的巨大潜力，未来研究应关注动态监管合规机制、新型智能体协作、对抗鲁棒性以及跨国信用生态系统的大规模实施。

Abstract: Significant digitalization of financial services in a short period of time has led to an urgent demand to have autonomous, transparent and real-time credit risk decision making systems. The traditional machine learning models are effective in pattern recognition, but do not have the adaptive reasoning, situational awareness, and autonomy needed in modern financial operations. As a proposal, this paper presents an Agentic AI framework, or a system where AI agents view the world of dynamic credit independent of human observers, who then make actions based on their articulable decision-making paths. The research introduces a multi-agent system with reinforcing learning, natural language reasoning, explainable AI modules, and real-time data absorption pipelines as a means of assessing the risk profiles of borrowers with few humans being involved. The processes consist of agent collaboration protocol, risk-scoring engines, interpretability layers, and continuous feedback learning cycles. Findings indicate that decision speed, transparency and responsiveness is better than traditional credit scoring models. Nevertheless, there are still some practical limitations such as risks of model drift, inconsistencies in interpreting high dimensional data and regulatory uncertainties as well as infrastructure limitations in low-resource settings. The suggested system has a high prospective to transform credit analytics and future studies ought to be directed on dynamic regulatory compliance mobilizers, new agent teamwork, adversarial robustness, and large-scale implementation in cross-country credit ecosystems.

</details>


### [79] [CogCanvas: Compression-Resistant Cognitive Artifacts for Long LLM Conversations](https://arxiv.org/abs/2601.00821)
*Tao An*

Main category: cs.AI

TL;DR: CogCanvas是一个无需训练的框架，通过提取对话中的认知构件并组织成时序感知图，解决大语言模型在长对话中的上下文限制和信息保真度问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长对话中面临上下文窗口限制和信息保真度的基本矛盾。现有方法（截断和摘要）要么丢弃早期信息，要么丢失细节信息。

Method: CogCanvas是一个无需训练的框架，从对话轮次中提取基于原文的认知构件（决策、事实、提醒），并将其组织成时序感知图，实现抗压缩检索。

Result: 在LoCoMo基准测试中，CogCanvas达到34.7%整体准确率，优于RAG（25.6%）和GraphRAG（13.7%）。在时序推理上优势最明显：31.5% vs. 9.3%（RAG）和5.0%（GraphRAG），相对提升530%。在多跳因果推理上达到81.0%通过率，而GraphRAG为40.0%。

Conclusion: 虽然经过大量优化的方法通过专门训练能达到更高绝对分数，但CogCanvas这种无需训练的方法为实践者提供了可立即部署的替代方案，显著优于标准基线方法。

Abstract: Large language models face a fundamental tension between context window limits and information fidelity in long conversations. Existing approaches--truncation and summarization--either discard early information or lose nuanced details. We introduce CogCanvas, a training-free framework that extracts verbatim-grounded cognitive artifacts (decisions, facts, reminders) from conversation turns and organizes them into a temporal-aware graph for compression-resistant retrieval.
  On the LoCoMo benchmark, CogCanvas achieves 34.7% overall accuracy, outperforming RAG (25.6%, +9.1pp) and GraphRAG (13.7%, +21.0pp). The advantage is most pronounced on temporal reasoning: 31.5% vs. 9.3% (RAG) and 5.0% (GraphRAG)--a +530% relative improvement. On multi-hop causal reasoning, CogCanvas achieves 81.0% pass rate vs. 40.0% for GraphRAG (+41.0pp). Controlled benchmarks show 97.5% recall (+78.5pp vs. summarization) with 93.0% exact match preservation.
  While heavily-optimized approaches achieve higher absolute scores through dedicated training (EverMemOS: approximately 92%), our training-free approach provides practitioners with an immediately-deployable alternative that significantly outperforms standard baselines. Code and data: https://github.com/tao-hpu/cog-canvas.

</details>


### [80] [Energy-Aware Routing to Large Reasoning Models](https://arxiv.org/abs/2601.00823)
*Austin R. Ellis-Mohr,Max Hartman,Lav R. Varshney*

Main category: cs.AI

TL;DR: 论文提出在大型推理模型系统中，通过方差感知的路由和调度策略来优化能源效率，在临界状态下平衡基础能源和辅助能源的使用。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型具有异构的推理能耗，系统性能取决于平均能源供应与随机波动之间的平衡。需要找到既能避免基础能源浪费，又能减少对辅助能源依赖的优化策略。

Method: 提出方差感知的路由和调度框架，基于训练计算和推理计算的缩放定律来设计调度策略，在时间、模型和执行选择三个维度上吸收变异性。

Result: 建立了临界状态下的理论分析框架，展示了如何通过二阶特性表征系统性能，为开发能源感知的模型路由策略提供了理论基础。

Conclusion: 方差感知的路由和调度是系统设计的核心原则，能够有效优化大型推理模型系统的能源效率，平衡基础能源和辅助能源的使用。

Abstract: Large reasoning models (LRMs) have heterogeneous inference energy costs based on which model is used and how much it reasons. To reduce energy, it is important to choose the right LRM and operate it in the right way. As a result, the performance of systems that dispatch tasks to different individual LRMs depend on the balance between mean energy provisioning and stochastic fluctuations. The critical regime is the unique operating point at which neither auxiliary energy nor baseline energy is systematically wasted. Increasing baseline supply shifts the system toward persistent over-supply and baseline-energy waste, while reducing supply induces persistent reliance on auxiliary energy. Yet in this regime, performance remains volatility-limited and so a second-order characterization provides further insights that we develop. Here, performance is governed by how variability is absorbed across time, models, and execution choices. This perspective highlights variance-aware routing and dispatch as a principled design axis, and provides a theoretical basis for developing energy-aware model routing policies. Routing behavior is characterized when dispatch policies are based on training-compute and inference-compute scaling laws for LRMs.

</details>


### [81] [Decomposing LLM Self-Correction: The Accuracy-Correction Paradox and Error Depth Hypothesis](https://arxiv.org/abs/2601.00828)
*Yin Li*

Main category: cs.AI

TL;DR: 研究发现大型语言模型的内在自我修正能力存在"准确率-修正悖论"：较弱模型（GPT-3.5）的自我修正率反而比较强模型（DeepSeek）高1.6倍，挑战了模型能力与自我改进呈线性关系的假设。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型被认为具有自我修正能力，但近期研究表明内在自我修正（无需外部反馈）效果有限。本研究旨在系统分解自我修正能力，探究不同模型在自我修正方面的表现差异。

Method: 将自我修正分解为三个子能力：错误检测、错误定位和错误修正。在GSM8K-Complex数据集上进行跨模型实验（n=500/模型，共346个错误），使用三种主要LLM（GPT-3.5、DeepSeek、Claude），分析不同模型的自我修正表现。

Result: 发现"准确率-修正悖论"：较弱模型（GPT-3.5，66%准确率）的内在修正率（26.8%）比较强模型（DeepSeek，94%准确率）的修正率（16.7%）高1.6倍。提出"错误深度假说"：强模型错误更少但更深，难以自我修正。错误检测率在不同架构间差异巨大（10%-82%），但检测能力不能预测修正成功率（Claude仅检测10%错误但修正29%）。提供错误位置提示反而损害所有模型表现。

Conclusion: 研究挑战了模型能力与自我改进呈线性关系的假设，对自我精炼流程设计有重要启示。强模型虽然准确率高，但内在自我修正能力反而较弱，这需要重新思考如何有效利用LLM的自我修正能力。

Abstract: Large Language Models (LLMs) are widely believed to possess self-correction capabilities, yet recent studies suggest that intrinsic self-correction--where models correct their own outputs without external feedback--remains largely ineffective. In this work, we systematically decompose self-correction into three distinct sub-capabilities: error detection, error localization, and error correction. Through cross-model experiments on GSM8K-Complex (n=500 per model, 346 total errors) with three major LLMs, we uncover a striking Accuracy-Correction Paradox: weaker models (GPT-3.5, 66% accuracy) achieve 1.6x higher intrinsic correction rates than stronger models (DeepSeek, 94% accuracy)--26.8% vs 16.7%. We propose the Error Depth Hypothesis: stronger models make fewer but deeper errors that resist self-correction. Error detection rates vary dramatically across architectures (10% to 82%), yet detection capability does not predict correction success--Claude detects only 10% of errors but corrects 29% intrinsically. Surprisingly, providing error location hints hurts all models. Our findings challenge linear assumptions about model capability and self-improvement, with important implications for the design of self-refinement pipelines.

</details>


### [82] [Can We Trust AI Explanations? Evidence of Systematic Underreporting in Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.00830)
*Deep Pankajbhai Mehta*

Main category: cs.AI

TL;DR: AI模型的逐步推理解释并不能真实反映影响其决策的因素，模型会注意到提示但选择不报告，即使被监视也无济于事


<details>
  <summary>Details</summary>
Motivation: 验证AI系统逐步推理解释是否真的揭示了影响其答案的实际因素，测试模型是否会报告嵌入问题中的提示信息

Method: 在问题中嵌入提示信息，测试11个领先AI模型在9000多个测试案例中的表现，测量模型是否自发提及提示，以及在被直接询问时的反应

Result: 模型几乎从不自发提及提示，但被直接询问时会承认注意到提示；监视模型无帮助；强制报告提示会导致虚假报告和准确性下降；迎合用户偏好的提示最危险

Conclusion: 仅仅观察AI推理过程不足以发现隐藏的影响因素，当前的解释机制存在严重缺陷，需要更可靠的透明度方法

Abstract: When AI systems explain their reasoning step-by-step, practitioners often assume these explanations reveal what actually influenced the AI's answer. We tested this assumption by embedding hints into questions and measuring whether models mentioned them. In a study of over 9,000 test cases across 11 leading AI models, we found a troubling pattern: models almost never mention hints spontaneously, yet when asked directly, they admit noticing them. This suggests models see influential information but choose not to report it. Telling models they are being watched does not help. Forcing models to report hints works, but causes them to report hints even when none exist and reduces their accuracy. We also found that hints appealing to user preferences are especially dangerous-models follow them most often while reporting them least. These findings suggest that simply watching AI reasoning is not enough to catch hidden influences.

</details>


### [83] [OmniNeuro: A Multimodal HCI Framework for Explainable BCI Feedback via Generative AI and Sonification](https://arxiv.org/abs/2601.00843)
*Ayda Aghaei Nia*

Main category: cs.AI

TL;DR: OmniNeuro是一个新型HCI框架，将BCI从黑盒解码器转变为透明的反馈伙伴，通过物理、混沌和量子启发的可解释性引擎提供实时神经声化和生成式AI临床报告。


<details>
  <summary>Details</summary>
Motivation: 深度学习虽然提高了脑机接口的解码精度，但其"黑盒"特性阻碍了临床采用，导致用户挫折感和神经可塑性结果不佳。需要提高BCI系统的透明度和可解释性。

Method: 提出OmniNeuro框架，集成三种可解释性引擎：(1)物理(能量)分析，(2)混沌(分形复杂性)分析，(3)量子启发的确定性建模。这些指标驱动实时神经声化和生成式AI临床报告，框架与解码器无关，可作为任何最先进架构的可解释性层。

Result: 在PhysioNet数据集(N=109)上评估，系统平均准确率达到58.52%。定性试点研究(N=3)证实，可解释的反馈有助于用户调节心理努力，减少"试错"阶段。

Conclusion: OmniNeuro成功将BCI从沉默解码器转变为透明反馈伙伴，通过多模态可解释性引擎改善了用户体验和临床效果，为任何最先进BCI架构提供了必要的可解释性层。

Abstract: While Deep Learning has improved Brain-Computer Interface (BCI) decoding accuracy, clinical adoption is hindered by the "Black Box" nature of these algorithms, leading to user frustration and poor neuroplasticity outcomes. We propose OmniNeuro, a novel HCI framework that transforms the BCI from a silent decoder into a transparent feedback partner. OmniNeuro integrates three interpretability engines: (1) Physics (Energy), (2) Chaos (Fractal Complexity), and (3) Quantum-Inspired uncertainty modeling. These metrics drive real-time Neuro-Sonification and Generative AI Clinical Reports. Evaluated on the PhysioNet dataset ($N=109$), the system achieved a mean accuracy of 58.52%, with qualitative pilot studies ($N=3$) confirming that explainable feedback helps users regulate mental effort and reduces the "trial-and-error" phase. OmniNeuro is decoder-agnostic, acting as an essential interpretability layer for any state-of-the-art architecture.

</details>


### [84] [ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems](https://arxiv.org/abs/2601.00994)
*Michael Bao*

Main category: cs.AI

TL;DR: ElecTwit是一个模拟社交媒体政治选举中多智能体说服交互的框架，通过真实环境实验发现LLM广泛使用25种说服技巧，模型架构差异影响说服动态，并观察到独特现象如"真相核心"消息和"墨水"强迫症。


<details>
  <summary>Details</summary>
Motivation: 克服以往研究中基于游戏模拟的局限性，在更真实的环境中研究多智能体系统中的说服行为，特别是在社交媒体政治选举场景下。

Method: 开发ElecTwit模拟框架，在模拟社交媒体政治选举的逼真环境中测试多个LLM模型，分析它们使用的说服技巧和交互动态。

Result: 观察到所有测试的LLM广泛使用25种特定说服技巧，范围超过以往报告；不同模型在技巧使用和整体说服输出上存在显著差异；发现了"真相核心"消息和"墨水"强迫症等独特现象。

Conclusion: 该研究为在真实世界环境中评估有说服力的LLM智能体奠定了基础，有助于确保对齐并防止危险结果，模型架构和训练的差异显著影响社会模拟中的说服动态。

Abstract: This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as "kernel of truth" messages and spontaneous developments with an "ink" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.

</details>


### [85] [Enhancing Temporal Awareness in LLMs for Temporal Point Processes](https://arxiv.org/abs/2601.00845)
*Lili Chen,Wensheng Gan,Shuang Liang,Philip S. Yu*

Main category: cs.AI

TL;DR: TPP-TAL是一个新颖的即插即用框架，通过显式对齐时间动态与语义上下文来增强LLMs在时序点过程中的时间感知能力，显著提升了事件建模性能。


<details>
  <summary>Details</summary>
Motivation: 时序点过程在金融、医疗等领域很重要，但现有方法难以有效捕捉时间信息与语义上下文之间的复杂交互，限制了LLMs在连续时间事件建模中的应用。

Method: 提出TPP-TAL框架，不同于传统简单拼接时间和事件类型嵌入的方法，而是显式对齐时间动态与语义上下文，然后将对齐后的信息输入LLM，以增强时间依赖和长程交互的感知。

Result: 在多个基准数据集上的实验表明，TPP-TAL在时间似然估计和事件预测准确性方面带来了显著改进。

Conclusion: 增强LLMs的时间感知能力对于连续时间事件建模至关重要，TPP-TAL框架通过时间-语义对齐有效解决了现有方法的局限性。

Abstract: Temporal point processes (TPPs) are crucial for analyzing events over time and are widely used in fields such as finance, healthcare, and social systems. These processes are particularly valuable for understanding how events unfold over time, accounting for their irregularity and dependencies. Despite the success of large language models (LLMs) in sequence modeling, applying them to temporal point processes remains challenging. A key issue is that current methods struggle to effectively capture the complex interaction between temporal information and semantic context, which is vital for accurate event modeling. In this context, we introduce TPP-TAL (Temporal Point Processes with Enhanced Temporal Awareness in LLMs), a novel plug-and-play framework designed to enhance temporal reasoning within LLMs. Rather than using the conventional method of simply concatenating event time and type embeddings, TPP-TAL explicitly aligns temporal dynamics with contextual semantics before feeding this information into the LLM. This alignment allows the model to better perceive temporal dependencies and long-range interactions between events and their surrounding contexts. Through comprehensive experiments on several benchmark datasets, it is shown that TPP-TAL delivers substantial improvements in temporal likelihood estimation and event prediction accuracy, highlighting the importance of enhancing temporal awareness in LLMs for continuous-time event modeling. The code is made available at https://github.com/chenlilil/TPP-TAL

</details>


### [86] [COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs](https://arxiv.org/abs/2601.01836)
*Dasol Choi,DongGeon Lee,Brigitta Jesica Kartono,Helena Berndt,Taeyoun Kwon,Joonwon Jang,Haon Park,Hwanjo Yu,Minsuk Kahng*

Main category: cs.AI

TL;DR: COMPASS是首个评估LLM是否符合组织允许列表和禁止列表政策的系统框架，发现现有模型在处理合法请求时表现良好（>95%准确率），但在执行禁令时严重失败（仅拒绝13-40%的违规请求）。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在医疗、金融等高风险企业应用中的部署，确保模型遵守组织特定政策变得至关重要。然而现有的安全评估只关注通用危害，缺乏针对组织政策的评估框架。

Method: 提出COMPASS框架，应用于八个不同行业场景，生成并验证了5,920个查询，测试常规合规性和通过策略性设计的边缘案例测试对抗鲁棒性。评估了七个最先进的模型。

Result: 发现基本不对称性：模型可靠处理合法请求（>95%准确率），但在执行禁令时灾难性失败，仅拒绝13-40%的对抗性禁止列表违规请求。

Conclusion: 当前LLM缺乏政策关键部署所需的鲁棒性，COMPASS成为组织AI安全的重要评估框架。

Abstract: As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.

</details>


### [87] [Temporal Attack Pattern Detection in Multi-Agent AI Workflows: An Open Framework for Training Trace-Based Security Models](https://arxiv.org/abs/2601.00848)
*Ron F. Del Rosario*

Main category: cs.AI

TL;DR: 该论文提出了一种基于OpenTelemetry追踪分析的语言模型微调方法，用于检测多智能体AI工作流中的时序攻击模式，通过精心策划的数据集和迭代训练，将检测准确率从42.86%提升至74.29%。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可重现的框架来构建针对多智能体AI工作流时序攻击检测的自定义安全模型，特别是在资源受限环境下。需要一种系统化方法来训练语言模型识别复杂的攻击模式。

Method: 1. 收集80,851个来自18个公共网络安全源的示例和35,026个合成的OpenTelemetry追踪数据；2. 在NVIDIA DGX Spark等ARM64硬件上使用QLoRA进行迭代微调；3. 通过三次训练迭代和策略性数据增强优化模型；4. 开发自定义基准测试评估性能。

Result: 自定义基准测试准确率从42.86%提升至74.29%，实现了31.4个百分点的显著提升。针对特定知识差距的定向训练示例优于无差别扩展。提供了完整的数据集、训练脚本和评估基准。

Conclusion: 该研究建立了首个可重现的框架，使从业者能够构建适应其威胁环境的自定义智能体安全模型。虽然实际部署仍需人工监督（因误报率），但证明了训练数据组成对模型行为的决定性影响，并提供了完整的开源工具链。

Abstract: We present an openly documented methodology for fine-tuning language models to detect temporal attack patterns in multi-agent AI workflows using OpenTelemetry trace analysis. We curate a dataset of 80,851 examples from 18 public cybersecurity sources and 35,026 synthetic OpenTelemetry traces. We apply iterative QLoRA fine-tuning on resource-constrained ARM64 hardware (NVIDIA DGX Spark) through three training iterations with strategic augmentation. Our custom benchmark accuracy improves from 42.86% to 74.29%, a statistically significant 31.4-point gain. Targeted examples addressing specific knowledge gaps outperform indiscriminate scaling. Key contributions include: (1) synthetic trace generation methodology for multi-agent coordination attacks and regulatory violations, (2) empirical evidence that training data composition fundamentally determines behavior, and (3) complete open release of datasets, training scripts, and evaluation benchmarks on HuggingFace. While practical deployment requires human oversight due to false positive rates, this work establishes the first reproducible framework enabling practitioners to build custom agentic security models adapted to their threat landscapes.

</details>


### [88] [Theory Trace Card: Theory-Driven Socio-Cognitive Evaluation of LLMs](https://arxiv.org/abs/2601.01878)
*Farzan Karimi-Malekabadi,Suhaib Abdurahman,Zhivar Sourati,Jackson Trager,Morteza Dehghani*

Main category: cs.AI

TL;DR: 论文指出当前大语言模型的社会认知基准测试存在理论缺口，导致评估结果被过度泛化，提出了理论追踪卡作为解决方案来明确评估的理论基础。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的社会认知基准测试存在评估-部署差距，即使模型在基准测试中得分很高，也无法预测真实世界行为。现有研究主要归因于测量和效度问题，但作者认为更根本的问题是缺乏明确的理论基础，导致对模型能力的过度泛化。

Method: 论文提出两个贡献：1) 诊断并形式化理论缺口问题，指出这是导致测量失效和结果过度泛化的根本原因；2) 引入理论追踪卡(TTC)，这是一种轻量级文档工具，用于明确记录评估的理论基础、目标能力组件、操作化过程及其局限性。

Result: 理论追踪卡通过明确理论-任务操作化-评分-局限性的完整效度链，增强了社会认知评估的可解释性和可重用性，无需修改现有基准测试或要求单一理论共识。

Conclusion: 解决社会认知评估中的理论缺口对于避免系统性效度幻觉至关重要。理论追踪卡提供了一种实用方法，通过明确评估的理论基础来改善评估质量，促进更负责任的AI评估实践。

Abstract: Socio-cognitive benchmarks for large language models (LLMs) often fail to predict real-world behavior, even when models achieve high benchmark scores. Prior work has attributed this evaluation-deployment gap to problems of measurement and validity. While these critiques are insightful, we argue that they overlook a more fundamental issue: many socio-cognitive evaluations proceed without an explicit theoretical specification of the target capability, leaving the assumptions linking task performance to competence implicit. Without this theoretical grounding, benchmarks that exercise only narrow subsets of a capability are routinely misinterpreted as evidence of broad competence: a gap that creates a systemic validity illusion by masking the failure to evaluate the capability's other essential dimensions. To address this gap, we make two contributions. First, we diagnose and formalize this theory gap as a foundational failure that undermines measurement and enables systematic overgeneralization of benchmark results. Second, we introduce the Theory Trace Card (TTC), a lightweight documentation artifact designed to accompany socio-cognitive evaluations, which explicitly outlines the theoretical basis of an evaluation, the components of the target capability it exercises, its operationalization, and its limitations. We argue that TTCs enhance the interpretability and reuse of socio-cognitive evaluations by making explicit the full validity chain, which links theory, task operationalization, scoring, and limitations, without modifying benchmarks or requiring agreement on a single theory.

</details>


### [89] [Comment on: Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Tasks](https://arxiv.org/abs/2601.00856)
*Milos Stankovic,Ella Hirche,Sarah Kollatzsch,Julia Nadine Doetsch*

Main category: cs.AI

TL;DR: 这是一篇对Kosmyna等人(2025)关于ChatGPT对大脑认知影响研究的评论文章，指出了该研究在样本量、可重复性、EEG分析方法、结果报告一致性和透明度等方面存在的问题。


<details>
  <summary>Details</summary>
Motivation: 对Kosmyna等人关于AI助手对写作任务中认知影响的研究进行建设性评论，旨在提高该研究的学术严谨性和发表准备度，因为某些结果可能需要更保守的解释。

Method: 通过系统性分析，从五个方面提出批评意见：研究设计（样本量有限）、分析可重复性、EEG分析方法问题、结果报告不一致性、研究过程和发现透明度不足。

Result: 指出了原研究在多个关键方面存在缺陷，包括样本代表性不足、分析方法可能不可重复、EEG数据处理方法问题、结果报告存在不一致、以及缺乏足够的透明度。

Conclusion: 虽然认可Kosmyna等人研究的重要性和价值，但建议在发表前需要解决这些方法学问题，以确保研究结果的可靠性和科学严谨性。

Abstract: Recently published work titled Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Task by Kosmyna et al. (2025) has sparked a vivid debate on the topic of artificial intelligence (AI) and human performance. We sincerely congratulate Kosmyna et al. for initiating such important research, collecting a valuable dataset, and establishing highly automated pipelines for Natural Language Processing (NLP) analyses and scoring. We aim to provide constructive comments that may improve the manuscript's readiness for peer-reviewed publication, as some results by Kosmyna et al. (2025) could be interpreted more conservatively. Our primary concerns focus on: (i) study design considerations, including the limited sample size; (ii) the reproducibility of the analyses; (iii) methodological issues related to the EEG analysis; (iv) inconsistencies in the reporting of results; and (v) limited transparency in several aspects of the study's procedures and findings.

</details>


### [90] [Cultural Encoding in Large Language Models: The Existence Gap in AI-Mediated Brand Discovery](https://arxiv.org/abs/2601.00869)
*Huang Junyao,Situ Ruimin,Ye Renqin*

Main category: cs.AI

TL;DR: 研究发现LLM训练数据的地理分布导致品牌推荐存在系统性差异，中国LLM的品牌提及率比国际LLM高30.6个百分点，提出了"存在鸿沟"和"数据护城河"框架。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地中介消费者信息发现，品牌面临算法不可见性问题。研究旨在探究LLM中的文化编码现象——由训练数据构成导致的品牌推荐系统性差异。

Method: 分析了1,909个纯英文查询，覆盖6个LLM（GPT-4o、Claude、Gemini、Qwen3、DeepSeek、Doubao）和30个品牌。通过案例研究智子边界（OmniEdge）平台验证发现。

Result: 中国LLM的品牌提及率比国际LLM高30.6个百分点（88.9% vs. 58.3%），这种差异在相同的英文查询中持续存在，表明训练数据的地理分布而非语言驱动了这种效应。提出了"存在鸿沟"概念。

Conclusion: 提出了数据护城河框架，将AI可见内容概念化为VRIN战略资源。操作化了"算法无处不在"作为生成引擎优化的战略目标。在AI中介的市场中，品牌的"数据边界"决定了其"市场前沿"的界限。

Abstract: As artificial intelligence systems increasingly mediate consumer information discovery,
  brands face algorithmic invisibility. This study investigates Cultural Encoding in Large
  Language Models (LLMs) -- systematic differences in brand recommendations arising from
  training data composition. Analyzing 1,909 pure-English queries across 6 LLMs (GPT-4o,
  Claude, Gemini, Qwen3, DeepSeek, Doubao) and 30 brands, we find Chinese LLMs exhibit 30.6
  percentage points higher brand mention rates than International LLMs (88.9% vs. 58.3%,
  p<.001). This disparity persists in identical English queries, indicating training data
  geography -- not language -- drives the effect. We introduce the Existence Gap: brands
  absent from LLM training corpora lack "existence" in AI responses regardless of quality.
  Through a case study of Zhizibianjie (OmniEdge), a collaboration platform with 65.6%
  mention rate in Chinese LLMs but 0% in International models (p<.001), we demonstrate how
  Linguistic Boundary Barriers create invisible market entry obstacles. Theoretically, we
  contribute the Data Moat Framework, conceptualizing AI-visible content as a VRIN strategic
  resource. We operationalize Algorithmic Omnipresence -- comprehensive brand visibility
  across LLM knowledge bases -- as the strategic objective for Generative Engine Optimization
  (GEO). Managerially, we provide an 18-month roadmap for brands to build Data Moats
  through semantic coverage, technical depth, and cultural localization. Our findings reveal
  that in AI-mediated markets, the limits of a brand's "Data Boundaries" define the limits
  of its "Market Frontiers."

</details>


### [91] [Universal Conditional Logic: A Formal Language for Prompt Engineering](https://arxiv.org/abs/2601.00880)
*Anthony Mikinka*

Main category: cs.AI

TL;DR: UCL是一个将提示工程从启发式实践转变为系统化优化的数学框架，通过系统评估显示能显著减少token使用（29.8%），其结构开销函数揭示了过度指定悖论，核心机制得到验证，且最优配置因模型架构而异。


<details>
  <summary>Details</summary>
Motivation: 当前提示工程主要依赖启发式实践，缺乏系统化的优化框架。作者希望建立一个数学框架，将提示工程转变为可系统优化的过程，提高LLM交互的效率并降低成本。

Method: 提出了通用条件逻辑（UCL）框架，包含指示函数、结构开销函数、早期绑定等核心机制。通过系统评估（N=305，11个模型，4次迭代）验证框架效果，并引入结构开销函数O_s(A)来解释版本特定的性能差异，特别是过度指定悖论现象。

Result: 显著减少了token使用（29.8%），具有统计学意义（t(10)=6.36, p < 0.001, Cohen's d = 2.01），带来相应的成本节约。验证了UCL的核心机制，发现最优UCL配置因模型架构而异，某些模型需要版本特定的适配。

Conclusion: UCL作为一个可校准的框架，为高效的LLM交互提供了系统化方法。模型家族特定的优化是未来关键研究方向，框架的成功应用表明可以将提示工程从启发式实践转变为基于数学的系统优化。

Abstract: We present Universal Conditional Logic (UCL), a mathematical framework for prompt optimization that transforms prompt engineering from heuristic practice into systematic optimization. Through systematic evaluation (N=305, 11 models, 4 iterations), we demonstrate significant token reduction (29.8%, t(10)=6.36, p < 0.001, Cohen's d = 2.01) with corresponding cost savings. UCL's structural overhead function O_s(A) explains version-specific performance differences through the Over-Specification Paradox: beyond threshold S* = 0.509, additional specification degrades performance quadratically. Core mechanisms -- indicator functions (I_i in {0,1}), structural overhead (O_s = gamma * sum(ln C_k)), early binding -- are validated. Notably, optimal UCL configuration varies by model architecture -- certain models (e.g., Llama 4 Scout) require version-specific adaptations (V4.1). This work establishes UCL as a calibratable framework for efficient LLM interaction, with model-family-specific optimization as a key research direction.

</details>


### [92] [Counterfactual Self-Questioning for Stable Policy Optimization in Language Models](https://arxiv.org/abs/2601.00885)
*Mandar Parab*

Main category: cs.AI

TL;DR: 提出Counterfactual Self-Questioning框架，让单个语言模型生成并评估自身推理的反事实批评，通过内部生成的监督实现可扩展的自我改进。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型自我改进方法依赖外部批评者、学习奖励模型或集成采样，增加了复杂性和训练不稳定性。需要一种更简单、更稳定的自我改进方法。

Method: 提出反事实自我提问框架：1) 生成初始推理轨迹；2) 针对潜在失败点提出针对性问题；3) 生成暴露错误假设或无效步骤的替代推理轨迹；4) 利用这些反事实轨迹作为结构化相对反馈进行策略优化。

Result: 在多个数学推理基准测试中，反事实自我提问提高了准确性和训练稳定性，特别是对于较小模型，仅使用内部生成的监督就能实现可扩展的自我改进。

Conclusion: Counterfactual Self-Questioning提供了一种简单有效的自我改进框架，无需外部模型辅助，通过内部生成的反事实批评实现稳定训练和性能提升。

Abstract: Recent work on language model self-improvement shows that models can refine their own reasoning through reflection, verification, debate, or self-generated rewards. However, most existing approaches rely on external critics, learned reward models, or ensemble sampling, which increases complexity and training instability. We propose Counterfactual Self-Questioning, a framework in which a single language model generates and evaluates counterfactual critiques of its own reasoning. The method produces an initial reasoning trace, formulates targeted questions that challenge potential failure points, and generates alternative reasoning trajectories that expose incorrect assumptions or invalid steps. These counterfactual trajectories provide structured relative feedback that can be directly used for policy optimization without auxiliary models. Experiments on multiple mathematical reasoning benchmarks show that counterfactual self-questioning improves accuracy and training stability, particularly for smaller models, enabling scalable self-improvement using internally generated supervision alone.

</details>


### [93] [Context Collapse: In-Context Learning and Model Collapse](https://arxiv.org/abs/2601.00923)
*Josef Ott*

Main category: cs.AI

TL;DR: 该论文研究了LLMs中的上下文学习和模型崩溃现象，在线性变压器中证明了最小化上下文损失会导致参数相变，并分析了模型崩溃的收敛条件，最后提出了上下文崩溃的概念。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型中两个关键现象：上下文学习（ICL）和模型崩溃，旨在理解这些现象背后的数学机制和动态特性。

Method: 1. 在线性回归任务上训练具有绑定权重的线性变压器，分析上下文学习；2. 使用鞅和随机游走理论分析线性回归和高斯拟合的简化设置；3. 将线性变压器的前向传播简化为预条件梯度下降并分析最优预条件器。

Result: 1. 最小化上下文损失会导致参数相变，超过临界上下文长度时解会发展出斜对称分量；2. 证明了模型崩溃的几乎必然收敛，除非数据增长足够快或随时间保留；3. 提出了上下文崩溃概念，连接了上下文学习动态与生成模型的长期稳定性挑战。

Conclusion: 论文揭示了上下文学习和模型崩溃的数学机制，证明了相变现象和收敛条件，并提出了上下文崩溃作为连接这两个现象的新概念，为理解LLMs的动态行为提供了理论框架。

Abstract: This thesis investigates two key phenomena in large language models (LLMs): in-context learning (ICL) and model collapse. We study ICL in a linear transformer with tied weights trained on linear regression tasks, and show that minimising the in-context loss leads to a phase transition in the learned parameters. Above a critical context length, the solution develops a skew-symmetric component. We prove this by reducing the forward pass of the linear transformer under weight tying to preconditioned gradient descent, and then analysing the optimal preconditioner. This preconditioner includes a skew-symmetric component, which induces a rotation of the gradient direction. For model collapse, we use martingale and random walk theory to analyse simplified settings - linear regression and Gaussian fitting - under both replacing and cumulative data regimes. We strengthen existing results by proving almost sure convergence, showing that collapse occurs unless the data grows sufficiently fast or is retained over time. Finally, we introduce the notion of context collapse: a degradation of context during long generations, especially in chain-of-thought reasoning. This concept links the dynamics of ICL with long-term stability challenges in generative models.

</details>


### [94] [Reinforcement Learning Enhanced Multi-hop Reasoning for Temporal Knowledge Question Answering](https://arxiv.org/abs/2601.01195)
*Wuzhenghong Wen,Chao Xue,Su Pan,Yuwei Sun,Minlong Peng*

Main category: cs.AI

TL;DR: 提出MRE框架，通过增强前向和后向推理来改进TKGQA中的多跳推理，使用Tree-Group Relative Policy Optimization优化推理轨迹选择


<details>
  <summary>Details</summary>
Motivation: TKGQA中，大型语言模型在每跳推理时会检索大量时间相似且语义复杂的关系子图，导致次优决策和错误传播风险增加

Method: 提出MRE框架：1) 提示工程生成多样推理轨迹；2) 选择有效轨迹进行监督微调作为冷启动；3) 引入T-GRPO，一种递归树结构的学习探索方法，建立强因果依赖关系

Result: 在两个TKGQA基准测试中，MRE模型持续超越SOTA方法，在处理复杂多跳查询方面表现优异，同时提高了可解释性和对噪声时间标注的鲁棒性

Conclusion: MRE框架通过增强前向和后向推理，有效解决了TKGQA中的多跳推理挑战，提高了推理轨迹的全局最优性和系统鲁棒性

Abstract: Temporal knowledge graph question answering (TKGQA) involves multi-hop reasoning over temporally constrained entity relationships in the knowledge graph to answer a given question. However, at each hop, large language models (LLMs) retrieve subgraphs with numerous temporally similar and semantically complex relations, increasing the risk of suboptimal decisions and error propagation. To address these challenges, we propose the multi-hop reasoning enhanced (MRE) framework, which enhances both forward and backward reasoning to improve the identification of globally optimal reasoning trajectories. Specifically, MRE begins with prompt engineering to guide the LLM in generating diverse reasoning trajectories for a given question. Valid reasoning trajectories are then selected for supervised fine-tuning, serving as a cold-start strategy. Finally, we introduce Tree-Group Relative Policy Optimization (T-GRPO), a recursive, tree-structured learning-by-exploration approach. At each hop, exploration establishes strong causal dependencies on the previous hop, while evaluation is informed by multi-path exploration feedback from subsequent hops. Experimental results on two TKGQA benchmarks indicate that the proposed MRE-based model consistently surpasses state-of-the-art (SOTA) approaches in handling complex multi-hop queries. Further analysis highlights improved interpretability and robustness to noisy temporal annotations.

</details>


### [95] [Accelerating Monte-Carlo Tree Search with Optimized Posterior Policies](https://arxiv.org/abs/2601.01301)
*Keith Frankston,Benjamin Howard*

Main category: cs.AI

TL;DR: 提出递归AlphaZero风格的蒙特卡洛树搜索算法RMCTS，比传统MCTS-UCB快40倍以上，训练时间减少三分之二


<details>
  <summary>Details</summary>
Motivation: 传统AlphaZero的MCTS-UCB算法存在GPU延迟问题，网络推理无法批量处理，导致搜索速度较慢。需要一种更高效的树搜索算法来加速训练过程。

Method: 提出递归蒙特卡洛树搜索(RMCTS)，采用广度优先搜索方式，使网络推理能够批量处理。基于Grill等人提出的后验策略优化方法，从叶子节点向根节点递归计算优化后验策略。树结构由先验网络策略定义而非自适应构建。

Result: 在单个根状态搜索时，RMCTS比MCTS-UCB快40倍以上；在批量根状态搜索时快约3倍。使用RMCTS训练的网络质量与MCTS-UCB相当，但训练时间减少约三分之二。在Connect-4、Dots-and-Boxes和Othello三个游戏中验证了性能优势。

Conclusion: RMCTS通过递归和批量处理显著加速了蒙特卡洛树搜索，虽然牺牲了树的自适应性，但速度优势明显，能够大幅减少训练时间而不损失网络质量。

Abstract: We introduce a recursive AlphaZero-style Monte--Carlo tree search algorithm, "RMCTS". The advantage of RMCTS over AlphaZero's MCTS-UCB is speed. In RMCTS, the search tree is explored in a breadth-first manner, so that network inferences naturally occur in large batches. This significantly reduces the GPU latency cost. We find that RMCTS is often more than 40 times faster than MCTS-UCB when searching a single root state, and about 3 times faster when searching a large batch of root states.
  The recursion in RMCTS is based on computing optimized posterior policies at each game state in the search tree, starting from the leaves and working back up to the root. Here we use the posterior policy explored in "Monte--Carlo tree search as regularized policy optimization" (Grill, et al.) Their posterior policy is the unique policy which maximizes the expected reward given estimated action rewards minus a penalty for diverging from the prior policy.
  The tree explored by RMCTS is not defined in an adaptive manner, as it is in MCTS-UCB. Instead, the RMCTS tree is defined by following prior network policies at each node. This is a disadvantage, but the speedup advantage is more significant, and in practice we find that RMCTS-trained networks match the quality of MCTS-UCB-trained networks in roughly one-third of the training time. We include timing and quality comparisons of RMCTS vs. MCTS-UCB for three games: Connect-4, Dots-and-Boxes, and Othello.

</details>


### [96] [Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models](https://arxiv.org/abs/2601.01321)
*Rong Zhou,Dongping Chen,Zihan Jia,Yao Su,Yixin Liu,Yiwen Lu,Dongwei Shi,Yue Huang,Tianyang Xu,Yi Pan,Xinliang Li,Yohannes Abate,Qingyu Chen,Zhengzhong Tu,Yu Yang,Yu Zhang,Qingsong Wen,Gengchen Mai,Sunyang Fu,Jiachen Li,Xuyu Wang,Ziran Wang,Jing Huang,Tianming Liu,Yong Chen,Lichao Sun,Lifang He*

Main category: cs.AI

TL;DR: 本文提出了一个统一的四阶段框架，系统描述人工智能在数字孪生全生命周期中的集成，涵盖建模、镜像、干预和自主管理四个阶段，并分析了AI技术如何将数字孪生从被动仿真工具转变为智能自主实体。


<details>
  <summary>Details</summary>
Motivation: 数字孪生作为物理系统的精确数字表示，正从被动仿真工具向智能自主实体演进。然而，AI技术在数字孪生中的集成缺乏系统性框架，需要统一的理论指导来理解AI如何在不同阶段赋能数字孪生，并推动其向认知系统发展。

Method: 提出统一的四阶段框架：1) 通过物理基础和物理信息AI方法建模物理孪生；2) 通过实时同步将物理系统镜像为数字孪生；3) 通过预测建模、异常检测和优化策略干预物理孪生；4) 通过大语言模型、基础模型和智能代理实现自主管理。通过跨11个应用领域的综述分析技术趋势和挑战。

Result: 建立了系统性的AI集成框架，分析了物理建模与数据驱动学习的协同作用，揭示了从传统数值求解器向物理信息模型和基础模型的转变。识别了生成式AI技术如何将数字孪生转变为具有推理、通信和创造性场景生成能力的认知系统，并指出了可扩展性、可解释性和可信度等共同挑战。

Conclusion: AI技术正在彻底改变数字孪生的本质，使其从被动仿真工具转变为主动、自改进的认知系统。统一的四阶段框架为理解AI在数字孪生生命周期中的集成提供了系统性视角，未来需要解决可扩展性、可解释性和可信度等挑战，推动负责任AI驱动的数字孪生系统发展。

Abstract: Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.

</details>


### [97] [Beyond Gemini-3-Pro: Revisiting LLM Routing and Aggregation at Scale](https://arxiv.org/abs/2601.01330)
*Shengji Tang,Weihao Lin,Jingqi Ye,Hao Li,Bo Zhang,Shuyue Hu,Tao Chen,Wangli Ouyang,Lei Bai,Peng Ye*

Main category: cs.AI

TL;DR: JiSi框架通过查询-响应混合路由、支持集聚合器选择和自适应路由-聚合切换，使开源LLM协作超越Gemini-3-Pro性能，成本仅47%


<details>
  <summary>Details</summary>
Motivation: 探索集体智能作为替代单体模型扩展的新路径，解决当前LLM路由和聚合的三个关键瓶颈：查询式路由局限、静态聚合方法、路由与聚合互补性未充分利用

Method: 提出JiSi框架，包含三个创新：1) 查询-响应混合路由同时捕捉语义信息和问题难度；2) 支持集聚合器选择评估聚合能力和领域能力；3) 自适应路由-聚合切换动态利用路由和聚合优势

Result: 在9个基准测试中，JiSi通过协调10个开源LLM，以仅47%的成本超越了Gemini-3-Pro性能，同时优于主流基线方法

Conclusion: 集体智能代表了通往AGI的新路径，通过有效协作开源LLM可以实现超越顶级单体模型的性能

Abstract: Large Language Models (LLMs) have rapidly advanced, with Gemini-3-Pro setting a new performance milestone. In this work, we explore collective intelligence as an alternative to monolithic scaling, and demonstrate that open-source LLMs' collaboration can surpass Gemini-3-Pro. We first revisit LLM routing and aggregation at scale and identify three key bottlenecks: (1) current train-free routers are limited by a query-based paradigm focusing solely on textual similarity; (2) recent aggregation methods remain largely static, failing to select appropriate aggregators for different tasks;(3) the complementarity of routing and aggregation remains underutilized. To address these problems, we introduce JiSi, a novel framework designed to release the full potential of LLMs' collaboration through three innovations: (1) Query-Response Mixed Routing capturing both semantic information and problem difficulty; (2) Support-Set-based Aggregator Selection jointly evaluating the aggregation and domain capacity of aggregators; (3) Adaptive Routing-Aggregation Switch dynamically leveraging the advantages of routing and aggregation. Comprehensive experiments on nine benchmarks demonstrate that JiSi can surpass Gemini-3-Pro with only 47% costs by orchestrating ten open-source LLMs, while outperforming mainstream baselines. It suggests that collective intelligence represents a novel path towards Artificial General Intelligence (AGI).

</details>


### [98] [A unified multimodal understanding and generation model for cross-disciplinary scientific research](https://arxiv.org/abs/2601.01363)
*Xiaomeng Yang,Zhiyu Tan,Xiaohui Zhong,Mengping Yang,Qiusheng Huang,Lei Chen,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: FuXi-Uni是一个统一的多模态科学模型，能够在单一架构中理解和生成跨学科的科学数据，在气象预报和生物医学问答等任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前科学发现需要整合跨学科的高维异构数据，但现有AI模型通常是领域特定的，缺乏同时理解和生成多模态科学数据的能力。许多全球性挑战和科学问题本质上是跨学科的，需要多个领域的协同进展。

Method: FuXi-Uni将跨学科的科学标记与自然语言标记对齐，并使用科学解码器重建科学标记，从而同时支持自然语言对话和科学数值预测。模型在统一的多模态架构中处理异构科学模态。

Result: 在地球科学中：1) 生成0.25°分辨率的10天全球天气预报，超越最先进的物理预报系统；2) 热带气旋路径和强度预测优于最先进物理模型；3) 生成的高分辨率区域天气场超越标准插值基线。在生物医学中：在多个生物医学视觉问答基准上优于领先的多模态大语言模型。

Conclusion: FuXi-Uni通过在原生共享潜在空间中统一异构科学模态，同时保持强大的领域特定性能，为更通用的多模态科学模型迈出了重要一步。

Abstract: Scientific discovery increasingly relies on integrating heterogeneous, high-dimensional data across disciplines nowadays. While AI models have achieved notable success across various scientific domains, they typically remain domain-specific or lack the capability of simultaneously understanding and generating multimodal scientific data, particularly for high-dimensional data. Yet, many pressing global challenges and scientific problems are inherently cross-disciplinary and require coordinated progress across multiple fields. Here, we present FuXi-Uni, a native unified multimodal model for scientific understanding and high-fidelity generation across scientific domains within a single architecture. Specifically, FuXi-Uni aligns cross-disciplinary scientific tokens within natural language tokens and employs science decoder to reconstruct scientific tokens, thereby supporting both natural language conversation and scientific numerical prediction. Empirically, we validate FuXi-Uni in Earth science and Biomedicine. In Earth system modeling, the model supports global weather forecasting, tropical cyclone (TC) forecast editing, and spatial downscaling driven by only language instructions. FuXi-Uni generates 10-day global forecasts at 0.25° resolution that outperform the SOTA physical forecasting system. It shows superior performance for both TC track and intensity prediction relative to the SOTA physical model, and generates high-resolution regional weather fields that surpass standard interpolation baselines. Regarding biomedicine, FuXi-Uni outperforms leading multimodal large language models on multiple biomedical visual question answering benchmarks. By unifying heterogeneous scientific modalities within a native shared latent space while maintaining strong domain-specific performance, FuXi-Uni provides a step forward more general-purpose, multimodal scientific models.

</details>


### [99] [KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models](https://arxiv.org/abs/2601.01366)
*Zixian Liu,Sihao Liu,Yuqi Zhao*

Main category: cs.AI

TL;DR: KGCE是一个针对教育场景中跨平台任务执行的新型基准测试平台，通过知识库增强和双图评估框架解决现有基准在私有领域软件任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准框架在教育场景的跨平台任务支持不足，特别是在处理学校专用软件（如小雅智能助手、华师匣子等）时，由于缺乏对这些私有领域软件结构细节的理解，代理效率显著下降。同时，当前评估方法过度依赖粗粒度指标，难以捕捉复杂任务中的详细执行和效率。

Method: 提出了KGCE平台，整合知识库增强和双图评估框架。构建了包含104个教育相关任务的数据集，涵盖Windows、Android和跨平台协作任务。引入双图评估框架将任务分解为多个子目标并验证完成状态，提供细粒度评估指标。开发了增强的代理系统，包含针对学校专用软件的知识库。

Result: 开发了KGCE基准测试平台，包含104个教育任务的数据集，实现了知识库增强的代理系统和双图评估框架，代码已在GitHub开源。

Conclusion: KGCE通过知识库增强和双图评估框架有效解决了教育场景中跨平台任务基准测试的现有缺陷，为多模态大语言模型在自主代理中的教育应用提供了更精细的评估工具。

Abstract: With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform tasks in educational contexts, especially when dealing with school-specific software (such as XiaoYa Intelligent Assistant, HuaShi XiaZi, etc.), where the efficiency of agents often significantly decreases due to a lack of understanding of the structural specifics of these private-domain software. Additionally, current evaluation methods heavily rely on coarse-grained metrics like goal orientation or trajectory matching, making it challenging to capture the detailed execution and efficiency of agents in complex tasks. To address these issues, we propose KGCE (Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models), a novel benchmarking platform that integrates knowledge base enhancement and a dual-graph evaluation framework. We first constructed a dataset comprising 104 education-related tasks, covering Windows, Android, and cross-platform collaborative tasks. KGCE introduces a dual-graph evaluation framework that decomposes tasks into multiple sub-goals and verifies their completion status, providing fine-grained evaluation metrics. To overcome the execution bottlenecks of existing agents in private-domain tasks, we developed an enhanced agent system incorporating a knowledge base specific to school-specific software. The code can be found at https://github.com/Kinginlife/KGCE.

</details>


### [100] [Empowering Small Language Models with Factual Hallucination-Aware Reasoning for Financial Classification](https://arxiv.org/abs/2601.01378)
*Han Yuan,Yilin Wu,Li Zhang,Zheng Ma*

Main category: cs.AI

TL;DR: 提出AAAI三阶段流程，通过减少事实幻觉来提升小语言模型在金融分类任务中的性能


<details>
  <summary>Details</summary>
Motivation: 小语言模型在金融分类中推理速度快、可本地部署，但相比大模型更容易产生事实幻觉且分类性能较弱，需要探索减少事实幻觉是否能提升其金融分类能力

Method: 提出AAAI三阶段流程：关联识别、自动检测和自适应推理。首先识别事实关联，然后使用编码器验证器检测事实幻觉，最后通过事实错误反馈实现自适应推理

Result: 实验发现：1）事实幻觉与错误分类正相关；2）编码器验证器能有效检测事实幻觉；3）结合事实错误反馈的自适应推理能提升分类性能

Conclusion: AAAI流程有助于提升小语言模型在金融领域的可信度和有效性应用

Abstract: Small language models (SLMs) are increasingly used for financial classification due to their fast inference and local deployability. However, compared with large language models, SLMs are more prone to factual hallucinations in reasoning and exhibit weaker classification performance. This raises a natural question: Can mitigating factual hallucinations improve SLMs' financial classification? To address this, we propose a three-step pipeline named AAAI (Association Identification, Automated Detection, and Adaptive Inference). Experiments on three representative SLMs reveal that: (1) factual hallucinations are positively correlated with misclassifications; (2) encoder-based verifiers effectively detect factual hallucinations; and (3) incorporating feedback on factual errors enables SLMs' adaptive inference that enhances classification performance. We hope this pipeline contributes to trustworthy and effective applications of SLMs in finance.

</details>


### [101] [A construction of an optimal base for conditional attribute and attributional condition implications in triadic contexts](https://arxiv.org/abs/2601.01467)
*Romuald Kwessy Mouona,Blaise Blériot Koguep Njionou,Etienne Romuald Temgoua Alomo,Rokia Missaoui,Leonard Kwuida*

Main category: cs.AI

TL;DR: 本文研究三元上下文中的蕴涵关系，重点分析Ganter和Obiedkov引入的条件属性蕴涵和属性条件蕴涵，旨在为这些蕴涵构建最优基


<details>
  <summary>Details</summary>
Motivation: 在三元上下文（triadic contexts）中，现有的蕴涵理论需要进一步扩展和完善。Ganter和Obiedkov引入的条件属性蕴涵和属性条件蕴涵为三元数据分析提供了新的工具，但缺乏系统的最优基构造方法

Method: 研究三元上下文中的蕴涵关系，分析条件属性蕴涵和属性条件蕴涵的特性，开发构造这些蕴涵最优基的算法或方法

Result: 提出了为三元上下文中的条件属性蕴涵和属性条件蕴涵构造最优基的方法，可能包括基的完备性、非冗余性和最小性等性质

Conclusion: 成功扩展了形式概念分析中的蕴涵理论到三元上下文，为三元数据分析提供了有效的蕴涵基构造工具，增强了三元数据挖掘的能力

Abstract: This article studies implications in triadic contexts. Specifically, we focus on those introduced by Ganter and Obiedkov, namely conditional attribute and attributional condition implications. Our aim is to construct an optimal base for these implications.

</details>


### [102] [Reading Between the Lines: Deconfounding Causal Estimates using Text Embeddings and Deep Learning](https://arxiv.org/abs/2601.01511)
*Ahmed Dawoud,Osama El-Shamy*

Main category: cs.AI

TL;DR: 提出神经网络增强的双重机器学习框架，利用文本嵌入解决未观测混杂变量问题，相比传统树模型能显著减少估计偏差


<details>
  <summary>Details</summary>
Motivation: 在观测性研究中，未观测混杂变量导致的选择偏差是因果效应估计的主要挑战。传统计量方法在混杂变量与结构化协变量正交时效果不佳，而高维非结构化文本数据通常包含这些潜在变量的丰富代理信息。

Method: 提出神经网络增强的双重机器学习框架，利用文本嵌入进行因果识别。该方法通过深度学习架构建模嵌入流形的连续拓扑结构，优化架构设计以有效捕获混杂信息。

Result: 使用合成基准测试表明：1）非结构化文本嵌入能捕获结构化表格数据中缺失的关键混杂信息；2）标准树基DML估计器由于无法建模嵌入流形的连续拓扑，仍存在显著偏差（+24%）；3）提出的深度学习方法将偏差降至-0.86%，有效恢复了真实因果参数。

Conclusion: 当基于高维自然语言数据进行条件分析时，深度学习架构对于满足无混杂假设至关重要。神经网络增强的DML框架能有效利用文本嵌入中的混杂信息，显著提高因果效应估计的准确性。

Abstract: Estimating causal treatment effects in observational settings is frequently compromised by selection bias arising from unobserved confounders. While traditional econometric methods struggle when these confounders are orthogonal to structured covariates, high-dimensional unstructured text often contains rich proxies for these latent variables. This study proposes a Neural Network-Enhanced Double Machine Learning (DML) framework designed to leverage text embeddings for causal identification. Using a rigorous synthetic benchmark, we demonstrate that unstructured text embeddings capture critical confounding information that is absent from structured tabular data. However, we show that standard tree-based DML estimators retain substantial bias (+24%) due to their inability to model the continuous topology of embedding manifolds. In contrast, our deep learning approach reduces bias to -0.86% with optimized architectures, effectively recovering the ground-truth causal parameter. These findings suggest that deep learning architectures are essential for satisfying the unconfoundedness assumption when conditioning on high-dimensional natural language data

</details>


### [103] [Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making](https://arxiv.org/abs/2601.01522)
*Danial Amin*

Main category: cs.AI

TL;DR: 提出贝叶斯成本感知的多LLM协同框架，将LLM视为近似似然模型而非分类器，在非对称错误成本场景中显著降低总成本并提升公平性


<details>
  <summary>Details</summary>
Motivation: 当前LLM在非对称错误成本的自主决策场景（如招聘、医疗分诊、欺诈检测）中，主流方法是查询单个LLM获取后验概率并基于"置信度"阈值行动，但这种方法在序列决策中成本处理不足

Method: 提出贝叶斯成本感知的多LLM协同框架：1) 通过对比提示为每个候选状态获取似然；2) 使用稳健统计方法聚合多个不同模型的输出；3) 在新证据到达时使用贝叶斯规则在显式先验下更新信念；4) 支持连贯的信念更新、期望成本行动选择、基于信息价值的原则性信息收集

Result: 在简历筛选实验中（错失人才成本40000美元，面试成本2500美元，电话筛选成本150美元），使用5个LLM（GPT-4o、Claude 4.5 Sonnet、Gemini Pro、Grok、DeepSeek）处理1000份简历，相比最佳单LLM基线降低总成本294000美元（34%），并将人口统计公平性提升45%（最大群体差距从22个百分点降至5个百分点）

Conclusion: 正确的概率基础理论在多LLM决策中具有重要价值：多LLM聚合贡献51%的成本节省，序列更新贡献43%，分歧触发的信息收集贡献20%，证明了贝叶斯框架在非对称成本序列决策中的有效性

Abstract: Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds "confidence," and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.

</details>


### [104] [Aletheia: Quantifying Cognitive Conviction in Reasoning Models via Regularized Inverse Confusion Matrix](https://arxiv.org/abs/2601.01532)
*Fanzhe Fu*

Main category: cs.AI

TL;DR: 该论文提出了Project Aletheia框架，用于量化System 2推理模型的"认知信念强度"，通过Tikhonov正则化反演评估者的混淆矩阵，并引入对齐信念分数来确保安全性。


<details>
  <summary>Details</summary>
Motivation: 当前AGI评估范式面临认识论危机：静态基准测试只能衡量知识广度，无法量化信念深度。虽然Simhi等人(2025)在标准QA中定义了CHOKE现象，但需要扩展该框架来量化System 2推理模型的认知信念强度。

Method: 提出Project Aletheia认知物理学框架，采用Tikhonov正则化反演评估者的混淆矩阵。为避免依赖不透明的私有数据，实施了合成代理协议。引入对齐信念分数(S_aligned)来验证信念强度不会损害安全性。

Result: 对2025年基线模型(如DeepSeek-R1、OpenAI o1)的初步试点研究表明：推理模型虽然起到"认知缓冲"作用，但在对抗压力下可能表现出"防御性过度思考"现象。

Conclusion: 该工作为衡量AI科学完整性提供了蓝图，提出的框架能够量化认知信念强度，同时通过对齐信念分数确保安全性，为AGI评估提供了新的方法论基础。

Abstract: In the progressive journey toward Artificial General Intelligence (AGI), current evaluation paradigms face an epistemological crisis. Static benchmarks measure knowledge breadth but fail to quantify the depth of belief. While Simhi et al. (2025) defined the CHOKE phenomenon in standard QA, we extend this framework to quantify "Cognitive Conviction" in System 2 reasoning models. We propose Project Aletheia, a cognitive physics framework that employs Tikhonov Regularization to invert the judge's confusion matrix. To validate this methodology without relying on opaque private data, we implement a Synthetic Proxy Protocol. Our preliminary pilot study on 2025 baselines (e.g., DeepSeek-R1, OpenAI o1) suggests that while reasoning models act as a "cognitive buffer," they may exhibit "Defensive OverThinking" under adversarial pressure. Furthermore, we introduce the Aligned Conviction Score (S_aligned) to verify that conviction does not compromise safety. This work serves as a blueprint for measuring AI scientific integrity.

</details>


### [105] [Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation](https://arxiv.org/abs/2601.01546)
*Letian Kong,Qianran,Jin,Renyu Zhang*

Main category: cs.AI

TL;DR: 论文提出两阶段框架改善LLM在复杂决策环境中的行为对齐：上下文形成（明确实验设计）和上下文导航（指导推理过程），验证表明复杂任务需要两阶段，简单任务仅需第一阶段。


<details>
  <summary>Details</summary>
Motivation: LLM在模拟人类行为时，在需要预测他人行动和基于观察形成信念的复杂决策环境中，系统性地偏离人类决策，需要改进行为对齐方法。

Method: 提出两阶段框架：1) 上下文形成阶段明确指定实验设计，建立决策任务和上下文的准确表示；2) 上下文导航阶段在该表示内指导推理过程做出决策。通过三个实验验证：顺序购买游戏、众筹游戏和需求估计任务。

Result: 在四个SOTA模型（GPT-4o、GPT-5、Claude-4.0-Sonnet-Thinking、DeepSeek-R1）上验证发现：复杂决策环境需要两阶段才能实现与人类基准的行为对齐，而简单的需求估计任务仅需上下文形成阶段。

Conclusion: 研究阐明了每个阶段在何时必要，为设计和诊断LLM社会模拟作为行为研究中人类受试者的补充提供了系统方法。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior in experimental settings, but they systematically diverge from human decisions in complex decision-making environments, where participants must anticipate others' actions and form beliefs based on observed behavior. We propose a two-stage framework for improving behavioral alignment. The first stage, context formation, explicitly specifies the experimental design to establish an accurate representation of the decision task and its context. The second stage, context navigation, guides the reasoning process within that representation to make decisions. We validate this framework through a focal replication of a sequential purchasing game with quality signaling (Kremer and Debo, 2016), extending to a crowdfunding game with costly signaling (Cason et al., 2025) and a demand-estimation task (Gui and Toubia, 2025) to test generalizability across decision environments. Across four state-of-the-art (SOTA) models (GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1), we find that complex decision-making environments require both stages to achieve behavioral alignment with human benchmarks, whereas the simpler demand-estimation task requires only context formation. Our findings clarify when each stage is necessary and provide a systematic approach for designing and diagnosing LLM social simulations as complements to human subjects in behavioral research.

</details>


### [106] [Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training and Document Knowledge Enhancement](https://arxiv.org/abs/2601.01562)
*Mingyu Xu,Cheng Fang,Keyue Jiang,Yuqian Zheng,Yanghua Xiao,Baojian Zhou,Qifang Zhao,Suhang Zheng,Xiuwen Zhu,Jiyang Tang,Yongchi Zhao,Yijia Luo,Zhiqi Bai,Yuchi Xu,Wenbo Su,Wei Wang,Bing Zhao,Lin Qu,Xiaoxiao Xu*

Main category: cs.AI

TL;DR: Logics-STEM是一个针对STEM领域推理任务的最先进模型，通过数据算法协同设计在10M规模高质量数据集上微调，在STEM基准测试中平均提升4.68%


<details>
  <summary>Details</summary>
Motivation: 针对STEM领域推理任务，现有模型在科学、技术、工程和数学推理方面存在不足，需要大规模高质量数据和算法协同优化来提升推理能力

Method: 采用数据算法协同设计引擎：数据方面通过5阶段数据策展引擎构建10M规模高质量数据集；算法方面使用故障驱动后训练框架，针对SFT阶段的失败区域进行针对性知识检索和数据合成

Result: 在STEM相关基准测试中表现优异，相比8B规模次优模型平均提升4.68%，展示了大规模开源数据与精心设计合成数据结合的潜力

Conclusion: 数据算法协同设计通过后训练显著提升推理能力，证明了大规模高质量数据与针对性算法优化的结合对STEM推理任务的重要性，相关模型和数据集已开源

Abstract: We present Logics-STEM, a state-of-the-art reasoning model fine-tuned on Logics-STEM-SFT-Dataset, a high-quality and diverse dataset at 10M scale that represents one of the largest-scale open-source long chain-of-thought corpora. Logics-STEM targets reasoning tasks in the domains of Science, Technology, Engineering, and Mathematics (STEM), and exhibits exceptional performance on STEM-related benchmarks with an average improvement of 4.68% over the next-best model at 8B scale. We attribute the gains to our data-algorithm co-design engine, where they are jointly optimized to fit a gold-standard distribution behind reasoning. Data-wise, the Logics-STEM-SFT-Dataset is constructed from a meticulously designed data curation engine with 5 stages to ensure the quality, diversity, and scalability, including annotation, deduplication, decontamination, distillation, and stratified sampling. Algorithm-wise, our failure-driven post-training framework leverages targeted knowledge retrieval and data synthesis around model failure regions in the Supervised Fine-tuning (SFT) stage to effectively guide the second-stage SFT or the reinforcement learning (RL) for better fitting the target distribution. The superior empirical performance of Logics-STEM reveals the vast potential of combining large-scale open-source data with carefully designed synthetic data, underscoring the critical role of data-algorithm co-design in enhancing reasoning capabilities through post-training. We make both the Logics-STEM models (8B and 32B) and the Logics-STEM-SFT-Dataset (10M and downsampled 2.2M versions) publicly available to support future research in the open-source community.

</details>


### [107] [CaveAgent: Transforming LLMs into Stateful Runtime Operators](https://arxiv.org/abs/2601.01569)
*Maohao Ran,Zhenglin Wan,Cooper Lin,Yanting Zhang,Hongyu Xin,Hongwei Fan,Yibo Xu,Beier Luo,Yaxin Zhou,Wangbo Zhao,Lijie Yang,Lang Feng,Fuchao Yang,Jingxuan Wu,Yiqiao Huang,Chendong Ma,Dailing Jiang,Jianbo Deng,Sihui Han,Bo An,Yike Guo,Jun Song*

Main category: cs.AI

TL;DR: CaveAgent是一个将LLM从文本生成器转变为运行时操作员的框架，通过双流架构分离状态管理，支持Python对象持久化，显著提升复杂任务执行效率并减少token消耗。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体系统受限于文本中心范式，传统的JSON函数调用在处理长时程任务时存在脆弱的多轮依赖和上下文漂移问题，需要更强大的状态管理和执行能力。

Method: 提出CaveAgent框架，采用双流上下文架构：轻量级语义流用于推理，持久化确定性Python运行时流用于执行。引入状态化运行时管理，支持复杂Python对象的注入、操作和跨轮次持久化。

Result: 在Tau²-bench、BFCL等基准测试中表现优异：零售任务成功率提升10.5%，多轮场景总token消耗减少28.4%，数据密集型任务token消耗减少59%，能够处理导致其他智能体上下文溢出的海量数据。

Conclusion: CaveAgent通过将LLM转变为运行时操作员，解决了传统智能体系统的上下文漂移和灾难性遗忘问题，为复杂、长时程任务提供了高效、可靠的执行框架。

Abstract: LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms. Traditional approaches rely on procedural JSON-based function calling, which often struggles with long-horizon tasks due to fragile multi-turn dependencies and context drift. In this paper, we present CaveAgent, a framework that transforms the paradigm from "LLM-as-Text-Generator" to "LLM-as-Runtime-Operator." We introduce a Dual-stream Context Architecture that decouples state management into a lightweight semantic stream for reasoning and a persistent, deterministic Python Runtime stream for execution. In addition to leveraging code generation to efficiently resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, we introduce \textit{Stateful Runtime Management} in CaveAgent. Distinct from existing code-based approaches that remain text-bound and lack the support for external object injection and retrieval, CaveAgent injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns. This persistence mechanism acts as a high-fidelity external memory to eliminate context drift, avoid catastrophic forgetting, while ensuring that processed data flows losslessly to downstream applications. Comprehensive evaluations on Tau$^2$-bench, BFCL and various case studies across representative SOTA LLMs demonstrate CaveAgent's superiority. Specifically, our framework achieves a 10.5\% success rate improvement on retail tasks and reduces total token consumption by 28.4\% in multi-turn scenarios. On data-intensive tasks, direct variable storage and retrieval reduces token consumption by 59\%, allowing CaveAgent to handle large-scale data that causes context overflow failures in both JSON-based and Code-based agents.

</details>


### [108] [Structured Decomposition for LLM Reasoning: Cross-Domain Validation and Semantic Web Integration](https://arxiv.org/abs/2601.01609)
*Albert Sadowski,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: 论文提出了一种结合LLM灵活性和符号推理保证的集成框架，通过LLM将非结构化文本转换为ABox断言，再使用SWRL推理器进行确定性规则应用。


<details>
  <summary>Details</summary>
Motivation: 在需要可审计和可解释决策的领域（如临床协议、法律证据规则、科学标准），需要既有解释灵活性又有形式化保证的规则推理。LLM提供灵活性但无法保证一致性，符号系统提供保证但需要结构化输入。

Method: 提出集成模式：LLM作为本体填充引擎，将非结构化文本转换为基于专家编写TBox规范的ABox断言，SWRL推理器应用规则提供确定性保证。框架将推理分解为实体识别、断言提取和符号验证。

Result: 在三个领域（法律传闻确定、科学方法任务应用、临床试验资格）和11个语言模型上的实验验证了方法有效性。结构化分解在总体上比few-shot提示有统计显著改进，所有三个领域都有增益。消融研究确认符号验证提供了超越结构化提示的实质性好处。

Conclusion: 该框架结合了LLM的灵活性和符号推理的保证，填充的ABox可与标准语义Web工具集成进行检查和查询，为更丰富的推理模式奠定了基础。

Abstract: Rule-based reasoning over natural language input arises in domains where decisions must be auditable and justifiable: clinical protocols specify eligibility criteria in prose, evidence rules define admissibility through textual conditions, and scientific standards dictate methodological requirements. Applying rules to such inputs demands both interpretive flexibility and formal guarantees. Large language models (LLMs) provide flexibility but cannot ensure consistent rule application; symbolic systems provide guarantees but require structured input. This paper presents an integration pattern that combines these strengths: LLMs serve as ontology population engines, translating unstructured text into ABox assertions according to expert-authored TBox specifications, while SWRL-based reasoners apply rules with deterministic guarantees. The framework decomposes reasoning into entity identification, assertion extraction, and symbolic verification, with task definitions grounded in OWL 2 ontologies. Experiments across three domains (legal hearsay determination, scientific method-task application, clinical trial eligibility) and eleven language models validate the approach. Structured decomposition achieves statistically significant improvements over few-shot prompting in aggregate, with gains observed across all three domains. An ablation study confirms that symbolic verification provides substantial benefit beyond structured prompting alone. The populated ABox integrates with standard semantic web tooling for inspection and querying, positioning the framework for richer inference patterns that simpler formalisms cannot express.

</details>


### [109] [Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications](https://arxiv.org/abs/2601.01718)
*YuanLab. ai,:,Shawn Wu,Sean Wang,Louie Li,Darcy Chen,Allen Wang,Jiangang Luo,Xudong Zhao,Joseph Shen,Gawain Ma,Jasper Jia,Marcus Mao,Claire Wang,Hunter He,Carol Wang,Zera Zhang,Jason Wang,Chonly Shen,Leo Zhang,Logan Chen,Qasim Meng,James Gong,Danied Zhao,Penn Zheng,Owen Zhu,Tong Yu*

Main category: cs.AI

TL;DR: Yuan3.0 Flash是一个开源的MoE多模态大语言模型，具有3.7B激活参数和40B总参数，专为企业任务优化，同时保持通用任务竞争力，并引入RAPO算法解决大推理模型的过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型中的"过度思考"现象，同时开发一个既能处理企业导向任务（如RAG、复杂表格理解、摘要）又能在通用任务上保持竞争力的高效模型。

Method: 采用混合专家（MoE）架构，提出Reflection-aware Adaptive Policy Optimization (RAPO)强化学习训练算法来调节过度思考行为，模型具有3.7B激活参数和40B总参数。

Result: 在企业任务（RAG、表格理解、摘要）上表现优异，在数学、科学等推理领域达到前沿模型相当的准确率，同时平均token消耗仅为1/4到1/2。

Conclusion: Yuan3.0 Flash是一个高效的多模态大语言模型，通过RAPO算法有效解决了过度思考问题，在企业任务和通用推理任务上都表现出色，已完全开源供研究和实际部署。

Abstract: We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.

</details>


### [110] [AI Agent Systems: Architectures, Applications, and Evaluation](https://arxiv.org/abs/2601.01743)
*Bin Xu*

Main category: cs.AI

TL;DR: 本文对AI智能体架构进行了系统性综述，涵盖推理、规划、工具调用等核心组件，提出了统一的分类体系，并讨论了设计权衡、评估挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: AI智能体结合基础模型与推理、规划、记忆和工具使用能力，正成为连接自然语言意图与现实世界计算的关键接口。需要系统性地梳理这一快速发展的领域，为研究人员和从业者提供清晰的架构框架和设计指导。

Method: 采用文献综述方法，将现有工作组织成统一的分类体系：1）智能体组件（策略/LLM核心、记忆、世界模型、规划器、工具路由器、批评器）；2）编排模式（单智能体vs.多智能体，集中式vs.去中心化协调）；3）部署设置（离线分析vs.在线交互，安全关键vs.开放任务）。

Result: 建立了全面的AI智能体架构分类框架，识别了关键设计权衡（延迟vs.准确性、自主性vs.可控性、能力vs.可靠性），总结了评估实践的复杂性（非确定性、长时程信用分配、工具和环境变异性、隐藏成本），并提出了测量和基准测试方法。

Conclusion: AI智能体架构研究面临诸多开放挑战：工具动作的验证和防护、可扩展的记忆和上下文管理、智能体决策的可解释性、以及真实工作负载下的可重复评估。需要在这些方向上进一步研究以推动领域发展。

Abstract: AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\ multi-agent; centralized vs.\ decentralized coordination), and deployment settings (offline analysis vs.\ online interactive assistance; safety-critical vs.\ open-ended tasks). We discuss key design trade-offs -- latency vs.\ accuracy, autonomy vs.\ controllability, and capability vs.\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.

</details>


### [111] [A New Benchmark for the Appropriate Evaluation of RTL Code Optimization](https://arxiv.org/abs/2601.01765)
*Yao Lu,Shang Liu,Hangan Zhou,Wenji Fang,Qijun Zhang,Zhiyao Xie*

Main category: cs.AI

TL;DR: RTL-OPT是一个用于评估大语言模型在RTL代码优化能力的基准测试，包含36个手工设计的数字电路，涵盖多种实现类别，并提供自动化评估框架来验证功能正确性和量化PPA改进。


<details>
  <summary>Details</summary>
Motivation: 当前AI在集成电路设计中的快速发展需要高效的RTL代码生成，但现有基准主要评估语法正确性而非优化质量（功耗、性能、面积）。缺乏能够评估LLMs在RTL优化能力的标准化基准。

Method: 创建RTL-OPT基准，包含36个手工设计的数字电路，涵盖组合逻辑、流水线数据路径、有限状态机和存储器接口等类别。每个任务提供次优版本和人工优化的参考版本，反映工业验证的优化模式。集成自动化评估框架验证功能正确性并量化PPA改进。

Result: RTL-OPT提供了标准化的评估框架，能够验证生成模型在硬件设计优化中的能力，特别是量化功耗、性能和面积方面的改进，填补了现有基准在优化质量评估方面的空白。

Conclusion: RTL-OPT基准为评估LLMs在RTL优化能力提供了标准化工具，能够更全面地评估生成模型在硬件设计优化中的实际效果，推动AI在集成电路设计领域的应用发展。

Abstract: The rapid progress of artificial intelligence increasingly relies on efficient integrated circuit (IC) design. Recent studies have explored the use of large language models (LLMs) for generating Register Transfer Level (RTL) code, but existing benchmarks mainly evaluate syntactic correctness rather than optimization quality in terms of power, performance, and area (PPA). This work introduces RTL-OPT, a benchmark for assessing the capability of LLMs in RTL optimization. RTL-OPT contains 36 handcrafted digital designs that cover diverse implementation categories including combinational logic, pipelined datapaths, finite state machines, and memory interfaces. Each task provides a pair of RTL codes, a suboptimal version and a human-optimized reference that reflects industry-proven optimization patterns not captured by conventional synthesis tools. Furthermore, RTL-OPT integrates an automated evaluation framework to verify functional correctness and quantify PPA improvements, enabling standardized and meaningful assessment of generative models for hardware design optimization.

</details>


### [112] [Can Large Language Models Solve Engineering Equations? A Systematic Comparison of Direct Prediction and Solver-Assisted Approaches](https://arxiv.org/abs/2601.01774)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.AI

TL;DR: LLMs在求解超越方程时，直接数值预测误差较大，而结合传统迭代求解器的混合架构能显著降低误差67.9%-81.8%，表明LLMs更适合作为传统数值求解器的智能接口而非独立计算引擎。


<details>
  <summary>Details</summary>
Motivation: 超越方程在工程实践中普遍存在，需要迭代数值求解。研究旨在评估LLMs能否直接求解这些方程，还是需要结合传统迭代求解器的混合架构更有效。

Method: 测试6个最先进的LLM模型（GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5）在7个工程领域的100个问题上，比较直接预测与求解器辅助计算（LLMs制定控制方程并提供初始条件，牛顿-拉弗森迭代执行数值求解）。

Result: 直接预测的平均相对误差为0.765-1.262，而求解器辅助计算为0.225-0.301，误差降低了67.9%-81.8%。电子学领域改进最大（93.1%），流体力学改进最小（7.2%）。

Conclusion: 当代LLMs擅长符号操作和领域知识检索，但在精度关键的迭代算术方面表现不佳，建议将其作为传统数值求解器的智能接口而非独立计算引擎进行部署。

Abstract: Transcendental equations requiring iterative numerical solution pervade engineering practice, from fluid mechanics friction factor calculations to orbital position determination. We systematically evaluate whether Large Language Models can solve these equations through direct numerical prediction or whether a hybrid architecture combining LLM symbolic manipulation with classical iterative solvers proves more effective. Testing six state-of-the-art models (GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5) on 100 problems spanning seven engineering domains, we compare direct prediction against solver-assisted computation where LLMs formulate governing equations and provide initial conditions while Newton-Raphson iteration performs numerical solution. Direct prediction yields mean relative errors of 0.765 to 1.262 across models, while solver-assisted computation achieves 0.225 to 0.301, representing error reductions of 67.9% to 81.8%. Domain-specific analysis reveals dramatic improvements in Electronics (93.1%) due to exponential equation sensitivity, contrasted with modest gains in Fluid Mechanics (7.2%) where LLMs exhibit effective pattern recognition. These findings establish that contemporary LLMs excel at symbolic manipulation and domain knowledge retrieval but struggle with precision-critical iterative arithmetic, suggesting their optimal deployment as intelligent interfaces to classical numerical solvers rather than standalone computational engines.

</details>


### [113] [PsychEval: A Multi-Session and Multi-Therapy Benchmark for High-Realism and Comprehensive AI Psychological Counselor](https://arxiv.org/abs/2601.01802)
*Qianjun Pan,Junyi Wang,Jie Zhou,Yutao Yang,Junsong Li,Kaiyin Xu,Yougen Zhou,Yihan Li,Jingyuan Zhao,Qin Chen,Ningning Zhou,Kai Chen,Liang He*

Main category: cs.AI

TL;DR: PsychEval：一个用于心理评估AI的多会话、多疗法、高真实度基准，包含数据集、评估框架和强化学习环境，旨在训练和评估临床负责且自适应的AI咨询师。


<details>
  <summary>Details</summary>
Motivation: 开发可靠的心理评估AI面临三个关键挑战：1）如何训练高度真实的AI咨询师（需要持续记忆和动态目标跟踪）；2）如何训练多疗法AI咨询师（复杂案例需要灵活切换不同疗法）；3）如何系统评估AI咨询师。现有模型通常只关注单一疗法，缺乏真实的多会话场景和全面的评估框架。

Method: 构建多会话基准（6-10个会话，分三个阶段），包含677个元技能和4577个原子技能的专业标注；创建涵盖五种治疗模式（心理动力学、行为主义、CBT、人本存在主义、后现代主义）和整合疗法的多样化数据集；建立包含18个治疗特定和共享指标的全面评估框架，并构建2000多个多样化客户档案。

Result: 实验分析充分验证了数据集的高质量和临床保真度。PsychEval超越了静态基准测试，可作为高保真强化学习环境，支持临床负责且自适应的AI咨询师的自我进化训练。

Conclusion: PsychEval为解决心理评估AI的三个关键挑战提供了全面解决方案，通过多会话、多疗法的高真实度基准和评估框架，为训练和评估临床负责的AI咨询师奠定了基础，并可作为强化学习环境促进AI咨询师的自我进化。

Abstract: To develop a reliable AI for psychological assessment, we introduce \texttt{PsychEval}, a multi-session, multi-therapy, and highly realistic benchmark designed to address three key challenges: \textbf{1) Can we train a highly realistic AI counselor?} Realistic counseling is a longitudinal task requiring sustained memory and dynamic goal tracking. We propose a multi-session benchmark (spanning 6-10 sessions across three distinct stages) that demands critical capabilities such as memory continuity, adaptive reasoning, and longitudinal planning. The dataset is annotated with extensive professional skills, comprising over 677 meta-skills and 4577 atomic skills. \textbf{2) How to train a multi-therapy AI counselor?} While existing models often focus on a single therapy, complex cases frequently require flexible strategies among various therapies. We construct a diverse dataset covering five therapeutic modalities (Psychodynamic, Behaviorism, CBT, Humanistic Existentialist, and Postmodernist) alongside an integrative therapy with a unified three-stage clinical framework across six core psychological topics. \textbf{3) How to systematically evaluate an AI counselor?} We establish a holistic evaluation framework with 18 therapy-specific and therapy-shared metrics across Client-Level and Counselor-Level dimensions. To support this, we also construct over 2,000 diverse client profiles. Extensive experimental analysis fully validates the superior quality and clinical fidelity of our dataset. Crucially, \texttt{PsychEval} transcends static benchmarking to serve as a high-fidelity reinforcement learning environment that enables the self-evolutionary training of clinically responsible and adaptive AI counselors.

</details>


### [114] [Admissibility Alignment](https://arxiv.org/abs/2601.01816)
*Chris Duffey*

Main category: cs.AI

TL;DR: 提出"可容许对齐"框架，将AI对齐重新定义为在不确定性下对结果分布的可容许行动和决策选择属性，并通过MAP-AI系统架构实现概率化、决策论的对齐评估。


<details>
  <summary>Details</summary>
Motivation: 传统AI对齐方法通常将对齐视为静态或二元条件，缺乏在不确定性下对决策政策行为的分布性评估。需要一种能够处理不确定性、干预效应、价值模糊性和治理约束的实用对齐框架。

Method: 提出MAP-AI（蒙特卡洛对齐政策）系统架构，通过蒙特卡洛估计结果分布和可容许控制政策选择来实施对齐。该框架在可信未来集合中评估决策政策，明确建模不确定性、干预效应、价值模糊性和治理约束。

Result: 建立了基于分布属性（包括期望效用、方差、尾部风险和不对齐概率）的对齐评估方法，而非基于准确性或排名性能。提供了可执行的方法论来评估企业级AI系统的信任和对齐。

Conclusion: 该框架为治理AI系统提供了实用基础，其影响由政策行为在分布和尾部事件中的表现决定而非单个预测。展示了如何将对齐评估整合到决策过程中，实现不重新训练或修改底层模型的可容许控制行动选择。

Abstract: This paper introduces Admissibility Alignment: a reframing of AI alignment as a property of admissible action and decision selection over distributions of outcomes under uncertainty, evaluated through the behavior of candidate policies. We present MAP-AI (Monte Carlo Alignment for Policy) as a canonical system architecture for operationalizing admissibility alignment, formalizing alignment as a probabilistic, decision-theoretic property rather than a static or binary condition.
  MAP-AI, a new control-plane system architecture for aligned decision-making under uncertainty, enforces alignment through Monte Carlo estimation of outcome distributions and admissibility-controlled policy selection rather than static model-level constraints. The framework evaluates decision policies across ensembles of plausible futures, explicitly modeling uncertainty, intervention effects, value ambiguity, and governance constraints. Alignment is assessed through distributional properties including expected utility, variance, tail risk, and probability of misalignment rather than accuracy or ranking performance. This approach distinguishes probabilistic prediction from decision reasoning under uncertainty and provides an executable methodology for evaluating trust and alignment in enterprise and institutional AI systems. The result is a practical foundation for governing AI systems whose impact is determined not by individual forecasts, but by policy behavior across distributions and tail events. Finally, we show how distributional alignment evaluation can be integrated into decision-making itself, yielding an admissibility-controlled action selection mechanism that alters policy behavior under uncertainty without retraining or modifying underlying models.

</details>


### [115] [Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation](https://arxiv.org/abs/2601.01844)
*Udiptaman Das,Krishnasai B. Atmakuri,Duy Ho,Chi Lee,Yugyung Lee*

Main category: cs.AI

TL;DR: 提出一个端到端框架，利用多智能体提示和模式约束的检索增强生成策略，直接从自由文本构建临床知识图谱，特别针对肿瘤学领域，无需依赖黄金标准标注。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖结构化输入，缺乏对事实准确性和语义一致性的鲁棒验证，这在肿瘤学领域尤其成问题。需要直接从非结构化临床叙述构建高质量知识图谱的方法。

Method: 采用多智能体提示和模式约束的KG-RAG策略，包括：(1) 提示驱动的实体、属性和关系抽取；(2) 基于熵的不确定性评分；(3) 本体对齐的RDF/OWL模式生成；(4) 多LLM共识验证用于幻觉检测和语义精炼。

Result: 应用于PDAC和BRCA两个肿瘤队列，该方法产生了可解释、SPARQL兼容且临床基础的知识图谱。实验结果显示在精确度、相关性和本体一致性方面相比基线方法有持续提升。

Conclusion: 该框架支持连续精炼和自我监督评估，能够迭代改进图谱质量，为直接从自由文本构建临床知识图谱提供了有效解决方案，特别适用于肿瘤学领域。

Abstract: Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.

</details>


### [116] [Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios](https://arxiv.org/abs/2601.01857)
*Defei Xia,Bingfeng Pi,Shenbin Zhang,Song Hua,Yunfei Wei,Lei Zuo*

Main category: cs.AI

TL;DR: 本文提出Jenius-Agent框架，通过自适应提示生成、上下文感知工具编排和分层内存机制三大创新，提升LLM智能体的任务准确性20%，同时降低token成本、响应延迟和调用失败率。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能体系统发展，提升自主智能体在上下文理解、工具使用和响应生成方面的任务性能变得日益重要。尽管先前研究改进了LLM智能体的整体设计，但对其内部推理和工具使用流程的系统优化仍显不足。

Method: 提出Jenius-Agent框架，包含三大关键创新：1) 自适应提示生成策略，根据智能体状态和任务目标调整提示以提高可靠性和鲁棒性；2) 上下文感知工具编排模块，基于用户意图和上下文进行工具分类、语义检索和自适应调用；3) 分层内存机制，整合会话内存、任务历史和外部摘要，通过动态摘要和压缩提高相关性和效率。框架还集成了基于MCP的工具、文件I/O和执行反馈等优化。

Result: 实验显示任务准确性提升20%，同时降低了token成本、响应延迟和调用失败率。该框架已在Jenius平台部署，为稳健、协议兼容的自主智能体提供了轻量级可扩展解决方案。

Conclusion: Jenius-Agent框架通过系统优化智能体的内部推理和工具使用流程，显著提升了任务性能，为实际应用中的自主智能体提供了有效的解决方案。

Abstract: As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.

</details>


### [117] [Toward Auditable Neuro-Symbolic Reasoning in Pathology: SQL as an Explicit Trace of Evidence](https://arxiv.org/abs/2601.01875)
*Kewen Cao,Jianxu Chen,Yongbing Zhang,Ye Zhang,Hongxiao Wang*

Main category: cs.AI

TL;DR: 提出SQL中心代理框架，通过可执行SQL查询将病理图像特征测量与诊断推理关联，增强模型决策的可解释性和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 当前病理图像分析缺乏可验证的证据支持模型决策，视觉语言模型生成的解释多为相关性描述而非可验证证据，需要建立可审计的特征测量和推理框架。

Method: 1) 提取人类可解释的细胞特征；2) 特征推理代理通过SQL查询在特征表上聚合视觉证据为量化发现；3) 知识比较代理将发现与病理知识对比，模拟病理学家从可测量观察到诊断的推理过程。

Result: 在两个病理视觉问答数据集上的实验表明，该方法提高了可解释性和决策可追溯性，同时生成可执行的SQL追踪，将细胞测量与诊断结论联系起来。

Conclusion: SQL中心代理框架通过可审计的特征测量和推理，为病理图像分析提供了可验证的解释，增强了模型决策的透明度和可信度。

Abstract: Automated pathology image analysis is central to clinical diagnosis, but clinicians still ask which slide features drive a model's decision and why. Vision-language models can produce natural language explanations, but these are often correlational and lack verifiable evidence. In this paper, we introduce an SQL-centered agentic framework that enables both feature measurement and reasoning to be auditable. Specifically, after extracting human-interpretable cellular features, Feature Reasoning Agents compose and execute SQL queries over feature tables to aggregate visual evidence into quantitative findings. A Knowledge Comparison Agent then evaluates these findings against established pathological knowledge, mirroring how pathologists justify diagnoses from measurable observations. Extensive experiments evaluated on two pathology visual question answering datasets demonstrate our method improves interpretability and decision traceability while producing executable SQL traces that link cellular measurements to diagnostic conclusions.

</details>


### [118] [MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning](https://arxiv.org/abs/2601.01910)
*Minh Hieu Ha,Khanh Ly Ta,Hung Phan,Tung Doan,Tung Dao,Dao Tran,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: MMP-A*：结合视觉语言模型空间感知与自适应衰减机制的多模态路径规划框架，在复杂环境中实现接近最优轨迹并显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 传统A*算法在大规模场景中计算和内存成本过高，而基于大语言模型的路径规划方法缺乏空间感知能力，在拓扑复杂环境中容易产生错误路径点，导致昂贵的修正扩展

Method: 提出MMP-A*多模态框架，集成视觉语言模型的空间感知能力与新颖的自适应衰减机制，将高层推理锚定在物理几何中，动态调节不确定路径点在启发式函数中的影响

Result: 在具有严重杂乱和拓扑复杂性的挑战性环境中测试，MMP-A*实现了接近最优的轨迹，同时显著降低了操作成本

Conclusion: MMP-A*展示了作为感知基础和计算高效的自主导航范式的潜力，通过空间感知和自适应机制解决了纯文本规划器的局限性

Abstract: Autonomous path planning requires a synergy between global reasoning and geometric precision, especially in complex or cluttered environments. While classical A* is valued for its optimality, it incurs prohibitive computational and memory costs in large-scale scenarios. Recent attempts to mitigate these limitations by using Large Language Models for waypoint guidance remain insufficient, as they rely only on text-based reasoning without spatial grounding. As a result, such models often produce incorrect waypoints in topologically complex environments with dead ends, and lack the perceptual capacity to interpret ambiguous physical boundaries. These inconsistencies lead to costly corrective expansions and undermine the intended computational efficiency.
  We introduce MMP-A*, a multimodal framework that integrates the spatial grounding capabilities of vision-language models with a novel adaptive decay mechanism. By anchoring high-level reasoning in physical geometry, the framework produces coherent waypoint guidance that addresses the limitations of text-only planners. The adaptive decay mechanism dynamically regulates the influence of uncertain waypoints within the heuristic, ensuring geometric validity while substantially reducing memory overhead. To evaluate robustness, we test the framework in challenging environments characterized by severe clutter and topological complexity. Experimental results show that MMP-A* achieves near-optimal trajectories with significantly reduced operational costs, demonstrating its potential as a perception-grounded and computationally efficient paradigm for autonomous navigation.

</details>


### [119] [OpenSocInt: A Multi-modal Training Environment for Human-Aware Social Navigation](https://arxiv.org/abs/2601.01939)
*Victor Sanchez,Chris Reinke,Ahamed Mohamed,Xavier Alameda-Pineda*

Main category: cs.AI

TL;DR: OpenSocInt是一个开源的多模态社交交互仿真器，提供模块化架构训练社交智能体，已应用于社交导航任务。


<details>
  <summary>Details</summary>
Motivation: 开发一个开源的多模态社交交互仿真平台，支持研究不同感知特征、编码融合方法以及多种智能体在社交交互中的应用。

Method: 构建了一个模块化的开源软件包，包含多模态社交交互仿真器和可扩展的智能体训练架构，通过社交导航任务验证其功能。

Result: 成功开发了OpenSocInt软件包，已在GitLab上以GPL协议开源，展示了其在社交导航任务中的实用价值。

Conclusion: OpenSocInt为多模态社交交互研究提供了一个灵活的开源平台，支持探索不同感知特征和智能体架构，促进社交智能研究的发展。

Abstract: In this paper, we introduce OpenSocInt, an open-source software package providing a simulator for multi-modal social interactions and a modular architecture to train social agents. We described the software package and showcased its interest via an experimental protocol based on the task of social navigation. Our framework allows for exploring the use of different perceptual features, their encoding and fusion, as well as the use of different agents. The software is already publicly available under GPL at https://gitlab.inria.fr/robotlearn/OpenSocInt/.

</details>


### [120] [CNC-TP: Classifier Nominal Concept Based on Top-Pertinent Attributes](https://arxiv.org/abs/2601.01976)
*Yasmine Souissi,Fabrice Boissier,Nida Meddouri*

Main category: cs.AI

TL;DR: 本文对基于形式概念分析(FCA)的分类器进行了综述，提出了一种从名义数据计算闭包算子的新方法，并构建了关注最相关概念的部分概念格。


<details>
  <summary>Details</summary>
Motivation: 知识发现(KDD)旨在从海量数据中提取隐藏知识，其中分类是核心数据挖掘技术之一。形式概念分析(FCA)因其可解释性和可解释性学习而被认为是有效的分类方法，但需要更高效的方法来处理名义数据并构建概念格。

Method: 1. 对FCA基分类器进行最新综述；2. 探索从名义数据计算闭包算子的多种方法；3. 提出构建部分概念格的新方法，重点关注最相关的概念。

Result: 通过实验验证了所提方法的效率，展示了在构建概念格和分类任务中的性能表现。

Conclusion: 本文提出的方法能够有效处理名义数据并构建部分概念格，为基于FCA的分类器提供了更高效的实现方案，有助于提升知识发现过程中的分类性能。

Abstract: Knowledge Discovery in Databases (KDD) aims to exploit the vast amounts of data generated daily across various domains of computer applications. Its objective is to extract hidden and meaningful knowledge from datasets through a structured process comprising several key steps: data selection, preprocessing, transformation, data mining, and visualization. Among the core data mining techniques are classification and clustering. Classification involves predicting the class of new instances using a classifier trained on labeled data. Several approaches have been proposed in the literature, including Decision Tree Induction, Bayesian classifiers, Nearest Neighbor search, Neural Networks, Support Vector Machines, and Formal Concept Analysis (FCA). The last one is recognized as an effective approach for interpretable and explainable learning. It is grounded in the mathematical structure of the concept lattice, which enables the generation of formal concepts and the discovery of hidden relationships among them. In this paper, we present a state-of-theart review of FCA-based classifiers. We explore various methods for computing closure operators from nominal data and introduce a novel approach for constructing a partial concept lattice that focuses on the most relevant concepts. Experimental results are provided to demonstrate the efficiency of the proposed method.

</details>


### [121] [ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems](https://arxiv.org/abs/2601.01982)
*Noel Thomas*

Main category: cs.AI

TL;DR: ChaosBench-Logic是一个评估大语言模型在混沌动力系统中逻辑推理能力的基准测试，包含30个系统、621个问题，发现前沿LLMs在单项准确率可达91-94%，但在组合推理上得分为0%，对话准确率53.1-75.5%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言任务上表现出色，但在需要精确逻辑和符号推理的领域仍然脆弱。混沌动力系统提供了一个特别严格的测试环境，因为混沌是确定性的，但常被误解为随机性或复杂性。

Method: 引入ChaosBench-Logic基准测试，使用统一的一阶逻辑本体评估LLM在30个不同动力系统中的推理能力。每个系统用11个语义谓词的真值分配进行标注，生成621个问题，涵盖七个推理类别，包括多跳推理、跨系统类比、反事实推理、偏见探测和多轮对话。

Result: 前沿LLMs（GPT-4、Claude 3.5 Sonnet、Gemini 2.5 Flash、LLaMA-3 70B）在单项准确率达到91-94%，但在组合项目上得分为0%，表现出脆弱的全局一致性。对话级准确率从53.1%（GPT-4 CoT）到75.5%（LLaMA-3零样本）。

Conclusion: ChaosBench-Logic为诊断LLM在逻辑推理上的失败提供了一个严格的测试平台，并为开发改进LLM科学推理能力的神经符号方法奠定了基础。

Abstract: Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.

</details>


### [122] [MindChat: A Privacy-preserving Large Language Model for Mental Health Support](https://arxiv.org/abs/2601.01993)
*Dong Xue,Jicheng Tu,Ming Wang,Xin Yan,Fangzhou Liu,Jie Hu*

Main category: cs.AI

TL;DR: MindChat是一个保护隐私的心理健康支持大语言模型，配合MindCorpus合成咨询数据集，通过多智能体角色扮演和联邦学习实现高质量数据生成和隐私保护


<details>
  <summary>Details</summary>
Motivation: 现有心理健康支持大语言模型面临真实咨询对话稀缺且敏感的问题，同时存在隐私泄露风险，需要开发既能保护隐私又能提供高质量咨询服务的解决方案

Method: 1) 通过多智能体角色扮演框架构建MindCorpus合成咨询数据集，采用双闭环反馈设计：回合级批判修订和会话级策略优化；2) 使用联邦学习配合LoRA适配器和差分隐私优化来训练MindChat模型，保护数据隐私

Result: MindCorpus提高了训练效果，MindChat在自动LLM评估和人工评估中与现有通用和咨询导向的LLM基线相当，同时在成员推理攻击下表现出减少的隐私泄露

Conclusion: 该方法成功解决了心理健康支持LLM的数据稀缺和隐私问题，通过合成数据生成和隐私保护技术的结合，为开发实用的心理健康支持系统提供了可行方案

Abstract: Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.

</details>


### [123] [XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging](https://arxiv.org/abs/2601.02008)
*Midhat Urooj,Ayan Banerjee,Sandeep Gupta*

Main category: cs.AI

TL;DR: XAIMeD是一个可解释的医学AI框架，通过神经符号架构整合临床专家知识，提升分布偏移下的鲁棒性、罕见类别敏感性，并提供临床对齐的解释。


<details>
  <summary>Details</summary>
Motivation: 医学AI面临可解释性、领域泛化和罕见类别可靠性的关键挑战，深度模型在真实世界分布偏移下经常失败，并对不常见的临床条件表现出偏见。

Method: 将临床专业知识编码为原子医学命题的逻辑连接，转化为机器可检查的类别特定规则；通过加权特征满足分数量化诊断效用；符号推理分支补充神经预测；置信度加权融合集成符号和深度输出；基于熵不平衡增益和罕见类别基尼系数的自适应路由机制。

Result: 在四个挑战性任务上评估，包括癫痫发作区定位和糖尿病视网膜病变分级，显示显著性能提升：跨领域泛化提升6%，罕见类别F1分数提升10%，远超最先进的深度学习基线。

Conclusion: XAIMeD提供了一个原则性、临床忠实且可解释的多模态医学AI方法，临床基础的符号组件作为有效的正则化器，确保对分布偏移的鲁棒性。

Abstract: Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.

</details>


### [124] [Simulated Reasoning is Reasoning](https://arxiv.org/abs/2601.02043)
*Hendrik Kempt,Alon Lavie*

Main category: cs.AI

TL;DR: 论文认为基础模型通过模仿"出声思考"过程、测试和迭代推理路径，实现了不同于人类符号推理的新型推理能力，这改变了我们对推理必要条件的理解，需要重新评估"随机鹦鹉"隐喻的适用性。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为推理是通过符号推理实现理解的路径，但基础模型展示了不同的推理方式：通过模仿"出声思考"过程、测试和迭代推理路径来解决问题。这种新型推理能力挑战了传统对推理必要条件的理解，需要重新审视相关哲学概念和安全考量。

Method: 论文采用哲学分析方法，讨论基础模型推理现象的不同哲学解释，论证"随机鹦鹉"隐喻已失去相关性，并反思从这些推理模型及其增长能力中产生的安全和适当性考虑中的规范性要素。

Result: 基础模型通过模仿思考过程而非符号推理实现了有效的推理能力，但这种推理缺乏人类推理的根基和常识，导致推理过程的脆弱性。这显著改变了我们对推理及其必要条件的评估，并影响了安全性和鲁棒性防御方法。

Conclusion: 基础模型的推理能力代表了与人类符号推理不同的新型推理形式，需要放弃过时的"随机鹦鹉"隐喻，重新思考相关的哲学概念，并建立适应这种新型推理模式的安全框架和规范性考量。

Abstract: Reasoning has long been understood as a pathway between stages of understanding. Proper reasoning leads to understanding of a given subject. This reasoning was conceptualized as a process of understanding in a particular way, i.e., "symbolic reasoning". Foundational Models (FM) demonstrate that this is not a necessary condition for many reasoning tasks: they can "reason" by way of imitating the process of "thinking out loud", testing the produced pathways, and iterating on these pathways on their own. This leads to some form of reasoning that can solve problems on its own or with few-shot learning, but appears fundamentally different from human reasoning due to its lack of grounding and common sense, leading to brittleness of the reasoning process. These insights promise to substantially alter our assessment of reasoning and its necessary conditions, but also inform the approaches to safety and robust defences against this brittleness of FMs. This paper offers and discusses several philosophical interpretations of this phenomenon, argues that the previously apt metaphor of the "stochastic parrot" has lost its relevance and thus should be abandoned, and reflects on different normative elements in the safety- and appropriateness-considerations emerging from these reasoning models and their growing capacity.

</details>


### [125] [Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management](https://arxiv.org/abs/2601.02061)
*Faizan Ahmed,Aniket Dixit,James Brusey*

Main category: cs.AI

TL;DR: 该论文研究使用高阶导数惩罚（特别是三阶导数惩罚，即加速度变化率最小化）来平滑深度强化学习中的控制行为，在连续控制基准和建筑能源管理系统中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习代理通常表现出不稳定、高频的控制行为，这在现实世界部署中会导致过度能耗和机械磨损。需要解决RL优化与实际操作约束之间的差距。

Method: 系统研究动作平滑正则化方法，采用高阶导数惩罚策略，从连续控制基准的理论理解扩展到建筑能源管理的实际验证，重点关注三阶导数惩罚（加速度变化率最小化）。

Result: 在四个连续控制环境中，三阶导数惩罚（加速度变化率最小化）能持续实现更平滑的控制，同时保持竞争力性能。在HVAC控制系统中，平滑策略使设备切换减少60%，带来显著的操作效益。

Conclusion: 高阶动作正则化是连接RL优化与能源关键应用中操作约束的有效桥梁，为实际部署提供了重要方法。

Abstract: Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.

</details>


### [126] [FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations](https://arxiv.org/abs/2601.02071)
*Adeshola Okubena,Yusuf Ali Mohammed,Moe Elbadawi*

Main category: cs.AI

TL;DR: 研究探索了将大型语言模型（LLM）应用于药物3D打印配方开发，通过微调四种LLM架构在1400多个FDM配方数据集上，用于推荐辅料和预测丝材机械性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的药物3D打印研究大多局限于狭窄领域，未能全面解决配方开发中的复杂挑战。随着人工通用智能概念的发展，需要探索LLM在药物配方开发中的应用潜力，超越传统的预测建模，实现更通用的推理能力。

Method: 使用包含1400多个熔融沉积成型（FDM）配方的数据集，对四种LLM架构进行微调，系统评估微调和生成参数配置。研究重点关注LLM在推荐适合的辅料（基于API剂量）和预测丝材机械性能方面的应用。

Result: Llama2模型在推荐FDM配方辅料方面表现最佳；模型选择和参数化显著影响性能，较小的LLM出现灾难性遗忘现象；即使相对较小的数据集（1400多个配方）也可能导致模型灾难性遗忘；标准的LLM评估指标仅衡量语言性能而非配方可加工性；基于生物医学相关数据训练的LLM并不总是产生最佳结果。

Conclusion: 解决这些挑战对于推动LLM超越语言能力，发展成为药物配方开发的可靠系统至关重要。研究强调了在药物3D打印领域应用LLM时需要克服的关键技术障碍，包括灾难性遗忘、评估指标局限性和数据相关性等问题。

Abstract: Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.

</details>


### [127] [EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning](https://arxiv.org/abs/2601.02163)
*Chuanrui Hu,Xingze Gao,Zuyi Zhou,Dannong Xu,Yi Bai,Xintong Li,Hui Zhang,Tong Li,Chong Zhang,Lidong Bing,Yafeng Deng*

Main category: cs.AI

TL;DR: EverMemOS：受记忆印迹启发的自组织记忆操作系统，通过生命周期管理实现长期交互中用户状态的持续跟踪与冲突解决


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为长期交互代理时，有限的上下文窗口难以维持连贯行为，现有记忆系统存储孤立记录且检索碎片化，无法有效整合演化中的用户状态和解决冲突

Method: 提出记忆印迹启发的计算记忆生命周期：1) 情景痕迹形成：对话流转换为MemCells，捕捉情景痕迹、原子事实和时间边界前瞻信号；2) 语义整合：MemCells组织为主题MemScenes，提炼稳定语义结构并更新用户画像；3) 重构回忆：MemScene引导的智能检索，为下游推理组合必要且充分的上下文

Result: 在LoCoMo和LongMemEval基准测试中达到记忆增强推理任务的最先进性能，在PersonaMem v2上的画像研究及定性案例展示了用户画像和前瞻等聊天导向能力

Conclusion: EverMemOS通过自组织记忆操作系统有效解决了长期交互中的记忆管理问题，实现了用户状态的持续跟踪和冲突解决，为LLM作为长期代理提供了系统化记忆管理方案

Abstract: Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.

</details>


### [128] [Streaming Hallucination Detection in Long Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.02170)
*Haolang Lu,Minghui Pan,Ripeng Li,Guoshun Nan,Jialin Zhuang,Zijie Zhao,Zhongxiang Sun,Kun Wang,Yang Liu*

Main category: cs.AI

TL;DR: 提出将长链思维推理中的幻觉视为演化潜在状态而非一次性错误事件，引入累积前缀级幻觉信号来追踪整个推理轨迹的全局演化，实现流式幻觉检测


<details>
  <summary>Details</summary>
Motivation: 长链思维推理虽然能提升大语言模型性能，但其中的幻觉往往以微妙方式出现并在推理步骤间传播。现有方法将幻觉视为一次性错误事件，未能捕捉其演化特性

Method: 将步骤级幻觉判断视为局部观测，引入累积前缀级幻觉信号来追踪整个推理轨迹的全局演化状态，实现流式幻觉检测

Result: 提出的方法能够在长链思维推理中实现实时、可解释的幻觉检测，提供全局演化视角而非孤立判断

Conclusion: 将幻觉视为演化潜在状态而非一次性错误事件，通过累积前缀级信号追踪全局演化，为长链思维推理中的幻觉检测提供了更准确、实时的解决方案

Abstract: Long chain-of-thought (CoT) reasoning improves the performance of large language models, yet hallucinations in such settings often emerge subtly and propagate across reasoning steps. We suggest that hallucination in long CoT reasoning is better understood as an evolving latent state rather than a one-off erroneous event. Accordingly, we treat step-level hallucination judgments as local observations and introduce a cumulative prefix-level hallucination signal that tracks the global evolution of the reasoning state over the entire trajectory. Overall, our approach enables streaming hallucination detection in long CoT reasoning, providing real-time, interpretable evidence.

</details>


### [129] [Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents](https://arxiv.org/abs/2601.02314)
*Sourena Khanzadeh*

Main category: cs.AI

TL;DR: 论文提出Project Ariadne框架，使用结构因果模型和反事实逻辑来审计LLM代理推理的因果完整性，发现当前代理架构存在"忠实性差距"，推理痕迹常为"推理剧场"而非真实决策驱动。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理越来越多地承担高风险自主决策任务，其推理过程的透明度成为关键安全关切。虽然思维链提示允许代理生成人类可读的推理痕迹，但这些痕迹究竟是模型输出的忠实生成驱动还是仅仅是事后合理化，尚不清楚。

Method: 引入Project Ariadne框架，利用结构因果模型和反事实逻辑来审计代理推理的因果完整性。该方法对中间推理节点执行硬干预（do-演算），系统性地反转逻辑、否定前提和颠倒事实主张，以测量终端答案的因果敏感性。

Result: 对最先进模型的实证评估揭示了持续的"忠实性差距"。定义并检测到一种广泛的故障模式称为"因果解耦"，代理在事实和科学领域中违反密度高达0.77。在这些情况下，代理尽管内部逻辑矛盾却得出相同结论，证明其推理痕迹只是"推理剧场"，而决策由潜在参数先验控制。

Conclusion: 当前代理架构本质上容易产生不忠实的解释，提出Ariadne分数作为对齐陈述逻辑与模型行为的新基准。

Abstract: As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \textbf{faithful} generative drivers of the model's output or merely \textbf{post-hoc rationalizations}. We introduce \textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as "Reasoning Theater" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.

</details>


### [130] [Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.02346)
*Falcon LLM Team,Iheb Chaabane,Puneesh Khanna,Suhail Mohmad,Slim Frikha,Shi Hu,Abdalgader Abubaker,Reda Alami,Mikhail Lubinets,Mohamed El Amine Seddik,Hakim Hacid*

Main category: cs.AI

TL;DR: Falcon-H1R是一个7B参数的推理优化模型，证明了小语言模型也能实现有竞争力的推理性能，在多个推理基准测试中匹配或超越2-7倍大的SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 探索小语言模型能否通过精心设计的数据策展和训练策略实现与大型模型相媲美的推理性能，解决推理效率问题。

Method: 采用混合并行架构设计实现更快推理，结合高效监督微调(SFT)和强化学习(RL)扩展策略，利用DeepConf方法实现最先进的测试时扩展效率。

Result: 在多个推理密集型基准测试中，Falcon-H1R-7B模型匹配或超越了2-7倍大的SOTA模型，在推理速度、token效率和准确性方面都表现出色。

Conclusion: 紧凑模型通过针对性的训练策略和架构选择，能够提供强大且可扩展的推理性能，为需要大量思维链生成和并行测试时扩展的场景提供了实用的骨干模型。

Abstract: This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\times$ to $7\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [131] [Application of the learning from errors principle in tufting machines](https://arxiv.org/abs/2601.00813)
*Longxiang Shao,Dominik Huesener,Michael Schluse,Juergen Rossmann*

Main category: eess.SY

TL;DR: 提出结合数字孪生、增强现实和Petri网建模的集成培训方法，让簇绒机操作员在安全环境中通过模拟错误进行学习


<details>
  <summary>Details</summary>
Motivation: 工业环境中"从错误中学习"的教学原则通常因安全和设备风险而难以实施，需要一种既能应用这一教学原则又能确保安全的培训方法

Method: 采用混合数字孪生、增强现实和基于Petri网的建模方法。通过可实验数字孪生模拟操作员行为和错误，用AR可视化错误后果，Petri网模型形式化表示过程、典型故障和恢复路径，使用VEROSIM和SOML++实现

Result: 该混合框架为AR引导的培训系统提供了可扩展的基础，能够降低风险并加速技能获取

Conclusion: 提出的集成培训方法成功地将"从错误中学习"原则应用于工业环境，通过安全模拟错误实现了有效的体验式学习

Abstract: The principle of learning from errors is pedagogically powerful but often impractical in industrial settings due to risks to safety and equipment. This paper presents an integrated training approach specifically designed for tufting machine operators. It uses hybrid digital twins, augmented reality (AR), and Petri Net-based modelling to apply the learning from errors principle effectively. Operator actions and errors are simulated via experimentable digital twins (EDTs), and the consequences of errors are visualized in AR, enabling safe, experiential learning. A Petri Net model formally represents the process, including typical faults and recovery paths, and is implemented in VEROSIM using SOML++. This hybrid framework provides a scalable foundation for AR-guided training systems that reduce risk and accelerate skill acquisition.

</details>


### [132] [Scalable Data-Driven Reachability Analysis and Control via Koopman Operators with Conformal Coverage Guarantees](https://arxiv.org/abs/2601.01076)
*Devesh Nath,Haoran Yin,Glen Chou*

Main category: eess.SY

TL;DR: 提出一个基于可达性的概率性数据驱动安全验证框架，用于未知非线性动力学系统，结合Koopman理论、神经网络和保形预测来保证统计有效的安全边界。


<details>
  <summary>Details</summary>
Motivation: 需要为未知非线性动力学系统提供可扩展的、概率性的安全验证方法，以应对高维机器人控制任务中的安全挑战。

Method: 1) 使用Koopman理论和神经网络提升函数学习动力学的近似线性表示；2) 在提升空间中设计线性控制器进行轨迹跟踪；3) 在提升空间高效计算可达集并通过神经网络验证工具映射回原始状态空间；4) 应用保形预测处理模型不匹配问题，生成统计有效的误差边界。

Result: 在高维MuJoCo任务（11D Hopper、28D Swimmer）和12D四旋翼无人机上验证，相比现有方法在可达集覆盖率、计算效率和保守性方面均有改进。

Conclusion: 该框架为未知非线性系统提供了可扩展的概率安全验证方法，能够处理高维控制任务，且误差边界可跨参考轨迹重用，无需重新计算。

Abstract: We propose a scalable reachability-based framework for probabilistic, data-driven safety verification of unknown nonlinear dynamics. We use Koopman theory with a neural network (NN) lifting function to learn an approximate linear representation of the dynamics and design linear controllers in this space to enable closed-loop tracking of a reference trajectory distribution. Closed-loop reachable sets are efficiently computed in the lifted space and mapped back to the original state space via NN verification tools. To capture model mismatch between the Koopman dynamics and the true system, we apply conformal prediction to produce statistically-valid error bounds that inflate the reachable sets to ensure the true trajectories are contained with a user-specified probability. These bounds generalize across references, enabling reuse without recomputation. Results on high-dimensional MuJoCo tasks (11D Hopper, 28D Swimmer) and 12D quadcopters show improved reachable set coverage rate, computational efficiency, and conservativeness over existing methods.

</details>


### [133] [Time Series Based CO2 Emission Forecasting and Energy Mix Analysis for Net Zero Transitions: A Multi Country Study](https://arxiv.org/abs/2601.01105)
*Salim Oyinlola,Joshua Ajayi,Gozie Ibekwe*

Main category: eess.SY

TL;DR: 该研究分析了五个主要经济体（尼日利亚、美国、中国、巴西、俄罗斯）的长期CO₂排放轨迹，结合能源结构特征和时间序列预测模型，发现不同国家因能源结构差异呈现不同的脱碳路径。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解主要经济体如何基于其能源结构特征实现长期CO₂减排，评估不同预测模型在排放轨迹分析中的准确性，并为各国制定符合全球气候目标的能源政策提供依据。

Method: 方法包括：分析2000-2023年年度排放数据和能源生产数据；将国家分类为化石依赖型、转型阶段或可再生能源加速型；评估三种时间序列预测模型（ARIMA、SARIMA、Holt-Winters指数平滑法）的准确性；使用MAE、RMSE、MAPE和R²指标进行模型评估；进行2024-2060年的长期排放预测。

Result: 结果显示：Holt-Winters模型对尼日利亚、美国、中国和巴西的预测最准确，而SARIMA模型对俄罗斯的预测最佳（因其排放相对稳定）。长期预测表明：巴西因可再生能源主导的能源系统最接近低排放未来；尼日利亚因化石依赖排放持续上升；美国和中国的排放适度下降但需加速减排以实现净零承诺；俄罗斯排放基本保持平稳。

Conclusion: 结论强调能源结构对国家脱碳前景有强烈影响，需要有针对性的能源政策改革以符合全球气候目标。不同国家需要根据其能源特征采取差异化的减排策略。

Abstract: This study examines long-term CO$_2$ emission trajectories across five major economies: Nigeria, the United States, China, Brazil, and Russia, by integrating national energy-mix characteristics with time-series forecasting models. Annual emissions from 2000-2023 were analyzed alongside energy production data to classify countries into fossil-dependent, transition-phase, or renewable-accelerated profiles. Three forecasting models (ARIMA, SARIMA, and Holt-Winters exponential smoothing) were evaluated using MAE, RMSE, MAPE, and R$^2$ metrics. Results show that Holt-Winters provided the most accurate forecasts for Nigeria, the United States, China, and Brazil, while SARIMA performed best for Russia due to its relatively stable emissions. Long-term projections from 2024 to 2060 indicate divergent decarbonization pathways. Brazil aligns most closely with a low-emission future owing to its renewable-dominant energy system, whereas Nigeria continues on an upward emissions trajectory driven by fossil dependence. The United States and China maintain moderate declines but require accelerated mitigation to reach their respective net-zero commitments. Russia's emissions remain largely flat under current conditions. These findings highlight the strong influence of energy structures on national decarbonization prospects and underscore the need for targeted energy policy reforms to align with global climate objectives.

</details>


### [134] [Compensating Star-Trackers Misalignments with Adaptive Multi-Model Estimation](https://arxiv.org/abs/2601.01130)
*Ridma Ganganath,Simone Servadio,David Daeyoung Lee*

Main category: eess.SY

TL;DR: 提出自适应多模型框架，用于GPS缺失的深空立方星任务中联合估计航天器姿态和星敏感器失准角，采用MEKF估计姿态参数，MMAE处理失准角假设网格，引入多样性度量触发自适应网格细化。


<details>
  <summary>Details</summary>
Motivation: 针对资源受限的深空立方星任务，在GPS缺失环境下需要自主、准确、计算高效的在轨校准方法，特别是处理星敏感器安装失准问题，传统方法可能计算复杂或精度不足。

Method: 结合MEKF（估计姿态、角速度、陀螺偏差）和MMAE（处理失准角假设网格）。单失准角时MEKF处理陀螺和TRIAD姿态观测，MMAE更新三维失准向量网格；双失准角时保留MEKF动态，MMAE直接处理两个星敏感器的视线测量，形成六维网格。引入多样性度量Ψ触发自适应网格细化。

Result: 蒙特卡洛仿真显示能达到角秒级失准角估计和亚度级姿态误差，估计误差保持良好有界，证明方法的鲁棒性和一致性。双星敏感器失准角估计成为深空立方星任务的实用选择。

Conclusion: 提出的MEKF-MMAE架构为资源受限航天器提供了准确、自主、计算高效的在轨校准能力，特别适用于深空立方星任务，解决了星敏感器失准角估计的实际问题。

Abstract: This paper presents an adaptive multi-model framework for jointly estimating spacecraft attitude and star-tracker misalignments in GPS-denied deep-space CubeSat missions. A Multiplicative Extended Kalman Filter (MEKF) estimates attitude, angular velocity, and gyro bias, while a Bayesian Multiple-Model Adaptive Estimation (MMAE) layer operates on a discrete grid of body-to-sensor misalignment hypotheses. In the single-misalignment case, the MEKF processes gyroscope measurements and TRIAD-based attitude observations, and the MMAE updates a three-dimensional grid over the misalignment vector. For a dual-misalignment configuration, the same MEKF dynamics are retained, and the MMAE bank is driven directly by stacked line-of-sight measurements from two star trackers, forming a six-dimensional grid over the two misalignment quaternions without augmenting the continuous-state dimension. A novel diversity metric, $Ψ$, is introduced to trigger adaptive refinement of the misalignment grid around a weighted-mean estimate, thereby preventing premature collapse of the model probabilities and concentrating computation in the most likely region of the parameter space. Monte Carlo simulations show arcsecond-level misalignment estimation and sub-degree attitude errors for both estimation problems, with estimation errors remaining well-bounded, proving robustness and consistency. These results indicate that the proposed MEKF--MMAE architecture enables accurate, autonomous, and computationally efficient in-flight calibration for resource-constrained spacecraft, and establishes dual star-tracker misalignment estimation as a practical option for deep-space CubeSat missions.

</details>


### [135] [Tube-based robust nonlinear model predictive control of anaerobic co-digestion](https://arxiv.org/abs/2601.01157)
*Davide Carecci,Laurent Dewasme,Alessio La Bella,Gianni Ferretti,Alain Vande Wouwer*

Main category: eess.SY

TL;DR: 提出基于管道的鲁棒非线性模型预测控制（NMPC）来调节生物甲烷生产，处理饲料变化期间的不确定性，确保安全运行


<details>
  <summary>Details</summary>
Motivation: 为满足生物甲烷生产需求，厌氧消化器需要处理多种饲料共消化，且需要随时间优化调整饲料配方。然而，实际工厂通常设备有限，数据信息含量低，模型识别困难，存在高参数不确定性。

Method: 采用基于管道的鲁棒非线性模型预测控制（NMPC）方法，通过数值模拟评估其在饲料变化期间调节生物甲烷生产的能力，模拟设计尽可能接近实验设置。

Result: 通过数值模拟评估了NMPC在饲料变化期间调节生物甲烷生产的能力，为即将在真实小型中试工厂上的验证做准备。

Conclusion: 基于管道的鲁棒NMPC能够处理生物甲烷生产中的饲料变化控制问题，应对不确定性并保证安全运行，为实际工厂应用提供了有前景的解决方案。

Abstract: To match the growing demand for bio-methane production, anaerobic digesters need to embrace the co-digestion of different feedstocks; in addition, to improve the techno-economic performance, an optimal and time-varying adaptation of the input diet is required. These operation modes constitute a very hard challenge for the limited instrumentation and control equipment typically installed aboard full-scale plants. A model-based predictive approach may be able to handle such control problem, but the identification of reliable predictive models is limited by the low information content typical of the data available from full-scale plants' operations, which entail high parametric uncertainty. In this work, the application of a tube-based robust nonlinear model predictive control (NMPC) is proposed to regulate bio-methane production over a period of diet change in time, while warranting safe operation and dealing with uncertainties. In view of its upcoming validation on a true small pilot-scale plant, the NMPC capabilities are assessed via numerical simulations designed to resemble as much as possible the experimental setup, along with some practical final considerations.

</details>


### [136] [Transient Power Allocation Control Scheme for Hybrid Hydrogen Electrolyzer-Supercapacitor System with Autonomous Inertia Response](https://arxiv.org/abs/2601.01170)
*Pengfeng Lin,Guangjie Gao,Jianjun Ma,Miao Zhu,Xinan Zhang,Ahmed Abu-Siada*

Main category: eess.SY

TL;DR: 提出混合氢电解槽-超级电容系统(HESS)及新型控制策略，用于可再生能源主导电网。系统包含碱性电解槽、质子交换膜电解槽和超级电容，通过惯性模拟控制协调，实现不同频率功率分量自主分配。


<details>
  <summary>Details</summary>
Motivation: 可再生能源主导电网面临频率波动和惯性不足问题，需要储能系统提供快速响应。传统电解槽响应慢、寿命短，需要结合超级电容提供瞬态功率支持，同时降低电解槽能量损耗。

Method: 1. 设计混合系统：碱性电解槽(AEL)采用传统直流功率控制，质子交换膜电解槽(PEMEL)采用动态惯性控制，超级电容(SC)采用电容惯性控制
2. 提出惯性模拟控制策略协调三种控制
3. 建立基于混合势理论的大信号数学模型
4. 实施SOC恢复控制增强超级电容循环寿命
5. 通过硬件在环实验验证

Result: 1. 系统实现高频瞬态功率由SC自主处理，稳定频率功率由PEMEL调节，低频稳态功率由AEL处理
2. SC显著减轻电解槽能量损耗，提供足够惯性恢复能力且无需额外通信
3. SOC恢复控制使SC承受超过3倍稳定性放电循环
4. 数学模型提供清晰系统参数稳定边界
5. 硬件在环实验结果完全验证系统可行性

Conclusion: 提出的混合氢电解槽-超级电容系统及控制策略有效解决可再生能源电网的惯性问题，通过功率分量自主分配优化系统性能，延长设备寿命，为高比例可再生能源电网提供可行解决方案。

Abstract: This paper proposes a hybrid hydrogen electrolyzer-supercapacitor system (HESS) with a novel control strategy for renewable-dominant power grids. The HESS consists of alkaline electrolyzers (AEL), proton exchange membrane electrolyzers (PEMEL), and supercapacitors (SC). The interfacing inverters between HESS and power grid are regulated by an inertia emulation control strategy. From HESS, AEL is with conventional DC power control, whereas PEMEL and SC are designed with the proposed dynamic inertia control and capacitive inertia control, respectively. Benefitting from the coordination of three controls, within the HESS, high-frequency transient power components are autonomously handled by SC, stable frequency power components are regulated by PEMEL, and low-frequency steady-state power is addressed by AEL, characterized by low operational gains and longer lifetimes. SC delivers transient power, significantly alleviating energy losses on electrolyzers and achieving adequate inertia recovery capabilities while requiring no additional communication. Implementing SOC recovery control enables the SC to withstand more than three times more stability discharge cycles compared to an SC without SOC recovery. Furthermore, a large-signal mathematical model based on mixed potential theory is established, providing clear stability boundaries for system parameters. Dynamic analyses theoretically verify system feasibility, and extensive hardware-in-the-loop experimental results fully validate the proposed HESS along with the corresponding transient power allocation controls.

</details>


### [137] [Reinforcement Learning Based Whittle Index Policy for Scheduling Wireless Sensors](https://arxiv.org/abs/2601.01179)
*Sokipriala Jonah,Seong Ki Yoo,Saurav Sthapit*

Main category: eess.SY

TL;DR: 提出基于强化学习的WIQL调度策略，结合AoII和边缘挖掘技术，在无线传感器网络中实现高效数据传输，减少70%的数据包传输。


<details>
  <summary>Details</summary>
Motivation: 传统基于AoI的无线传感器网络调度只关注信息新鲜度，不考虑更新数据的实际价值，导致资源受限的传感器网络效率低下，增加能耗和存储需求。

Method: 将调度问题建模为Restless Multi-Armed Bandit (RMAB)，提出基于Whittle Index的Q-learning (WIQL)策略动态选择最有价值的传感器节点，并采用边缘挖掘技术在本地预处理数据。

Result: WIQL策略实现了接近最优的性能，同时将传输数据包数量减少高达70%，为资源受限的WSN提供了可扩展的自适应解决方案。

Conclusion: 基于强化学习的WIQL调度策略结合AoII和边缘挖掘技术，能够有效提高无线传感器网络的数据传输效率，显著降低能耗，适用于资源受限的监测应用。

Abstract: Wireless Sensor nodes used in remote monitoring applications typically transmit data in a timely manner, often optimising for the Age of Information (AoI). However, this approach focuses solely on keeping updates at the sink fresh without considering the actual value of each update. This method is inefficient for sensor networks, which have limited resources. Transmitting data indiscriminately without evaluating its significance not only increases energy consumption but also raises storage requirements. To address this challenge, we propose a reinforcement learning-based scheduling strategy that prioritises sensor transmissions based on the Age of Incorrect Information (AoII) using an edge mining technique. This ensures that the most valuable updates are received while reducing energy consumption. We frame the scheduling problem as a Restless Multi-Armed Bandit (RMAB) and introduce a Whittle Index-based Q-learning (WIQL) policy to dynamically select the most informative sensors. Additionally, we employ an edge mining technique, where raw sensor data is processed locally before transmission, enhancing state estimation at the sink. Experimental results demonstrate that WIQL achieves near-optimal performance while significantly reducing the number of transmitted packets by up to 70%. This reinforcement learning-based approach provides a scalable and adaptive solution for efficient data scheduling in resource-constrained WSNs.

</details>


### [138] [Multi-Objective Operational Optimization of Energy Storage Systems in User-Side Microgrids](https://arxiv.org/abs/2601.01256)
*Jinzhou Xu,Yuanxin Zhuo,Paola Tapia*

Main category: eess.SY

TL;DR: 本文提出了一种面向用户的微电网储能系统运行优化策略，通过多目标优化框架降低电费成本、减少碳排放并提高可再生能源利用率，在实际运行数据验证中实现了13.47%的平均电费降低。


<details>
  <summary>Details</summary>
Motivation: 针对微电网储能系统在实际用户侧应用中的需求，需要开发一种能够同时考虑经济性、环保性和可再生能源利用的优化策略，以解决现有用户运行策略在灵活性、适应性和多目标平衡方面的不足。

Method: 首先建立基础ESS模型描述系统动态和运行约束，然后构建多目标运行优化框架，同时最小化电费成本、减少碳排放并提高可再生能源利用率，使用Gurobi优化求解器确保计算效率和可扩展性。

Result: 使用实际微电网运行数据验证，ESS模型能准确表征真实系统约束；相比现有用户运行策略，平均降低电费成本13.47%；通过动态调整多目标权重因子，显著提高了运行灵活性和场景适应性。

Conclusion: 本文提出的用户中心优化设计通过多目标权重分配增强了运行灵活性和场景适应性，为实际微电网ESS运行提供了实用且可扩展的解决方案，并为用户侧微电网参与余电上网政策提供了决策支持。

Abstract: An operational optimization strategy for microgrid energy storage systems (ESSs) is developed to address practical user-oriented application requirements, and its effectiveness is validated using real-world operational data. First, a fundamental ESS model is established to characterize system dynamics and operational constraints, providing a theoretical basis for optimization. Subsequently, a multi-objective operational optimization framework is formulated to simultaneously minimize electricity cost, reduce carbon emissions, and enhance renewable energy utilization. To ensure computational efficiency and scalability, the commercial optimization solver Gurobi is employed. The proposed strategy is evaluated using actual microgrid operational data, demonstrating that the developed ESS model accurately represents real system constraints. Compared with existing user operational strategies, the proposed approach achieves an average reduction of 13.47% in electricity cost. Moreover, by dynamically adjusting the weighting factors of the multi-objective formulation, the strategy enables flexible operational modes and significantly improves adaptability to varying operating scenarios. In addition, the proposed framework provides decision support for user-side microgrids participating in surplus electricity feed-in policies. The main contribution of this work lies in its user-centric optimization design, which enhances operational flexibility and scenario adaptability through multi-objective weight allocation, offering a practical and scalable solution for real-world microgrid ESS operation.

</details>


### [139] [An Energy-Efficient Smart Bus Transport Management System with Blind-Spot Collision Detection Ability](https://arxiv.org/abs/2601.01274)
*Md. Sadman Haque,Zobaer Ibn Razzaque,Robiul Awoul Robin,Fahim Hafiz,Riasat Azim*

Main category: eess.SY

TL;DR: 提出智慧公交系统，包含深度学习盲点预警、自动公交站识别、物联网太阳能智能站台、RFID乘客追踪、智能车门系统及实时公交追踪，提升安全、效率与可持续性。


<details>
  <summary>Details</summary>
Motivation: 发展中国家公交系统缺乏实时位置更新，乘客体验差；非指定地点停车造成安全隐患和交通拥堵；盲点和交通违规增加事故风险。

Method: 1) 深度学习盲点预警系统；2) 自动公交站检测；3) 物联网太阳能智能站台，显示实时乘客数；4) RFID卡系统追踪乘客上下车；5) 智能车门系统；6) 实时公交追踪；7) HTTP服务器连接所有子系统。

Result: 实时盲点检测效率约99%；精确停靠公交站；服务器提供实时位置更新给用户和站台；节能站台节省12.71kWh能源。

Conclusion: 提出的智慧公交系统能有效提升安全性、效率和可持续性，适用于发展中国家公交系统现代化改造。

Abstract: Public bus transport systems in developing countries often suffer from a lack of real-time location updates and for users, making commuting inconvenient and unreliable for passengers. Furthermore, stopping at undesired locations rather than designated bus stops creates safety risks and contributes to roadblocks, often causing traffic congestion. Additionally, issues such as blind spots, along with a lack of following traffic laws, increase the chances of accidents. In this work, we address these challenges by proposing a smart public bus system along with intelligent bus stops that enhance safety, efficiency, and sustainability. Our approach includes a deep learning-based blind-spot warning system to help drivers avoid accidents with automated bus-stop detection to accurately identify bus stops, improving transit efficiency. We also introduce IoT-based solar-powered smart bus stops that show real-time passenger counts, along with an RFID-based card system to track where passengers board and exit. A smart door system ensures safer and more organised boarding, while real-time bus tracking keeps passengers informed. To connect all these features, we use an HTTP-based server for seamless communication between the interconnected network systems. Our proposed system demonstrated approximately 99% efficiency in real-time blind spot detection while stopping precisely at the bus stops. Furthermore, the server showed real-time location updates both to the users and at the bus stops, enhancing commuting efficiency. The proposed energy-efficient bus stop demonstrated 12.71kWh energy saving, promoting sustainable architecture. Full implementation and source code are available at: https://github.com/sadman-adib/MoveMe-IoT

</details>


### [140] [Simple yet Effective Anti-windup Techniques for Amplitude and Rate Saturation: An Autonomous Underwater Vehicle Case Study](https://arxiv.org/abs/2601.01302)
*Pouria Sarhadi*

Main category: eess.SY

TL;DR: 本文重新审视经典抗饱和控制方法，通过改进PID和LQI控制器，使其能同时处理执行器幅值和速率饱和问题，在SISO系统中取得与复杂现代方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 执行器幅值和速率饱和（A&RSat）及其引发的积分饱和问题是控制系统的长期挑战。虽然现代抗饱和方法（MAW）能处理两种饱和，但其推导复杂、条件严格，工程应用困难。而经典抗饱和方法（CAW）通常只处理幅值饱和，文献中很少探讨其处理两种饱和的潜力。

Method: 对两种经典控制器（PID和LQI）进行改进，使其能够同时处理执行器幅值和速率饱和。通过修改控制结构，使这些经典方法能在A&RSat条件下有效工作。

Result: 在REMUS AUV偏航控制问题上进行基准测试，并与约束MPC比较。结果表明，改进后的经典方法在SISO系统中能提供简单有效的解决方案，性能与复杂方法相当。

Conclusion: 经典抗饱和技术经过适当改进后，仍能提供简单有效的解决方案，至少对于SISO系统，其性能可与复杂现代方法媲美。这为仅需少量额外调参且易于实现的解决方案提供了研究思路。

Abstract: Actuator amplitude and rate saturation (A\&RSat), together with their consequent windup problem, have long been recognised as challenges in control systems. Anti-windup (AW) solutions have been developed over the past decades, which can generally be categorised into two main groups: classical and modern anti-windup (CAW and MAW) approaches. Classical methods have provided simple and effective results, mainly addressing amplitude saturation. In contrast, modern approaches offer powerful and theoretically sound solutions capable of handling both amplitude and rate saturations. However, MAW's derivation process often imposes restrictive conditions and can be complex to apply in practical engineering problems. Nevertheless, the literature has paid limited attention (if not entirely ignored) to the potential of simple yet effective CAW schemes that can operate in the presence of both A\&RSat elements. This paper revisits this issue and proposes modifications to two well-known controllers: PID and LQI. The obtained results, benchmarked on the REMUS AUV yaw control problem and compared with constrained MPC, indicate that these classical techniques can still provide simple yet effective solutions with comparable performance, at least for SISO systems. These findings may stimulate further research into solutions that achieve comparable performance with only one (or a limited number of) additional tuning parameters and straightforward implementation.

</details>


### [141] [Neural-network-based Self-triggered Observed Platoon Control for Autonomous Vehicles](https://arxiv.org/abs/2601.01335)
*Zihan Li,Ziming Wang,Chenning Liu,Xin Wang*

Main category: eess.SY

TL;DR: 提出自适应一致性跟踪控制框架，用于解决不确定动态和间歇通信下的自动驾驶车队控制问题，结合神经网络、分布式观测器和自触发机制


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车队在不确定动态和间歇通信条件下的控制是智能交通系统的关键挑战，需要解决非线性、未知扰动和通信限制问题

Method: 集成反步设计、非线性采样数据观测器、径向基函数神经网络和自触发通信机制，神经网络近似未知非线性和时变扰动，分布式观测器估计邻居状态，自触发机制确定触发时刻

Result: 理论分析证明所有闭环信号一致最终有界，跟踪误差收敛到紧集；仿真结果显示方法具有高鲁棒性、适应性和通信效率

Conclusion: 所提方法适用于实际网络化车辆系统，能有效处理不确定动态和间歇通信问题，避免Zeno行为

Abstract: This paper investigates autonomous vehicle (AV) platoon control under uncertain dynamics and intermittent communication, which remains a critical challenge in intelligent transportation systems. To address these issues, this paper proposes an adaptive consensus tracking control framework for nonlinear multi-agent systems (MASs). The proposed approach integrates backstepping design, a nonlinear sampled-data observer, radial basis function neural networks, and a self-triggered communication mechanism. The radial basis function neural networks approximate unknown nonlinearities and time-varying disturbances, thereby enhancing system robustness. A distributed observer estimates neighboring states based on limited and intermittent measurements, thereby reducing dependence on continuous communication. Moreover, self-triggered mechanism is developed to determine triggering instants, guaranteeing a strictly positive minimum inter-event time and preventing Zeno behavior. The theoretical analysis proves that all closed-loop signals are uniformly ultimately bounded (UUB), and tracking errors converge to a compact set. Simulation results demonstrate that the proposed approach achieves high robustness, adaptability, and communication efficiency, making it suitable for real-world networked vehicle systems.

</details>


### [142] [Sampling Strategy Design for Model Predictive Path Integral Control on Legged Robot Locomotion](https://arxiv.org/abs/2601.01409)
*Chuyuan Tao,Fanxin Wang,Haolong Jiang,Jia He,Yiyang Chen,Qinglei Bu*

Main category: eess.SY

TL;DR: 本文系统研究了MPPI框架中采样策略设计对四足机器人运动控制的影响，比较了非结构化和样条基方法，通过仿真评估了不同策略对控制平滑性、任务性能、鲁棒性和采样效率的影响。


<details>
  <summary>Details</summary>
Motivation: MPPI控制已成为处理复杂非线性高维系统的强大采样优化控制方法，但直接应用于足式机器人系统面临诸多挑战。本文旨在系统研究MPPI框架中采样策略设计对足式机器人运动控制的作用。

Method: 基于结构化控制参数化的思想，在MPPI框架内探索并比较了多种采样策略，包括非结构化和样条基方法。通过四足机器人平台的大量仿真实验，评估不同采样策略对控制性能的影响。

Result: 研究结果为采样设计在复杂足式系统上部署MPPI的实际意义提供了新的见解，揭示了不同采样策略在控制平滑性、任务性能、鲁棒性和采样效率方面的差异。

Conclusion: 本文系统研究了MPPI框架中采样策略设计对足式机器人运动控制的重要性，为在实际复杂足式系统中有效部署MPPI控制提供了指导性见解。

Abstract: Model Predictive Path Integral (MPPI) control has emerged as a powerful sampling-based optimal control method for complex, nonlinear, and high-dimensional systems. However, directly applying MPPI to legged robotic systems presents several challenges. This paper systematically investigates the role of sampling strategy design within the MPPI framework for legged robot locomotion. Based upon the idea of structured control parameterization, we explore and compare multiple sampling strategies within the framework, including both unstructured and spline-based approaches. Through extensive simulations on a quadruped robot platform, we evaluate how different sampling strategies affect control smoothness, task performance, robustness, and sample efficiency. The results provide new insights into the practical implications of sampling design for deploying MPPI on complex legged systems.

</details>


### [143] [Reliable Grid Forecasting: State Space Models for Safety-Critical Energy Systems](https://arxiv.org/abs/2601.01410)
*Jisoo Lee,Sunki Hong*

Main category: eess.SY

TL;DR: 论文提出了一个针对电网负荷预测的评估框架，强调传统对称误差指标无法反映运营风险，并证明S-Mamba模型在99.5%尾风险储备代理下具有最优可靠性。


<details>
  <summary>Details</summary>
Motivation: 电网负荷预测的准确性对运营安全至关重要：预测不足可能导致供电短缺，而传统的对称误差指标无法反映这种运营不对称性。需要开发能直接衡量运营风险而非仅统计准确性的评估框架。

Method: 提出了电网特定的评估框架，包括非对称MAPE、预测不足率和储备裕度。使用该框架系统评估了基于Mamba的状态空间模型在加州电网预测中的表现，数据集涵盖2023年11月至2025年11月的84,498小时记录，覆盖5个传输区域。

Result: 标准准确性指标是运营安全的差代理：具有相同MAPE的模型可能需要完全不同的储备裕度。预测误差与温度呈弱但显著相关（r=0.16，p<10^{-16}），表明需要天气感知建模。S-Mamba模型实现了最低的Reserve_{99.5}%裕度（14.12%），优于iTransformer的16.66%。

Conclusion: 电网负荷预测评估需要关注运营风险而非仅统计准确性，提出的评估框架能更好衡量模型可靠性。S-Mamba在尾风险场景下表现出色，为电网运营提供了更安全的预测保障。

Abstract: Accurate grid load forecasting is safety-critical: under-predictions risk supply shortfalls, while symmetric error metrics mask this operational asymmetry. We introduce a grid-specific evaluation framework--Asymmetric MAPE, Under-Prediction Rate, and Reserve Margin--that directly measures operational risk rather than statistical accuracy alone.
  Using this framework, we conduct a systematic evaluation of Mamba-based State Space Models for California grid forecasting on a weather-aligned CAISO TAC-area dataset spanning Nov 2023--Nov 2025 (84,498 hourly records across 5 transmission areas). Our analysis reveals that standard accuracy metrics are poor proxies for operational safety: models with identical MAPE can require vastly different reserve margins.
  We demonstrate that forecast errors are weakly but significantly associated with temperature (r = 0.16, p < 10^{-16}), motivating weather-aware modeling rather than loss function modification alone. The S-Mamba model achieves the lowest Reserve_{99.5}% margin (14.12%) compared to 16.66% for iTransformer, demonstrating superior forecast reliability under a 99.5th-percentile tail-risk reserve proxy.

</details>


### [144] [Context-Aware Information Transfer via Digital Semantic Communication in UAV-Based Networks](https://arxiv.org/abs/2601.01430)
*Poorvi Joshi,Mohan Gurusamy*

Main category: eess.SY

TL;DR: DSC-UAV模型通过上下文自适应的数字语义通信框架，结合提示感知编码、动态无人机中继和用户移动性优化的强化学习，显著提升了带宽受限无人机网络中的关键任务数据传输效率。


<details>
  <summary>Details</summary>
Motivation: 在智慧城市中，带宽受限的无人机往往无法及时中继关键任务数据，影响实时决策。这凸显了需要更快、更高效地传输最相关信息的需求。

Method: 提出DSC-UAV模型，包含三个核心组件：1) 提示感知编码：使用Vision Transformer结合提示文本编码器生成基于上下文（通用或特定对象）的语义特征；2) 动态无人机中继：通过无人机网络传输量化后的语义特征；3) 用户移动性优化的强化学习：使用截断分位数批评家技术联合优化轨迹和资源分配，相比标准SAC和TD3具有更好的稳定性和精度。

Result: 仿真结果显示，相比数字和先前的无人机语义通信基线，性能显著提升：语义结构相似性提高22%，信息年龄减少14%。

Conclusion: 通过将移动控制与上下文驱动的视觉抽象相结合，DSC-UAV为带宽受限环境中的下一代无人机网络推进了具有弹性的、以信息为中心的监控能力。

Abstract: In smart cities, bandwidth-constrained Unmanned Aerial Vehicles (UAVs) often fail to relay mission-critical data in time, compromising real-time decision-making. This highlights the need for faster and more efficient transmission of only the most relevant information. To address this, we propose DSC-UAV model, leveraging a context-adaptive Digital Semantic Communication (DSC) framework. This model redefines aerial data transmission through three core components: prompt-aware encoding, dynamic UAV-enabled relaying, and user mobility-optimized reinforcement learning. Ground users transmit context-driven visual content. Images are encoded via Vision Transformer combined with a prompt-text encoder to generate semantic features based on the desired context (generic or object-specific). These features are then quantized and transmitted over a UAV network that dynamically relays the data. Joint trajectory and resource allocation are optimized using Truncated Quantile Critic (TQC)-aided reinforcement learning technique, which offers greater stability and precision over standard SAC and TD3 due to its resistance to overestimation bias. Simulations demonstrate significant performance improvement, up to 22\% gain in semantic-structural similarity and 14\% reduction in Age of Information (AoI) compared to digital and prior UAV-semantic communication baselines. By integrating mobility control with context-driven visual abstraction, DSC-UAV advances resilient, information-centric surveillance for next-generation UAV networks in bandwidth-constrained environments.

</details>


### [145] [Full-Time-Scale Power Management Strategy for Hybrid AC/DC/DS Microgrid with Dynamic Concatenation and Autonomous Frequency / Voltage Restorations](https://arxiv.org/abs/2601.01629)
*Qingzuo Meng,Pengfeng Lin,Yujie Wang,Miao Zhu,Amer M. Ghias,Syed Islam,Frede Blaabjerg*

Main category: eess.SY

TL;DR: 提出全时标功率管理策略，统一暂态惯量共享和稳态功率分配，消除下垂控制引起的频率电压偏差


<details>
  <summary>Details</summary>
Motivation: 现有方法要么关注稳态功率分配，要么关注暂态惯量支持，很少结合两者，且常忽略下垂控制引起的频率电压偏差，可能损害敏感负载

Method: 提出全时标功率管理策略，通过动态连接器统一暂态惯量共享和稳态功率分配；引入自主频率/电压恢复消除各子网稳态偏差；开发全局等效电路模型简化系统分析和设计

Result: 实验证实该方法在稳态下保持额定频率和电压，同时实现所有时间尺度上暂态惯量支持和比例功率分配的无缝过渡

Conclusion: 提出的全时标功率管理策略有效解决了混合交直流微电网中功率管理的综合问题，统一了暂态和稳态控制，消除了传统下垂控制的偏差问题

Abstract: Hybrid AC/DC microgrids with distributed energy storage (DS) improve power reliability in remote areas. Existing power management methods either focus on steady-state power sharing or transient inertia support, but rarely combine both. They also often ignore frequency and voltage deviations caused by droop control, which can harm sensitive loads. To overcome these issues, this paper proposes a full-time-scale (FTS) power management strategy that unifies transient inertia sharing and steady-state power allocation through a novel dynamic concatenator. It also introduces autonomous frequency/voltage restoration to eliminate steady-state deviations in each subgrid. Additionally, a global equivalent circuit model (GECM) is developed to simplify system analysis and design. Experiments confirm that the approach maintains nominal frequency and voltage in steady state while enabling seamless transition between transient inertia support and proportional power sharing across all time scales.

</details>


### [146] [Cross-Directional Modelling and Control of Slot-Die Battery Electrode Coating](https://arxiv.org/abs/2601.01691)
*Hyuntae Kim,Idris Kempf*

Main category: eess.SY

TL;DR: 开发基于物理建模的电池电极制造薄膜厚度控制管道，通过CFD仿真获取数据建立低阶模型，设计可用于实时自动反馈和手动前馈操作的控制器


<details>
  <summary>Details</summary>
Motivation: 随着全球电池需求增长，电池电极制造需要实时过程控制，但目前狭缝涂布线大多仍采用开环手动操作，缺乏自动化控制

Method: 基于计算流体动力学(CFD)仿真获取数据，识别和校准低阶横向模型，设计控制器支持实时自动反馈和手动前馈操作

Result: 数值仿真显示CFD模型与低阶横向模型高度一致，开发的控制器可用于生产线调试和实时自动化操作

Conclusion: 该研究为电池电极制造中的薄膜厚度控制提供了有效的物理建模与控制管道，能够实现从手动操作向自动化实时控制的转变

Abstract: As global battery demand increases, real-time process control becomes increasingly important for battery electrode manufacturing, yet slot-die lines are still mostly manually operated in open loop. This paper develops a physics-based modelling-and-control pipeline for film-thickness regulation. Computational fluid dynamics (CFD) simulations provide the data from which a low-order cross-directional model is identified and calibrated. Numerical simulations demonstrate close agreement between the CFD and the cross-directional model, which is used to design a controller that can be used in both real-time, automated feedback operation and manual feedforward operation during line commissioning.

</details>


### [147] [Host-Aware Control of Gene Expression using Data-Enabled Predictive Control](https://arxiv.org/abs/2601.01693)
*Liam Perreault,Idris Kempf,Kirill Sechkar,Jean-Baptiste Lugagne,Antonis Papachristodoulou*

Main category: eess.SY

TL;DR: DeePC控制器在细菌基因表达控制中表现出优越的数据效率和鲁棒性，无需大量数据或重新训练即可适应新系统。


<details>
  <summary>Details</summary>
Motivation: 传统AI控制器需要大量数据和重新训练才能适应新系统，限制了其在细菌基因表达控制中的应用。需要一种更高效、更灵活的控制方法。

Method: 采用数据驱动的预测控制（DeePC），结合基函数处理非线性问题，应用于具有两个输入（光遗传控制和培养基浓度）和两个输出（目标基因表达和宿主生长速率）的系统。

Result: DeePC在参数变化下保持鲁棒性，在所有控制策略中表现最佳，同时使用最少的数据量。

Conclusion: DeePC为细菌基因表达控制提供了一种高效、鲁棒的数据驱动方法，在工程生物学、药物开发和生物制造中具有应用潜力。

Abstract: Cybergenetic gene expression control in bacteria enables applications in engineering biology, drug development, and biomanufacturing. AI-based controllers offer new possibilities for real-time, single-cell-level regulation but typically require large datasets and re-training for new systems. Data-enabled Predictive Control (DeePC) offers better sample efficiency without prior modelling. We apply DeePC to a system with two inputs, optogenetic control and media concentration, and two outputs, expression of gene of interest and host growth rate. Using basis functions to address nonlinearities, we demonstrate that DeePC remains robust to parameter variations and performs among the best control strategies while using the least data.

</details>


### [148] [Policy Optimization with Differentiable MPC: Convergence Analysis under Uncertainty](https://arxiv.org/abs/2601.01940)
*Riccardo Zuliani,Efe C. Balta,John Lygeros*

Main category: eess.SY

TL;DR: 将梯度策略优化与递归系统辨识结合，确保收敛到最优控制器设计


<details>
  <summary>Details</summary>
Motivation: 基于模型的策略优化框架在控制应用中广泛使用，但控制器性能和优化算法收敛性严重依赖模型精度。现有方法在模型预测控制中嵌入显式动态模型，但模型不准确会影响性能。

Method: 结合梯度策略优化与递归系统辨识，在优化过程中持续更新和改进系统模型

Result: 该方法确保收敛到最优控制器设计，并在多个控制示例中展示了有效性

Conclusion: 梯度策略优化与递归系统辨识的结合为解决模型精度依赖问题提供了有效方案，能保证收敛到最优控制器

Abstract: Model-based policy optimization is a well-established framework for designing reliable and high-performance controllers across a wide range of control applications. Recently, this approach has been extended to model predictive control policies, where explicit dynamical models are embedded within the control law. However, the performance of the resulting controllers, and the convergence of the associated optimization algorithms, critically depends on the accuracy of the models. In this paper, we demonstrate that combining gradient-based policy optimization with recursive system identification ensures convergence to an optimal controller design and showcase our finding in several control examples.

</details>


### [149] [Asymptotic Behavior of an Unforced Duhem-Type Hysteretic Oscillator](https://arxiv.org/abs/2601.01951)
*Mihails Milehins,Dan B. Marghitu*

Main category: eess.SY

TL;DR: 分析具有Duhem型粘弹塑性滞回元件的无外力机械振荡器的基本解析性质，包括解的存在性、唯一性和收敛性


<details>
  <summary>Details</summary>
Motivation: 研究具有Duhem型滞回元件的机械振荡器的数学性质，为理解这类非线性系统的行为提供理论基础

Method: 采用数学分析方法，研究无外力机械振荡器模型，分析其解的存在性、唯一性和收敛性等基本性质

Result: 证明了该系统的全局解存在性、解的唯一性，以及每个解都收敛到平衡点

Conclusion: 具有Duhem型粘弹塑性滞回元件的无外力机械振荡器具有良好的数学性质，为实际应用提供了理论保证

Abstract: The article describes fundamental analytical properties of an unforced mechanical oscillator with a Duhem-type viscoelastoplastic hysteretic element. These properties include global existence of solutions, uniqueness of solutions, and convergence of each solution to an equilibrium point.

</details>


### [150] [Finite-State Decentralized Policy-Based Control With Guaranteed Ground Coverage](https://arxiv.org/abs/2601.02109)
*Hossein Rastgoftar*

Main category: eess.SY

TL;DR: 提出一个有限状态、去中心化的多智能体地面覆盖决策与控制框架，通过深度神经网络结构设计和基于策略的去中心化覆盖控制实现高效覆盖。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体地面覆盖问题，需要一种可扩展、计算高效的去中心化方法，能够适应环境目标集的变化并实现最优配置。

Method: 将问题分解为两部分：1）基于智能体参考配置设计深度神经网络结构；2）基于策略的去中心化覆盖控制。将智能体分为锚点和跟随者，形成三角形通信结构，DNN权重编码团队空间配置。建立计算高效的去中心化马尔可夫决策过程，引入Anyway Output Controllability概念。

Result: 建立了去中心化收敛到期望配置的理论保证，该配置能够最优地表示环境目标集，实现了可扩展且计算高效的覆盖控制。

Conclusion: 提出的框架通过结合深度神经网络和去中心化控制，为多智能体地面覆盖问题提供了一种新颖且有效的解决方案，具有可扩展性和计算效率优势。

Abstract: We propose a finite-state, decentralized decision and control framework for multi-agent ground coverage. The approach decomposes the problem into two coupled components: (i) the structural design of a deep neural network (DNN) induced by the reference configuration of the agents, and (ii) policy-based decentralized coverage control. Agents are classified as anchors and followers, yielding a generic and scalable communication architecture in which each follower interacts with exactly three in-neighbors from the preceding layer, forming an enclosing triangular communication structure. The DNN training weights implicitly encode the spatial configuration of the agent team, thereby providing a geometric representation of the environmental target set. Within this architecture, we formulate a computationally efficient decentralized Markov decision process (MDP) whose components are time-invariant except for a time-varying cost function defined by the deviation from the centroid of the target set contained within each agent communication triangle. By introducing the concept of Anyway Output Controllability (AOC), we assume each agent is AOC and establish decentralized convergence to a desired configuration that optimally represents the environmental target.

</details>


### [151] [Optimal Dispatch of Electricity and Water in Renewable-Integrated Desalination Plants](https://arxiv.org/abs/2601.02243)
*Ahmed S. Alahmed,Audun Botterud,Saurabh Amin,Ali T. Al-Awami*

Main category: eess.SY

TL;DR: 提出海水淡化厂作为混合发电-负荷资源的最优调度数学框架，通过阈值结构实现水-电耦合优化


<details>
  <summary>Details</summary>
Motivation: 海水淡化厂整合热力发电、膜基可控负荷和可再生能源，具有独特的运行灵活性，可同时参与水电两个市场，但需要有效的调度框架来最大化利润

Method: 建立利润最大化的海水淡化厂调度决策模型，捕捉水-电流的运行、技术和市场耦合，推导出基于阈值的结构，提供计算可行的协调机制

Result: 阈值以闭式解析形式表示为技术和电价参数的显函数，小参数变化对利润影响可分析，仿真显示相比基准算法有显著改进

Conclusion: 阈值结构为大规模部署提供计算可行的协调，热力发电和膜基负荷互补提供连续双向灵活性，显著提升海水淡化厂的经济效益

Abstract: We develop a mathematical framework for the optimal dispatch of flexible water desalination plants (WDPs) as hybrid generator-load resources. WDPs integrate thermal generation, membrane-based controllable loads, and renewable energy sources, offering unique operational flexibility for power system operations. They can simultaneously participate in two markets: selling desalinated water to a water utility, and bidirectionally transacting electricity with the grid based on their net electricity demand. We formulate the dispatch decision problem of a profit-maximizing WDP, capturing operational, technological, and market-based coupling between water and electricity flows. The threshold-based structure we derive provides computationally tractable coordination suitable for large-scale deployment, offering operational insights into how thermal generation and membrane-based loads complementarily provide continuous bidirectional flexibility. The thresholds are analytically characterized in closed form as explicit functions of technology and tariff parameters. We examine how small changes in the exogenous tariff and technology parameters affect the WDP's profit. Extensive simulations illustrate the optimal WDP's operation, profit, and water-electricity exchange, demonstrating significant improvements relative to benchmark algorithms.

</details>


### [152] [Characterizing All Locally Exponentially Stabilizing Controllers as a Linear Feedback Plus Learnable Nonlinear Youla Dynamics](https://arxiv.org/abs/2601.02244)
*Luca Furieri*

Main category: eess.SY

TL;DR: 论文提出了一种非线性输入仿射连续时间系统的状态空间控制器参数化方法，将动态状态反馈控制器分解为线性稳定部分和局部指数稳定内部动态部分，并证明这种分解的充分必要性。


<details>
  <summary>Details</summary>
Motivation: 为非线性系统设计稳定控制器是控制理论的核心问题。现有Youla参数化方法主要针对线性系统或全局渐近稳定性，缺乏针对非线性系统局部指数稳定性的状态空间参数化方法。

Method: 推导出所有使非线性输入仿射连续时间系统平衡点局部指数稳定的动态状态反馈控制器的状态空间特征。证明任何控制器都可以表示为：线性状态反馈部分（Kx，稳定线性化系统）加上内部局部指数稳定控制器动态的输出。

Result: 建立了局部指数稳定控制器的充分必要条件：任何局部指数稳定控制器都允许这样的分解，反之，任何这样的分解都产生局部指数稳定控制器。这可以看作是非线性Youla型参数化在局部指数稳定性下的特化。

Conclusion: 该结果为非线性控制提供了新的状态空间参数化框架，剩余的内部动态可以用稳定循环神经网络实现并作为神经ODE训练，从而在非线性控制任务中实现高性能闭环控制。

Abstract: We derive a state-space characterization of all dynamic state-feedback controllers that make an equilibrium of a nonlinear input-affine continuous-time system locally exponentially stable. Specirically, any controller obtained as the sum of a linear state-feedback $u=Kx$, with $K$ stabilizing the linearized system, and the output of internal locally exponentially stable controller dynamics is itself locally exponentially stabilizing. Conversely, every dynamic state-feedback controller that locally exponentially stabilizes the equilibrium admits such a decomposition. The result can be viewed as a state-space nonlinear Youla-type parametrization specialized to local, rather than global, and exponential, rather than asymptotic, closed-loop stability. The residual locally exponentially stable controller dynamics can be implemented with stable recurrent neural networks and trained as neural ODEs to achieve high closed-loop performance in nonlinear control tasks.

</details>


### [153] [Machine Learning Guided Cooling Optimization for Data Centers](https://arxiv.org/abs/2601.02275)
*Shrenik Jadhav,Zheng Liu*

Main category: eess.SY

TL;DR: 提出一个三阶段物理引导机器学习框架，用于识别和减少高性能计算设施中的冷却能耗浪费，通过构建单调性约束的梯度提升代理模型量化冷却效率低下，并评估安全设定点调整以回收高达96%的过量能耗。


<details>
  <summary>Details</summary>
Motivation: 数据中心冷却系统对可靠运行至关重要，但通常存在效率低下问题，导致能源消耗过度。需要一种系统方法来识别和减少冷却能耗浪费，特别是在高性能计算设施中。

Method: 三阶段物理引导机器学习框架：1) 使用单调性约束梯度提升建立预测设施辅助功率的代理模型；2) 使用代理模型作为物理一致基线量化过量冷却能耗；3) 评估护栏约束的反事实调整（供应温度和子回路流量），在尊重热限值和操作约束下回收过量能耗。

Result: 代理模型实现0.026 MW的平均绝对误差，98.7%测试样本的功率使用效率预测误差在0.01以内。识别出约85 MWh的年效率低下，集中在特定月份、小时和操作状态。通过安全设定点调整可回收高达96%的过量能耗。

Conclusion: 该框架提供可解释的建议，支持反事实分析（如低负载期间流量减少和冷却回路间热负荷重新分配），为减少辅助功率提供实用途径。框架兼容模型预测控制，可扩展到其他液冷数据中心。

Abstract: Effective data center cooling is crucial for reliable operation; however, cooling systems often exhibit inefficiencies that result in excessive energy consumption. This paper presents a three-stage, physics-guided machine learning framework for identifying and reducing cooling energy waste in high-performance computing facilities. Using one year of 10-minute resolution operational data from the Frontier exascale supercomputer, we first train a monotonicity-constrained gradient boosting surrogate that predicts facility accessory power from coolant flow rates, temperatures, and server power. The surrogate achieves a mean absolute error of 0.026 MW and predicts power usage effectiveness within 0.01 of measured values for 98.7% of test samples. In the second stage, the surrogate serves as a physics-consistent baseline to quantify excess cooling energy, revealing approximately 85 MWh of annual inefficiency concentrated in specific months, hours, and operating regimes. The third stage evaluates guardrail-constrained counterfactual adjustments to supply temperature and subloop flows, demonstrating that up to 96% of identified excess can be recovered through small, safe setpoint changes while respecting thermal limits and operational constraints. The framework yields interpretable recommendations, supports counterfactual analyses such as flow reduction during low-load periods and redistribution of thermal duty across cooling loops, and provides a practical pathway toward quantifiable reductions in accessory power. The developed framework is readily compatible with model predictive control and can be extended to other liquid-cooled data centers with different configurations and cooling requirements.

</details>


### [154] [Multi-mode Fault Diagnosis Datasets of Three-phase Asynchronous Motor Under Variable Working Conditions](https://arxiv.org/abs/2601.02278)
*Shijin Chen,Zeyi Liu,Chenyang Li,Dongliang Zou,Xiao He,Donghua Zhou*

Main category: eess.SY

TL;DR: 该论文构建了一个三相异步电机在多种故障类型和严重程度下的综合数据集，包含振动和电流信号，支持在变工况条件下的鲁棒故障诊断方法开发。


<details>
  <summary>Details</summary>
Motivation: 三相异步电机是工业系统的核心部件，故障会导致严重停机和经济损失。实际应用中电机常在变工况（如转速和负载波动）下运行，这使故障诊断变得复杂，需要更全面的数据集来支持鲁棒诊断方法开发。

Method: 收集三相异步电机在多种故障类型和严重程度下的综合数据集，包括单一故障和机电复合故障（如转子不平衡、定子绕组短路、轴承故障及其组合）。数据采集涵盖稳态和过渡状态，信号包括三轴振动、三相电流、扭矩和键相信号。

Result: 构建了一个全面的数据集，支持在多样化速度、负载条件和故障类型下开发验证电机故障诊断方法，特别关注实际应用中的变工况场景。

Conclusion: 该数据集为开发适用于实际工业环境中变工况条件的鲁棒电机故障诊断方法提供了重要基础，有助于提高电机健康监测的准确性和可靠性。

Abstract: Three-phase asynchronous motor are fundamental components in industrial systems, and their failure can lead to significant operational downtime and economic losses. Vibration and current signals are effective indicators for monitoring motor health and diagnosing faults. However, motors in real applications often operate under variable conditions such as fluctuating speeds and loads, which complicate the fault diagnosis process. This paper presents a comprehensive dataset collected from a three-phase asynchronous motor under various fault types and severities, operating under diverse speed and load conditions. The dataset includes both single faults and mechanical-electrical compound faults, such as rotor unbalance, stator winding short circuits, bearing faults, and their combinations. Data were acquired under both steady and transitional conditions, with signals including triaxial vibration, three-phase currents, torque, and key-phase signals. This dataset supports the development and validation of robust fault diagnosis methods for electric motors under realistic operating conditions.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [155] [Almost-Exact Simulation Scheme for Heston-type Models: Bermudan and American Option Pricing](https://arxiv.org/abs/2601.00815)
*Mara Kalicanin Dimitrov,Marko Dimitrov,Anatoliy Malyarenko,Ying Ni*

Main category: q-fin.PR

TL;DR: 本文扩展了几乎精确模拟(AES)方案，用于在Heston和双Heston模型下定价百慕大和美式期权，通过使用非中心卡方分布提高蒙特卡洛模拟效率。


<details>
  <summary>Details</summary>
Motivation: 现有的AES方案仅用于Heston模型下的欧式期权定价，需要扩展到更复杂的百慕大和美式期权定价，并扩展到双Heston模型。

Method: 推导双Heston模型的AES方案，使用非中心卡方分布模拟方差过程，并与欧拉方案进行性能比较。特别地，当模拟步数与百慕大期权行权日期匹配时，AES能达到更高精度。

Result: 数值实验验证了AES方案在两种模型下都能提供准确的期权价格并减少计算时间，显示出其鲁棒性。AES在精度和计算效率方面优于欧拉方案。

Conclusion: AES方案对于Heston和双Heston模型下的百慕大和美式期权定价是有效且高效的，特别是在模拟步数与行权日期匹配时表现最优。

Abstract: Recently, an Almost-Exact Simulation (AES) scheme was introduced for the Heston stochastic volatility model and tested for European option pricing. This paper extends this scheme for pricing Bermudan and American options under both Heston and double Heston models. The AES improves Monte Carlo simulation efficiency by using the non-central chi-square distribution for the variance process. We derive the AES scheme for the double Heston model and compare the performance of the AES schemes under both models with the Euler scheme. Our numerical experiments validate the effectiveness of the AES scheme in providing accurate option prices with reduced computational time, highlighting its robustness for both models. In particular, the AES achieves higher accuracy and computational efficiency when the number of simulation steps matches the exercise dates for Bermudan options.

</details>


### [156] [Reinforcement Learning for Option Hedging: Static Implied-Volatility Fit versus Shortfall-Aware Performance](https://arxiv.org/abs/2601.01709)
*Ziheng Chen,Minxuan Hu,Jiayu Yi,Wenxi Sun*

Main category: q-fin.PR

TL;DR: 将风险厌恶和交易成本纳入QLBS框架，提出RLOP方法，在SPY和XOP期权数据上评估静态定价和动态对冲性能


<details>
  <summary>Details</summary>
Motivation: 现有期权定价模型通常只关注静态定价准确性，而忽略了实际对冲效果的重要性。研究旨在开发在考虑市场摩擦（交易成本）和风险厌恶的情况下，既能提高定价准确性又能改善动态对冲性能的方法。

Method: 1. 扩展QLBS框架，纳入风险厌恶和交易成本；2. 提出新的RLOP（复制学习期权定价）方法；3. 两种方法都与标准强化学习算法兼容；4. 使用SPY和XOP期权数据进行评估；5. 从静态（定价准确性）和动态（对冲性能）两个维度评估。

Result: 1. Adaptive-QLBS在隐含波动率空间实现了更高的静态定价准确性；2. RLOP在动态对冲方面表现更优，降低了短缺概率；3. 两种方法都能在市场摩擦下有效运行。

Conclusion: 评估期权定价模型不应仅限于静态拟合，而应更重视实际对冲结果。RLOP在动态对冲方面的优越性表明，将复制学习与强化学习结合是改善期权风险管理的重要方向。

Abstract: We extend the Q-learner in Black-Scholes (QLBS) framework by incorporating risk aversion and trading costs, and propose a novel Replication Learning of Option Pricing (RLOP) approach. Both methods are fully compatible with standard reinforcement learning algorithms and operate under market frictions. Using SPY and XOP option data, we evaluate performance along static and dynamic dimensions. Adaptive-QLBS achieves higher static pricing accuracy in implied volatility space, while RLOP delivers superior dynamic hedging performance by reducing shortfall probability. These results highlight the importance of evaluating option pricing models beyond static fit, emphasizing realized hedging outcomes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [157] [Horizon Reduction as Information Loss in Offline Reinforcement Learning](https://arxiv.org/abs/2601.00831)
*Uday Kumar Nidadala,Venkata Bhumika Guthi*

Main category: cs.LG

TL;DR: 论文证明在离线强化学习中，视野缩减（horizon reduction）可能导致不可恢复的信息损失，即使有无限数据和完美函数逼近，最优策略也可能与次优策略在统计上无法区分。


<details>
  <summary>Details</summary>
Motivation: 尽管经验证据表明视野缩减可以改善离线强化学习的扩展性，但其理论影响尚未充分发展。本文旨在揭示视野缩减可能导致的基本信息损失问题。

Method: 将视野缩减形式化为从固定长度轨迹片段学习，并通过最小反例马尔可夫决策过程（MDPs）识别三种结构失效模式：前缀不可区分性、截断回报导致的目标错误指定、以及离线数据集支持和表示混叠。

Result: 证明在固定长度轨迹片段的学习范式下，最优策略可能与次优策略在统计上无法区分，即使有无限数据和完美函数逼近。建立了视野缩减安全应用的必要条件。

Conclusion: 视野缩减在离线强化学习中存在固有局限性，这些限制无法仅通过算法改进克服。研究结果补充了关于保守目标和分布偏移的算法工作，为安全使用视野缩减提供了理论指导。

Abstract: Horizon reduction is a common design strategy in offline reinforcement learning (RL), used to mitigate long-horizon credit assignment, improve stability, and enable scalable learning through truncated rollouts, windowed training, or hierarchical decomposition (Levine et al., 2020; Prudencio et al., 2023; Park et al., 2025). Despite recent empirical evidence that horizon reduction can improve scaling on challenging offline RL benchmarks, its theoretical implications remain underdeveloped (Park et al., 2025). In this paper, we show that horizon reduction can induce fundamental and irrecoverable information loss in offline RL. We formalize horizon reduction as learning from fixed-length trajectory segments and prove that, under this paradigm and any learning interface restricted to fixed-length trajectory segments, optimal policies may be statistically indistinguishable from suboptimal ones even with infinite data and perfect function approximation. Through a set of minimal counterexample Markov decision processes (MDPs), we identify three distinct structural failure modes: (i) prefix indistinguishability leading to identifiability failure, (ii) objective misspecification induced by truncated returns, and (iii) offline dataset support and representation aliasing. Our results establish necessary conditions under which horizon reduction can be safe and highlight intrinsic limitations that cannot be overcome by algorithmic improvements alone, complementing algorithmic work on conservative objectives and distribution shift that addresses a different axis of offline RL difficulty (Fujimoto et al., 2019; Kumar et al., 2020; Gulcehre et al., 2020).

</details>


### [158] [ShrimpXNet: A Transfer Learning Framework for Shrimp Disease Classification with Augmented Regularization, Adversarial Training, and Explainable AI](https://arxiv.org/abs/2601.00832)
*Israk Hasan Jone,D. M. Rafiun Bin Masud,Promit Sarker,Sayed Fuad Al Labib,Nazmul Islam,Farhad Billah*

Main category: cs.LG

TL;DR: 该研究提出基于深度学习的虾病自动分类方法，使用6种预训练模型在包含1149张图像的4类疾病数据集上进行评估，ConvNeXt-Tiny模型取得最佳性能（96.88%准确率）。


<details>
  <summary>Details</summary>
Motivation: 虾养殖是全球重要的水产养殖产业，但疾病爆发严重威胁其可持续发展。传统疾病检测方法存在时效性和准确性不足的问题，需要自动化分类方法提供及时准确的疾病检测。

Method: 使用包含1149张图像的4类疾病数据集，部署ResNet50、EfficientNet、DenseNet201、MobileNet、ConvNeXt-Tiny和Xception六种预训练深度学习模型。采用背景去除和Keras图像管道标准化预处理，使用FGSM进行对抗训练增强模型鲁棒性，应用CutMix和MixUp数据增强策略防止过拟合，并采用Grad-CAM、Grad-CAM++和XGrad-CAM等后解释方法进行可视化分析。

Result: ConvNeXt-Tiny模型在测试集上取得最高性能，达到96.88%的准确率。经过1000次迭代后，模型99%的置信区间为[0.953,0.971]，显示出良好的分类性能。

Conclusion: 深度学习模型能够有效实现虾病的自动分类，ConvNeXt-Tiny在该任务中表现最佳。该方法为虾养殖业的疾病监测提供了可行的自动化解决方案，有助于提高疾病检测的及时性和准确性。

Abstract: Shrimp is one of the most widely consumed aquatic species globally, valued for both its nutritional content and economic importance. Shrimp farming represents a significant source of income in many regions; however, like other forms of aquaculture, it is severely impacted by disease outbreaks. These diseases pose a major challenge to sustainable shrimp production. To address this issue, automated disease classification methods can offer timely and accurate detection. This research proposes a deep learning-based approach for the automated classification of shrimp diseases. A dataset comprising 1,149 images across four disease classes was utilized. Six pretrained deep learning models, ResNet50, EfficientNet, DenseNet201, MobileNet, ConvNeXt-Tiny, and Xception were deployed and evaluated for performance. The images background was removed, followed by standardized preprocessing through the Keras image pipeline. Fast Gradient Sign Method (FGSM) was used for enhancing the model robustness through adversarial training. While advanced augmentation strategies, including CutMix and MixUp, were implemented to mitigate overfitting and improve generalization. To support interpretability, and to visualize regions of model attention, post-hoc explanation methods such as Grad-CAM, Grad-CAM++, and XGrad-CAM were applied. Exploratory results demonstrated that ConvNeXt-Tiny achieved the highest performance, attaining a 96.88% accuracy on the test dataset. After 1000 iterations, the 99% confidence interval for the model is [0.953,0.971].

</details>


### [159] [Intrinsic-Metric Physics-Informed Neural Networks (IM-PINN) for Reaction-Diffusion Dynamics on Complex Riemannian Manifolds](https://arxiv.org/abs/2601.00834)
*Julian Evan Chrisnanto,Salsabila Rahma Alia,Nurfauzi Fadillah,Yulison Herry Chrisnanto*

Main category: cs.LG

TL;DR: IM-PINN：一种无需网格的几何深度学习框架，通过将黎曼度量张量嵌入自动微分图，直接在连续参数域求解复杂流形上的非线性反应扩散方程，解决了传统方法在极端曲率表面上的计算难题。


<details>
  <summary>Details</summary>
Motivation: 在复杂非欧几里得流形上模拟非线性反应扩散动力学面临两大挑战：高保真网格生成成本高昂，以及离散时间步进方案中的辛漂移问题。传统自适应细化方法无法解析具有极端高斯曲率波动的各向异性图灵不稳定性。

Method: 提出内在度量物理信息神经网络（IM-PINN），通过将黎曼度量张量嵌入自动微分图，解析重建拉普拉斯-贝尔特拉米算子，将解复杂度与几何离散化解耦。采用双流架构和傅里叶特征嵌入来缓解谱偏差。

Result: 在具有极端高斯曲率波动（K∈[-2489,3580]）的"随机布料"流形上验证，IM-PINN恢复了Gray-Scott模型的"分裂斑点"和"迷宫"模式。相比表面有限元法（SFEM），IM-PINN实现了更好的全局质量守恒（误差0.157 vs 0.258），消除了半隐式积分固有的质量漂移。

Conclusion: IM-PINN提供了一个内存高效、分辨率无关的范式，用于在演化表面上模拟生物模式形成，弥合了微分几何和物理信息机器学习之间的鸿沟，作为热力学一致的全局求解器优于传统方法。

Abstract: Simulating nonlinear reaction-diffusion dynamics on complex, non-Euclidean manifolds remains a fundamental challenge in computational morphogenesis, constrained by high-fidelity mesh generation costs and symplectic drift in discrete time-stepping schemes. This study introduces the Intrinsic-Metric Physics-Informed Neural Network (IM-PINN), a mesh-free geometric deep learning framework that solves partial differential equations directly in the continuous parametric domain. By embedding the Riemannian metric tensor into the automatic differentiation graph, our architecture analytically reconstructs the Laplace-Beltrami operator, decoupling solution complexity from geometric discretization. We validate the framework on a "Stochastic Cloth" manifold with extreme Gaussian curvature fluctuations ($K \in [-2489, 3580]$), where traditional adaptive refinement fails to resolve anisotropic Turing instabilities. Using a dual-stream architecture with Fourier feature embeddings to mitigate spectral bias, the IM-PINN recovers the "splitting spot" and "labyrinthine" regimes of the Gray-Scott model. Benchmarking against the Surface Finite Element Method (SFEM) reveals superior physical rigor: the IM-PINN achieves global mass conservation error of $\mathcal{E}_{mass} \approx 0.157$ versus SFEM's $0.258$, acting as a thermodynamically consistent global solver that eliminates mass drift inherent in semi-implicit integration. The framework offers a memory-efficient, resolution-independent paradigm for simulating biological pattern formation on evolving surfaces, bridging differential geometry and physics-informed machine learning.

</details>


### [160] [SLO-Conditioned Action Routing for Retrieval-Augmented Generation: Objective Ablation and Failure Modes](https://arxiv.org/abs/2601.00841)
*Bharath Nunepalli*

Main category: cs.LG

TL;DR: 该论文研究了RAG系统中的查询级控制问题，提出通过选择检索深度和生成模式（防护vs自动）或拒绝来满足成本、拒绝率和幻觉风险等SLO目标，并通过离线数据集评估了两种简单的策略学习方法。


<details>
  <summary>Details</summary>
Motivation: RAG系统面临一个实际的控制问题：需要为每个查询选择适当的检索深度和生成行为，以满足成本、拒绝率和幻觉风险等服务级别目标（SLOs）。当前缺乏对这类控制问题的系统性研究。

Method: 将每个查询的控制建模为离散动作：选择检索深度和生成模式（防护vs自动），或拒绝。使用SQuAD 2.0构建离线日志数据集，记录每个动作的准确性、token成本、幻觉/拒绝指标和SLO加权奖励。评估两种简单的策略学习目标：基于每个状态最佳动作的监督分类（Argmax-CE）及其奖励加权变体（Argmax-CE-WT）。

Result: 在评估的设置中，一个固定的强基线（低k、防护提示）表现具有竞争力；学习到的策略主要在质量导向的SLO下提供额外的成本节省，而在廉价SLO下当拒绝被高度奖励时可能出现拒绝崩溃现象。

Conclusion: 本文提供了一个可复现的RAG管道SLO感知控制案例研究，强调失败模式和报告规范，而不是提出新的检索器或语言模型。研究揭示了简单策略学习方法的局限性和实际部署中的挑战。

Abstract: Retrieval-augmented generation (RAG) introduces a practical control problem: retrieval depth and generation behavior must be chosen per query to satisfy service-level objectives (SLOs) such as cost, refusal rate, and hallucination risk. This work models per-query control as a small discrete action: choose a retrieval depth and a generation mode (guarded vs. auto), or refuse. An offline logged dataset is constructed from SQuAD 2.0 by executing each action and recording accuracy, token cost, hallucination/refusal indicators, and an SLO-weighted reward. Two simple policy-learning objectives are evaluated: supervised classification of the per-state best action (Argmax-CE) and a reward-weighted variant (Argmax-CE-WT). Across the evaluated settings, a strong fixed baseline (low k, guarded prompting) performs competitively; learned policies mainly provide additional cost savings under a quality-focused SLO and can exhibit refusal collapse under a cheap SLO when refusal is heavily rewarded. The contribution is a reproducible case study of SLO-aware control for RAG pipelines, emphasizing failure modes and reporting conventions rather than proposing a new retriever or language model.

</details>


### [161] [Value-guided action planning with JEPA world models](https://arxiv.org/abs/2601.00844)
*Matthieu Destrade,Oumayma Bounou,Quentin Le Lidec,Jean Ponce,Yann LeCun*

Main category: cs.LG

TL;DR: 提出一种增强JEPA世界模型规划能力的方法，通过塑造表示空间，使负目标条件值函数近似于状态嵌入间的距离，从而显著提升规划性能。


<details>
  <summary>Details</summary>
Motivation: 虽然JEPA框架能通过自监督预测目标学习环境动态表示，但其支持有效行动规划的能力有限。需要增强JEPA世界模型的规划能力。

Method: 通过塑造JEPA的表示空间，使特定环境中到达成本的负目标条件值函数近似于状态嵌入间的距离（或准距离）。提出了一种在训练中强制执行此约束的实用方法。

Result: 在简单控制任务上，该方法相比标准JEPA模型显著提升了规划性能。

Conclusion: 通过约束表示空间使值函数近似于嵌入距离，可以有效增强JEPA世界模型的规划能力，为构建能推理环境动态的深度学习模型提供了改进方向。

Abstract: Building deep learning models that can reason about their environment requires capturing its underlying dynamics. Joint-Embedded Predictive Architectures (JEPA) provide a promising framework to model such dynamics by learning representations and predictors through a self-supervised prediction objective. However, their ability to support effective action planning remains limited. We propose an approach to enhance planning with JEPA world models by shaping their representation space so that the negative goal-conditioned value function for a reaching cost in a given environment is approximated by a distance (or quasi-distance) between state embeddings. We introduce a practical method to enforce this constraint during training and show that it leads to significantly improved planning performance compared to standard JEPA models on simple control tasks.

</details>


### [162] [You Only Need Your Transformer 25% of the Time: Meaning-First Execution for Eliminating Unnecessary Inference](https://arxiv.org/abs/2601.00847)
*Ryan Shamim*

Main category: cs.LG

TL;DR: MFEE框架将推理重构为控制平面决策问题，通过语义分析选择性执行transformer，在保持100%准确率的同时减少78.1%的计算量。


<details>
  <summary>Details</summary>
Motivation: 当前AI推理系统将transformer执行视为强制性的，混淆了模型能力与执行必要性。需要区分何时必须执行transformer，何时可以通过替代路径保持正确性。

Method: 提出Meaning-First Execution (MFEE)控制平面架构，作为现有堆栈之上的门控层，不修改模型、权重或参数。通过语义分析决定何时需要执行transformer推理。

Result: 在1000个多样化提示的确定性解码下，MFEE实现78.1%的执行减少，同时保持100%的精确匹配等价性。相比基于模式的路由器最多53.3%的避免率且有错误，MFEE通过语义分析达到100%避免且零错误。

Conclusion: 证明了仅基于有限特征图的路由器无法同时保证零错误跳过和正避免率。执行治理应成为ML系统基础设施的基础层，与模型级优化技术正交。

Abstract: Modern AI inference systems treat transformer execution as mandatory, conflating model capability with execution necessity. We reframe inference as a control-plane decision problem: determining when execution is necessary versus when correctness can be preserved through alternative pathways. We introduce Meaning-First Execution (MFEE), a control-plane architecture implementing this framework, selectively invoking transformer inference only when required. MFEE operates as a gating layer above existing stacks without modifying models, weights, or parameters. Across 1,000 diverse prompts under deterministic decoding, MFEE achieves 78.1% execution reduction while maintaining 100% exact-match equivalence for invoked executions. Comparative evaluation reveals pattern-based routers achieve at most 53.3% avoidance with correctness failures, while MFEE reaches 100% avoidance with zero failures through semantic analysis. We prove this limitation via Theorem 1: any router operating solely on finite feature maps cannot simultaneously guarantee zero false skips and positive avoidance on feature-collision pairs. These results establish execution governance as a foundational layer in ML systems infrastructure, orthogonal to model-level optimization techniques.

</details>


### [163] [EdgeJury: Cross-Reviewed Small-Model Ensembles for Truthful Question Answering on Serverless Edge Inference](https://arxiv.org/abs/2601.00850)
*Aayush Kumar*

Main category: cs.LG

TL;DR: EdgeJury是一个轻量级集成框架，使用小型指令调优语言模型（3B-8B）通过四阶段协调流程提升问答的真实性和鲁棒性，特别适用于资源受限的边缘部署场景。


<details>
  <summary>Details</summary>
Motivation: 幻觉问题阻碍了可靠的问答系统，特别是在资源受限的边缘部署场景中，前沿规模模型或检索管道可能不切实际。需要一种轻量级解决方案来提升小型模型在问答任务中的真实性和鲁棒性。

Method: EdgeJury采用四阶段协调框架：1) 并行角色专业化生成；2) 匿名交叉评审，包含结构化批评和排名；3) 主席合成，整合最强内容并解决标记的问题；4) 基于模型间一致性的声明级一致性标记。该框架仅使用小型指令调优语言模型（3B-8B），适合无服务器边缘推理。

Result: 在TruthfulQA（MC1）上达到76.2%准确率，相比单个8B基线（62.8%）提升21.4%；在200个对抗性EdgeCases问题上获得48.2%的相对增益；人工分析显示事实性幻觉错误减少约55%；在Cloudflare Workers AI上部署实现8.4秒中位端到端延迟。

Conclusion: 协调的小型模型集成可以在不依赖外部检索或专有大模型API的情况下，显著提升在误解密集型问答基准上的真实性，为资源受限的边缘部署提供了可行的解决方案。

Abstract: Hallucinations hinder reliable question answering, especially in resource-constrained deployments where frontier-scale models or retrieval pipelines may be impractical. We present EdgeJury, a lightweight ensemble framework that improves truthfulness and robustness using only small instruction-tuned language models (3B-8B) suitable for serverless edge inference. EdgeJury orchestrates four stages: (1) parallel role-specialized generation, (2) anonymized cross-review with structured critiques and rankings, (3) chairman synthesis that integrates the strongest content while addressing flagged issues, and (4) claim-level consistency labeling based on inter-model agreement. On TruthfulQA (MC1), EdgeJury achieves 76.2% accuracy (95% CI: 72.8-79.6%), a +21.4% relative improvement over a single 8B baseline (62.8%), and outperforms standard baselines including self-consistency and majority voting under transparent compute accounting (total tokens and platform cost reported). On a 200-question adversarial EdgeCases set, EdgeJury yields +48.2% relative gains (95% CI: 44.0-52.4%). Manual analysis on 100 incorrect answers shows an approximately 55% reduction in factual hallucination errors versus the single-model baseline. Deployed on Cloudflare Workers AI, EdgeJury achieves 8.4 s median end-to-end latency, demonstrating that coordinated small-model ensembles can improve truthfulness on misconception-heavy QA benchmarks without external retrieval or proprietary large-model APIs.

</details>


### [164] [FedSCAM (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation): Scam-resistant SAM for Robust Federated Optimization in Heterogeneous Environments](https://arxiv.org/abs/2601.00853)
*Sameer Rahil,Zain Abdullah Ahmad,Talha Asif*

Main category: cs.LG

TL;DR: FedSCAM提出了一种联邦学习中基于客户端异质性动态调整SAM扰动半径和聚合权重的算法，通过异质性感知的加权聚合机制提升模型收敛速度和最终精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据的统计异质性（特别是非独立同分布标签分布）对模型收敛和泛化带来挑战。现有方法通常对所有客户端使用统一的SAM扰动半径，忽略了客户端特定的异质性差异。

Method: 提出FedSCAM算法：1) 为每个客户端计算异质性分数；2) 根据异质性分数反向调制SAM扰动半径，防止高方差客户端破坏全局模型稳定性；3) 引入异质性感知的加权聚合机制，优先考虑与全局优化方向一致的客户端更新。

Result: 在CIFAR-10和Fashion-MNIST数据集上，基于不同Dirichlet标签偏斜程度的实验表明，FedSCAM在收敛速度和最终测试精度方面优于FedSAM、FedLESAM等基线方法。

Conclusion: FedSCAM通过动态调整SAM扰动半径和异质性感知聚合，有效解决了联邦学习中的统计异质性问题，提升了模型性能和稳定性。

Abstract: Federated Learning (FL) enables collaborative model training across decentralized edge devices while preserving data privacy. However, statistical heterogeneity among clients, often manifested as non-IID label distributions, poses significant challenges to convergence and generalization. While Sharpness-Aware Minimization (SAM) has been introduced to FL to seek flatter, more robust minima, existing approaches typically apply a uniform perturbation radius across all clients, ignoring client-specific heterogeneity. In this work, we propose \textbf{FedSCAM} (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation), a novel algorithm that dynamically adjusts the SAM perturbation radius and aggregation weights based on client-specific heterogeneity scores. By calculating a heterogeneity metric for each client and modulating the perturbation radius inversely to this score, FedSCAM prevents clients with high variance from destabilizing the global model. Furthermore, we introduce a heterogeneity-aware weighted aggregation mechanism that prioritizes updates from clients that align with the global optimization direction. Extensive experiments on CIFAR-10 and Fashion-MNIST under various degrees of Dirichlet-based label skew demonstrate that FedSCAM achieves competitive performance among state-of-the-art baselines, including FedSAM, FedLESAM, etc. in terms of convergence speed and final test accuracy.

</details>


### [165] [Harvesting AlphaEarth: Benchmarking the Geospatial Foundation Model for Agricultural Downstream Tasks](https://arxiv.org/abs/2601.00857)
*Yuchi Ma,Yawen Shen,Anu Swatantran,David B. Lobell*

Main category: cs.LG

TL;DR: 评估AlphaEarth Foundation地理空间基础模型在农业监测任务中的表现，发现其在产量预测和耕作制图方面表现良好，但存在空间可迁移性、可解释性和时间敏感性等限制


<details>
  <summary>Details</summary>
Motivation: 虽然AlphaEarth Foundation等地理空间基础模型在土地覆盖分类任务中表现出色，但缺乏在农业监测关键下游任务中的深入评估，以及与基于传统遥感方法的模型的全面比较

Method: 在美国的三个农业下游任务中评估AEF嵌入：作物产量预测、耕作制图和覆盖作物制图。使用公共和私人来源的数据集，在不同尺度和位置进行综合评估，并训练基于遥感的模型作为对比

Result: AEF模型在所有任务中表现良好，在产量预测和县级耕作制图方面与专门构建的遥感模型竞争力相当。但发现AEF嵌入存在空间可迁移性有限、可解释性低和时间敏感性不足等限制

Conclusion: AEF嵌入在农业应用中需谨慎使用，特别是在时间敏感性、泛化能力和可解释性要求较高的场景中。虽然在某些任务中表现良好，但当前版本存在重要限制

Abstract: Geospatial foundation models (GFMs) have emerged as a promising approach to overcoming the limitations in existing featurization methods. More recently, Google DeepMind has introduced AlphaEarth Foundation (AEF), a GFM pre-trained using multi-source EOs across continuous time. An annual and global embedding dataset is produced using AEF that is ready for analysis and modeling. The internal experiments show that AEF embeddings have outperformed operational models in 15 EO tasks without re-training. However, those experiments are mostly about land cover and land use classification. Applying AEF and other GFMs to agricultural monitoring require an in-depth evaluation in critical agricultural downstream tasks. There is also a lack of comprehensive comparison between the AEF-based models and traditional remote sensing (RS)-based models under different scenarios, which could offer valuable guidance for researchers and practitioners. This study addresses some of these gaps by evaluating AEF embeddings in three agricultural downstream tasks in the U.S., including crop yield prediction, tillage mapping, and cover crop mapping. Datasets are compiled from both public and private sources to comprehensively evaluate AEF embeddings across tasks at different scales and locations, and RS-based models are trained as comparison models. AEF-based models generally exhibit strong performance on all tasks and are competitive with purpose-built RS-based models in yield prediction and county-level tillage mapping when trained on local data. However, we also find several limitations in current AEF embeddings, such as limited spatial transferability compared to RS-based models, low interpretability, and limited time sensitivity. These limitations recommend caution when applying AEF embeddings in agriculture, where time sensitivity, generalizability, and interpretability is important.

</details>


### [166] [Path Integral Solution for Dissipative Generative Dynamics](https://arxiv.org/abs/2601.00860)
*Xidi Wang*

Main category: cs.LG

TL;DR: 该论文证明纯机械系统通过耗散量子动力学和非局域上下文聚合可以生成智能语言，而守恒定律会导致根本性失败。语言生成本质上是耗散量子场论。


<details>
  <summary>Details</summary>
Motivation: 探究纯机械系统是否能够生成智能语言，理解语言生成的基本物理原理，特别是耗散与守恒在智能涌现中的作用。

Method: 使用Koopman算子和闭式路径积分传播子分析耗散量子动力学，通过谱分析揭示特征值结构（衰减模式、增长模式、中性模式），研究不可逆计算的基本要求。

Result: 证明耗散量子动力学能够产生连贯的文本生成，而哈密顿约束会消除耗散模式并导致性能下降。谱分析显示特征值结构分离为衰减、增长和中性模式，这是定向信息流的基本要素。

Conclusion: 语言生成本质上是耗散量子场论，机械系统通过耗散和非局域性的结合获得智能，而非通过守恒。不可逆计算需要受控信息耗散和因果上下文聚合。

Abstract: Can purely mechanical systems generate intelligent language? We prove that dissipative quantum dynamics with analytically tractable non-local context aggregation produce coherent text generation, while conservation laws cause fundamental failure. Employing Koopman operators with closed-form path integral propagators, we show irreversible computation fundamentally requires both controlled information dissipation and causal context aggregation. Spectral analysis reveals emergent eigenvalue structure, separating into decay modes (forgetting), growth modes (amplification), and neutral modes (preservation) -- the essential ingredients for directed information flow. Hamiltonian constraints force the elimination of these dissipative modes and degrading performance despite unchanged model capacity. This establishes language generation as dissipative quantum field theory, proving mechanical systems acquire intelligence through the combination of dissipation and non-locality, not through conservation.

</details>


### [167] [Universal Battery Degradation Forecasting Driven by Foundation Model Across Diverse Chemistries and Conditions](https://arxiv.org/abs/2601.00862)
*Joey Chan,Huan Wang,Haoyu Pan,Wei Wu,Zirong Wang,Zhen Chen,Ershun Pan,Min Xie,Lifeng Xi*

Main category: cs.LG

TL;DR: 提出一个统一的电池容量衰减预测框架，通过时间序列基础模型和参数高效微调，在多种化学体系和工况下保持鲁棒性能


<details>
  <summary>Details</summary>
Motivation: 电池容量衰减预测对储能系统的安全、可靠和长期效率至关重要，但由于不同电池化学体系、外形尺寸和运行条件的强异质性，难以构建一个能泛化到训练域之外的单一模型

Method: 整合20个公开老化数据集构建大规模语料库（1,704个电池，3,961,195个充放电循环段），采用时间序列基础模型（TSFM）作为骨干，结合参数高效的LoRA微调和物理引导的对比表示学习来捕捉共享的退化模式

Result: 单一统一模型在已见和刻意保留的未见数据集上均取得与强基线相当或更优的精度，同时在训练中排除的化学体系、容量尺度和运行条件下保持稳定性能

Conclusion: 基于TSFM的架构展示了作为可扩展和可迁移解决方案的潜力，可用于实际电池管理系统中的容量退化预测

Abstract: Accurate forecasting of battery capacity fade is essential for the safety, reliability, and long-term efficiency of energy storage systems. However, the strong heterogeneity across cell chemistries, form factors, and operating conditions makes it difficult to build a single model that generalizes beyond its training domain. This work proposes a unified capacity forecasting framework that maintains robust performance across diverse chemistries and usage scenarios. We curate 20 public aging datasets into a large-scale corpus covering 1,704 cells and 3,961,195 charge-discharge cycle segments, spanning temperatures from $-5\,^{\circ}\mathrm{C}$ to $45\,^{\circ}\mathrm{C}$, multiple C-rates, and application-oriented profiles such as fast charging and partial cycling. On this corpus, we adopt a Time-Series Foundation Model (TSFM) backbone and apply parameter-efficient Low-Rank Adaptation (LoRA) together with physics-guided contrastive representation learning to capture shared degradation patterns. Experiments on both seen and deliberately held-out unseen datasets show that a single unified model achieves competitive or superior accuracy compared with strong per-dataset baselines, while retaining stable performance on chemistries, capacity scales, and operating conditions excluded from training. These results demonstrate the potential of TSFM-based architectures as a scalable and transferable solution for capacity degradation forecasting in real battery management systems.

</details>


### [168] [Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery](https://arxiv.org/abs/2601.00863)
*Markus J. Buehler*

Main category: cs.LG

TL;DR: 论文提出materiomusic生成框架，将物质的层次结构与音乐创作逻辑相连接，通过可逆映射将分子振动、蜘蛛网结构等物理模式转化为音乐元素，探索科学与艺术在约束下的生成共性。


<details>
  <summary>Details</summary>
Motivation: 探索物质结构与音乐创作之间的深层联系，建立科学与艺术的统一框架，通过声音作为科学探针来理解物理现象，同时利用物理约束启发艺术创新。

Method: 使用可逆映射方法：1) 分子光谱映射到音调；2) 三维网络结构映射到可演奏乐器；3) 枚举所有2^12音乐音阶分析文化显著性；4) 采用群体智能AI模型作曲；5) 应用选择性不完美机制平衡连贯性与适应性。

Result: 发现文化显著的音乐系统聚集在中熵、中缺陷的走廊区域，与材料科学中的Hall-Petch最优缺陷密度直接对应；AI生成的音乐表现出类人的小世界连接性、模块化整合和长程连贯性等结构特征。

Conclusion: 科学与艺术都是在约束下的世界构建生成行为，振动作为跨尺度的共享语法组织结构；选择性不完美是平衡连贯性与适应性的机制；音乐与物质的映射创造了人类创造力与物理学的生产性碰撞。

Abstract: We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.

</details>


### [169] [Distribution Matching for Graph Quantification Under Structural Covariate Shift](https://arxiv.org/abs/2601.00864)
*Clemens Damke,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 本文提出了一种针对图数据的量化学习方法，通过结构重要性采样扩展KDEy方法，以处理训练和测试数据之间的结构偏移问题。


<details>
  <summary>Details</summary>
Motivation: 在社交网络等图数据中，传统量化学习方法基于先验概率偏移假设，但当训练和测试数据来自图的不同区域时，这种假设不成立，导致性能下降。需要专门处理图结构偏移的量化学习方法。

Method: 将结构重要性采样的思想扩展到最先进的KDEy量化方法中。通过考虑图的结构特性，调整训练和测试数据之间的分布差异，使模型能够适应图结构的变化。

Result: 提出的方法能够适应结构偏移，在存在图结构变化的情况下，性能优于标准的量化学习方法。

Conclusion: 通过将结构重要性采样与KDEy方法结合，成功解决了图数据量化学习中的结构偏移问题，为图环境下的标签分布估计提供了更有效的解决方案。

Abstract: Graphs are commonly used in machine learning to model relationships between instances. Consider the task of predicting the political preferences of users in a social network; to solve this task one should consider, both, the features of each individual user and the relationships between them. However, oftentimes one is not interested in the label of a single instance but rather in the distribution of labels over a set of instances; e.g., when predicting the political preferences of users, the overall prevalence of a given opinion might be of higher interest than the opinion of a specific person. This label prevalence estimation task is commonly referred to as quantification learning (QL). Current QL methods for tabular data are typically based on the so-called prior probability shift (PPS) assumption which states that the label-conditional instance distributions should remain equal across the training and test data. In the graph setting, PPS generally does not hold if the shift between training and test data is structural, i.e., if the training data comes from a different region of the graph than the test data. To address such structural shifts, an importance sampling variant of the popular adjusted count quantification approach has previously been proposed. In this work, we extend the idea of structural importance sampling to the state-of-the-art KDEy quantification approach. We show that our proposed method adapts to structural shifts and outperforms standard quantification approaches.

</details>


### [170] [A-PINN: Auxiliary Physics-informed Neural Networks for Structural Vibration Analysis in Continuous Euler-Bernoulli Beam](https://arxiv.org/abs/2601.00866)
*Shivani Saini,Ramesh Kumar Vats,Arup Kumar Sahoo*

Main category: cs.LG

TL;DR: 提出改进的辅助物理信息神经网络（A-PINN）框架，结合平衡自适应优化器，用于结构振动分析，在Euler-Bernoulli梁方程求解中相比基线模型提升至少40%性能。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络（PINNs）及其变体在求解微分方程控制的正反问题方面表现出色，但需要改进以更准确地表示结构系统、捕捉振动现象并确保可靠的预测分析。

Method: 提出改进的辅助物理信息神经网络（A-PINN）框架，结合平衡自适应优化器，专门针对结构振动问题设计。通过数值模拟评估模型在Euler-Bernoulli梁方程各种场景下的表现。

Result: 数值结果表明，该模型在数值稳定性和预测准确性方面均有显著提升，相比基线模型至少提高40%的性能。

Conclusion: 改进的A-PINN框架在结构振动问题分析中表现出优越性能，为科学机器学习模型在振动问题求解中的鲁棒性提供了深入见解。

Abstract: Recent advancements in physics-informed neural networks (PINNs) and their variants have garnered substantial focus from researchers due to their effectiveness in solving both forward and inverse problems governed by differential equations. In this research, a modified Auxiliary physics-informed neural network (A-PINN) framework with balanced adaptive optimizers is proposed for the analysis of structural vibration problems. In order to accurately represent structural systems, it is critical for capturing vibration phenomena and ensuring reliable predictive analysis. So, our investigations are crucial for gaining deeper insight into the robustness of scientific machine learning models for solving vibration problems. Further, to rigorously evaluate the performance of A-PINN, we conducted different numerical simulations to approximate the Euler-Bernoulli beam equations under the various scenarios. The numerical results substantiate the enhanced performance of our model in terms of both numerical stability and predictive accuracy. Our model shows improvement of at least 40% over the baselines.

</details>


### [171] [Hierarchical topological clustering](https://arxiv.org/abs/2601.00892)
*Ana Carpio,Gema Duro*

Main category: cs.LG

TL;DR: 提出一种分层拓扑聚类算法，适用于任意距离选择，能识别任意形状的聚类和异常值


<details>
  <summary>Details</summary>
Motivation: 拓扑方法能在不假设数据结构的情况下探索数据云，需要一种能处理任意形状聚类和异常值的聚类算法

Method: 提出分层拓扑聚类算法，可与任意距离度量结合使用，通过层次结构推断异常值和任意形状聚类的持久性

Result: 在图像、医疗和经济数据等包含相关异常值的数据集上展示了算法的潜力，这些情况下其他技术往往失效

Conclusion: 该方法能在其他技术失败的情况下提供有意义的聚类，展示了拓扑方法在探索复杂数据云中的价值

Abstract: Topological methods have the potential of exploring data clouds without making assumptions on their the structure. Here we propose a hierarchical topological clustering algorithm that can be implemented with any distance choice. The persistence of outliers and clusters of arbitrary shape is inferred from the resulting hierarchy. We demonstrate the potential of the algorithm on selected datasets in which outliers play relevant roles, consisting of images, medical and economic data. These methods can provide meaningful clusters in situations in which other techniques fail to do so.

</details>


### [172] [SmartFlow Reinforcement Learning and Agentic AI for Bike-Sharing Optimisation](https://arxiv.org/abs/2601.00868)
*Aditya Sreevatsa K,Arun Kumar Raveendran,Jesrael K Mani,Prakash G Shigli,Rajkumar Rangadore,Narayana Darapaneni,Anwesh Reddy Paduri*

Main category: cs.LG

TL;DR: SmartFlow是一个多层框架，结合强化学习和智能体AI解决城市共享单车动态再平衡问题，通过战略DQN学习策略、战术模块优化调度、通信层生成可执行指令，显著减少网络不平衡并降低运营成本。


<details>
  <summary>Details</summary>
Motivation: 解决城市共享单车系统中的动态再平衡问题，传统方法难以应对复杂动态环境，需要结合机器智能与人工操作的可扩展解决方案，减少闲置时间、提高单车可用性、降低运营成本。

Method: 多层框架：战略层使用DQN在纽约Citi Bike网络高保真模拟中学习再平衡策略；战术层确定性模块优化多段行程和调度；通信层基于LLM的智能体AI将物流计划转化为可执行指令。

Result: 在多次种子运行评估中，SmartFlow将网络不平衡减少超过95%，同时需要最小行驶距离，实现高卡车利用率，显著改善单车可用性并降低运营成本。

Conclusion: SmartFlow为复杂城市移动网络提供了可解释、AI驱动的物流蓝图，成功桥接机器智能与人工操作，展示了在动态城市环境中可扩展的智能物流解决方案。

Abstract: SmartFlow is a multi-layered framework that integrates Reinforcement Learning and Agentic AI to address the dynamic rebalancing problem in urban bike-sharing services. Its architecture separates strategic, tactical, and communication functions for clarity and scalability. At the strategic level, a Deep Q-Network (DQN) agent, trained in a high-fidelity simulation of New Yorks Citi Bike network, learns robust rebalancing policies by modelling the challenge as a Markov Decision Process. These high-level strategies feed into a deterministic tactical module that optimises multi-leg journeys and schedules just-in-time dispatches to minimise fleet travel. Evaluation across multiple seeded runs demonstrates SmartFlows high efficacy, reducing network imbalance by over 95% while requiring minimal travel distance and achieving strong truck utilisation. A communication layer, powered by a grounded Agentic AI with a Large Language Model (LLM), translates logistical plans into clear, actionable instructions for operational staff, ensuring interpretability and execution readiness. This integration bridges machine intelligence with human operations, offering a scalable solution that reduces idle time, improves bike availability, and lowers operational costs. SmartFlow provides a blueprint for interpretable, AI-driven logistics in complex urban mobility networks.

</details>


### [173] [Conformal Prediction Under Distribution Shift: A COVID-19 Natural Experiment](https://arxiv.org/abs/2601.00908)
*Chorok Lee*

Main category: cs.LG

TL;DR: 研究显示，在分布偏移下，保形预测的覆盖保证会下降。通过对8个供应链任务的分析发现，即使特征完全变化，覆盖下降程度差异巨大（0%-86.7%）。单特征依赖与灾难性失败高度相关，而稳健任务会将重要性分散到多个特征。季度重训练对脆弱任务有效，但对稳健任务无益。


<details>
  <summary>Details</summary>
Motivation: 保形预测在分布偏移下的性能保证会下降，但具体下降程度和机制尚不清楚。研究旨在通过COVID-19作为自然实验，分析供应链任务中分布偏移对保形预测的影响，并识别导致灾难性失败的因素。

Method: 使用COVID-19作为自然实验，分析8个供应链任务。通过SHAP分析特征重要性分布，计算特征集中度指标。比较不同任务在严重特征变化下的覆盖下降情况，并测试季度重训练的效果。扩展分析4个具有中等特征稳定性的任务。

Result: 1) 即使特征完全变化（Jaccard≈0），覆盖下降程度差异巨大（0%-86.7%）；2) 灾难性失败与单特征依赖高度相关（ρ=0.714, p=0.047）；3) 脆弱任务特征重要性集中度增加4.5倍，稳健任务分散到10-20个特征；4) 季度重训练可将脆弱任务覆盖从22%提升到41%，但对稳健任务无益（99.8%覆盖）。

Conclusion: 特征重要性集中度是预测保形预测在严重分布偏移下脆弱性的关键指标。提出了决策框架：部署前监控SHAP集中度；如果集中度>40%则进行季度重训练；如果任务稳健则可跳过重训练。特征稳定性（而非集中度）决定中等偏移下的稳健性。

Abstract: Conformal prediction guarantees degrade under distribution shift. We study this using COVID-19 as a natural experiment across 8 supply chain tasks. Despite identical severe feature turnover (Jaccard approximately 0), coverage drops vary from 0% to 86.7%, spanning two orders of magnitude. Using SHapley Additive exPlanations (SHAP) analysis, we find catastrophic failures correlate with single-feature dependence (rho = 0.714, p = 0.047). Catastrophic tasks concentrate importance in one feature (4.5x increase), while robust tasks redistribute across many (10-20x). Quarterly retraining restores catastrophic task coverage from 22% to 41% (+19 pp, p = 0.04), but provides no benefit for robust tasks (99.8% coverage). Exploratory analysis of 4 additional tasks with moderate feature stability (Jaccard 0.13-0.86) reveals feature stability, not concentration, determines robustness, suggesting concentration effects apply specifically to severe shifts. We provide a decision framework: monitor SHAP concentration before deployment; retrain quarterly if vulnerable (>40% concentration); skip retraining if robust.

</details>


### [174] [Quantum Machine Learning Approaches for Coordinated Stealth Attack Detection in Distributed Generation Systems](https://arxiv.org/abs/2601.00873)
*Osasumwen Cedric Ogiesoba-Eguakun,Suman Rath*

Main category: cs.LG

TL;DR: 量子机器学习在检测微电网分布式发电单元协同隐蔽攻击中的应用研究，混合量子经典模型在低维数据集上表现最佳


<details>
  <summary>Details</summary>
Motivation: 协同隐蔽攻击是分布式发电系统的严重网络安全威胁，它们修改控制和测量信号但保持接近正常行为，使得传统入侵检测方法难以发现。需要探索量子机器学习方法来提高检测能力。

Method: 使用高质量模拟测量数据创建平衡的二元分类数据集，包含三个特征：DG1的无功功率、相对于标称值的频率偏差和端电压幅值。评估了经典机器学习基线、完全量子变分分类器和混合量子经典模型。

Result: 混合量子经典模型（量子特征嵌入与经典RBF支持向量机结合）在低维数据集上取得了最佳整体性能，在准确率和F1分数上比经典SVM基线有适度提升。完全量子模型由于训练不稳定性和当前NISQ硬件的限制表现较差。

Conclusion: 混合模型训练更可靠，表明量子特征映射即使在完全量子学习尚不实用的情况下也能增强入侵检测能力。量子机器学习在网络安全领域具有应用潜力，但当前硬件限制使得混合方法更为实用。

Abstract: Coordinated stealth attacks are a serious cybersecurity threat to distributed generation systems because they modify control and measurement signals while remaining close to normal behavior, making them difficult to detect using standard intrusion detection methods. This study investigates quantum machine learning approaches for detecting coordinated stealth attacks on a distributed generation unit in a microgrid. High-quality simulated measurements were used to create a balanced binary classification dataset using three features: reactive power at DG1, frequency deviation relative to the nominal value, and terminal voltage magnitude. Classical machine learning baselines, fully quantum variational classifiers, and hybrid quantum classical models were evaluated. The results show that a hybrid quantum classical model combining quantum feature embeddings with a classical RBF support vector machine achieves the best overall performance on this low dimensional dataset, with a modest improvement in accuracy and F1 score over a strong classical SVM baseline. Fully quantum models perform worse due to training instability and limitations of current NISQ hardware. In contrast, hybrid models train more reliably and demonstrate that quantum feature mapping can enhance intrusion detection even when fully quantum learning is not yet practical.

</details>


### [175] [Revisiting Weighted Strategy for Non-stationary Parametric Bandits and MDPs](https://arxiv.org/abs/2601.01069)
*Jing Wang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种改进的分析框架，简化了非平稳参数化赌博机中加权策略的算法设计和理论分析，在线性赌博机中实现了更简单高效的算法，并将该框架扩展到广义线性赌博机、自协调赌博机和具有函数逼近的MDPs，获得了更好的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 非平稳参数化赌博机中，加权策略在处理渐变漂移模式时很常用，但之前的理论分析复杂且算法要么计算效率低要么统计次优。本文旨在解决加权策略分析复杂、算法设计繁琐的问题。

Method: 提出了一个精炼的分析框架，简化了加权策略的推导过程。在线性赌博机中，基于该框架设计了更简单的加权算法。然后将该框架扩展到广义线性赌博机、自协调赌博机以及具有函数逼近的马尔可夫决策过程（线性混合MDP和多项Logit混合MDP）。

Result: 在线性赌博机中，新算法与窗口/重启算法同样高效，且保持了相同的遗憾界。在广义线性赌博机中，获得了$\tilde{O}(k_μ^{5/4} c_μ^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$的遗憾界，优于之前的$\tilde{O}(k_μ^{2} c_μ^{-1}d^{9/10} P_T^{1/5}T^{4/5})$。该框架还成功应用于自协调赌博机和两类MDPs。

Conclusion: 提出的精炼分析框架显著简化了非平稳参数化赌博机中加权策略的理论分析和算法设计，在线性赌博机中实现了简单高效的算法，并在广义线性赌博机、自协调赌博机和具有函数逼近的MDPs中获得了改进的遗憾界。

Abstract: Non-stationary parametric bandits have attracted much attention recently. There are three principled ways to deal with non-stationarity, including sliding-window, weighted, and restart strategies. As many non-stationary environments exhibit gradual drifting patterns, the weighted strategy is commonly adopted in real-world applications. However, previous theoretical studies show that its analysis is more involved and the algorithms are either computationally less efficient or statistically suboptimal. This paper revisits the weighted strategy for non-stationary parametric bandits. In linear bandits (LB), we discover that this undesirable feature is due to an inadequate regret analysis, which results in an overly complex algorithm design. We propose a \emph{refined analysis framework}, which simplifies the derivation and, importantly, produces a simpler weight-based algorithm that is as efficient as window/restart-based algorithms while retaining the same regret as previous studies. Furthermore, our new framework can be used to improve regret bounds of other parametric bandits, including Generalized Linear Bandits (GLB) and Self-Concordant Bandits (SCB). For example, we develop a simple weighted GLB algorithm with an $\tilde{O}(k_μ^{5/4} c_μ^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$ regret, improving the $\tilde{O}(k_μ^{2} c_μ^{-1}d^{9/10} P_T^{1/5}T^{4/5})$ bound in prior work, where $k_μ$ and $c_μ$ characterize the reward model's nonlinearity, $P_T$ measures the non-stationarity, $d$ and $T$ denote the dimension and time horizon. Moreover, we extend our framework to non-stationary Markov Decision Processes (MDPs) with function approximation, focusing on Linear Mixture MDP and Multinomial Logit (MNL) Mixture MDP. For both classes, we propose algorithms based on the weighted strategy and establish dynamic regret guarantees using our analysis framework.

</details>


### [176] [LLMize: A Framework for Large Language Model-Based Numerical Optimization](https://arxiv.org/abs/2601.00874)
*M. Rizki Oktavian*

Main category: cs.LG

TL;DR: LLMize是一个开源Python框架，通过迭代提示和上下文学习实现LLM驱动的优化，将优化问题转化为自然语言生成候选解、外部评估、反馈改进的黑盒过程。


<details>
  <summary>Details</summary>
Motivation: 大语言模型展现出超越传统语言任务的推理能力，激发了将其用于数值优化的探索。传统优化方法需要数学编程或元启发式设计专业知识，而LLM可以通过自然语言注入约束和领域知识。

Method: 将优化问题转化为黑盒过程：用自然语言生成候选解，通过外部目标函数评估，利用解-分数反馈进行迭代改进。支持多种策略，包括优化提示（OPRO）以及受进化算法和模拟退火启发的混合LLM方法。

Result: 在凸优化、线性规划、旅行商问题、神经网络超参数调优和核燃料晶格优化等任务上评估。结果显示，对于简单问题，LLM优化不如经典求解器有竞争力，但对于约束和启发式难以形式化的复杂领域特定任务，提供了一种实用且易用的方法。

Conclusion: LLMize为复杂、领域特定的优化问题提供了一个实用且易用的框架，能够通过自然语言直接注入约束和领域知识，降低了优化问题的门槛，特别适用于难以形式化的复杂任务。

Abstract: Large language models (LLMs) have recently shown strong reasoning capabilities beyond traditional language tasks, motivating their use for numerical optimization. This paper presents LLMize, an open-source Python framework that enables LLM-driven optimization through iterative prompting and in-context learning. LLMize formulates optimization as a black-box process in which candidate solutions are generated in natural language, evaluated by an external objective function, and refined over successive iterations using solution-score feedback. The framework supports multiple optimization strategies, including Optimization by Prompting (OPRO) and hybrid LLM-based methods inspired by evolutionary algorithms and simulated annealing. A key advantage of LLMize is the ability to inject constraints, rules, and domain knowledge directly through natural language descriptions, allowing practitioners to define complex optimization problems without requiring expertise in mathematical programming or metaheuristic design. LLMize is evaluated on convex optimization, linear programming, the Traveling Salesman Problem, neural network hyperparameter tuning, and nuclear fuel lattice optimization. Results show that while LLM-based optimization is not competitive with classical solvers for simple problems, it provides a practical and accessible approach for complex, domain-specific tasks where constraints and heuristics are difficult to formalize.

</details>


### [177] [Wittgenstein's Family Resemblance Clustering Algorithm](https://arxiv.org/abs/2601.01127)
*Golbahar Amanpour,Benyamin Ghojogh*

Main category: cs.LG

TL;DR: 基于维特根斯坦家族相似性哲学概念提出的新型聚类算法，无需预设簇数量或形状假设


<details>
  <summary>Details</summary>
Motivation: 将维特根斯坦的家族相似性哲学概念（来自《哲学研究》）应用于机器学习聚类问题，该概念认为类别成员通过重叠相似性而非单一定义属性连接，自然适合图学习方法

Method: 提出WFR（维特根斯坦家族相似性）聚类算法及其核变体kernel WFR：计算相邻数据实例间的相似度得分，阈值处理后构建相似图，图的连通分量形成最终聚类

Result: 在基准数据集上的模拟实验表明，WFR是一种有效的非线性聚类算法，无需预先知道簇数量或对其形状做出假设

Conclusion: 成功将哲学概念转化为实用的机器学习算法，为聚类问题提供了新的图基方法，特别适用于复杂形状和非线性数据分布的场景

Abstract: This paper, introducing a novel method in philomatics, draws on Wittgenstein's concept of family resemblance from analytic philosophy to develop a clustering algorithm for machine learning. According to Wittgenstein's Philosophical Investigations (1953), family resemblance holds that members of a concept or category are connected by overlapping similarities rather than a single defining property. Consequently, a family of entities forms a chain of items sharing overlapping traits. This philosophical idea naturally lends itself to a graph-based approach in machine learning. Accordingly, we propose the Wittgenstein's Family Resemblance (WFR) clustering algorithm and its kernel variant, kernel WFR. This algorithm computes resemblance scores between neighboring data instances, and after thresholding these scores, a resemblance graph is constructed. The connected components of this graph define the resulting clusters. Simulations on benchmark datasets demonstrate that WFR is an effective nonlinear clustering algorithm that does not require prior knowledge of the number of clusters or assumptions about their shapes.

</details>


### [178] [LearnAD: Learning Interpretable Rules for Brain Networks in Alzheimer's Disease Classification](https://arxiv.org/abs/2601.00877)
*Thomas Andrews,Mark Law,Sara Ahmadi-Abhari,Alessandra Russo*

Main category: cs.LG

TL;DR: LearnAD是一种神经符号方法，用于从脑部MRI数据预测阿尔茨海默病，学习完全可解释的规则，性能接近黑盒模型但保持完全可解释性。


<details>
  <summary>Details</summary>
Motivation: 在临床神经科学中，需要既能准确预测阿尔茨海默病又能提供可解释规则的模型，以理解GNN等黑盒模型的行为并加深对疾病机制的理解。

Method: 使用统计模型、决策树、随机森林或GNN识别相关脑连接，然后应用FastLAS学习全局规则，形成神经符号混合方法。

Result: 最佳实例优于决策树，匹配支持向量机准确率，性能略低于随机森林和GNN，但保持完全可解释性；消融研究显示神经符号方法在保持可比性能的同时提高了可解释性。

Conclusion: LearnAD展示了符号学习如何加深对临床神经科学中GNN行为的理解，为开发既准确又可解释的阿尔茨海默病预测模型提供了有效途径。

Abstract: We introduce LearnAD, a neuro-symbolic method for predicting Alzheimer's disease from brain magnetic resonance imaging data, learning fully interpretable rules. LearnAD applies statistical models, Decision Trees, Random Forests, or GNNs to identify relevant brain connections, and then employs FastLAS to learn global rules. Our best instance outperforms Decision Trees, matches Support Vector Machine accuracy, and performs only slightly below Random Forests and GNNs trained on all features, all while remaining fully interpretable. Ablation studies show that our neuro-symbolic approach improves interpretability with comparable performance to pure statistical models. LearnAD demonstrates how symbolic learning can deepen our understanding of GNN behaviour in clinical neuroscience.

</details>


### [179] [Sparse Bayesian Message Passing under Structural Uncertainty](https://arxiv.org/abs/2601.01207)
*Yoonhyuk Choi,Jiho Choi,Chanran Kim,Yumin Lee,Hawon Shin,Yeowon Jeon,Minjeong Kim,Jiwoo Kang*

Main category: cs.LG

TL;DR: 提出一种基于贝叶斯后验分布建模的稀疏符号消息传递网络，通过建模带符号邻接矩阵的后验分布来处理异质图结构中的边噪声和异质性。


<details>
  <summary>Details</summary>
Motivation: 现实世界图中的半监督学习常面临异质性挑战，现有图神经网络要么依赖固定邻接结构，要么通过正则化处理结构噪声，缺乏对结构不确定性的显式建模。

Method: 1) 建模带符号邻接矩阵的后验分布，每条边可以是正、负或不存在；2) 提出稀疏符号消息传递网络，从贝叶斯视角解释其鲁棒性；3) 结合后验边缘化和稀疏符号消息聚合。

Result: 在异质性基准测试中，该方法在合成和真实世界结构噪声下均优于强基线模型。

Conclusion: 通过显式建模结构不确定性，该方法为处理边噪声和异质性提供了原则性方法，稀疏符号消息传递网络在异质图上表现出色。

Abstract: Semi-supervised learning on real-world graphs is frequently challenged by heterophily, where the observed graph is unreliable or label-disassortative. Many existing graph neural networks either rely on a fixed adjacency structure or attempt to handle structural noise through regularization. In this work, we explicitly capture structural uncertainty by modeling a posterior distribution over signed adjacency matrices, allowing each edge to be positive, negative, or absent. We propose a sparse signed message passing network that is naturally robust to edge noise and heterophily, which can be interpreted from a Bayesian perspective. By combining (i) posterior marginalization over signed graph structures with (ii) sparse signed message aggregation, our approach offers a principled way to handle both edge noise and heterophily. Experimental results demonstrate that our method outperforms strong baseline models on heterophilic benchmarks under both synthetic and real-world structural noise.

</details>


### [180] [Outlier Detection Using Vector Cosine Similarity by Adding a Dimension](https://arxiv.org/abs/2601.00883)
*Zhongyang Shen*

Main category: cs.LG

TL;DR: 提出基于向量余弦相似度的多维异常检测方法，通过添加零值维度构建新数据集，比较观测点到测量点及其他点的向量相似度来识别异常


<details>
  <summary>Details</summary>
Motivation: 现有多维异常检测方法可能复杂或效果有限，需要更简单有效的技术来识别高维数据中的异常点

Method: 1) 在原始数据中添加零值维度构建新数据集；2) 选择测量点并创建观测点（原点，仅在新维度有非零值）；3) 形成从观测点到测量点及其他点的向量；4) 比较这些向量的余弦相似度来检测异常

Result: 开发了MDOD优化实现并在PyPI发布，提供了一种新的多维异常检测工具

Conclusion: 该方法通过创新的向量相似度比较机制，为多维异常检测提供了新的有效解决方案

Abstract: We propose a new outlier detection method for multi-dimensional data. The method detects outliers based on vector cosine similarity, using a new dataset constructed by adding a dimension with zero values to the original data. When a point in the new dataset is selected as the measured point, an observation point is created as the origin, differing only in the new dimension by having a non-zero value compared to the measured point. Vectors are then formed from the observation point to the measured point and to other points in the dataset. By comparing the cosine similarities of these vectors, abnormal data can be identified. An optimized implementation (MDOD) is available on PyPI: https://pypi.org/project/mdod/.

</details>


### [181] [Adaptive Conformal Prediction via Bayesian Uncertainty Weighting for Hierarchical Healthcare Data](https://arxiv.org/abs/2601.01223)
*Marzieh Amiri Shahbazi,Ali Baheri,Nasibeh Azadeh-Fard*

Main category: cs.LG

TL;DR: 提出混合贝叶斯-保形框架，结合贝叶斯层次随机森林与组感知保形校准，为临床决策提供分布自由的覆盖保证和风险自适应精度。


<details>
  <summary>Details</summary>
Motivation: 临床决策需要不确定性量化，既要有分布自由的覆盖保证，又要有风险自适应的精度，而现有方法无法同时满足这两个要求。

Method: 集成贝叶斯层次随机森林与组感知保形校准，使用后验不确定性对保形分数进行加权，同时保持严格的覆盖有效性。

Result: 在61,538例入院患者（3,793家美国医院，4个地区）评估中，达到目标覆盖率（94.3% vs 95%目标），低不确定性病例区间宽度减少21%，高风险预测适当加宽。纯贝叶斯不确定性严重覆盖不足（14.1%）。

Conclusion: 该框架支持风险分层临床协议、高效资源规划（高置信度预测）和保守分配（不确定病例），为多样化医疗环境提供不确定性感知决策支持。

Abstract: Clinical decision-making demands uncertainty quantification that provides both distribution-free coverage guarantees and risk-adaptive precision, requirements that existing methods fail to jointly satisfy. We present a hybrid Bayesian-conformal framework that addresses this fundamental limitation in healthcare predictions. Our approach integrates Bayesian hierarchical random forests with group-aware conformal calibration, using posterior uncertainties to weight conformity scores while maintaining rigorous coverage validity. Evaluated on 61,538 admissions across 3,793 U.S. hospitals and 4 regions, our method achieves target coverage (94.3% vs 95% target) with adaptive precision: 21% narrower intervals for low-uncertainty cases while appropriately widening for high-risk predictions. Critically, we demonstrate that well-calibrated Bayesian uncertainties alone severely under-cover (14.1%), highlighting the necessity of our hybrid approach. This framework enables risk-stratified clinical protocols, efficient resource planning for high-confidence predictions, and conservative allocation with enhanced oversight for uncertain cases, providing uncertainty-aware decision support across diverse healthcare settings.

</details>


### [182] [FANoS: Friction-Adaptive Nosé--Hoover Symplectic Momentum for Stiff Objectives](https://arxiv.org/abs/2601.00889)
*Nalin Dhiman*

Main category: cs.LG

TL;DR: FANoS是一种受物理启发的优化器，结合了二阶动力系统、Nosé-Hoover恒温器和半隐式积分器，在某些非凸问题上表现良好，但总体上不如现代基线方法。


<details>
  <summary>Details</summary>
Motivation: 将分子动力学中的结构保持积分和恒温器思想应用于优化问题，开发一种物理启发的优化启发式方法。

Method: 结合：(1) 离散化二阶动力系统的动量更新，(2) 使用动能反馈调整标量摩擦系数的Nosé-Hoover类恒温器变量，(3) 半隐式（辛欧拉）积分器，可选对角RMS预处理器。

Result: 在Rosenbrock-100D基准测试中，FANoS-RMS（1.74×10⁻²）优于未剪裁的AdamW（48.50）和SGD+momentum（90.76），但不如梯度剪裁的AdamW（1.87×10⁻³）和L-BFGS（≈4.4×10⁻¹⁰）。在病态凸二次问题和PINN预热任务中表现不佳。

Conclusion: FANoS是现有思想的可解释综合，在某些刚性非凸谷问题上可能有帮助，但不是现代基线的普遍优越替代品，其行为对温度调度和超参数选择敏感。

Abstract: We study a physics-inspired optimizer, \emph{FANoS} (Friction-Adaptive Nosé--Hoover Symplectic momentum), which combines (i) a momentum update written as a discretized second-order dynamical system, (ii) a Nosé--Hoover-like thermostat variable that adapts a scalar friction coefficient using kinetic-energy feedback, and (iii) a semi-implicit (symplectic-Euler) integrator, optionally with a diagonal RMS preconditioner. The method is motivated by structure-preserving integration and thermostat ideas from molecular dynamics, but is used here purely as an optimization heuristic.
  We provide the algorithm and limited theoretical observations in idealized settings. On the deterministic Rosenbrock-100D benchmark with 3000 gradient evaluations, FANoS-RMS attains a mean final objective value of $1.74\times 10^{-2}$, improving substantially over unclipped AdamW ($48.50$) and SGD+momentum ($90.76$) in this protocol. However, AdamW with gradient clipping is stronger, reaching $1.87\times 10^{-3}$, and L-BFGS reaches $\approx 4.4\times 10^{-10}$. On ill-conditioned convex quadratics and in a small PINN warm-start suite (Burgers and Allen--Cahn), the default FANoS configuration underperforms AdamW and can be unstable or high-variance.
  Overall, the evidence supports a conservative conclusion: FANoS is an interpretable synthesis of existing ideas that can help on some stiff nonconvex valleys, but it is not a generally superior replacement for modern baselines, and its behavior is sensitive to temperature-schedule and hyperparameter choices.

</details>


### [183] [When to Ponder: Adaptive Compute Allocation for Code Generation via Test-Time Training](https://arxiv.org/abs/2601.00894)
*Gihyeon Sim*

Main category: cs.LG

TL;DR: PonderTTT：一种基于自监督重建损失的门控策略，用于选择性触发测试时训练更新，无需训练，仅需单个标量阈值，在代码语言建模中显著优于随机跳过基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对所有输入应用统一计算，无论难度如何。这可能导致计算资源浪费，特别是对于简单输入。需要一种智能机制来选择性触发测试时训练更新，以优化计算效率。

Method: 提出PonderTTT门控策略，利用TTT层的自监督重建损失来决定是否触发测试时训练更新。门控决策本身无需训练，不需要学习分类器或辅助网络，仅需在未标记数据上初始校准单个标量阈值，并通过指数移动平均持续调整以维持目标更新率。

Result: 在GPT-2模型（124M到1.5B参数）上的代码语言建模实验（The Stack v2数据集，教师强制困惑度）显示，该方法无需真实标签，重建门控达到82-89%的Oracle恢复率，完全无需训练，显著优于随机跳过基线（在OOD语言上损失降低达16%）。

Conclusion: PonderTTT提供了一种高效、无需训练的门控机制，能够智能选择性地触发测试时训练更新，在保持性能的同时优化计算资源使用，特别适用于代码语言建模等任务。

Abstract: Large language models apply uniform computation to all inputs, regardless of difficulty. We propose PonderTTT, a gating strategy using the TTT layer's self-supervised reconstruction loss to selectively trigger Test-Time Training (TTT) updates. The gating decision itself is training-free--requiring no learned classifier or auxiliary networks; only a single scalar threshold is initially calibrated on unlabeled data and continuously adapted via EMA to maintain target update rates. Our experiments with GPT-2 models (124M to 1.5B) on code language modeling (The Stack v2, teacher-forced perplexity) demonstrate that this signal is inference-compatible, requiring no ground-truth labels. Our Reconstruction Gating achieves 82-89% Oracle Recovery while being fully training-free, significantly outperforming Random Skip baselines (up to 16% lower loss on OOD languages).

</details>


### [184] [Improving Variational Autoencoder using Random Fourier Transformation: An Aviation Safety Anomaly Detection Case-Study](https://arxiv.org/abs/2601.01016)
*Ata Akbari Asanjan,Milad Memarzadeh,Bryan Matthews,Nikunj Oza*

Main category: cs.LG

TL;DR: 该研究探讨了在自编码器和变分自编码器中使用随机傅里叶变换改进训练和推理，通过频率原理分析发现RFT模型能同时学习高低频特征，而传统DNN只能从低频开始逐步学习高频。


<details>
  <summary>Details</summary>
Motivation: 研究动机是改进深度神经网络的训练过程和推理性能，特别是在自编码器和变分自编码器的异常检测任务中。传统DNN在学习高频特征时存在困难，需要探索更有效的频率学习方法。

Method: 使用随机傅里叶变换（RFT）及其可训练变体，结合频率原理分析模型训练行为。在低维合成数据集进行数据表示分析，在高维航空安全数据集（Dashlink）进行基于重建的异常检测实验。

Result: 结果表明：1）使用傅里叶变换的模型优于传统模型；2）RFT模型能同时学习低频和高频特征，而传统DNN只能从低频开始逐步学习；3）可训练傅里叶变换与随机变体相比的优势尚不确定。

Conclusion: 傅里叶变换能显著改进自编码器和变分自编码器的性能，特别是在异常检测任务中。RFT使模型能同时学习不同频率特征，但可训练变体的优势需要进一步研究验证。

Abstract: In this study, we focus on the training process and inference improvements of deep neural networks (DNNs), specifically Autoencoders (AEs) and Variational Autoencoders (VAEs), using Random Fourier Transformation (RFT). We further explore the role of RFT in model training behavior using Frequency Principle (F-Principle) analysis and show that models with RFT turn to learn low frequency and high frequency at the same time, whereas conventional DNNs start from low frequency and gradually learn (if successful) high-frequency features. We focus on reconstruction-based anomaly detection using autoencoder and variational autoencoder and investigate the RFT's role. We also introduced a trainable variant of RFT that uses the existing computation graph to train the expansion of RFT instead of it being random. We showcase our findings with two low-dimensional synthetic datasets for data representation, and an aviation safety dataset, called Dashlink, for high-dimensional reconstruction-based anomaly detection. The results indicate the superiority of models with Fourier transformation compared to the conventional counterpart and remain inconclusive regarding the benefits of using trainable Fourier transformation in contrast to the Random variant.

</details>


### [185] [Towards a Principled Muon under $μ\mathsf{P}$: Ensuring Spectral Conditions throughout Training](https://arxiv.org/abs/2601.01306)
*John Zhao*

Main category: cs.LG

TL;DR: 本文提出Muon++，一种改进的矩阵优化器，能够在整个训练过程中可靠地满足μP的谱条件，无需显式权重谱归一化，降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: μ参数化（μP）为LLM训练提供了理论基础，但现有研究在矩阵优化器（如Muon）上存在局限性：要么无法在整个训练过程中保证谱条件，要么需要重复的谱归一化导致计算开销大。

Method: 提出Muon++变体，关键见解是对于中等规模模型，仅需在优化器更新层面保持谱控制即可维持μP兼容的缩放，无需显式权重谱归一化。还首次引入数据依赖的自适应谱条件。

Result: Muon++能够在整个训练过程中满足μP的谱条件，填补了μP理论承诺与矩阵优化器实际部署之间的差距，同时降低了计算开销。

Conclusion: 该工作为矩阵优化器在长时程LLM训练中的实际部署提供了可靠方案，通过仅控制更新层面的谱条件而非权重本身，实现了μP兼容的缩放，并首次引入自适应谱条件以适应数据依赖效应。

Abstract: The $μ$-parameterization ($μ$P) provides a principled foundation for large language model (LLM) training by prescribing width-independent learning dynamics, which in turn enables predictable scaling behavior and robust hyperparameter transfer across model sizes. A central requirement of $μ$P is the satisfaction of certain spectral conditions on weight matrices, which ensure consistent feature learning and optimization behavior as model width grows. While these conditions are well understood in theory, guaranteeing their validity in practical training for matrix-based optimizers such as Muon is still under studied. Existing works that study Muon under $μ$P exhibit important limitations: they either do not ensure that the spectral conditions hold throughout the entire training horizon, or require repeated spectral normalization (or Newton-Schulz iterations) applied to both weights and updates, leading to significant computational overhead and reduced practicality. In this work, we show how to reliably guarantee the spectral conditions required by $μ$P for Muon during the entire training process. Our key insight is that for moderately large models, maintaining spectral control at the level of optimizer updates alone is sufficient to preserve $μ$P-compatible scaling, eliminating the need for explicit spectral normalization of the weights. Based on this principle, we develop a variant of Muon, namely Muon++, that satisfies spectral condition throughout the training process. Our results bridge the gap between the theoretical promises of $μ$P and the practical deployment of matrix-based optimizers in long-horizon training. We also take the first step towards an adaptive spectral condition by incorporating data-dependent effects, making it better suited for long-horizon LLM training.

</details>


### [186] [Learning with Monotone Adversarial Corruptions](https://arxiv.org/abs/2601.02193)
*Kasper Green Larsen,Chirag Pabbaraju,Abhishek Shetty*

Main category: cs.LG

TL;DR: 研究表明，即使面对看似有益的单调对抗性数据污染，标准机器学习算法也会因为过度依赖数据的可交换性而性能下降。


<details>
  <summary>Details</summary>
Motivation: 探索标准机器学习算法对数据可交换性和独立性的依赖程度，通过引入单调对抗性污染模型来测试算法在面对看似有益的数据污染时的鲁棒性。

Method: 提出单调对抗性污染模型：攻击者观察干净的i.i.d.数据集后，插入自己选择的"污染"数据点，但这些污染点必须遵循单调性约束，即根据真实目标函数进行标注。

Result: 发现所有已知的最优二分类算法在这种设置下都会在新测试点上获得次优的期望误差；而基于一致收敛的算法则不会降低性能保证。

Conclusion: 最优学习算法在面对看似有益的单调污染时会崩溃，暴露了它们对数据可交换性的过度依赖，而基于一致收敛的算法则更加鲁棒。

Abstract: We study the extent to which standard machine learning algorithms rely on exchangeability and independence of data by introducing a monotone adversarial corruption model. In this model, an adversary, upon looking at a "clean" i.i.d. dataset, inserts additional "corrupted" points of their choice into the dataset. These added points are constrained to be monotone corruptions, in that they get labeled according to the ground-truth target function. Perhaps surprisingly, we demonstrate that in this setting, all known optimal learning algorithms for binary classification can be made to achieve suboptimal expected error on a new independent test point drawn from the same distribution as the clean dataset. On the other hand, we show that uniform convergence-based algorithms do not degrade in their guarantees. Our results showcase how optimal learning algorithms break down in the face of seemingly helpful monotone corruptions, exposing their overreliance on exchangeability.

</details>


### [187] [Dichotomous Diffusion Policy Optimization](https://arxiv.org/abs/2601.00898)
*Ruiming Liang,Yinan Zheng,Kexin Zheng,Tianyi Tan,Jianxiong Li,Liyuan Mao,Zhihao Wang,Guang Chen,Hangjun Ye,Jingjing Liu,Jinqiao Wang,Xianyuan Zhan*

Main category: cs.LG

TL;DR: DIPOLE是一种新颖的强化学习算法，通过分解为奖励最大化和最小化的二分策略，实现稳定可控的扩散策略优化。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略在强化学习中面临训练不稳定或计算效率低的问题，需要一种既能稳定训练又能灵活控制贪婪度的扩散策略优化方法。

Method: 提出DIPOLE算法，基于KL正则化目标重新设计，通过贪婪化策略正则化方案将最优策略分解为一对二分策略：一个最大化奖励，一个最小化奖励。推理时通过线性组合两个策略的分数生成动作。

Result: 在ExORL和OGBench的离线和离线到在线RL设置中验证了有效性，并成功训练了用于端到端自动驾驶的大型视觉语言动作模型，在NAVSIM基准上表现优异。

Conclusion: DIPOLE为扩散策略提供了稳定可控的优化框架，在复杂决策任务中表现出色，具有实际应用潜力。

Abstract: Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of greediness.Evaluations in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.

</details>


### [188] [Tiny Machine Learning for Real-Time Aquaculture Monitoring: A Case Study in Morocco](https://arxiv.org/abs/2601.01065)
*Achraf Hsain,Yahya Zaki,Othman Abaakil,Hibat-allah Bekkar,Yousra Chtouki*

Main category: cs.LG

TL;DR: 该论文提出将基于TinyML的低功耗边缘设备集成到水产养殖系统中，实现实时自动化监测和控制，以解决传统人工监测效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 水产养殖业面临水质波动、疾病爆发和饲料管理低效等挑战，传统人工监测方法耗时且可能导致问题处理延迟，需要更智能的自动化解决方案。

Method: 集成基于TinyML的低功耗边缘设备，通过传感器实时收集pH值、温度、溶解氧、氨含量等参数数据，实现异常检测和自动报警，同时考虑传感器选择、算法设计、硬件约束和伦理因素。

Result: 系统能够提供实时水质监测、异常警报，收集的数据可用于优化水处理过程、饲料分配和模式分析，提高饲料效率并降低运营成本。

Conclusion: TinyML技术在水产养殖监测中具有可行性，能够实现更可持续和高效的养殖实践，为行业发展提供智能化解决方案。

Abstract: Aquaculture, the farming of aquatic organisms, is a rapidly growing industry facing challenges such as water quality fluctuations, disease outbreaks, and inefficient feed management. Traditional monitoring methods often rely on manual labor and are time consuming, leading to potential delays in addressing issues. This paper proposes the integration of low-power edge devices using Tiny Machine Learning (TinyML) into aquaculture systems to enable real-time automated monitoring and control, such as collecting data and triggering alarms, and reducing labor requirements. The system provides real-time data on the required parameters such as pH levels, temperature, dissolved oxygen, and ammonia levels to control water quality, nutrient levels, and environmental conditions enabling better maintenance, efficient resource utilization, and optimal management of the enclosed aquaculture space. The system enables alerts in case of anomaly detection. The data collected by the sensors over time can serve for important decision-making regarding optimizing water treatment processes, feed distribution, feed pattern analysis and improve feed efficiency, reducing operational costs. This research explores the feasibility of developing TinyML-based solutions for aquaculture monitoring, considering factors such as sensor selection, algorithm design, hardware constraints, and ethical considerations. By demonstrating the potential benefits of TinyML in aquaculture, our aim is to contribute to the development of more sustainable and efficient farming practices.

</details>


### [189] [Accelerating Decentralized Optimization via Overlapping Local Steps](https://arxiv.org/abs/2601.01493)
*Yijie Zhou,Shi Pu*

Main category: cs.LG

TL;DR: OLDSGD是一种通过计算-通信重叠加速去中心化训练的新方法，在保持与Local SGD相同平均更新的同时减少网络空闲时间，提升实际训练速度。


<details>
  <summary>Details</summary>
Motivation: 去中心化优化在分布式学习中很重要，但现有方法因节点间频繁同步而面临通信瓶颈问题，需要减少网络空闲时间以加速训练。

Method: 提出重叠局部去中心化SGD（OLDSGD），通过精心设计的更新机制实现计算和通信的重叠，在保持与Local SGD相同平均更新的同时避免通信导致的停滞。

Result: 理论上证明了OLDSGD在光滑非凸目标上具有非渐进收敛率，保持与标准Local Decentralized SGD相同的迭代复杂度但改善每次迭代运行时间。实验显示在不同通信延迟下都能提升实际时间收敛速度。

Conclusion: OLDSGD通过对现有框架的最小修改，提供了不牺牲理论保证的更快去中心化学习实用方案，有效解决了通信瓶颈问题。

Abstract: Decentralized optimization has emerged as a critical paradigm for distributed learning, enabling scalable training while preserving data privacy through peer-to-peer collaboration. However, existing methods often suffer from communication bottlenecks due to frequent synchronization between nodes. We present Overlapping Local Decentralized SGD (OLDSGD), a novel approach to accelerate decentralized training by computation-communication overlapping, significantly reducing network idle time. With a deliberately designed update, OLDSGD preserves the same average update as Local SGD while avoiding communication-induced stalls. Theoretically, we establish non-asymptotic convergence rates for smooth non-convex objectives, showing that OLDSGD retains the same iteration complexity as standard Local Decentralized SGD while improving per-iteration runtime. Empirical results demonstrate OLDSGD's consistent improvements in wall-clock time convergence under different levels of communication delays. With minimal modifications to existing frameworks, OLDSGD offers a practical solution for faster decentralized learning without sacrificing theoretical guarantees.

</details>


### [190] [Latent-Constrained Conditional VAEs for Augmenting Large-Scale Climate Ensembles](https://arxiv.org/abs/2601.00915)
*Jacquelyn Shelton,Przemyslaw Polewski,Alexander Robel,Matthew Hoffman,Stephen Price*

Main category: cs.LG

TL;DR: 提出LC-CVAE方法，通过强制潜在空间在锚点位置的一致性，解决传统CVAE在气候模型集合生成中的泛化问题，实现从有限真实化生成新气候场。


<details>
  <summary>Details</summary>
Motivation: 大型气候模型集合计算成本高，但许多下游分析需要更多统计一致的气候变量实现。传统条件变分自编码器在跨真实化训练时潜在空间碎片化，无法泛化到未见过的集合成员。

Method: 提出潜在约束条件变分自编码器(LC-CVAE)，强制潜在嵌入在共享地理"锚点"位置具有跨真实化同质性。使用多输出高斯过程回归在潜在空间预测新真实化中未采样位置的坐标，然后解码生成完整时间序列场。

Result: 实验表明：1) 单真实化训练不稳定；2) 纳入约5个真实化后收益递减；3) 空间覆盖与重建质量之间存在权衡，与潜在空间平均邻近距离密切相关。

Conclusion: LC-CVAE通过潜在空间约束解决了跨气候真实化的泛化问题，为从有限模型运行生成统计一致的新气候实现提供了有效方法，在计算成本与生成质量间取得平衡。

Abstract: Large climate-model ensembles are computationally expensive; yet many downstream analyses would benefit from additional, statistically consistent realizations of spatiotemporal climate variables. We study a generative modeling approach for producing new realizations from a limited set of available runs by transferring structure learned across an ensemble. Using monthly near-surface temperature time series from ten independent reanalysis realizations (ERA5), we find that a vanilla conditional variational autoencoder (CVAE) trained jointly across realizations yields a fragmented latent space that fails to generalize to unseen ensemble members. To address this, we introduce a latent-constrained CVAE (LC-CVAE) that enforces cross-realization homogeneity of latent embeddings at a small set of shared geographic 'anchor' locations. We then use multi-output Gaussian process regression in the latent space to predict latent coordinates at unsampled locations in a new realization, followed by decoding to generate full time series fields. Experiments and ablations demonstrate (i) instability when training on a single realization, (ii) diminishing returns after incorporating roughly five realizations, and (iii) a trade-off between spatial coverage and reconstruction quality that is closely linked to the average neighbor distance in latent space.

</details>


### [191] [Distributed Federated Learning by Alternating Periods of Training](https://arxiv.org/abs/2601.01793)
*Shamik Bhattacharyya,Rachel Kalpana Kalaimani*

Main category: cs.LG

TL;DR: 提出分布式联邦学习框架，通过多服务器架构解决传统联邦学习的可扩展性和容错性问题


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习依赖单一中央服务器，在大规模客户端场景下存在可扩展性限制和单点故障风险，需要更健壮的分布式解决方案

Method: 设计分布式联邦学习框架，包含多个具有服务器间通信能力的服务器，每个服务器管理一组不相交的客户端；提出DFL算法，交替进行客户端本地训练和服务器间全局训练

Result: 在适当参数选择下，DFL算法确保所有服务器收敛到接近理想模型的共同模型值，有效整合本地和全局训练模型

Conclusion: 分布式联邦学习框架通过多服务器架构成功解决了传统联邦学习的可扩展性和容错性限制，理论分析和数值模拟验证了方法的有效性

Abstract: Federated learning is a privacy-focused approach towards machine learning where models are trained on client devices with locally available data and aggregated at a central server. However, the dependence on a single central server is challenging in the case of a large number of clients and even poses the risk of a single point of failure. To address these critical limitations of scalability and fault-tolerance, we present a distributed approach to federated learning comprising multiple servers with inter-server communication capabilities. While providing a fully decentralized approach, the designed framework retains the core federated learning structure where each server is associated with a disjoint set of clients with server-client communication capabilities. We propose a novel DFL (Distributed Federated Learning) algorithm which uses alternating periods of local training on the client data followed by global training among servers. We show that the DFL algorithm, under a suitable choice of parameters, ensures that all the servers converge to a common model value within a small tolerance of the ideal model, thus exhibiting effective integration of local and global training models. Finally, we illustrate our theoretical claims through numerical simulations.

</details>


### [192] [Attention Needs to Focus: A Unified Perspective on Attention Allocation](https://arxiv.org/abs/2601.00919)
*Zichuan Fu,Wentao Song,Guojing Li,Yejing Wang,Xian Wu,Yimin Deng,Hanyu Yan,Yefeng Zheng,Xiangyu Zhao*

Main category: cs.LG

TL;DR: Lazy Attention机制通过位置区分和弹性Softmax解决注意力过载和欠载问题，缓解表示坍缩和注意力沉没，在保持性能的同时实现高达59.58%的注意力稀疏性。


<details>
  <summary>Details</summary>
Motivation: Transformer注意力机制存在表示坍缩和注意力沉没两个问题，先前研究孤立处理这些问题，本文认为两者根源相同——注意力分配不当，需要统一解决方案。

Method: 提出Lazy Attention机制：1) 通过位置区分（跨头和维度）缓解注意力过载，增强token区分度；2) 使用弹性Softmax缓解注意力欠载，抑制无关token的注意力分配。

Result: 在FineWeb-Edu语料库和九个基准测试中，Lazy Attention成功缓解注意力沉没，性能与标准注意力及现代架构相当，同时实现高达59.58%的注意力稀疏性。

Conclusion: 注意力过载和欠载是Transformer注意力机制的根本问题，Lazy Attention通过统一框架有效解决这两个问题，为注意力机制设计提供了新思路。

Abstract: The Transformer architecture, a cornerstone of modern Large Language Models (LLMs), has achieved extraordinary success in sequence modeling, primarily due to its attention mechanism. However, despite its power, the standard attention mechanism is plagued by well-documented issues: representational collapse and attention sink. Although prior work has proposed approaches for these issues, they are often studied in isolation, obscuring their deeper connection. In this paper, we present a unified perspective, arguing that both can be traced to a common root -- improper attention allocation. We identify two failure modes: 1) Attention Overload, where tokens receive comparable high weights, blurring semantic features that lead to representational collapse; 2) Attention Underload, where no token is semantically relevant, yet attention is still forced to distribute, resulting in spurious focus such as attention sink. Building on this insight, we introduce Lazy Attention, a novel mechanism designed for a more focused attention distribution. To mitigate overload, it employs positional discrimination across both heads and dimensions to sharpen token distinctions. To counteract underload, it incorporates Elastic-Softmax, a modified normalization function that relaxes the standard softmax constraint to suppress attention on irrelevant tokens. Experiments on the FineWeb-Edu corpus, evaluated across nine diverse benchmarks, demonstrate that Lazy Attention successfully mitigates attention sink and achieves competitive performance compared to both standard attention and modern architectures, while reaching up to 59.58% attention sparsity.

</details>


### [193] [MODE: Efficient Time Series Prediction with Mamba Enhanced by Low-Rank Neural ODEs](https://arxiv.org/abs/2601.00920)
*Xingsheng Chen,Regina Zhang,Bo Gao,Xingwei He,Xiaofeng Liu,Pietro Lio,Kwok-Yan Lam,Siu-Ming Yiu*

Main category: cs.LG

TL;DR: MODE是一个统一的时间序列预测框架，结合了低秩神经ODE和增强Mamba架构，通过低秩近似和动态选择性扫描机制，在保持表达力的同时提高计算效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法在处理长程依赖和不规则采样数据时，难以平衡效率、可扩展性和准确性。特别是在金融、医疗、能源系统和环境建模等领域，需要更高效且准确的预测模型。

Method: 提出MODE框架：1）线性标记化层处理输入序列；2）多个Mamba编码器块，每个包含增强Mamba层（因果卷积、SiLU激活、低秩神经ODE增强）；3）低秩神经ODE减少计算开销；4）分段选择性扫描机制（受伪ODE动态启发）自适应聚焦重要子序列。

Result: 在基准数据集上的大量实验表明，MODE在预测准确性和计算效率方面均优于现有基线方法。

Conclusion: MODE为长期时间序列建模提供了一个统一高效架构，通过Mamba选择性扫描与低秩神经ODE的集成增强了时间表示能力，低秩近似和动态选择性扫描显著提升了效率和可扩展性。

Abstract: Time series prediction plays a pivotal role across diverse domains such as finance, healthcare, energy systems, and environmental modeling. However, existing approaches often struggle to balance efficiency, scalability, and accuracy, particularly when handling long-range dependencies and irregularly sampled data. To address these challenges, we propose MODE, a unified framework that integrates Low-Rank Neural Ordinary Differential Equations (Neural ODEs) with an Enhanced Mamba architecture. As illustrated in our framework, the input sequence is first transformed by a Linear Tokenization Layer and then processed through multiple Mamba Encoder blocks, each equipped with an Enhanced Mamba Layer that employs Causal Convolution, SiLU activation, and a Low-Rank Neural ODE enhancement to efficiently capture temporal dynamics. This low-rank formulation reduces computational overhead while maintaining expressive power. Furthermore, a segmented selective scanning mechanism, inspired by pseudo-ODE dynamics, adaptively focuses on salient subsequences to improve scalability and long-range sequence modeling. Extensive experiments on benchmark datasets demonstrate that MODE surpasses existing baselines in both predictive accuracy and computational efficiency. Overall, our contributions include: (1) a unified and efficient architecture for long-term time series modeling, (2) integration of Mamba's selective scanning with low-rank Neural ODEs for enhanced temporal representation, and (3) substantial improvements in efficiency and scalability enabled by low-rank approximation and dynamic selective scanning.

</details>


### [194] [Practical Geometric and Quantum Kernel Methods for Predicting Skeletal Muscle Outcomes in chronic obstructive pulmonary disease](https://arxiv.org/abs/2601.00921)
*Azadeh Alavi,Hamidreza Khalili,Stanley H. Chan,Fatemeh Kouchmeshki,Ross Vlahos*

Main category: cs.LG

TL;DR: 研究使用几何感知和量子核方法预测COPD相关骨骼肌功能障碍，在低数据、低特征生物医学预测中显示出可测量的优势。


<details>
  <summary>Details</summary>
Motivation: 慢性阻塞性肺疾病（COPD）的骨骼肌功能障碍是重要的肺外表现，与全身和气道炎症密切相关。需要从可纵向获取的微创生物标志物预测肌肉结果。

Method: 使用213只动物的临床前数据集，比较调优的经典基线、几何感知对称正定描述符与Stein散度、以及为低维表格数据设计的量子核模型。使用血液C反应蛋白、中性粒细胞计数、支气管肺泡灌洗细胞性和条件作为输入特征。

Result: 在肌肉重量预测中，量子核岭回归获得测试RMSE 4.41 mg和R² 0.605，优于相同特征集的岭回归基线（4.70 mg和0.553）。几何感知Stein散度原型距离在仅生物标志物设置中也获得一致改进（4.55 mg vs 4.79 mg）。筛查式评估的ROC-AUC高达0.90。

Conclusion: 几何和量子核提升在低数据、低特征的生物医学预测问题中能提供可测量的优势，同时保持可解释性和透明的模型选择。

Abstract: Skeletal muscle dysfunction is a clinically relevant extra-pulmonary manifestation of chronic obstructive pulmonary disease (COPD) and is closely linked to systemic and airway inflammation. This motivates predictive modelling of muscle outcomes from minimally invasive biomarkers that can be acquired longitudinally. We study a small-sample preclinical dataset comprising 213 animals across two conditions (Sham versus cigarette-smoke exposure), with blood and bronchoalveolar lavage fluid measurements and three continuous targets: tibialis anterior muscle weight (milligram: mg), specific force (millinewton: mN), and a derived muscle quality index (mN per mg). We benchmark tuned classical baselines, geometry-aware symmetric positive definite (SPD) descriptors with Stein divergence, and quantum kernel models designed for low-dimensional tabular data. In the muscle-weight setting, quantum kernel ridge regression using four interpretable inputs (blood C-reactive protein, neutrophil count, bronchoalveolar lavage cellularity, and condition) attains a test root mean squared error of 4.41 mg and coefficient of determination of 0.605, improving over a matched ridge baseline on the same feature set (4.70 mg and 0.553). Geometry-informed Stein-divergence prototype distances yield a smaller but consistent gain in the biomarker-only setting (4.55 mg versus 4.79 mg). Screening-style evaluation, obtained by thresholding the continuous outcome at 0.8 times the training Sham mean, achieves an area under the receiver operating characteristic curve (ROC-AUC) of up to 0.90 for detecting low muscle weight. These results indicate that geometric and quantum kernel lifts can provide measurable benefits in low-data, low-feature biomedical prediction problems, while preserving interpretability and transparent model selection.

</details>


### [195] [Complexity-based code embeddings](https://arxiv.org/abs/2601.00924)
*Rares Folea,Radu Iacob,Emil Slusanschi,Traian Rebedea*

Main category: cs.LG

TL;DR: 提出一种将算法源代码转换为数值嵌入的通用方法，通过动态分析程序行为并定制复杂度函数，基于r-Complexity构建代码嵌入，在Codeforces真实代码数据集上实现多标签分类


<details>
  <summary>Details</summary>
Motivation: 需要将算法源代码转换为数值表示（嵌入），以便进行机器学习任务，特别是对编程竞赛中的代码进行分类和分析

Method: 通过动态分析程序在不同输入下的行为，为分析指标定制多个通用复杂度函数，基于r-Complexity构建算法嵌入，使用这些嵌入训练XGBoost模型

Result: 在包含11个类别的多标签数据集上（基于Codeforces平台真实代码片段）实现了平均F1分数，表明该方法有效

Conclusion: 提出的通用代码嵌入方法能够有效表示算法源代码，支持机器学习应用，在真实编程竞赛代码分类任务中表现良好

Abstract: This paper presents a generic method for transforming the source code of various algorithms to numerical embeddings, by dynamically analysing the behaviour of computer programs against different inputs and by tailoring multiple generic complexity functions for the analysed metrics. The used algorithms embeddings are based on r-Complexity . Using the proposed code embeddings, we present an implementation of the XGBoost algorithm that achieves an average F1-score on a multi-label dataset with 11 classes, built using real-world code snippets submitted for programming competitions on the Codeforces platform.

</details>


### [196] [Enhanced Data-Driven Product Development via Gradient Based Optimization and Conformalized Monte Carlo Dropout Uncertainty Estimation](https://arxiv.org/abs/2601.00932)
*Andrea Thomas Nava,Lijo Johny,Fabio Azzalini,Johannes Schneider,Arianna Casanova*

Main category: cs.LG

TL;DR: 提出了一种数据驱动的产品开发框架，使用联合神经网络优化多个相关属性，并引入ConfMC方法提供具有有限样本覆盖保证的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 传统产品开发中，需要同时优化多个相关属性，且需要可靠的不确定性估计来指导设计决策。现有方法在提供有限样本覆盖保证方面存在不足，且调整置信水平时需要重新训练模型。

Method: 1) 使用联合神经网络捕捉多个目标间的相互依赖关系；2) 采用投影梯度下降法寻找最优输入特征；3) 提出ConfMC方法，结合嵌套保形预测和蒙特卡洛dropout，提供模型无关的有限样本覆盖保证。

Result: 在五个真实世界数据集上的实验表明，该方法在性能上达到最先进水平，同时提供自适应、非均匀的预测区间，且调整置信水平时无需重新训练模型。

Conclusion: 该框架为数据驱动的产品开发提供了一种有效方法，能够同时优化多个相关属性，并提供可靠的不确定性估计，具有实际应用价值。

Abstract: Data-Driven Product Development (DDPD) leverages data to learn the relationship between product design specifications and resulting properties. To discover improved designs, we train a neural network on past experiments and apply Projected Gradient Descent to identify optimal input features that maximize performance. Since many products require simultaneous optimization of multiple correlated properties, our framework employs joint neural networks to capture interdependencies among targets. Furthermore, we integrate uncertainty estimation via \emph{Conformalised Monte Carlo Dropout} (ConfMC), a novel method combining Nested Conformal Prediction with Monte Carlo dropout to provide model-agnostic, finite-sample coverage guarantees under data exchangeability. Extensive experiments on five real-world datasets show that our method matches state-of-the-art performance while offering adaptive, non-uniform prediction intervals and eliminating the need for retraining when adjusting coverage levels.

</details>


### [197] [LOFA: Online Influence Maximization under Full-Bandit Feedback using Lazy Forward Selection](https://arxiv.org/abs/2601.00933)
*Jinyu Xu,Abhishek K. Umrawal*

Main category: cs.LG

TL;DR: 提出LOFA算法用于在线影响力最大化问题，在完全强盗反馈下通过利用子模性实现更低经验遗憾


<details>
  <summary>Details</summary>
Motivation: 在线影响力最大化问题中，现有算法虽然利用子模性实现了低遗憾，但仍有改进空间，需要进一步利用子模性来降低经验遗憾

Method: 提出Lazy Online Forward Algorithm (LOFA)，在完全强盗反馈模型下，进一步利用影响力函数的子模性特性设计算法

Result: 在真实社交网络上的实验表明，LOFA在累积遗憾和瞬时奖励方面优于现有的强盗算法

Conclusion: 通过更充分地利用子模性，LOFA算法在在线影响力最大化问题上实现了更优的性能表现

Abstract: We study the problem of influence maximization (IM) in an online setting, where the goal is to select a subset of nodes$\unicode{x2014}$called the seed set$\unicode{x2014}$at each time step over a fixed time horizon, subject to a cardinality budget constraint, to maximize the expected cumulative influence. We operate under a full-bandit feedback model, where only the influence of the chosen seed set at each time step is observed, with no additional structural information about the network or diffusion process. It is well-established that the influence function is submodular, and existing algorithms exploit this property to achieve low regret. In this work, we leverage this property further and propose the Lazy Online Forward Algorithm (LOFA), which achieves a lower empirical regret. We conduct experiments on a real-world social network to demonstrate that LOFA achieves superior performance compared to existing bandit algorithms in terms of cumulative regret and instantaneous reward.

</details>


### [198] [Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures](https://arxiv.org/abs/2601.00942)
*Kabir Grover*

Main category: cs.LG

TL;DR: 稀疏MoE架构在随机解码下的可靠性研究表明，指令微调而非架构稀疏性是决定模型在确定性任务中对解码随机性鲁棒性的主要因素。


<details>
  <summary>Details</summary>
Motivation: 随着稀疏混合专家(MoE)架构在大型语言模型中的普及，需要研究其在随机解码下的可靠性。虽然条件计算带来了计算效率的提升，但尚不清楚稀疏路由与基于温度的采样之间的交互是否会损害输出稳定性。

Method: 评估了三个代表性模型：OLMoE-7B（稀疏基础模型）、Mixtral-8x7B（稀疏指令微调模型）和Qwen2.5-3B（密集指令微调模型）。在具有客观可验证答案的确定性算术推理任务上进行测试，涵盖四种解码配置（从贪婪解码到T=1.0），评估准确性、格式合规性、重复生成的一致性和置信度指标，总计9,360次模型生成。

Result: 稀疏指令微调模型在所有解码温度下表现出与密集指令微调模型相当的稳定性，而稀疏基础模型随着温度升高显示出系统性退化。这表明指令微调，而非架构稀疏性，是决定模型在确定性任务中对解码随机性鲁棒性的主要因素。

Conclusion: 指令微调是确保稀疏语言模型在可靠性关键应用中稳定性的关键因素。研究结果为在不需要牺牲输出稳定性的场景中安全采用稀疏架构提供了指导。

Abstract: The increasing prevalence of sparse Mixture-of-Experts (MoE) architectures in large language models raises important questions regarding their reliability under stochastic decoding. While conditional computation enables substantial gains in computational efficiency, it remains unclear whether the interaction between sparse routing and temperature-based sampling compromises output stability relative to dense architectures. This work investigates whether conditional computation in MoE models amplifies decoding-induced randomness, leading to reduced reliability as temperature increases. We evaluate three representative models: OLMoE-7B (sparse base), Mixtral-8x7B (sparse instruction-tuned), and Qwen2.5-3B (dense instruction-tuned) on deterministic arithmetic reasoning tasks with objectively verifiable answers. Experiments span four decoding configurations, ranging from greedy decoding to T=1.0. Our evaluation encompasses accuracy, format compliance, output consistency across repeated generations, and confidence metrics, totaling 9,360 model generations. Results demonstrate that the sparse instruction-tuned model exhibits stability comparable to the dense instruction-tuned model across all decoding temperatures, while the sparse base model shows systematic degradation as temperature increases. These findings indicate that instruction tuning, rather than architectural sparsity, is the primary determinant of robustness to decoding randomness on deterministic tasks. We discuss the implications of these results for deploying sparse language models in reliability-critical applications, highlighting scenarios in which sparse architectures can be safely adopted without sacrificing output stability.

</details>


### [199] [Adapting Feature Attenuation to NLP](https://arxiv.org/abs/2601.00965)
*Tianshuo Yang,Ryan Rabinowitz,Terrance E. Boult,Jugal Kalita*

Main category: cs.LG

TL;DR: 本文研究文本开放集识别（OSR），将计算机视觉中的特征衰减假设移植到Transformer模型，在arXiv学科分类任务上评估COSTARR等方法，发现移植可行但效果提升不显著。


<details>
  <summary>Details</summary>
Motivation: Transformer分类器（如BERT）在闭集任务中表现出色，但在面对未见类别时表现脆弱，而这是部署NLP系统常见场景。需要研究文本开放集识别方法。

Method: 将计算机视觉中的COSTARR框架适配到两个语言模型（BERT base和GPT-2），在176个arXiv学科分类任务上训练。同时评估MSP、MaxLogit和温度缩放自由能分数，使用OOSA和AUOSCR指标。

Result: 1) COSTARR可扩展到NLP领域且无需重新训练，但与MaxLogit或MSP相比无统计学显著提升；2) 自由能分数在高类别数设置下落后于其他所有分数。

Conclusion: 研究显示了将视觉中心OSR思想移植到语言模型的潜力与当前局限性，指出需要更大的骨干网络和任务定制的衰减策略。

Abstract: Transformer classifiers such as BERT deliver impressive closed-set accuracy, yet they remain brittle when confronted with inputs from unseen categories--a common scenario for deployed NLP systems. We investigate Open-Set Recognition (OSR) for text by porting the feature attenuation hypothesis from computer vision to transformers and by benchmarking it against state-of-the-art baselines. Concretely, we adapt the COSTARR framework--originally designed for classification in computer vision--to two modest language models (BERT (base) and GPT-2) trained to label 176 arXiv subject areas. Alongside COSTARR, we evaluate Maximum Softmax Probability (MSP), MaxLogit, and the temperature-scaled free-energy score under the OOSA and AUOSCR metrics. Our results show (i) COSTARR extends to NLP without retraining but yields no statistically significant gain over MaxLogit or MSP, and (ii) free-energy lags behind all other scores in this high-class-count setting. The study highlights both the promise and the current limitations of transplanting vision-centric OSR ideas to language models, and points toward the need for larger backbones and task-tailored attenuation strategies.

</details>


### [200] [Explainability-Guided Defense: Attribution-Aware Model Refinement Against Adversarial Data Attacks](https://arxiv.org/abs/2601.00968)
*Longwei Wang,Mohammad Navid Nayyem,Abdullah Al Rakin,KC Santosh,Chaowei Zhang,Yang Zhou*

Main category: cs.LG

TL;DR: 提出一个基于LIME解释的对抗鲁棒性训练框架，通过抑制虚假特征来同时提升模型的可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在医疗和自动驾驶等安全关键领域，深度学习模型需要同时具备对抗鲁棒性和决策透明度。研究发现，通过LIME识别出的虚假、不稳定或语义无关的特征对对抗脆弱性有不成比例的贡献。

Method: 提出一个属性引导的精炼框架，将LIME从被动诊断工具转变为主动训练信号。方法包括特征掩蔽、敏感度感知正则化和对抗增强，形成闭环精炼流程，无需额外数据集或模型架构，可无缝集成到标准对抗训练中。

Result: 在CIFAR-10、CIFAR-10-C和CIFAR-100上的实验表明，该方法显著提升了对抗鲁棒性和分布外泛化能力。

Conclusion: 通过理论推导和实证验证，建立了可解释性对齐与鲁棒性之间的正式联系，为开发同时具备鲁棒性和可解释性的模型提供了有效框架。

Abstract: The growing reliance on deep learning models in safety-critical domains such as healthcare and autonomous navigation underscores the need for defenses that are both robust to adversarial perturbations and transparent in their decision-making. In this paper, we identify a connection between interpretability and robustness that can be directly leveraged during training. Specifically, we observe that spurious, unstable, or semantically irrelevant features identified through Local Interpretable Model-Agnostic Explanations (LIME) contribute disproportionately to adversarial vulnerability. Building on this insight, we introduce an attribution-guided refinement framework that transforms LIME from a passive diagnostic into an active training signal. Our method systematically suppresses spurious features using feature masking, sensitivity-aware regularization, and adversarial augmentation in a closed-loop refinement pipeline. This approach does not require additional datasets or model architectures and integrates seamlessly into standard adversarial training. Theoretically, we derive an attribution-aware lower bound on adversarial distortion that formalizes the link between explanation alignment and robustness. Empirical evaluations on CIFAR-10, CIFAR-10-C, and CIFAR-100 demonstrate substantial improvements in adversarial robustness and out-of-distribution generalization.

</details>


### [201] [Zero-shot Forecasting by Simulation Alone](https://arxiv.org/abs/2601.00970)
*Boris N. Oreshkin,Mayank Jauhari,Ravi Kiran Selvam,Malcolm Wolff,Wenhao Pan,Shankar Ramasubramanian,Kin G. Olivares,Tatiana Konstantinova,Andres Potapczynski,Mengfei Cao,Dmitry Efimov,Michael W. Mahoney,Andrew G. Wilson*

Main category: cs.LG

TL;DR: 提出SarSim0时间序列模拟器，基于SARIMA模型快速生成合成数据，用于训练神经网络实现零样本预测，在多个基准测试中超越统计方法和现有基线。


<details>
  <summary>Details</summary>
Motivation: 零样本时间序列预测面临数据有限、评估易泄漏、隐私和许可限制等挑战，需要一种实用的模拟方法来解决这些问题。

Method: 基于SARIMA模型构建SarSim0模拟器，采用三步法：1)从特征多项式稳定区域采样良好轨迹；2)通过叠加方案组合多个路径形成多季节性序列；3)添加基于速率的重尾噪声模型捕捉突发性和间歇性。

Result: SarSim0比基于核的生成器快几个数量级，能在约10亿个模拟序列上训练神经网络，在M-Series和GiftEval基准测试中超越统计预测器和近期基础模型，在GiftEval上甚至超过了生成数据的AutoARIMA模型。

Conclusion: SarSim0提供了一种快速实用的时间序列模拟方法，能够生成丰富的合成数据来训练神经网络，实现强大的零样本预测性能，为工业应用中的时间序列预测提供了新途径。

Abstract: Zero-shot time-series forecasting holds great promise, but is still in its infancy, hindered by limited and biased data corpora, leakage-prone evaluation, and privacy and licensing constraints. Motivated by these challenges, we propose the first practical univariate time series simulation pipeline which is simultaneously fast enough for on-the-fly data generation and enables notable zero-shot forecasting performance on M-Series and GiftEval benchmarks that capture trend/seasonality/intermittency patterns, typical of industrial forecasting applications across a variety of domains. Our simulator, which we call SarSim0 (SARIMA Simulator for Zero-Shot Forecasting), is based off of a seasonal autoregressive integrated moving average (SARIMA) model as its core data source. Due to instability in the autoregressive component, naive SARIMA simulation often leads to unusable paths. Instead, we follow a three-step procedure: (1) we sample well-behaved trajectories from its characteristic polynomial stability region; (2) we introduce a superposition scheme that combines multiple paths into rich multi-seasonality traces; and (3) we add rate-based heavy-tailed noise models to capture burstiness and intermittency alongside seasonalities and trends. SarSim0 is orders of magnitude faster than kernel-based generators, and it enables training on circa 1B unique purely simulated series, generated on the fly; after which well-established neural network backbones exhibit strong zero-shot generalization, surpassing strong statistical forecasters and recent foundation baselines, while operating under strict zero-shot protocol. Notably, on GiftEval we observe a "student-beats-teacher" effect: models trained on our simulations exceed the forecasting accuracy of the AutoARIMA generating processes.

</details>


### [202] [Contractive Diffusion Policies: Robust Action Diffusion via Contractive Score-Based Sampling with Differential Equations](https://arxiv.org/abs/2601.01003)
*Amin Abyaneh,Charlotte Morissette,Mohamad H. Danesh,Anas El Houssaini,David Meger,Gregory Dudek,Hsiu-Chin Lin*

Main category: cs.LG

TL;DR: 本文提出收缩扩散策略（CDPs），通过在扩散采样动力学中引入收缩行为来增强扩散策略的鲁棒性，减少求解器和分数匹配误差的影响，特别适用于数据稀缺的离线强化学习场景。


<details>
  <summary>Details</summary>
Motivation: 扩散策略虽然作为离线策略学习的强大生成模型，但其基于分数的SDE建模存在求解器和分数匹配误差、大数据需求以及动作生成不一致等问题。在连续控制任务中，这些不准确性会累积并导致失败，特别是在数据稀缺的情况下。

Method: 提出收缩扩散策略（CDPs），在扩散采样动力学中引入收缩行为。收缩将相邻的流拉近，增强对求解器和分数匹配误差的鲁棒性，同时减少不必要的动作方差。提供了理论分析框架和实际实现方法，可以最小化修改和计算成本地集成到现有扩散策略架构中。

Result: 在仿真和真实世界环境中进行了广泛的离线学习实验。在多个基准测试中，CDPs通常优于基线策略，在数据稀缺情况下表现出更明显的优势。

Conclusion: CDPs通过引入收缩行为有效解决了扩散策略在连续控制任务中的误差累积问题，提高了策略的鲁棒性和性能，特别是在数据有限的情况下，为扩散策略的实际应用提供了重要改进。

Abstract: Diffusion policies have emerged as powerful generative models for offline policy learning, whose sampling process can be rigorously characterized by a score function guiding a Stochastic Differential Equation (SDE). However, the same score-based SDE modeling that grants diffusion policies the flexibility to learn diverse behavior also incurs solver and score-matching errors, large data requirements, and inconsistencies in action generation. While less critical in image generation, these inaccuracies compound and lead to failure in continuous control settings. We introduce Contractive Diffusion Policies (CDPs) to induce contractive behavior in the diffusion sampling dynamics. Contraction pulls nearby flows closer to enhance robustness against solver and score-matching errors while reducing unwanted action variance. We develop an in-depth theoretical analysis along with a practical implementation recipe to incorporate CDPs into existing diffusion policy architectures with minimal modification and computational cost. We evaluate CDPs for offline learning by conducting extensive experiments in simulation and real-world settings. Across benchmarks, CDPs often outperform baseline policies, with pronounced benefits under data scarcity.

</details>


### [203] [Data-Driven Assessment of Concrete Mixture Compositions on Chloride Transport via Standalone Machine Learning Algorithms](https://arxiv.org/abs/2601.01009)
*Mojtaba Aliasghar-Mamaghani,Mohammadreza Khalafi*

Main category: cs.LG

TL;DR: 使用多种机器学习算法分析混凝土配合比对氯离子侵蚀时间演化的影响，发现KRR、GPR和MLP模型精度最高，揭示了配合比成分与氯离子含量之间的正负相关性。


<details>
  <summary>Details</summary>
Motivation: 评估受侵蚀环境下混凝土结构的服役寿命，需要理解混凝土配合比对氯离子侵蚀时间演化的影响。传统方法难以捕捉复杂的隐藏相关性，因此采用数据驱动的机器学习方法来建立可靠的预测模型。

Method: 采用简单和复杂的机器学习算法：线性回归、KNN回归、核岭回归（KRR）、支持向量回归（SVR）、高斯过程回归（GPR）、前馈神经网络（MLP）和门控循环单元（GRU）。使用综合数据集评估算法性能，并分析配合比成分对氯离子侵蚀的贡献。

Result: KRR、GPR和MLP表现出高精度，而GRU由于配合比多样性无法准确预测测试集响应。GPR模型揭示了清晰的潜在相关性趋势，MLP、SVR和KRR也能提供可接受的趋势估计。大多数配合比成分与氯离子含量呈负相关，少数成分呈正相关。

Conclusion: 机器学习方法能够有效描述氯离子侵蚀的物理过程和相关关系，为增强土木基础设施服役寿命提供了有前景的替代方法。GPR模型特别适合揭示可解释的趋势，而配合比成分的多样性对某些模型（如GRU）提出了挑战。

Abstract: This paper employs a data-driven approach to determine the impact of concrete mixture compositions on the temporal evolution of chloride in concrete structures. This is critical for assessing the service life of civil infrastructure subjected to aggressive environments. The adopted methodology relies on several simple and complex standalone machine learning (ML) algorithms, with the primary objective of establishing confidence in the unbiased prediction of the underlying hidden correlations. The simple algorithms include linear regression (LR), k-nearest neighbors (KNN) regression, and kernel ridge regression (KRR). The complex algorithms entail support vector regression (SVR), Gaussian process regression (GPR), and two families of artificial neural networks, including a feedforward network (multilayer perceptron, MLP) and a gated recurrent unit (GRU). The MLP architecture cannot explicitly handle sequential data, a limitation addressed by the GRU. A comprehensive dataset is considered. The performance of ML algorithms is evaluated, with KRR, GPR, and MLP exhibiting high accuracy. Given the diversity of the adopted concrete mixture proportions, the GRU was unable to accurately reproduce the response in the test set. Further analyses elucidate the contributions of mixture compositions to the temporal evolution of chloride. The results obtained from the GPR model unravel latent correlations through clear and explainable trends. The MLP, SVR, and KRR also provide acceptable estimates of the overall trends. The majority of mixture components exhibit an inverse relation with chloride content, while a few components demonstrate a direct correlation. These findings highlight the potential of surrogate approaches for describing the physical processes involved in chloride ingress and the associated correlations, toward the ultimate goal of enhancing the service life of civil infrastructure.

</details>


### [204] [Geometric and Dynamic Scaling in Deep Transformers](https://arxiv.org/abs/2601.01014)
*Haoran Su,Chenyu You*

Main category: cs.LG

TL;DR: 论文提出Transformer深度增加导致表示崩溃的根本原因是几何问题，而非优化问题，并提出了Manifold-Geometric Transformer (MGT)架构来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究将Transformer深度增加导致的表示崩溃归因于优化不稳定或梯度消失，但这些解释无法解释为什么在现代归一化和初始化方案下崩溃仍然存在。作者认为这是一个几何问题，标准残差更新缺乏约束更新方向或擦除过时信息的机制。

Method: 提出了统一的几何框架，包含两个正交原则：1) 流形约束超连接，限制残差更新到有效的局部切向方向，防止不受控制的流形漂移；2) 深度增量学习，引入数据依赖的非单调更新，实现冗余特征的反射和擦除而非无条件累积。

Result: 这些机制解耦了特征更新的方向和符号，实现了跨深度的稳定几何演化。提出的MGT架构预测，强制几何有效性同时允许动态擦除对于避免超深网络中的秩崩溃至关重要。

Conclusion: 几何而非深度本身是深度表示学习的关键限制因素。论文为超过100层的Transformer设计了评估协议来验证这一假设。

Abstract: Despite their empirical success, pushing Transformer architectures to extreme depth often leads to a paradoxical failure: representations become increasingly redundant, lose rank, and ultimately collapse. Existing explanations largely attribute this phenomenon to optimization instability or vanishing gradients, yet such accounts fail to explain why collapse persists even under modern normalization and initialization schemes. In this paper, we argue that the collapse of deep Transformers is fundamentally a geometric problem. Standard residual updates implicitly assume that feature accumulation is always beneficial, but offer no mechanism to constrain update directions or to erase outdated information. As depth increases, this leads to systematic drift off the semantic manifold and monotonic feature accumulation, causing representational degeneracy. We propose a unified geometric framework that addresses these failures through two orthogonal principles. First, manifold-constrained hyper-connections restrict residual updates to valid local tangent directions, preventing uncontrolled manifold drift. Second, deep delta learning introduces data-dependent, non-monotonic updates that enable reflection and erasure of redundant features rather than their unconditional accumulation. Together, these mechanisms decouple the direction and sign of feature updates, yielding a stable geometric evolution across depth. We term the resulting architecture the Manifold-Geometric Transformer (MGT). Our analysis predicts that enforcing geometric validity while allowing dynamic erasure is essential for avoiding rank collapse in ultra-deep networks. We outline an evaluation protocol for Transformers exceeding 100 layers to test the hypothesis that geometry, rather than depth itself, is the key limiting factor in deep representation learning.

</details>


### [205] [Expanding the Chaos: Neural Operator for Stochastic (Partial) Differential Equations](https://arxiv.org/abs/2601.01021)
*Dai Shi,Lequan Lin,Andi Han,Luke Thompson,José Miguel Hernández-Lobato,Zhiyong Wang,Junbin Gao*

Main category: cs.LG

TL;DR: 基于Wiener混沌展开的神经算子架构，用于学习SDE/SPDE的解算子，通过正交Wick Hermite特征投影噪声路径，用神经算子参数化确定性混沌系数，实现单次前向传播从噪声重构完整解轨迹。


<details>
  <summary>Details</summary>
Motivation: 随机微分方程（SDEs）和随机偏微分方程（SPDEs）是建模随机动力学的基础工具。开发深度学习模型来近似它们的解算子不仅能提供快速实用的求解器，还可能从新视角解决经典学习任务。传统方法在处理复杂随机系统时存在计算效率低或灵活性不足的问题。

Method: 基于经典Wiener混沌展开（WCE），设计用于SPDEs和SDEs的神经算子架构：将驱动噪声路径投影到正交Wick Hermite特征上，用神经算子参数化得到的确定性混沌系数，从而可以从噪声中单次前向传播重构完整解轨迹。理论方面，为多维SDEs和半线性SPDEs显式写出混沌系数的耦合ODE/PDE系统。

Result: 在多样化问题集上验证模型：经典SPDE基准测试、图像扩散一步采样、图拓扑插值、金融外推、参数估计、洪水预测的流形SDEs，展示了竞争性精度和广泛适用性。

Conclusion: 基于WCE的神经算子为跨多个领域学习SDE/SPDE解算子提供了实用且可扩展的方法，在保持理论严谨性的同时实现了高效计算和广泛适用性。

Abstract: Stochastic differential equations (SDEs) and stochastic partial differential equations (SPDEs) are fundamental tools for modeling stochastic dynamics across the natural sciences and modern machine learning. Developing deep learning models for approximating their solution operators promises not only fast, practical solvers, but may also inspire models that resolve classical learning tasks from a new perspective. In this work, we build on classical Wiener chaos expansions (WCE) to design neural operator (NO) architectures for SPDEs and SDEs: we project the driving noise paths onto orthonormal Wick Hermite features and parameterize the resulting deterministic chaos coefficients with neural operators, so that full solution trajectories can be reconstructed from noise in a single forward pass. On the theoretical side, we investigate the classical WCE results for the class of multi-dimensional SDEs and semilinear SPDEs considered here by explicitly writing down the associated coupled ODE/PDE systems for their chaos coefficients, which makes the separation between stochastic forcing and deterministic dynamics fully explicit and directly motivates our model designs. On the empirical side, we validate our models on a diverse suite of problems: classical SPDE benchmarks, diffusion one-step sampling on images, topological interpolation on graphs, financial extrapolation, parameter estimation, and manifold SDEs for flood prediction, demonstrating competitive accuracy and broad applicability. Overall, our results indicate that WCE-based neural operators provide a practical and scalable way to learn SDE/SPDE solution operators across diverse domains.

</details>


### [206] [Wireless Dataset Similarity: Measuring Distances in Supervised and Unsupervised Machine Learning](https://arxiv.org/abs/2601.01023)
*João Morais,Sadjad Alikhani,Akshay Malhotra,Shahab Hamidi-Rad,Ahmed Alkhateeb*

Main category: cs.LG

TL;DR: 论文提出了一个任务和模型感知的无线数据集相似性度量框架，用于预测跨数据集的可迁移性，并在CSI压缩和波束预测任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 无线通信中需要比较不同数据集的相似性，以支持数据集选择/增强、仿真到真实环境比较、任务特定合成数据生成等应用，但缺乏有效的度量方法。

Method: 提出任务和模型感知的框架，使用UMAP嵌入结合Wasserstein和欧几里得距离度量数据集距离，并通过监督UMAP和数据集不平衡惩罚来推导标签感知距离。

Result: 在CSI压缩任务中，数据集距离与跨数据集性能的Pearson相关系数超过0.85；在波束预测任务中，提出的距离度量优于传统基线，与模型可迁移性有更强相关性。

Conclusion: 该框架能够有效度量无线数据集的相似性，支持任务相关的数据集比较，为模型训练和适应新部署提供决策依据。

Abstract: This paper introduces a task- and model-aware framework for measuring similarity between wireless datasets, enabling applications such as dataset selection/augmentation, simulation-to-real (sim2real) comparison, task-specific synthetic data generation, and informing decisions on model training/adaptation to new deployments. We evaluate candidate dataset distance metrics by how well they predict cross-dataset transferability: if two datasets have a small distance, a model trained on one should perform well on the other. We apply the framework on an unsupervised task, channel state information (CSI) compression, using autoencoders. Using metrics based on UMAP embeddings, combined with Wasserstein and Euclidean distances, we achieve Pearson correlations exceeding 0.85 between dataset distances and train-on-one/test-on-another task performance. We also apply the framework to a supervised beam prediction in the downlink using convolutional neural networks. For this task, we derive a label-aware distance by integrating supervised UMAP and penalties for dataset imbalance. Across both tasks, the resulting distances outperform traditional baselines and consistently exhibit stronger correlations with model transferability, supporting task-relevant comparisons between wireless datasets.

</details>


### [207] [Coarse-Grained Kullback--Leibler Control of Diffusion-Based Generative AI](https://arxiv.org/abs/2601.01045)
*Tatsuaki Tsuruyama*

Main category: cs.LG

TL;DR: 提出基于信息论Lyapunov函数V-δ的投影反向扩散方法，用于在生成模型中显式控制粗粒度统计量（如图像块强度），同时保持像素级精度和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型缺乏理论描述粗粒度统计量（如空间块强度）在反向扩散过程中的演化机制，需要开发能显式控制这些统计量的方法。

Method: 移植信息论Lyapunov函数V-δ框架到生成模型的反向扩散过程，提出V-δ投影反向扩散方案，扩展V的单调性到时间非齐次块保持马尔可夫核，并在块常数图像和简化反向核的玩具模型中进行数值验证。

Result: 数值实验表明，所提方法能将块质量误差和泄漏容忍势函数控制在预设容差内，同时达到与非投影动力学相当的像素级精度和视觉质量。

Conclusion: 将生成采样重新解释为从噪声到数据的信息势函数下降过程，为具有显式粗粒度统计量控制的反向扩散过程提供了设计原则。

Abstract: Diffusion models and score-based generative models provide a powerful framework for synthesizing high-quality images from noise. However, there is still no satisfactory theory that describes how coarse-grained quantities, such as blockwise intensity or class proportions after partitioning an image into spatial blocks, are preserved and evolve along the reverse diffusion dynamics. In previous work, the author introduced an information-theoretic Lyapunov function V for non-ergodic Markov processes on a state space partitioned into blocks, defined as the minimal Kullback-Leibler divergence to the set of stationary distributions reachable from a given initial condition, and showed that a leak-tolerant potential V-delta with a prescribed tolerance for block masses admits a closed-form expression as a scaling-and-clipping operation on block masses.
  In this paper, I transplant this framework to the reverse diffusion process in generative models and propose a reverse diffusion scheme that is projected by the potential V-delta (referred to as the V-delta projected reverse diffusion). I extend the monotonicity of V to time-inhomogeneous block-preserving Markov kernels and show that, under small leakage and the V-delta projection, V-delta acts as an approximate Lyapunov function. Furthermore, using a toy model consisting of block-constant images and a simplified reverse kernel, I numerically demonstrate that the proposed method keeps the block-mass error and the leak-tolerant potential within the prescribed tolerance, while achieving pixel-wise accuracy and visual quality comparable to the non-projected dynamics. This study reinterprets generative sampling as a decrease of an information potential from noise to data, and provides a design principle for reverse diffusion processes with explicit control of coarse-grained quantities.

</details>


### [208] [A UCB Bandit Algorithm for General ML-Based Estimators](https://arxiv.org/abs/2601.01061)
*Yajing Liu,Erkao Bao,Linqi Song*

Main category: cs.LG

TL;DR: ML-UCB算法将任意机器学习模型集成到多臂老虎机框架中，通过直接建模估计器的学习曲线行为，无需模型特定的理论分析即可实现次线性遗憾


<details>
  <summary>Details</summary>
Motivation: 在序列决策中部署复杂ML模型的主要挑战是缺乏可处理的集中不等式来进行有原则的探索。现有方法通常需要针对特定模型进行理论分析，限制了通用性。

Method: 提出ML-UCB算法，假设均方误差随训练样本数呈幂律下降，推导出广义集中不等式，并证明算法能达到次线性遗憾。该方法仅需经验性地表征模型的学习曲线。

Result: 在协同过滤推荐系统的实验中，使用在线矩阵分解和模拟简化双塔模型的合成数据，ML-UCB相比LinUCB取得了显著改进。

Conclusion: ML-UCB为任意机器学习模型提供了有原则的多臂老虎机集成框架，通过建模学习曲线行为克服了传统方法需要模型特定理论分析的局限性，实现了通用且有效的序列决策。

Abstract: We present ML-UCB, a generalized upper confidence bound algorithm that integrates arbitrary machine learning models into multi-armed bandit frameworks. A fundamental challenge in deploying sophisticated ML models for sequential decision-making is the lack of tractable concentration inequalities required for principled exploration. We overcome this limitation by directly modeling the learning curve behavior of the underlying estimator. Specifically, assuming the Mean Squared Error decreases as a power law in the number of training samples, we derive a generalized concentration inequality and prove that ML-UCB achieves sublinear regret. This framework enables the principled integration of any ML model whose learning curve can be empirically characterized, eliminating the need for model-specific theoretical analysis. We validate our approach through experiments on a collaborative filtering recommendation system using online matrix factorization with synthetic data designed to simulate a simplified two-tower model, demonstrating substantial improvements over LinUCB

</details>


### [209] [SPoRC-VIST: A Benchmark for Evaluating Generative Natural Narrative in Vision-Language Models](https://arxiv.org/abs/2601.01062)
*Yunlin Zeng*

Main category: cs.LG

TL;DR: 本文提出了一种端到端的视觉播客生成管道，通过微调Qwen3-VL-32B模型，使用合成到真实的训练策略，在视觉叙事任务中显著提升了对话自然性和叙事深度。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在描述性任务上表现出色，但在生成引人入胜的长篇叙事（特别是多说话者播客对话）方面仍未被充分探索且难以评估。现有标准指标无法捕捉对话自然性、个性和叙事流畅性等细微差别。

Method: 1. 提出端到端视觉播客生成管道；2. 在4000个图像-对话对数据集上微调Qwen3-VL-32B模型；3. 采用合成到真实的训练策略：在SPoRC高质量播客对话与合成生成图像上训练，在VIST真实世界照片序列上评估；4. 提出综合评估框架，使用AI作为评判和新型风格指标。

Result: 微调的32B模型在对话自然性上显著优于235B基础模型（胜率>80%），叙事深度提升50%（平均对话轮次长度增加），同时保持相同的视觉基础能力（CLIPScore: 20.39）。

Conclusion: 通过合成到真实的训练策略和综合评估框架，视觉语言模型能够有效生成自然、深入的视觉播客对话，为长篇叙事生成提供了新的解决方案。

Abstract: Vision-Language Models (VLMs) have achieved remarkable success in descriptive tasks such as image captioning and visual question answering (VQA). However, their ability to generate engaging, long-form narratives -- specifically multi-speaker podcast dialogues -- remains under-explored and difficult to evaluate. Standard metrics like BLEU and ROUGE fail to capture the nuances of conversational naturalness, personality, and narrative flow, often rewarding safe, repetitive outputs over engaging storytelling. In this work, we present a novel pipeline for end-to-end visual podcast generation, and fine-tune a Qwen3-VL-32B model on a curated dataset of 4,000 image-dialogue pairs. Crucially, we use a synthetic-to-real training strategy: we train on high-quality podcast dialogues from the Structured Podcast Research Corpus (SPoRC) paired with synthetically generated imagery, and evaluate on real-world photo sequences from the Visual Storytelling Dataset (VIST). This rigorous setup tests the model's ability to generalize from synthetic training data to real-world visual domains. We propose a comprehensive evaluation framework that moves beyond textual overlap, and use AI-as-a-judge (Gemini 3 Pro, Claude Opus 4.5, GPT 5.2) and novel style metrics (average turn length, speaker switch rate) to assess quality. Our experiments demonstrate that our fine-tuned 32B model significantly outperforms a 235B base model in conversational naturalness ($>$80\% win rate) and narrative depth (+50\% turn length), while maintaining identical visual grounding capabilities (CLIPScore: 20.39).

</details>


### [210] [Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments](https://arxiv.org/abs/2601.01075)
*Hansen Jin Lillemark,Benhao Huang,Fangneng Zhan,Yilun Du,Thomas Anderson Keller*

Main category: cs.LG

TL;DR: 提出Flow Equivariant World Models框架，将自运动和外部物体运动统一为单参数李群"流"，利用群等变性实现稳定潜在世界表示，在2D/3D部分观测视频世界建模基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络世界模型忽略了现实世界中连续感官输入流所遵循的平滑时间参数化对称性结构，导致重复从数据中学习相同变换，缺乏效率。

Method: 将自运动和外部物体运动统一为单参数李群"流"，实现对这些变换的群等变性，从而提供数百时间步的稳定潜在世界表示。

Result: 在2D和3D部分观测视频世界建模基准上，Flow Equivariant World Models显著优于最先进的基于扩散和记忆增强的世界建模架构，特别是在智能体当前视野外存在可预测世界动态时表现突出，且对长序列预测具有良好泛化能力。

Conclusion: 通过将世界模型表示结构与内部和外部运动对齐，流等变性为数据高效、对称性引导的具身智能提供了一条可扩展的路径。

Abstract: Embodied systems experience the world as 'a symphony of flows': a combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, time-parameterized symmetries, which combine through a precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce 'Flow Equivariant World Models', a framework in which both self-motion and external object motion are unified as one-parameter Lie group 'flows'. We leverage this unification to implement group equivariance with respect to these transformations, thereby providing a stable latent world representation over hundreds of timesteps. On both 2D and 3D partially observed video world modeling benchmarks, we demonstrate that Flow Equivariant World Models significantly outperform comparable state-of-the-art diffusion-based and memory-augmented world modeling architectures -- particularly when there are predictable world dynamics outside the agent's current field of view. We show that flow equivariance is particularly beneficial for long rollouts, generalizing far beyond the training horizon. By structuring world model representations with respect to internal and external motion, flow equivariance charts a scalable route to data efficient, symmetry-guided, embodied intelligence. Project link: https://flowequivariantworldmodels.github.io.

</details>


### [211] [Discount Model Search for Quality Diversity Optimization in High-Dimensional Measure Spaces](https://arxiv.org/abs/2601.01082)
*Bryon Tjanaka,Henry Chen,Matthew C. Fontaine,Stefanos Nikolaidis*

Main category: cs.LG

TL;DR: DMS（折扣模型搜索）通过使用连续折扣模型替代直方图，解决了高维度量空间中QD优化的探索停滞问题，在图像等高维度量空间上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统QD算法在处理高维度量空间时存在失真问题，许多解映射到相似的度量值，导致探索停滞。CMA-MAE等现有方法使用直方图记录折扣值，但在高维空间中相似度量的解会落入同一单元格，获得相同折扣值，限制了探索能力。

Method: 提出折扣模型搜索（DMS），使用一个提供平滑连续折扣值表示的模型来指导探索。该模型能够区分具有相似度量的解，从而在高维度量空间中实现持续探索。特别适用于图像等高维度量空间，用户可以通过提供图像数据集而非手动设计度量函数来指定期望的度量。

Result: 在高维基准测试和图像度量空间的新应用中，DMS在性能上超越了CMA-MAE和其他黑盒QD算法，能够有效处理高维度量空间的探索问题。

Conclusion: DMS通过连续折扣模型解决了高维度量空间中QD优化的探索停滞问题，为QD应用开辟了新方向，特别是在用户可以通过图像数据集指定度量的场景中表现出色。

Abstract: Quality diversity (QD) optimization searches for a collection of solutions that optimize an objective while attaining diverse outputs of a user-specified, vector-valued measure function. Contemporary QD algorithms focus on low-dimensional measures because high-dimensional measures are prone to distortion, where many solutions found by the QD algorithm map to similar measures. For example, the CMA-MAE algorithm guides measure space exploration with a histogram in measure space that records so-called discount values. However, CMA-MAE stagnates in domains with high-dimensional measure spaces because solutions with similar measures fall into the same histogram cell and thus receive identical discount values. To address these limitations, we propose Discount Model Search (DMS), which guides exploration with a model that provides a smooth, continuous representation of discount values. In high-dimensional measure spaces, this model enables DMS to distinguish between solutions with similar measures and thus continue exploration. We show that DMS facilitates new QD applications by introducing two domains where the measure space is the high-dimensional space of images, which enables users to specify their desired measures by providing a dataset of images rather than hand-designing the measure function. Results in these domains and on high-dimensional benchmarks show that DMS outperforms CMA-MAE and other black-box QD algorithms.

</details>


### [212] [Central Dogma Transformer: Towards Mechanism-Oriented AI for Cellular Understanding](https://arxiv.org/abs/2601.01089)
*Nobuyuki Ota*

Main category: cs.LG

TL;DR: CDT是一个整合DNA、RNA和蛋白质预训练模型的架构，遵循中心法则方向性逻辑，通过交叉注意力机制生成虚拟细胞嵌入，在CRISPRi增强子扰动数据上验证有效。


<details>
  <summary>Details</summary>
Motivation: 当前针对DNA、RNA和蛋白质的领域特定基础模型虽然各自成功，但相互隔离，限制了我们对整合细胞过程建模的能力。需要遵循中心法则方向性逻辑的整合架构。

Method: 提出中心法则变换器(CDT)，整合DNA、RNA和蛋白质的预训练语言模型，采用方向性交叉注意力机制：DNA-to-RNA注意力建模转录调控，RNA-to-Protein注意力建模翻译关系，生成统一的虚拟细胞嵌入。

Result: 在K562细胞的CRISPRi增强子扰动数据上验证，Pearson相关系数达到0.503，占交叉实验变异性理论上限(r=0.797)的63%。注意力和梯度分析提供了互补的解释窗口，梯度分析识别出CTCF结合位点。

Conclusion: 与生物信息流对齐的AI架构既能实现预测准确性，又能获得机制可解释性，为整合多模态细胞建模提供了有前景的方向。

Abstract: Understanding cellular mechanisms requires integrating information across DNA, RNA, and protein - the three molecular systems linked by the Central Dogma of molecular biology. While domain-specific foundation models have achieved success for each modality individually, they remain isolated, limiting our ability to model integrated cellular processes. Here we present the Central Dogma Transformer (CDT), an architecture that integrates pre-trained language models for DNA, RNA, and protein following the directional logic of the Central Dogma. CDT employs directional cross-attention mechanisms - DNA-to-RNA attention models transcriptional regulation, while RNA-to-Protein attention models translational relationships - producing a unified Virtual Cell Embedding that integrates all three modalities. We validate CDT v1 - a proof-of-concept implementation using fixed (non-cell-specific) RNA and protein embeddings - on CRISPRi enhancer perturbation data from K562 cells, achieving a Pearson correlation of 0.503, representing 63% of the theoretical ceiling set by cross-experiment variability (r = 0.797). Attention and gradient analyses provide complementary interpretive windows: in detailed case studies, these approaches highlight largely distinct genomic regions, with gradient analysis identifying a CTCF binding site that Hi-C data showed as physically contacting both enhancer and target gene. These results suggest that AI architectures aligned with biological information flow can achieve both predictive accuracy and mechanistic interpretability.

</details>


### [213] [Community-Based Early-Stage Chronic Kidney Disease Screening using Explainable Machine Learning for Low-Resource Settings](https://arxiv.org/abs/2601.01119)
*Muhammad Ashad Kabir,Sirajam Munira,Dewan Tasnia Azad,Saleh Mohammed Ikram,Mohammad Habibur Rahman Sarker,Syed Manzoor Ahmed Hanifi*

Main category: cs.LG

TL;DR: 开发了一个可解释的机器学习框架，用于孟加拉国和南亚人群的早期慢性肾病社区筛查，相比现有工具显著提高了准确性和敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有慢性肾病筛查工具主要基于高收入国家人群开发，在孟加拉国和南亚地区表现不佳，因为这些地区的风险特征不同。这些工具通常使用简单的加性评分函数，基于晚期肾病患者数据，无法捕捉风险因素的复杂相互作用，也难以预测早期肾病。

Method: 使用孟加拉国首个社区慢性肾病数据集，评估了12种机器学习分类器。应用10种互补的特征选择技术识别稳健、可泛化的预测因子。最终模型采用10折交叉验证评估，并在印度、阿联酋和孟加拉国的三个独立数据集上进行外部验证。使用SHAP提供模型可解释性。

Result: 基于RFECV选择特征子集的机器学习模型达到90.40%的平衡准确率；仅使用最少非病理学测试特征也能达到89.23%的平衡准确率，通常优于更大或完整特征集。相比现有筛查工具，提出的模型在准确性和敏感性方面显著提高，同时需要更少且更易获取的输入。外部验证显示78%到98%的敏感性，证实了良好的泛化能力。

Conclusion: 该研究开发了一个可解释的机器学习框架，专门针对孟加拉国和南亚人群的早期慢性肾病筛查，在低资源环境中表现出色。模型不仅性能优于现有工具，而且通过SHAP解释识别出与已知肾病风险因素一致的临床有意义预测因子，为资源有限地区的早期肾病筛查提供了有效解决方案。

Abstract: Early detection of chronic kidney disease (CKD) is essential for preventing progression to end-stage renal disease. However, existing screening tools - primarily developed using populations from high-income countries - often underperform in Bangladesh and South Asia, where risk profiles differ. Most of these tools rely on simple additive scoring functions and are based on data from patients with advanced-stage CKD. Consequently, they fail to capture complex interactions among risk factors and are limited in predicting early-stage CKD. Our objective was to develop and evaluate an explainable machine learning (ML) framework for community-based early-stage CKD screening for low-resource settings, tailored to the Bangladeshi and South Asian population context. We used a community-based dataset from Bangladesh, the first such CKD dataset in South and South Asia, and evaluated twelve ML classifiers across multiple feature domains. Ten complementary feature selection techniques were applied to identify robust, generalizable predictors. The final models were assessed using 10-fold cross-validation. External validation was conducted on three independent datasets from India, the UAE, and Bangladesh. SHAP (SHapley Additive exPlanations) was used to provide model explainability. An ML model trained on an RFECV-selected feature subset achieved a balanced accuracy of 90.40%, whereas minimal non-pathology-test features demonstrated excellent predictive capability with a balanced accuracy of 89.23%, often outperforming larger or full feature sets. Compared with existing screening tools, the proposed models achieved substantially higher accuracy and sensitivity while requiring fewer and more accessible inputs. External validation confirmed strong generalizability with 78% to 98% sensitivity. SHAP interpretation identified clinically meaningful predictors consistent with established CKD risk factors.

</details>


### [214] [Learning from Historical Activations in Graph Neural Networks](https://arxiv.org/abs/2601.01123)
*Yaniv Galron,Hadar Sinai,Haggai Maron,Moshe Eliasof*

Main category: cs.LG

TL;DR: HISTOGRAPH：一种基于注意力的两阶段图聚合层，利用中间层激活历史来提升图分类性能，特别适用于深层GNN。


<details>
  <summary>Details</summary>
Motivation: 现有图池化方法仅使用最后一层GNN特征，未能充分利用前向传播过程中产生的中间层激活历史。这在节点表示可能发生显著变化的深层架构中尤为明显，且图特有的过平滑问题会加剧这一缺陷。

Method: 提出HISTOGRAPH，一种新颖的两阶段注意力聚合层：1）层间注意力：统一地对中间层激活进行加权；2）节点注意力：在层间聚合的基础上进行节点级注意力。通过建模节点表示在不同层间的演化过程，利用节点的激活历史和图结构来精炼最终预测特征。

Result: 在多个图分类基准测试上的实验结果表明，HISTOGRAPH提供了强大的性能，持续改进传统技术，在深层GNN中表现出特别强的鲁棒性。

Conclusion: HISTOGRAPH通过有效利用中间层激活历史，解决了现有图池化方法信息利用不足的问题，特别是在深层GNN架构中表现出优越的性能和鲁棒性。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable success in various domains such as social networks, molecular chemistry, and more. A crucial component of GNNs is the pooling procedure, in which the node features calculated by the model are combined to form an informative final descriptor to be used for the downstream task. However, previous graph pooling schemes rely on the last GNN layer features as an input to the pooling or classifier layers, potentially under-utilizing important activations of previous layers produced during the forward pass of the model, which we regard as historical graph activations. This gap is particularly pronounced in cases where a node's representation can shift significantly over the course of many graph neural layers, and worsened by graph-specific challenges such as over-smoothing in deep architectures. To bridge this gap, we introduce HISTOGRAPH, a novel two-stage attention-based final aggregation layer that first applies a unified layer-wise attention over intermediate activations, followed by node-wise attention. By modeling the evolution of node representations across layers, our HISTOGRAPH leverages both the activation history of nodes and the graph structure to refine features used for final prediction. Empirical results on multiple graph classification benchmarks demonstrate that HISTOGRAPH offers strong performance that consistently improves traditional techniques, with particularly strong robustness in deep GNNs.

</details>


### [215] [Self-Training the Neurochaos Learning Algorithm](https://arxiv.org/abs/2601.01146)
*Anusree M,Akhila Henry,Pramod P Nair*

Main category: cs.LG

TL;DR: 提出一种结合神经混沌学习与自训练的混合半监督学习架构，在数据稀缺和非平衡数据集上显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 实际应用中获取大量标注数据困难且昂贵，而无标注数据容易获得。传统监督学习方法在标注数据少或数据集不平衡时表现不佳。

Method: 结合神经混沌学习(NL)与基于阈值的自训练(ST)方法。NL将输入特征转换为混沌发放率表示以捕捉数据非线性关系，ST通过高置信度伪标注样本逐步扩展标注集。

Result: 在10个基准数据集和5个机器学习分类器上评估，仅使用15%标注数据和85%无标注数据。NL+ST架构相比单独ST模型获得显著性能提升，特别是在Iris(188.66%)、Wine(158.58%)和Glass Identification(110.48%)等有限、非线性和不平衡数据集上。

Conclusion: 基于混沌的特征提取与半监督学习结合，在低数据场景下提高了泛化能力、鲁棒性和分类准确性。

Abstract: In numerous practical applications, acquiring substantial quantities of labelled data is challenging and expensive, but unlabelled data is readily accessible. Conventional supervised learning methods frequently underperform in scenarios characterised by little labelled data or imbalanced datasets. This study introduces a hybrid semi-supervised learning (SSL) architecture that integrates Neurochaos Learning (NL) with a threshold-based Self-Training (ST) method to overcome this constraint. The NL architecture converts input characteristics into chaos-based ring-rate representations that encapsulate nonlinear relationships within the data, whereas ST progressively enlarges the labelled set utilising high-confidence pseudo-labelled samples. The model's performance is assessed using ten benchmark datasets and five machine learning classifiers, with 85% of the training data considered unlabelled and just 15% utilised as labelled data. The proposed Self-Training Neurochaos Learning (NL+ST) architecture consistently attains superior performance gain relative to standalone ST models, especially on limited, nonlinear and imbalanced datasets like Iris (188.66%), Wine (158.58%) and Glass Identification (110.48%). The results indicate that using chaos-based feature extraction with SSL improves generalisation, resilience, and classification accuracy in low-data contexts.

</details>


### [216] [Evo-TFS: Evolutionary Time-Frequency Domain-Based Synthetic Minority Oversampling Approach to Imbalanced Time Series Classification](https://arxiv.org/abs/2601.01150)
*Wenbin Pei,Ruohao Dai,Bing Xue,Mengjie Zhang,Qiang Zhang,Yiu-Ming Cheung*

Main category: cs.LG

TL;DR: Evo-TFS：一种结合时域和频域特征的进化过采样方法，用于解决不平衡时间序列分类问题，通过强类型遗传编程生成多样化的高质量时间序列样本。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法假设数据分布平衡，在不平衡时间序列数据中会忽略少数类；传统过采样方法依赖线性插值，难以保持时间动态特性和生成多样化样本。

Method: 提出Evo-TFS方法，使用强类型遗传编程进化生成多样化高质量时间序列，通过结合时域和频域特征的适应度函数指导进化过程。

Result: 在不平衡时间序列数据集上的实验表明，Evo-TFS优于现有过采样方法，显著提升了时域和频域分类器的性能。

Conclusion: Evo-TFS通过进化方法有效解决了不平衡时间序列分类问题，能够生成保持时间动态特性的多样化少数类样本，为实际应用提供了更好的解决方案。

Abstract: Time series classification is a fundamental machine learning task with broad real-world applications. Although many deep learning methods have proven effective in learning time-series data for classification, they were originally developed under the assumption of balanced data distributions. Once data distribution is uneven, these methods tend to ignore the minority class that is typically of higher practical significance. Oversampling methods have been designed to address this by generating minority-class samples, but their reliance on linear interpolation often hampers the preservation of temporal dynamics and the generation of diverse samples. Therefore, in this paper, we propose Evo-TFS, a novel evolutionary oversampling method that integrates both time- and frequency-domain characteristics. In Evo-TFS, strongly typed genetic programming is employed to evolve diverse, high-quality time series, guided by a fitness function that incorporates both time-domain and frequency-domain characteristics. Experiments conducted on imbalanced time series datasets demonstrate that Evo-TFS outperforms existing oversampling methods, significantly enhancing the performance of time-domain and frequency-domain classifiers.

</details>


### [217] [Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models](https://arxiv.org/abs/2601.01162)
*Zihua Yang,Xin Liao,Yiqun Zhang,Yiu-ming Cheung*

Main category: cs.LG

TL;DR: ARISE利用大语言模型为分类数据提供语义嵌入，增强聚类效果，在8个基准数据集上相比7个对比方法提升19-27%


<details>
  <summary>Details</summary>
Motivation: 分类数据（如医疗、营销、生物信息学）缺乏内在排序或距离度量，传统方法将不同值视为等距，造成语义鸿沟。现有方法依赖数据集内共现模式推断值间关系，但在样本有限时不可靠，未能充分利用数据的语义上下文。

Method: 提出ARISE方法，利用大语言模型的外部语义知识构建语义感知表示，补充分类数据的度量空间。具体使用LLM描述属性值以增强表示，将LLM增强的嵌入与原始数据结合，探索语义显著的聚类。

Result: 在8个基准数据集上的实验表明，相比7个代表性对比方法，ARISE取得了一致的改进，性能提升达到19-27%。

Conclusion: ARISE通过整合大语言模型的语义知识，有效解决了分类数据聚类中的语义鸿沟问题，显著提升了聚类质量，特别是在样本有限的情况下。

Abstract: Categorical data are prevalent in domains such as healthcare, marketing, and bioinformatics, where clustering serves as a fundamental tool for pattern discovery. A core challenge in categorical data clustering lies in measuring similarity among attribute values that lack inherent ordering or distance. Without appropriate similarity measures, values are often treated as equidistant, creating a semantic gap that obscures latent structures and degrades clustering quality. Although existing methods infer value relationships from within-dataset co-occurrence patterns, such inference becomes unreliable when samples are limited, leaving the semantic context of the data underexplored. To bridge this gap, we present ARISE (Attention-weighted Representation with Integrated Semantic Embeddings), which draws on external semantic knowledge from Large Language Models (LLMs) to construct semantic-aware representations that complement the metric space of categorical data for accurate clustering. That is, LLM is adopted to describe attribute values for representation enhancement, and the LLM-enhanced embeddings are combined with the original data to explore semantically prominent clusters. Experiments on eight benchmark datasets demonstrate consistent improvements over seven representative counterparts, with gains of 19-27%. Code is available at https://github.com/develop-yang/ARISE

</details>


### [218] [MentalGame: Predicting Personality-Job Fitness for Software Developers Using Multi-Genre Games and Machine Learning Approaches](https://arxiv.org/abs/2601.01206)
*Soroush Elyasi,Arya VarastehNezhad,Fattaneh Taghiyareh*

Main category: cs.LG

TL;DR: 使用多类型严肃游戏框架结合机器学习预测软件开发岗位适配性，通过游戏行为数据而非传统问卷评估人格特质，达到97%精度和94%准确率。


<details>
  <summary>Details</summary>
Motivation: 传统职业评估依赖自我报告问卷，存在回答偏差、疲劳和故意扭曲等问题。游戏化评估通过捕捉游戏中的隐式行为信号，提供更客观、可扩展且偏误更少的替代方案。

Method: 通过文献综述和软件工程师实证研究确定相关人格行为特质，设计定制移动游戏激发问题解决、规划、适应性等行为。采用两阶段建模策略，仅从游戏行为特征预测岗位适配性。

Result: 模型达到97%精度和94%准确率。合适候选人表现出独特游戏模式：解谜游戏获胜更多、完成更多侧挑战、更频繁导航菜单、较少暂停/重试/放弃行为。

Conclusion: 游戏过程中捕捉的隐式行为痕迹能有效预测软件开发适配性，无需显式人格测试。严肃游戏可作为职业评估的可扩展、吸引人且偏误更少的替代方案。

Abstract: Personality assessment in career guidance and personnel selection traditionally relies on self-report questionnaires, which are susceptible to response bias, fatigue, and intentional distortion. Game-based assessment offers a promising alternative by capturing implicit behavioral signals during gameplay. This study proposes a multi-genre serious-game framework combined with machine-learning techniques to predict suitability for software development roles. Developer-relevant personality and behavioral traits were identified through a systematic literature review and an empirical study of professional software engineers. A custom mobile game was designed to elicit behaviors related to problem solving, planning, adaptability, persistence, time management, and information seeking. Fine-grained gameplay event data were collected and analyzed using a two-phase modeling strategy where suitability was predicted exclusively from gameplay-derived behavioral features. Results show that our model achieved up to 97% precision and 94% accuracy. Behavioral analysis revealed that proper candidates exhibited distinct gameplay patterns, such as more wins in puzzle-based games, more side challenges, navigating menus more frequently, and exhibiting fewer pauses, retries, and surrender actions. These findings demonstrate that implicit behavioral traces captured during gameplay is promising in predicting software-development suitability without explicit personality testing, supporting serious games as a scalable, engaging, and less biased alternative for career assessment.

</details>


### [219] [The Dependency Divide: An Interpretable Machine Learning Framework for Profiling Student Digital Satisfaction in the Bangladesh Context](https://arxiv.org/abs/2601.01231)
*Md Muhtasim Munif Fahim,Humyra Ankona,Md Monimul Huq,Md Rezaul Karim*

Main category: cs.LG

TL;DR: 研究发现数字学习中的"依赖鸿沟"现象：高度投入的学生在基础设施不可靠时反而更易受挫，挑战了传统认为投入越多越受益的假设。


<details>
  <summary>Details</summary>
Motivation: 传统数字鸿沟框架无法解释在资源有限环境中，拥有相似连接条件的学生对数字学习平台满意度差异显著的问题。研究旨在探索在基础设施脆弱的后接入环境中，学生参与度与满意度之间的复杂关系。

Method: 对孟加拉国396名大学生进行横断面研究，采用三阶段分析方法：1) K-prototypes聚类识别学生特征；2) 随机森林模型结合SHAP和ALE分析确定满意度驱动因素；3) 倾向得分匹配进行正式交互分析验证依赖鸿沟假设。

Result: 识别出三类学生：偶尔参与型(58%)、高效学习者(35%)和高度投入型(7%)。发现教育设备使用时间与网络可靠性存在显著交互作用，证实了依赖鸿沟的存在。高度投入型学生尽管拥有复杂数字工作流程，却表现出最大脆弱性。政策模拟显示，针对高依赖用户的可靠性改善比统一干预效果高2.06倍。

Conclusion: 在基础设施脆弱的环境中，能力可能成为负担。数字转型政策应优先保障依赖倾向用户的可靠性，建立应急系统，并教育学生了解依赖风险，而非统一推广参与度。

Abstract: Background: While digital access has expanded rapidly in resource-constrained contexts, satisfaction with digital learning platforms varies significantly among students with seemingly equal connectivity. Traditional digital divide frameworks fail to explain these variations.
  Purpose: This study introduces the "Dependency Divide", a novel framework proposing that highly engaged students become conditionally vulnerable to infrastructure failures, challenging assumptions that engagement uniformly benefits learners in post-access environments.
  Methods: We conducted a cross-sectional study of 396 university students in Bangladesh using a three-stage analytical approach: (1) stability-validated K-prototypes clustering to identify student profiles, (2) profile-specific Random Forest models with SHAP and ALE analysis to determine satisfaction drivers, and (3) formal interaction analysis with propensity score matching to test the Dependency Divide hypothesis.
  Results: Three distinct profiles emerged: Casually Engaged (58%), Efficient Learners (35%), and Hyper-Engaged (7%). A significant interaction between educational device time and internet reliability (\b{eta} = 0.033, p = 0.028) confirmed the Dependency Divide: engagement increased satisfaction only when infrastructure remained reliable. Hyper-Engaged students showed greatest vulnerability despite or because of their sophisticated digital workflows. Policy simulations demonstrated that targeted reliability improvements for high-dependency users yielded 2.06 times greater returns than uniform interventions.
  Conclusions: In fragile infrastructure contexts, capability can become liability. Digital transformation policies must prioritize reliability for dependency-prone users, establish contingency systems, and educate students about dependency risks rather than uniformly promoting engagement.

</details>


### [220] [Benchmarking the Computational and Representational Efficiency of State Space Models against Transformers on Long-Context Dyadic Sessions](https://arxiv.org/abs/2601.01237)
*Abidemi Koledoye,Chinemerem Unachukwu,Gold Nwobu,Hasin Rana*

Main category: cs.LG

TL;DR: 该研究对比了Mamba SSM与LLaMA Transformer在长上下文序列建模中的性能，特别以治疗会话为测试案例，发现SSM在计算复杂度上具有线性优势，但在某些条件下Transformer仍表现更好。


<details>
  <summary>Details</summary>
Motivation: 随着长上下文序列建模需求的增长，需要寻找比Transformer更高效的架构。SSM因其线性计算复杂度而成为有前景的替代方案，但缺乏与Transformer在实际长上下文任务中的系统比较。

Method: 使用治疗会话作为代表性测试案例，从两个维度对比Mamba SSM和LLaMA Transformer：1) 计算效率（内存使用和推理速度，范围512-8192个token）；2) 表征效率（隐藏状态动态和注意力模式分析）。

Result: SSM在长序列上确实展现出线性计算复杂度的优势，内存使用和推理速度优于Transformer。但在某些特定条件下，Transformer在表征效率方面仍保持优势。研究为实践者提供了在不同条件下选择架构的具体指导。

Conclusion: SSM是Transformer在长上下文应用中的有效替代方案，特别是在计算效率要求高的场景下。研究明确了SSM相对于Transformer的优势条件，为实际应用提供了决策依据。

Abstract: State Space Models (SSMs) have emerged as a promising alternative to Transformers for long-context sequence modeling, offering linear $O(N)$ computational complexity compared to the Transformer's quadratic $O(N^2)$ scaling. This paper presents a comprehensive benchmarking study comparing the Mamba SSM against the LLaMA Transformer on long-context sequences, using dyadic therapy sessions as a representative test case. We evaluate both architectures across two dimensions: (1) computational efficiency, where we measure memory usage and inference speed from 512 to 8,192 tokens, and (2) representational efficiency, where we analyze hidden state dynamics and attention patterns. Our findings provide actionable insights for practitioners working with long-context applications, establishing precise conditions under which SSMs offer advantages over Transformers.

</details>


### [221] [Accelerated Full Waveform Inversion by Deep Compressed Learning](https://arxiv.org/abs/2601.01268)
*Maayan Gelboim,Amir Adler,Mauricio Araya-Polo*

Main category: cs.LG

TL;DR: 提出一种基于深度学习的全波形反演数据降维方法，通过压缩学习和分层选择减少计算成本


<details>
  <summary>Details</summary>
Motivation: 工业级全波形反演需要太字节级数据存储，计算成本过高，限制了复杂地下情况分析和多场景探索

Method: 使用带二值化感知层的深度神经网络进行压缩学习，从大量地下模型中学习简洁但重要的地震采集布局；然后通过自编码器计算数据的潜在表示，再用K-means聚类进一步选择最相关的数据

Result: 该方法在仅使用10%数据的情况下，在2D全波形反演中始终优于随机数据采样

Conclusion: 这种分层选择方法为大规模3D反演中加速全波形反演提供了可行路径

Abstract: We propose and test a method to reduce the dimensionality of Full Waveform Inversion (FWI) inputs as computational cost mitigation approach. Given modern seismic acquisition systems, the data (as input for FWI) required for an industrial-strength case is in the teraflop level of storage, therefore solving complex subsurface cases or exploring multiple scenarios with FWI become prohibitive. The proposed method utilizes a deep neural network with a binarized sensing layer that learns by compressed learning a succinct but consequential seismic acquisition layout from a large corpus of subsurface models. Thus, given a large seismic data set to invert, the trained network selects a smaller subset of the data, then by using representation learning, an autoencoder computes latent representations of the data, followed by K-means clustering of the latent representations to further select the most relevant data for FWI. Effectively, this approach can be seen as a hierarchical selection. The proposed approach consistently outperforms random data sampling, even when utilizing only 10% of the data for 2D FWI, these results pave the way to accelerating FWI in large scale 3D inversion.

</details>


### [222] [The Alchemy of Thought: Understanding In-Context Learning Through Supervised Classification](https://arxiv.org/abs/2601.01290)
*Harshita Narnoli,Mihai Surdeanu*

Main category: cs.LG

TL;DR: 本文通过比较ICL与基于ICL演示训练的监督分类器的行为，研究了ICL的工作原理，发现当演示相关性高时，LLMs行为类似于kNN分类器；当相关性低时，LLMs表现更好，因为它们可以利用参数记忆。


<details>
  <summary>Details</summary>
Motivation: 尽管ICL已成为快速定制LLMs的主要范式，但其工作原理尚不清楚。本文旨在通过比较ICL与监督分类器的行为来深入理解ICL的工作机制。

Method: 使用文本分类作为用例，在6个数据集和3个LLMs上，将ICL的行为与基于相同演示训练的监督分类器（梯度下降和k近邻）进行比较，研究三个核心问题。

Result: 当演示相关性高时，LLMs行为与这些分类器相似，且ICL更接近kNN而非逻辑回归；当演示相关性低时，LLMs表现优于这些分类器，因为LLMs可以回退到参数记忆。

Conclusion: ICL在演示相关性高时类似于kNN分类器，而在相关性低时LLMs可以利用参数记忆获得更好表现，这为理解ICL工作机制提供了实证证据。

Abstract: In-context learning (ICL) has become a prominent paradigm to rapidly customize LLMs to new tasks without fine-tuning. However, despite the empirical evidence of its usefulness, we still do not truly understand how ICL works. In this paper, we compare the behavior of in-context learning with supervised classifiers trained on ICL demonstrations to investigate three research questions: (1) Do LLMs with ICL behave similarly to classifiers trained on the same examples? (2) If so, which classifiers are closer, those based on gradient descent (GD) or those based on k-nearest neighbors (kNN)? (3) When they do not behave similarly, what conditions are associated with differences in behavior? Using text classification as a use case, with six datasets and three LLMs, we observe that LLMs behave similarly to these classifiers when the relevance of demonstrations is high. On average, ICL is closer to kNN than logistic regression, giving empirical evidence that the attention mechanism behaves more similarly to kNN than GD. However, when demonstration relevance is low, LLMs perform better than these classifiers, likely because LLMs can back off to their parametric memory, a luxury these classifiers do not have.

</details>


### [223] [Sobolev Approximation of Deep ReLU Network in Log-weighted Barron Space](https://arxiv.org/abs/2601.01295)
*Changhoon Song,Seungchan Ko,Youngjoon Hong*

Main category: cs.LG

TL;DR: 本文引入对数加权Barron空间，比经典Barron空间要求更弱的正则性条件，证明了深度ReLU网络在该空间中的逼近能力，阐明了深度如何降低高效表示所需的正则性要求。


<details>
  <summary>Details</summary>
Motivation: 经典Barron空间理论虽然解释了神经网络对某些函数的逼近能力，但仍需比Sobolev空间更强的正则性条件，且现有深度敏感结果常假设限制性条件（如sL ≤ 1/2）。需要更弱的函数空间来更好地解释深度模型在高维数据上的实际成功。

Method: 1. 引入对数加权Barron空间 ℬ^log，其正则性要求比任何ℬ^s（s>0）都弱；2. 研究该空间的嵌入性质并通过Rademacher复杂度进行统计分析；3. 证明ℬ^log中的函数可由深度ReLU网络以显式深度依赖逼近；4. 定义ℬ^{s,log}族，建立H^1范数下的逼近界，并确定保持这些速率的最大深度尺度。

Result: 1. 建立了对数加权Barron空间的嵌入性质和统计特性；2. 证明了深度ReLU网络在ℬ^log空间中的逼近能力；3. 获得了ℬ^{s,log}族在H^1范数下的逼近界；4. 确定了保持逼近速率的最大深度尺度。

Conclusion: 对数加权Barron空间为深度神经网络提供了更弱的正则性要求，阐明了深度如何降低高效表示所需的函数光滑性条件，为深度架构超越经典Barron设置的实际性能提供了更精确的解释，并为其在高维问题中的稳定使用提供了理论支持。

Abstract: Universal approximation theorems show that neural networks can approximate any continuous function; however, the number of parameters may grow exponentially with the ambient dimension, so these results do not fully explain the practical success of deep models on high-dimensional data. Barron space theory addresses this: if a target function belongs to a Barron space, a two-layer network with $n$ parameters achieves an $O(n^{-1/2})$ approximation error in $L^2$. Yet classical Barron spaces $\mathscr{B}^{s+1}$ still require stronger regularity than Sobolev spaces $H^s$, and existing depth-sensitive results often assume constraints such as $sL \le 1/2$. In this paper, we introduce a log-weighted Barron space $\mathscr{B}^{\log}$, which requires a strictly weaker assumption than $\mathscr{B}^s$ for any $s>0$. For this new function space, we first study embedding properties and carry out a statistical analysis via the Rademacher complexity. Then we prove that functions in $\mathscr{B}^{\log}$ can be approximated by deep ReLU networks with explicit depth dependence. We then define a family $\mathscr{B}^{s,\log}$, establish approximation bounds in the $H^1$ norm, and identify maximal depth scales under which these rates are preserved. Our results clarify how depth reduces regularity requirements for efficient representation, offering a more precise explanation for the performance of deep architectures beyond the classical Barron setting, and for their stable use in high-dimensional problems used today.

</details>


### [224] [ARGUS: Adaptive Rotation-Invariant Geometric Unsupervised System](https://arxiv.org/abs/2601.01297)
*Anantha Sharma*

Main category: cs.LG

TL;DR: Argus是一个用于高维数据流分布漂移检测的框架，通过固定空间分区跟踪局部统计量，具有正交变换不变性、O(N)复杂度、漂移传播图论分析和产品量化分块等特性。


<details>
  <summary>Details</summary>
Motivation: 高维数据流中的分布漂移检测面临三大挑战：全局比较方法扩展性差、基于投影的方法丢失几何结构、重新聚类方法存在身份不稳定性。需要一种既能保持高维结构又计算高效的方法。

Method: 将漂移检测重新定义为在数据流形固定空间分区上跟踪局部统计量。使用规范正交框架上的Voronoi细分获得正交变换不变的漂移度量，开发漂移传播的图论表征，引入产品量化细分扩展到超高维度。

Result: 证明了Voronoi细分在正交变换下的不变性，实现了O(N)复杂度并提供细胞级空间定位，能够区分连贯分布漂移与孤立扰动，在d>500的高维数据中有效工作，在坐标旋转下正确识别漂移而现有方法产生误报。

Conclusion: Argus框架为分布监控提供了原则性的几何基础，既保持了高维结构又避免了成对比较的计算负担，通过固定空间分区方法解决了高维数据流漂移检测的根本挑战。

Abstract: Detecting distributional drift in high-dimensional data streams presents fundamental challenges: global comparison methods scale poorly, projection-based approaches lose geometric structure, and re-clustering methods suffer from identity instability. This paper introduces Argus, A framework that reconceptualizes drift detection as tracking local statistics over a fixed spatial partition of the data manifold.
  The key contributions are fourfold. First, it is proved that Voronoi tessellations over canonical orthonormal frames yield drift metrics that are invariant to orthogonal transformations. The rotations and reflections that preserve Euclidean geometry. Second, it is established that this framework achieves O(N) complexity per snapshot while providing cell-level spatial localization of distributional change. Third, a graph-theoretic characterization of drift propagation is developed that distinguishes coherent distributional shifts from isolated perturbations. Fourth, product quantization tessellation is introduced for scaling to very high dimensions (d>500) by decomposing the space into independent subspaces and aggregating drift signals across subspaces.
  This paper formalizes the theoretical foundations, proves invariance properties, and presents experimental validation demonstrating that the framework correctly identifies drift under coordinate rotation while existing methods produce false positives. The tessellated approach offers a principled geometric foundation for distribution monitoring that preserves high-dimensional structure without the computational burden of pairwise comparisons.

</details>


### [225] [Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware](https://arxiv.org/abs/2601.01298)
*Jorge L. Ruiz Williams*

Main category: cs.LG

TL;DR: Warp Cortex是一个异步多智能体LLM框架，通过解耦智能体逻辑与物理内存，将内存复杂度从O(N*L)降低到O(1)权重和O(N*k)上下文，在单张RTX 4090上实现100个并发智能体仅需2.2GB显存。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体LLM框架存在线性内存扩展问题，使得"系统2"并行推理在消费级硬件上不切实际。需要解决内存瓶颈以实现大规模智能体认知扩展。

Method: 采用异步架构，通过Singleton权重共享和拓扑突触（受TDA混合地标技术启发）减少内存占用。将KV缓存视为潜在空间中的点云，应用见证复形稀疏化保留上下文流形的持久同调特征。引入非侵入式KV缓存更新机制Referential Injection。

Result: 在单张NVIDIA RTX 4090上，实现100个并发智能体仅需2.2GB总显存，理论容量超过1000个智能体（计算延迟成为瓶颈前）。内存复杂度从O(N*L)降低到O(1)权重和O(N*k)上下文。

Conclusion: Warp Cortex通过创新的内存优化和异步架构，理论上可实现百万智能体认知扩展，为大规模多智能体LLM系统在消费级硬件上的部署提供了可行方案。

Abstract: Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering "System 2" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.

</details>


### [226] [Spectral-Window Hybrid (SWH)](https://arxiv.org/abs/2601.01313)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: SWH是一种混合架构，通过并行全局分支（基于卷积定理建模长程衰减动态）和局部分支（滑动窗口注意力）来解耦序列建模，实现线性扩展至长序列，同时保持局部精度。


<details>
  <summary>Details</summary>
Motivation: 将序列建模扩展到极端上下文需要在计算效率和表示表达能力之间取得平衡。Transformer虽然通过注意力机制提供精确检索，但其二次方复杂度限制了在长序列任务中的应用。

Method: 提出Spectral-Window Hybrid (SWH)架构，将序列建模解耦为两个并行流：1) 全局分支利用卷积定理在O(T log T)时间内建模长程衰减动态；2) 局部分支采用滑动窗口注意力处理有界上下文内的token交互。通过聚合这些表示，SWH避免了全局注意力的计算瓶颈。

Result: SWH在短上下文上达到标准Transformer的困惑度水平，同时能够高效线性扩展到长序列。

Conclusion: SWH架构通过解耦全局和局部建模，在保持局部精度的同时实现了对长序列的高效线性扩展，解决了Transformer在长序列任务中的计算瓶颈问题。

Abstract: Scaling sequence modeling to extreme contexts requires balancing computational efficiency with representational expressivity. While Transformers provide precise retrieval via the attention mechanism, their quadratic $\mathcal{O}(T^2)$ complexity limits their application to long-horizon tasks. In this work, we propose the \textbf{Spectral-Window Hybrid (SWH)}, an architecture that decouples sequence modeling into two \textit{parallel} streams: a global branch utilizing the Convolution Theorem to model long-range decay dynamics in $\mathcal{O}(T \log T)$ time, and a local branch employing sliding-window attention for token interactions within a bounded context. By aggregating these representations, SWH avoids the computational bottleneck of global attention while retaining local precision. We demonstrate that SWH matches the perplexity of standard Transformers on short contexts while enabling efficient linear scaling to extended sequences. The code is available at https://github.com/VladimerKhasia/SWH

</details>


### [227] [From Classification to Generation: An Open-Ended Paradigm for Adverse Drug Reaction Prediction Based on Graph-Motif Feature Fusion](https://arxiv.org/abs/2601.01347)
*Yuyan Pi,Min Jin,Wentao Xie,Xinhua Liu*

Main category: cs.LG

TL;DR: GM-MLG提出了一种基于图-基序特征融合和多标签生成的开放式药物不良反应预测新范式，将ADR预测从多标签分类转变为基于Transformer解码器的多标签生成，显著扩展了预测空间并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前药物不良反应预测方法面临药物数据稀缺导致的冷启动问题、封闭标签集以及标签依赖关系建模不足等挑战，限制了预测能力和应用范围。

Method: 1) 构建原子级、局部分子级（通过BRICS算法动态提取细粒度基序）和全局分子级的双图表示架构；2) 将ADR预测从多标签分类转变为基于Transformer解码器的多标签生成；3) 将ADR标签视为离散标记序列，使用位置嵌入显式捕获标签依赖关系；4) 通过自回归解码动态扩展预测空间。

Result: GM-MLG实现了最高38%的性能提升，平均增益20%，将预测空间从200种扩展到超过10,000种。通过逆合成基序分析揭示了ADR与基序之间的非线性构效关系。

Conclusion: GM-MLG为药物安全性系统风险降低提供了可解释且创新的支持，通过开放式多标签生成范式有效解决了当前ADR预测的关键挑战。

Abstract: Computational biology offers immense potential for reducing the high costs and protracted cycles of new drug development through adverse drug reaction (ADR) prediction. However, current methods remain impeded by drug data scarcity-induced cold-start challenge, closed label sets, and inadequate modeling of label dependencies. Here we propose an open-ended ADR prediction paradigm based on Graph-Motif feature fusion and Multi-Label Generation (GM-MLG). Leveraging molecular structure as an intrinsic and inherent feature, GM-MLG constructs a dual-graph representation architecture spanning the atomic level, the local molecular level (utilizing fine-grained motifs dynamically extracted via the BRICS algorithm combined with additional fragmentation rules), and the global molecular level. Uniquely, GM-MLG pioneers transforming ADR prediction from multi-label classification into Transformer Decoder-based multi-label generation. By treating ADR labels as discrete token sequences, it employs positional embeddings to explicitly capture dependencies and co-occurrence relationships within large-scale label spaces, generating predictions via autoregressive decoding to dynamically expand the prediction space. Experiments demonstrate GM-MLG achieves up to 38% improvement and an average gain of 20%, expanding the prediction space from 200 to over 10,000 types. Furthermore, it elucidates non-linear structure-activity relationships between ADRs and motifs via retrosynthetic motif analysis, providing interpretable and innovative support for systematic risk reduction in drug safety.

</details>


### [228] [Towards LLM-enabled autonomous combustion research: A literature-aware agent for self-corrective modeling workflows](https://arxiv.org/abs/2601.01357)
*Ke Xiao,Haoze Zhang,Runze Mao,Han Li,Zhi X. Chen*

Main category: cs.LG

TL;DR: FlamePilot是一个专门用于燃烧建模的LLM智能体，通过自动化和自校正的CFD工作流程，将科学文献知识与计算流体动力学工具无缝集成，显著提升了燃烧模拟的效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型正在成为自主研究伙伴，但在燃烧建模等复杂科学领域仍存在关键差距。实际AI辅助需要将领域文献知识与专业计算工具（如CFD代码）的执行能力无缝集成。

Method: FlamePilot采用基于原子工具的架构，确保在OpenFOAM和DeepFlame等框架中稳健设置和执行复杂模拟。系统能够从科学文章中学习，提取关键信息指导从初始设置到优化结果的整个模拟过程。

Result: 在公共基准测试中，FlamePilot实现了完美的1.0可执行性得分和0.438的成功率，超过了先前最佳代理的0.625和0.250得分。在MILD燃烧模拟案例研究中，系统能够自主将研究论文转化为配置模拟、执行模拟、后处理结果、提出基于证据的改进建议，并在最少人工干预下管理多步骤参数研究直至收敛。

Conclusion: FlamePilot通过透明可解释的范式，为AI赋能的燃烧建模建立了基础框架，促进了研究人员与智能体之间的协作伙伴关系，其中智能体管理工作流程编排，使研究人员能够专注于高层次分析。

Abstract: The rapid evolution of large language models (LLMs) is transforming artificial intelligence into autonomous research partners, yet a critical gap persists in complex scientific domains such as combustion modeling. Here, practical AI assistance requires the seamless integration of domain literature knowledge with robust execution capabilities for expertise-intensive tools such as computational fluid dynamics (CFD) codes. To bridge this gap, we introduce FlamePilot, an LLM agent designed to empower combustion modeling research through automated and self-corrective CFD workflows. FlamePilot differentiates itself through an architecture that leverages atomic tools to ensure the robust setup and execution of complex simulations in both OpenFOAM and extended frameworks such as DeepFlame. The system is also capable of learning from scientific articles, extracting key information to guide the simulation from initial setup to optimized results. Validation on a public benchmark shows FlamePilot achieved a perfect 1.0 executability score and a 0.438 success rate, surpassing the prior best reported agent scores of 0.625 and 0.250, respectively. Furthermore, a detailed case study on Moderate or Intense Low-oxygen Dilution (MILD) combustion simulation demonstrates its efficacy as a collaborative research copilot, where FlamePilot autonomously translated a research paper into a configured simulation, conducted the simulation, post-processed the results, proposed evidence-based refinements, and managed a multi-step parameter study to convergence under minimal human intervention. By adopting a transparent and interpretable paradigm, FlamePilot establishes a foundational framework for AI-empowered combustion modeling, fostering a collaborative partnership where the agent manages workflow orchestration, freeing the researcher for high-level analysis.

</details>


### [229] [Causal discovery for linear causal model with correlated noise: an Adversarial Learning Approach](https://arxiv.org/abs/2601.01368)
*Mujin Zhou,Junzhe Zhang*

Main category: cs.LG

TL;DR: 提出基于f-GAN框架的因果发现方法，在存在未测量混杂因素的情况下学习二元因果结构，将结构学习问题转化为最小化贝叶斯自由能量，并通过Gumbel-Softmax松弛在离散图空间中进行梯度搜索。


<details>
  <summary>Details</summary>
Motivation: 在存在未测量混杂因素的情况下进行因果发现是一个具有挑战性的问题，现有方法通常依赖于特定的权重值假设，需要一种能够独立于具体权重值学习因果结构的方法。

Method: 基于f-GAN框架，将结构学习问题重新表述为最小化贝叶斯自由能量，并证明该问题等价于最小化真实数据分布与模型生成分布之间的f-散度。使用f-GAN框架将目标转化为min-max对抗优化问题，并通过Gumbel-Softmax松弛在离散图空间实现梯度搜索。

Result: 该方法能够在存在未测量混杂因素的情况下学习二元因果结构，不依赖于特定的权重值假设，通过对抗优化和离散空间梯度搜索实现了有效的因果发现。

Conclusion: 提出的基于f-GAN框架的方法为存在未测量混杂因素的因果发现问题提供了新的解决方案，通过将结构学习转化为贝叶斯自由能量最小化问题，并利用对抗优化和离散空间梯度搜索技术，实现了有效的因果结构学习。

Abstract: Causal discovery from data with unmeasured confounding factors is a challenging problem. This paper proposes an approach based on the f-GAN framework, learning the binary causal structure independent of specific weight values. We reformulate the structure learning problem as minimizing Bayesian free energy and prove that this problem is equivalent to minimizing the f-divergence between the true data distribution and the model-generated distribution. Using the f-GAN framework, we transform this objective into a min-max adversarial optimization problem. We implement the gradient search in the discrete graph space using Gumbel-Softmax relaxation.

</details>


### [230] [Data Complexity-aware Deep Model Performance Forecasting](https://arxiv.org/abs/2601.01383)
*Yen-Chia Chen,Hsing-Kuo Pao,Hanjuan Huang*

Main category: cs.LG

TL;DR: 提出一个轻量级的两阶段框架，可在训练前预测深度学习模型性能，无需实际训练或复杂模拟


<details>
  <summary>Details</summary>
Motivation: 深度学习模型选择通常依赖试错过程，耗时耗资源且难以自动化。现有性能预测方法要么需要部分训练，要么计算开销大，要么缺乏泛化能力

Method: 两阶段框架：第一阶段基于数据集的可测量属性预测基线性能；第二阶段结合模型架构和超参数信息调整估计。框架可泛化到不同数据集和模型类型

Result: 框架不仅能预测模型性能，还能指导架构选择、预处理流程，并在训练前检测潜在问题数据集。发现数据集方差等底层特征可作为数据质量的早期指标

Conclusion: 提出的轻量级框架为深度学习模型选择和性能预测提供了高效、可泛化的解决方案，减少了试错成本，同时提供了数据质量洞察

Abstract: Deep learning models are widely used across computer vision and other domains. When working on the model induction, selecting the right architecture for a given dataset often relies on repetitive trial-and-error procedures. This procedure is time-consuming, resource-intensive, and difficult to automate. While previous work has explored performance prediction using partial training or complex simulations, these methods often require significant computational overhead or lack generalizability. In this work, we propose an alternative approach: a lightweight, two-stage framework that can estimate model performance before training given the understanding of the dataset and the focused deep model structures. The first stage predicts a baseline based on the analysis of some measurable properties of the dataset, while the second stage adjusts the estimation with additional information on the model's architectural and hyperparameter details. The setup allows the framework to generalize across datasets and model types. Moreover, we find that some of the underlying features used for prediction - such as dataset variance - can offer practical guidance for model selection, and can serve as early indicators of data quality. As a result, the framework can be used not only to forecast model performance, but also to guide architecture choices, inform necessary preprocessing procedures, and detect potentially problematic datasets before training begins.

</details>


### [231] [Scale-Adaptive Power Flow Analysis with Local Topology Slicing and Multi-Task Graph Learning](https://arxiv.org/abs/2601.01387)
*Yongzhe Li,Lin Guan,Zihan Cai,Zuxian Lin,Jiyu Huang,Liukai Chen*

Main category: cs.LG

TL;DR: 提出SaMPFA框架，通过局部拓扑切片采样和多任务图学习模型，提升电力潮流分析在系统规模变化下的适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 开发具有强拓扑适应性的深度学习模型对潮流分析具有重要实践意义，需要增强模型在可变系统规模下的性能，并提高支路功率预测的鲁棒性。

Method: 提出SaMPFA框架，包含：1) 局部拓扑切片采样技术，从完整电网提取不同尺度的子图以增强跨尺度学习能力；2) 无参考多任务图学习模型，直接预测母线电压和支路功率而非相角，避免误差放大并引导模型学习相角差的物理关系；3) 损失函数中加入额外项，鼓励模型捕捉角差和功率传输的物理模式。

Result: 在IEEE 39节点系统和实际省级电网上的仿真表明，该模型在可变系统规模下实现了优越的适应性和泛化能力，准确率分别提高了4.47%和36.82%。

Conclusion: SaMPFA框架通过局部拓扑切片采样和多任务图学习，有效提升了深度学习模型在电力潮流分析中对拓扑变化的适应性和预测鲁棒性，具有实际应用价值。

Abstract: Developing deep learning models with strong adaptability to topological variations is of great practical significance for power flow analysis. To enhance model performance under variable system scales and improve robustness in branch power prediction, this paper proposes a Scale-adaptive Multi-task Power Flow Analysis (SaMPFA) framework. SaMPFA introduces a Local Topology Slicing (LTS) sampling technique that extracts subgraphs of different scales from the complete power network to strengthen the model's cross-scale learning capability. Furthermore, a Reference-free Multi-task Graph Learning (RMGL) model is designed for robust power flow prediction. Unlike existing approaches, RMGL predicts bus voltages and branch powers instead of phase angles. This design not only avoids the risk of error amplification in branch power calculation but also guides the model to learn the physical relationships of phase angle differences. In addition, the loss function incorporates extra terms that encourage the model to capture the physical patterns of angle differences and power transmission, further improving consistency between predictions and physical laws. Simulations on the IEEE 39-bus system and a real provincial grid in China demonstrate that the proposed model achieves superior adaptability and generalization under variable system scales, with accuracy improvements of 4.47% and 36.82%, respectively.

</details>


### [232] [A Graph-based Framework for Online Time Series Anomaly Detection Using Model Ensemble](https://arxiv.org/abs/2601.01403)
*Zewei Yu,Jianqiu Xu,Caimin Li*

Main category: cs.LG

TL;DR: 提出GDME框架，一种基于图模型集成的无监督在线时间序列异常检测方法，通过动态图结构和社区检测实现模型选择与概念漂移适应


<details>
  <summary>Details</summary>
Motivation: 工业系统中流数据量不断增加，在线异常检测成为关键任务。现有方法多为离线设计或难以有效处理异构流数据，需要能够适应快速演化数据模式的在线检测方法

Method: GDME框架维护动态模型池，通过剪枝表现不佳模型和引入新模型持续更新。使用动态图结构表示模型间关系，通过图上的社区检测选择合适子集进行集成。利用图结构变化监测概念漂移，适应演化中的流数据

Result: 在七个异构时间序列数据集上的实验表明，GDME优于现有在线异常检测方法，提升幅度最高达24%。其集成策略相比单个模型和平均集成提供更优检测性能，同时保持计算效率竞争力

Conclusion: GDME框架通过动态图模型集成有效解决了在线时间序列异常检测中的异构流数据处理问题，能够适应概念漂移并实现高性能检测，具有实际应用价值

Abstract: With the increasing volume of streaming data in industrial systems, online anomaly detection has become a critical task. The diverse and rapidly evolving data patterns pose significant challenges for online anomaly detection. Many existing anomaly detection methods are designed for offline settings or have difficulty in handling heterogeneous streaming data effectively. This paper proposes GDME, an unsupervised graph-based framework for online time series anomaly detection using model ensemble. GDME maintains a dynamic model pool that is continuously updated by pruning underperforming models and introducing new ones. It utilizes a dynamic graph structure to represent relationships among models and employs community detection on the graph to select an appropriate subset for ensemble. The graph structure is also used to detect concept drift by monitoring structural changes, allowing the framework to adapt to evolving streaming data. Experiments on seven heterogeneous time series demonstrate that GDME outperforms existing online anomaly detection methods, achieving improvements of up to 24%. In addition, its ensemble strategy provides superior detection performance compared with both individual models and average ensembles, with competitive computational efficiency.

</details>


### [233] [A Depth Hierarchy for Computing the Maximum in ReLU Networks via Extremal Graph Theory](https://arxiv.org/abs/2601.01417)
*Itay Safran*

Main category: cs.LG

TL;DR: 本文证明了计算d个实数最大值的ReLU神经网络存在深度层次结构：对于深度3≤k≤log₂(log₂(d))，需要宽度Ω(d^{1+1/(2^{k-2}-1)})才能精确表示最大值函数，这是首个针对该基本算子在深度k≥3时的无条件超线性下界。


<details>
  <summary>Details</summary>
Motivation: 最大值函数是神经网络中的基本算子，但其精确计算所需的网络规模尚不清楚。现有研究主要关注浅层网络，对于深层网络的表示能力缺乏理论理解。本文旨在探索最大值函数在深层ReLU网络中的计算复杂性，揭示其固有的几何结构复杂性。

Method: 采用组合论证方法，将最大值函数的不可微分脊线与计算网络第一隐藏层诱导的图中的团相关联。利用极值图论中的Turán定理，证明足够窄的网络无法捕捉最大值函数的非线性特性。通过分析网络深度与宽度的关系，建立深度层次结构。

Result: 证明了对于深度3≤k≤log₂(log₂(d))，精确计算d个实数最大值需要ReLU神经网络宽度至少为Ω(d^{1+1/(2^{k-2}-1)})。这是首个针对最大值函数在深度k≥3时的无条件超线性下界，即使深度随d缩放也成立。

Conclusion: 最大值函数虽然形式简单，但其几何结构中的不可微分超平面带来了固有的复杂性。该研究为深度神经网络下界证明提供了新方法，揭示了深度层次结构在计算基本算子时的重要性。

Abstract: We consider the problem of exact computation of the maximum function over $d$ real inputs using ReLU neural networks. We prove a depth hierarchy, wherein width $Ω\big(d^{1+\frac{1}{2^{k-2}-1}}\big)$ is necessary to represent the maximum for any depth $3\le k\le \log_2(\log_2(d))$. This is the first unconditional super-linear lower bound for this fundamental operator at depths $k\ge3$, and it holds even if the depth scales with $d$. Our proof technique is based on a combinatorial argument and associates the non-differentiable ridges of the maximum with cliques in a graph induced by the first hidden layer of the computing network, utilizing Turán's theorem from extremal graph theory to show that a sufficiently narrow network cannot capture the non-linearities of the maximum. This suggests that despite its simple nature, the maximum function possesses an inherent complexity that stems from the geometric structure of its non-differentiable hyperplanes, and provides a novel approach for proving lower bounds for deep neural networks.

</details>


### [234] [Unveiling the Heart-Brain Connection: An Analysis of ECG in Cognitive Performance](https://arxiv.org/abs/2601.01424)
*Akshay Sasi,Malavika Pradeep,Nusaibah Farrukh,Rahul Venugopal,Elizabeth Sherly*

Main category: cs.LG

TL;DR: 该研究探索使用心电图（ECG）替代脑电图（EEG）监测认知负荷，提出跨模态XGBoost框架将ECG特征映射到EEG认知空间，实现仅用ECG进行认知状态分类。


<details>
  <summary>Details</summary>
Motivation: 虽然EEG是评估心理负荷的金标准，但其便携性有限限制了实际应用。广泛可用的可穿戴设备ECG提供了一种实用替代方案，研究探索ECG信号是否能可靠反映认知负荷并作为EEG指标的代理。

Method: 收集工作记忆和被动听力任务的多模态数据，提取ECG时域HRV指标和Catch22描述符，对应EEG频谱和Catch22特征。提出跨模态XGBoost框架，将ECG特征投影到EEG代表的认知空间，实现仅用ECG进行工作负荷推断。

Result: ECG衍生的投影能显著捕捉认知状态变化，为准确分类提供良好支持。ECG特征能有效映射到EEG认知空间，实现可靠的认知状态监测。

Conclusion: ECG可作为可解释、实时、可穿戴的日常认知监测解决方案，为生理计算提供实用替代方案，克服EEG在便携性方面的限制。

Abstract: Understanding the interaction of neural and cardiac systems during cognitive activity is critical to advancing physiological computing. Although EEG has been the gold standard for assessing mental workload, its limited portability restricts its real-world use. Widely available ECG through wearable devices proposes a pragmatic alternative. This research investigates whether ECG signals can reliably reflect cognitive load and serve as proxies for EEG-based indicators. In this work, we present multimodal data acquired from two different paradigms involving working-memory and passive-listening tasks. For each modality, we extracted ECG time-domain HRV metrics and Catch22 descriptors against EEG spectral and Catch22 features, respectively. We propose a cross-modal XGBoost framework to project the ECG features onto EEG-representative cognitive spaces, thereby allowing workload inferences using only ECG. Our results show that ECG-derived projections expressively capture variation in cognitive states and provide good support for accurate classification. Our findings underpin ECG as an interpretable, real-time, wearable solution for everyday cognitive monitoring.

</details>


### [235] [Bayesian Subspace Gradient Estimation for Zeroth-Order Optimization of Large Language Models](https://arxiv.org/abs/2601.01452)
*Jian Feng,Zhihong Huang*

Main category: cs.LG

TL;DR: BSZO是一种贝叶斯子空间零阶优化方法，通过卡尔曼滤波结合多个扰动方向的有限差分信息，相比传统ZO方法提升了收敛速度，在保持接近推理内存使用的同时，在多个LLM上实现了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零阶优化方法依赖于随机扰动的一步梯度估计，存在效率限制。需要一种能够结合多个扰动方向信息、更有效地利用有限差分测量值的优化方法。

Method: 提出贝叶斯子空间零阶优化(BSZO)，将每个有限差分测量视为噪声观测，通过卡尔曼滤波构建投影梯度的后验分布，并使用基于残差的自适应机制调整扰动尺度。

Result: 理论分析显示BSZO将收敛速度提升了k/γ倍。在RoBERTa、Mistral和OPT模型上的实验表明，BSZO优于MeZO、MeZO-Adam和HiZOO，在OPT-13B上实现了最高6.67%的绝对平均提升，同时内存使用保持在推理基线的1.00-1.08倍。

Conclusion: BSZO通过贝叶斯方法有效结合多个扰动方向信息，显著提升了零阶优化的效率和性能，为内存受限环境下的大语言模型微调提供了更优的解决方案。

Abstract: Fine-tuning large language models (LLMs) with zeroth-order (ZO) optimization reduces memory by approximating gradients through function evaluations, but existing methods rely on one-step gradient estimates from random perturbations. We introduce Bayesian Subspace Zeroth-Order optimization (BSZO), a ZO optimizer that applies Kalman filtering to combine finite-difference information across multiple perturbation directions. By treating each finite-difference measurement as a noisy observation, BSZO builds a posterior distribution over the projected gradient and updates it through Bayesian inference, with a residual-based adaptive mechanism to adjust perturbation scales. Theoretical analysis shows that BSZO improves the convergence rate by a factor of $k/γ$ compared to standard ZO methods. Experiments on RoBERTa, Mistral, and OPT models show that BSZO outperforms MeZO, MeZO-Adam, and HiZOO across various tasks, achieving up to 6.67\% absolute average improvement on OPT-13B while keeping memory usage close to inference-only baselines (1.00$\times$--1.08$\times$ of MeZO).

</details>


### [236] [Leveraging Flatness to Improve Information-Theoretic Generalization Bounds for SGD](https://arxiv.org/abs/2601.01465)
*Ze Peng,Jian Zhang,Yisen Wang,Lei Qi,Yinghuan Shi,Yang Gao*

Main category: cs.LG

TL;DR: 该论文提出了一种新的信息论泛化界，能够更好地利用SGD的平坦性偏好，在数值上更紧且能正确反映平坦性改善时的泛化提升。


<details>
  <summary>Details</summary>
Motivation: 现有信息论泛化界虽然理论上依赖数据和算法特性，但未能有效捕捉SGD平坦性偏好对泛化的改善作用，且在数值上不够紧致。作者观察到平坦性偏差对SGD泛化至关重要，但现有界未能充分利用这一特性。

Method: 提出了一种更充分利用平坦性的信息论泛化界，引入"全知轨迹"技术。该界表明当最终权重协方差的大方差方向在损失景观中具有小局部曲率时，学习模型泛化更好。

Result: 在深度神经网络上的实验表明，新界不仅正确反映了平坦性改善时的泛化提升，而且在数值上更紧。应用于凸-Lipschitz-有界问题的梯度下降极小极大超额风险时，将代表性信息论界的Ω(1)率改进为O(1/√n)。

Conclusion: 通过"全知轨迹"技术，成功推导出能够更好利用SGD平坦性偏好的信息论泛化界，该界在理论和实验上都表现出优越性，并暗示了可以绕过记忆化-泛化权衡的可能性。

Abstract: Information-theoretic (IT) generalization bounds have been used to study the generalization of learning algorithms. These bounds are intrinsically data- and algorithm-dependent so that one can exploit the properties of data and algorithm to derive tighter bounds. However, we observe that although the flatness bias is crucial for SGD's generalization, these bounds fail to capture the improved generalization under better flatness and are also numerically loose. This is caused by the inadequate leverage of SGD's flatness bias in existing IT bounds. This paper derives a more flatness-leveraging IT bound for the flatness-favoring SGD. The bound indicates the learned models generalize better if the large-variance directions of the final weight covariance have small local curvatures in the loss landscape. Experiments on deep neural networks show our bound not only correctly reflects the better generalization when flatness is improved, but is also numerically much tighter. This is achieved by a flexible technique called "omniscient trajectory". When applied to Gradient Descent's minimax excess risk on convex-Lipschitz-Bounded problems, it improves representative IT bounds' $Ω(1)$ rates to $O(1/\sqrt{n})$. It also implies a by-pass of memorization-generalization trade-offs.

</details>


### [237] [Accelerating Storage-Based Training for Graph Neural Networks](https://arxiv.org/abs/2601.01473)
*Myung-Hwan Jang,Jeong-Min Park,Yunyong Ko,Sang-Wook Kim*

Main category: cs.LG

TL;DR: AGNES提出了一种基于存储的GNN训练框架，通过块级存储I/O处理和超批次处理来解决大规模图数据训练中的I/O瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于存储的GNN训练方法在处理大规模图数据时面临大量小型存储I/O操作的瓶颈问题，导致数据准备效率低下，限制了训练性能。

Method: AGNES采用块级存储I/O处理技术来充分利用高性能存储设备的I/O带宽，并结合基于现实图特征设计的超批次处理策略，进一步优化每个存储I/O的效率。

Result: 在五个真实世界图数据集上的综合实验表明，AGNES始终优于四种最先进的方法，比最佳竞争对手快达4.1倍。

Conclusion: AGNES通过创新的存储I/O优化策略，有效解决了大规模图神经网络训练中的存储瓶颈问题，显著提升了训练效率。

Abstract: Graph neural networks (GNNs) have achieved breakthroughs in various real-world downstream tasks due to their powerful expressiveness. As the scale of real-world graphs has been continuously growing, \textit{a storage-based approach to GNN training} has been studied, which leverages external storage (e.g., NVMe SSDs) to handle such web-scale graphs on a single machine. Although such storage-based GNN training methods have shown promising potential in large-scale GNN training, we observed that they suffer from a severe bottleneck in data preparation since they overlook a critical challenge: \textit{how to handle a large number of small storage I/Os}. To address the challenge, in this paper, we propose a novel storage-based GNN training framework, named \textsf{AGNES}, that employs a method of \textit{block-wise storage I/O processing} to fully utilize the I/O bandwidth of high-performance storage devices. Moreover, to further enhance the efficiency of each storage I/O, \textsf{AGNES} employs a simple yet effective strategy, \textit{hyperbatch-based processing} based on the characteristics of real-world graphs. Comprehensive experiments on five real-world graphs reveal that \textsf{AGNES} consistently outperforms four state-of-the-art methods, by up to 4.1$\times$ faster than the best competitor. Our code is available at https://github.com/Bigdasgit/agnes-kdd26.

</details>


### [238] [Multi-Subspace Multi-Modal Modeling for Diffusion Models: Estimation, Convergence and Mixture of Experts](https://arxiv.org/abs/2601.01475)
*Ruofeng Yang,Yongcan Li,Bo Jiang,Cheng Chen,Shuai Li*

Main category: cs.LG

TL;DR: 提出MoLR-MoG建模方法，将数据建模为K个线性子空间的并集，每个子空间采用混合高斯隐变量，解决了传统扩散模型维度诅咒问题，实现了更优的生成质量和理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽然在小数据集上表现良好，但估计误差受维度诅咒影响（n^{-1/D}）。虽然已有工作将数据建模为高斯隐变量的线性子空间并集，但高斯隐变量无法捕捉隐流形的多模态特性。

Method: 提出MoLR-MoG（低秩混合高斯混合子空间）建模：将目标数据建模为K个线性子空间的并集，每个子空间采用混合高斯隐变量（n_k个模态，维度d_k）。对应的得分函数具有混合专家结构，能捕捉多模态信息和非线性特性。

Result: 1. 实验表明MoE隐变量MoG神经网络的生成结果远优于MoE隐变量高斯得分；2. MoE隐变量MoG神经网络与参数多10倍的MoE隐变量Unet性能相当；3. 理论分析得到R^4√(Σn_k)√(Σn_kd_k)/√n的估计误差，避免了维度诅咒；4. 证明了在MoLR-MoG建模下的优化收敛保证。

Conclusion: MoLR-MoG建模合理且适用于真实世界数据，解释了为什么扩散模型只需少量训练样本和快速优化过程就能获得优异性能，为扩散模型的理论理解提供了新视角。

Abstract: Recently, diffusion models have achieved a great performance with a small dataset of size $n$ and a fast optimization process. However, the estimation error of diffusion models suffers from the curse of dimensionality $n^{-1/D}$ with the data dimension $D$. Since images are usually a union of low-dimensional manifolds, current works model the data as a union of linear subspaces with Gaussian latent and achieve a $1/\sqrt{n}$ bound. Though this modeling reflects the multi-manifold property, the Gaussian latent can not capture the multi-modal property of the latent manifold. To bridge this gap, we propose the mixture subspace of low-rank mixture of Gaussian (MoLR-MoG) modeling, which models the target data as a union of $K$ linear subspaces, and each subspace admits a mixture of Gaussian latent ($n_k$ modals with dimension $d_k$). With this modeling, the corresponding score function naturally has a mixture of expert (MoE) structure, captures the multi-modal information, and contains nonlinear property. We first conduct real-world experiments to show that the generation results of MoE-latent MoG NN are much better than MoE-latent Gaussian score. Furthermore, MoE-latent MoG NN achieves a comparable performance with MoE-latent Unet with $10 \times$ parameters. These results indicate that the MoLR-MoG modeling is reasonable and suitable for real-world data. After that, based on such MoE-latent MoG score, we provide a $R^4\sqrt{Σ_{k=1}^Kn_k}\sqrt{Σ_{k=1}^Kn_kd_k}/\sqrt{n}$ estimation error, which escapes the curse of dimensionality by using data structure. Finally, we study the optimization process and prove the convergence guarantee under the MoLR-MoG modeling. Combined with these results, under a setting close to real-world data, this work explains why diffusion models only require a small training sample and enjoy a fast optimization process to achieve a great performance.

</details>


### [239] [SGD-Based Knowledge Distillation with Bayesian Teachers: Theory and Guidelines](https://arxiv.org/abs/2601.01484)
*Itai Morad,Nir Shlezinger,Yonina C. Eldar*

Main category: cs.LG

TL;DR: 从贝叶斯视角分析知识蒸馏的理论基础，证明使用贝叶斯分类概率作为教师输出能降低方差、提升收敛稳定性，实验验证贝叶斯教师比确定性教师效果更好


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏在经验上很成功，但理论基础尚不完善。本文旨在从贝叶斯角度为知识蒸馏提供理论分析，理解其收敛行为和泛化特性

Method: 采用贝叶斯视角分析知识蒸馏，研究两种监督模式：1）教师提供精确的贝叶斯分类概率；2）使用噪声近似的贝叶斯分类概率。分析SGD训练的收敛行为，并实验验证贝叶斯深度学习模型作为教师的优势

Result: 理论分析表明：学习贝叶斯分类概率相比one-hot监督能降低方差、移除收敛边界中的邻域项。噪声水平影响泛化和准确性。实验证明：从贝叶斯教师蒸馏的学生准确率提升最高达4.27%，收敛稳定性提升（噪声减少达30%）

Conclusion: 贝叶斯视角为知识蒸馏提供了理论基础，使用贝叶斯深度学习模型作为教师能提供更好的贝叶斯分类概率估计，从而提升学生模型的准确率和收敛稳定性

Abstract: Knowledge Distillation (KD) is a central paradigm for transferring knowledge from a large teacher network to a typically smaller student model, often by leveraging soft probabilistic outputs. While KD has shown strong empirical success in numerous applications, its theoretical underpinnings remain only partially understood. In this work, we adopt a Bayesian perspective on KD to rigorously analyze the convergence behavior of students trained with Stochastic Gradient Descent (SGD). We study two regimes: $(i)$ when the teacher provides the exact Bayes Class Probabilities (BCPs); and $(ii)$ supervision with noisy approximations of the BCPs. Our analysis shows that learning from BCPs yields variance reduction and removes neighborhood terms in the convergence bounds compared to one-hot supervision. We further characterize how the level of noise affects generalization and accuracy. Motivated by these insights, we advocate the use of Bayesian deep learning models, which typically provide improved estimates of the BCPs, as teachers in KD. Consistent with our analysis, we experimentally demonstrate that students distilled from Bayesian teachers not only achieve higher accuracies (up to +4.27%), but also exhibit more stable convergence (up to 30% less noise), compared to students distilled from deterministic teachers.

</details>


### [240] [Advanced Global Wildfire Activity Modeling with Hierarchical Graph ODE](https://arxiv.org/abs/2601.01501)
*Fan Xu,Wei Gong,Hao Wu,Lilan Peng,Nan Wang,Qingsong Wen,Xian Wu,Kun Wang,Xibin Zhao*

Main category: cs.LG

TL;DR: HiGO：一种用于全球野火行为预测的多尺度连续时间深度学习框架，通过分层图ODE建模地球系统动态


<details>
  <summary>Details</summary>
Motivation: 野火受大气、海洋和陆地过程复杂相互作用影响，具有多时空尺度特性。虽然深度学习在天气预报方面取得突破，但在全球野火行为预测方面仍有待探索，需要能够捕捉多尺度连续动态的模型。

Method: 提出分层图ODE（HiGO）框架：1）将地球系统表示为多层图层次结构；2）设计自适应滤波消息传递机制，实现层内和层间信息流；3）在多个层次集成GNN参数化的神经ODE模块，显式学习各尺度的连续动态。

Result: 在SeasFire Cube数据集上的实验表明，HiGO在长期野火预测方面显著优于现有基准方法。其连续时间预测表现出很强的观测一致性，显示出实际应用潜力。

Conclusion: HiGO成功解决了全球野火行为预测的挑战，通过分层图ODE框架有效捕捉了多尺度连续动态，为野火预测提供了有前景的新方法。

Abstract: Wildfires, as an integral component of the Earth system, are governed by a complex interplay of atmospheric, oceanic, and terrestrial processes spanning a vast range of spatiotemporal scales. Modeling their global activity on large timescales is therefore a critical yet challenging task. While deep learning has recently achieved significant breakthroughs in global weather forecasting, its potential for global wildfire behavior prediction remains underexplored. In this work, we reframe this problem and introduce the Hierarchical Graph ODE (HiGO), a novel framework designed to learn the multi-scale, continuous-time dynamics of wildfires. Specifically, we represent the Earth system as a multi-level graph hierarchy and propose an adaptive filtering message passing mechanism for both intra- and inter-level information flow, enabling more effective feature extraction and fusion. Furthermore, we incorporate GNN-parameterized Neural ODE modules at multiple levels to explicitly learn the continuous dynamics inherent to each scale. Through extensive experiments on the SeasFire Cube dataset, we demonstrate that HiGO significantly outperforms state-of-the-art baselines on long-range wildfire forecasting. Moreover, its continuous-time predictions exhibit strong observational consistency, highlighting its potential for real-world applications.

</details>


### [241] [Utilizing Earth Foundation Models to Enhance the Simulation Performance of Hydrological Models with AlphaEarth Embeddings](https://arxiv.org/abs/2601.01558)
*Pengfei Qu,Wenyu Ouyang,Chi Zhang,Yikai Chai,Shuolong Xu,Lei Ye,Yongri Piao,Miao Zhang,Huchuan Lu*

Main category: cs.LG

TL;DR: 卫星图像学习的环境嵌入比传统流域属性更能有效预测无测站河流流量，通过选择相似流域作为数据源可提升预测精度


<details>
  <summary>Details</summary>
Motivation: 传统流域属性无法完全捕捉自然环境的复杂性，需要探索从卫星图像学习的环境嵌入是否能更有效地描述流域特征，从而改进无测站河流流量预测

Method: 使用AlphaEarth Foundation嵌入（从大量卫星图像学习的环境表示）作为流域特征，与传统属性对比，研究如何选择相似流域作为数据源来预测无测站区域流量

Result: 基于卫星嵌入的模型在未参与训练的流域上预测精度更高，表明其能更有效地捕捉关键物理差异；选择环境和水文行为相似的流域作为数据源可提升性能，而添加不相似流域会降低精度

Conclusion: 卫星信息驱动的环境表征能增强水文预测能力，支持开发更适应不同景观的水文模型，为无测站河流流量预测提供了新方法

Abstract: Predicting river flow in places without streamflow records is challenging because basins respond differently to climate, terrain, vegetation, and soils. Traditional basin attributes describe some of these differences, but they cannot fully represent the complexity of natural environments. This study examines whether AlphaEarth Foundation embeddings, which are learned from large collections of satellite images rather than designed by experts, offer a more informative way to describe basin characteristics. These embeddings summarize patterns in vegetation, land surface properties, and long-term environmental dynamics. We find that models using them achieve higher accuracy when predicting flows in basins not used for training, suggesting that they capture key physical differences more effectively than traditional attributes. We further investigate how selecting appropriate donor basins influences prediction in ungauged regions. Similarity based on the embeddings helps identify basins with comparable environmental and hydrological behavior, improving performance, whereas adding many dissimilar basins can reduce accuracy. The results show that satellite-informed environmental representations can strengthen hydrological forecasting and support the development of models that adapt more easily to different landscapes.

</details>


### [242] [The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs](https://arxiv.org/abs/2601.01580)
*Zibo Zhao,Yuanting Zha,Haipeng Zhang,Xingcheng Xu*

Main category: cs.LG

TL;DR: 该论文提出梯度归因属性理论，解释RL后训练如何使大语言模型获得自我反思能力，证明RL的成功源于决策能力的提升而非采样能力。


<details>
  <summary>Details</summary>
Motivation: 尽管RL后训练能让大语言模型获得自我反思能力，但统一的优化目标如何产生生成解决方案和评估何时修订这两种不同功能能力的机制尚不明确。

Method: 引入梯度归因属性来表征奖励梯度在策略组件中的分布，通过两阶段决策-采样假设将策略分解为用于生成的采样策略和用于验证的决策策略，理论分析不同训练方法的梯度归因特性。

Result: 证明替代奖励表现出平衡梯度归因，而SFT和KL惩罚表现出不平衡梯度归因，长度加权产生不对称正则化，约束采样策略而决策策略优化不足，这解释了RL成功而SFT失败的原因。

Conclusion: RL的优越泛化能力主要源于决策能力的改进而非采样能力，为思维模型中的自我修正提供了第一性原理的机制解释。

Abstract: Self-reflection capabilities emerge in Large Language Models after RL post-training, with multi-turn RL achieving substantial gains over SFT counterparts. Yet the mechanism of how a unified optimization objective gives rise to functionally distinct capabilities of generating solutions and evaluating when to revise them remains opaque. To address this question, we introduce the Gradient Attribution Property to characterize how reward gradients distribute across policy components, formalized through the Two-Stage Decision-Sampling (DS) Hypothesis, which decomposes the policy into sampling ($π_{sample}$) for generation and decision ($π_{d}$) for verification. We prove that surrogate rewards exhibit Balanced Gradient Attribution, while SFT and KL penalties exhibit Unbalanced Gradient Attribution, with length-weighting creating asymmetric regularization that constrains $π_{sample}$ while leaving $π_{d}$ under-optimized, providing an theoretical explanation of why RL succeeds where SFT fails. We also empirically validate our theoretical predictions on arithmetic reasoning demonstrates that RL's superior generalization stems primarily from improved decision-making ($π_{d}$) rather than sampling capabilities, providing a first-principles mechanistic explanation for self-correction in thinking models.

</details>


### [243] [REE-TTT: Highly Adaptive Radar Echo Extrapolation Based on Test-Time Training](https://arxiv.org/abs/2601.01605)
*Xin Di,Xinglin Piao,Fei Wang,Guodong Jing,Yong Zhang*

Main category: cs.LG

TL;DR: 提出REE-TTT模型，通过时空测试时训练机制增强雷达回波外推的泛化能力，解决传统方法在跨区域和极端天气下性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的雷达回波外推方法存在泛化能力差的问题，依赖高质量本地训练数据和静态模型参数，难以适应不同区域和极端天气事件。

Method: 提出REE-TTT模型，引入自适应测试时训练机制，核心是时空测试时训练块，用任务特定的注意力机制替代标准线性投影，增强对非平稳气象分布的适应能力。

Result: 在跨区域极端降水场景下的实验表明，REE-TTT在预测精度和泛化能力上显著优于现有基准模型，对数据分布偏移表现出卓越的适应性。

Conclusion: REE-TTT通过测试时训练机制有效解决了雷达回波外推的泛化问题，为降水临近预报提供了更稳健的解决方案。

Abstract: Precipitation nowcasting is critically important for meteorological forecasting. Deep learning-based Radar Echo Extrapolation (REE) has become a predominant nowcasting approach, yet it suffers from poor generalization due to its reliance on high-quality local training data and static model parameters, limiting its applicability across diverse regions and extreme events. To overcome this, we propose REE-TTT, a novel model that incorporates an adaptive Test-Time Training (TTT) mechanism. The core of our model lies in the newly designed Spatio-temporal Test-Time Training (ST-TTT) block, which replaces the standard linear projections in TTT layers with task-specific attention mechanisms, enabling robust adaptation to non-stationary meteorological distributions and thereby significantly enhancing the feature representation of precipitation. Experiments under cross-regional extreme precipitation scenarios demonstrate that REE-TTT substantially outperforms state-of-the-art baseline models in prediction accuracy and generalization, exhibiting remarkable adaptability to data distribution shifts.

</details>


### [244] [Real Time NILM Based Power Monitoring of Identical Induction Motors Representing Cutting Machines in Textile Industry](https://arxiv.org/abs/2601.01616)
*Md Istiauk Hossain Rifat,Moin Khan,Mohammad Zunaed*

Main category: cs.LG

TL;DR: 提出基于NILM的实时工业能耗监测框架，针对孟加拉国纺织业相同电机负载，开发硬件系统采集数据，评估MATNILM模型性能，发现总能耗估计准确但相同设备同时运行时分解困难。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国纺织业作为高能耗行业，现有监测方法落后导致能源使用效率低下和运营成本高昂，需要开发实时监测解决方案。

Method: 开发包含电压电流传感器、Arduino Mega和ESP8266的硬件系统，采集总负载和单个负载数据；创建包含三个相同感应电机和辅助负载的新数据集（超过18万个样本）；在云平台上处理数据；评估MATNILM模型在工业条件下的性能。

Result: 总能耗估计相对准确，但单个设备分解在多个相同机器同时运行时面临困难；集成系统通过Blynk应用实现了实用的实时远程监测。

Conclusion: NILM在工业应用中既有潜力也有局限性，未来改进方向包括更高频率数据采集、更大规模数据集以及处理相同负载的先进深度学习方法。

Abstract: The textile industry in Bangladesh is one of the most energy-intensive sectors, yet its monitoring practices remain largely outdated, resulting in inefficient power usage and high operational costs. To address this, we propose a real-time Non-Intrusive Load Monitoring (NILM)-based framework tailored for industrial applications, with a focus on identical motor-driven loads representing textile cutting machines. A hardware setup comprising voltage and current sensors, Arduino Mega and ESP8266 was developed to capture aggregate and individual load data, which was stored and processed on cloud platforms. A new dataset was created from three identical induction motors and auxiliary loads, totaling over 180,000 samples, to evaluate the state-of-the-art MATNILM model under challenging industrial conditions. Results indicate that while aggregate energy estimation was reasonably accurate, per-appliance disaggregation faced difficulties, particularly when multiple identical machines operated simultaneously. Despite these challenges, the integrated system demonstrated practical real-time monitoring with remote accessibility through the Blynk application. This work highlights both the potential and limitations of NILM in industrial contexts, offering insights into future improvements such as higher-frequency data collection, larger-scale datasets and advanced deep learning approaches for handling identical loads.

</details>


### [245] [Entropy-Aligned Decoding of LMs for Better Writing and Reasoning](https://arxiv.org/abs/2601.01714)
*Kareem Ahmed,Sameer Singh*

Main category: cs.LG

TL;DR: EPIC是一种无需超参数的解码方法，通过将未来轨迹的熵纳入语言模型解码，调节生成过程中的不确定性，使其与数据不确定性对齐，从而提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型解码算法依赖贪婪启发式方法，导致生成结果同质化、重复且不连贯，无法有效恢复真实语言分布。

Method: EPIC通过熵感知懒惰Gumbel-Max采样，将未来轨迹的熵显式纳入解码过程，使采样分布的熵与数据不确定性对齐，实现精确且高效的解码。

Result: 在创意写作和摘要任务中，EPIC在LM-as-judge偏好胜率上持续优于广泛使用的解码策略；在数学推理任务中，EPIC也超越了所有基线方法。

Conclusion: EPIC通过熵感知解码有效提升了语言模型生成质量，在多样性、忠实度和推理能力方面均有显著改进。

Abstract: Language models (LMs) are trained on billions of tokens in an attempt to recover the true language distribution. Still, vanilla random sampling from LMs yields low quality generations. Decoding algorithms attempt to restrict the LM distribution to a set of high-probability continuations, but rely on greedy heuristics that introduce myopic distortions, yielding sentences that are homogeneous, repetitive and incoherent. In this paper, we introduce EPIC, a hyperparameter-free decoding approach that incorporates the entropy of future trajectories into LM decoding. EPIC explicitly regulates the amount of uncertainty expressed at every step of generation, aligning the sampling distribution's entropy to the aleatoric (data) uncertainty. Through Entropy-Aware Lazy Gumbel-Max sampling, EPIC manages to be exact, while also being efficient, requiring only a sublinear number of entropy evaluations per step. Unlike current baselines, EPIC yields sampling distributions that are empirically well-aligned with the entropy of the underlying data distribution. Across creative writing and summarization tasks, EPIC consistently improves LM-as-judge preference win-rates over widely used decoding strategies. These preference gains are complemented by automatic metrics, showing that EPIC produces more diverse generations and more faithful summaries. We also evaluate EPIC on mathematical reasoning, where it outperforms all baselines.

</details>


### [246] [Communication-Efficient Federated AUC Maximization with Cyclic Client Participation](https://arxiv.org/abs/2601.01649)
*Umesh Vangapally,Wenhan Wu,Chen Chen,Zhishuai Guo*

Main category: cs.LG

TL;DR: 该论文提出了针对循环客户端参与场景的联邦AUC最大化算法，在平方替代损失下实现了$\widetilde{O}(1/ε^{1/2})$通信复杂度和$\widetilde{O}(1/ε)$迭代复杂度，在一般成对损失下也取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有联邦AUC最大化方法通常假设客户端完全可用，但实际联邦学习系统中客户端往往按固定循环时间表参与训练。这种循环参与模式给不可分解的AUC目标带来了独特的优化挑战。

Method: 研究两种关键设置：1）使用平方替代损失的AUC最大化，将其重新表述为非凸-强凹极小极大优化问题，利用Polyak-Łojasiewicz条件；2）一般成对AUC损失。针对循环客户端参与场景开发通信高效的算法。

Result: 在平方替代损失下实现了$\widetilde{O}(1/ε^{1/2})$通信复杂度和$\widetilde{O}(1/ε)$迭代复杂度的最先进性能；在一般成对损失下实现了$O(1/ε^3)$通信复杂度和$O(1/ε^4)$迭代复杂度，在PL条件下改进为$\widetilde{O}(1/ε^{1/2})$和$\widetilde{O}(1/ε)$。

Conclusion: 该论文提出的方法在图像分类、医学成像和欺诈检测等基准任务上展现出卓越的效率和有效性，为解决实际联邦学习系统中客户端循环参与下的AUC最大化问题提供了有效解决方案。

Abstract: Federated AUC maximization is a powerful approach for learning from imbalanced data in federated learning (FL). However, existing methods typically assume full client availability, which is rarely practical. In real-world FL systems, clients often participate in a cyclic manner: joining training according to a fixed, repeating schedule. This setting poses unique optimization challenges for the non-decomposable AUC objective. This paper addresses these challenges by developing and analyzing communication-efficient algorithms for federated AUC maximization under cyclic client participation. We investigate two key settings: First, we study AUC maximization with a squared surrogate loss, which reformulates the problem as a nonconvex-strongly-concave minimax optimization. By leveraging the Polyak-Łojasiewicz (PL) condition, we establish a state-of-the-art communication complexity of $\widetilde{O}(1/ε^{1/2})$ and iteration complexity of $\widetilde{O}(1/ε)$. Second, we consider general pairwise AUC losses. We establish a communication complexity of $O(1/ε^3)$ and an iteration complexity of $O(1/ε^4)$. Further, under the PL condition, these bounds improve to communication complexity of $\widetilde{O}(1/ε^{1/2})$ and iteration complexity of $\widetilde{O}(1/ε)$. Extensive experiments on benchmark tasks in image classification, medical imaging, and fraud detection demonstrate the superior efficiency and effectiveness of our proposed methods.

</details>


### [247] [Context-Free Recognition with Transformers](https://arxiv.org/abs/2601.01754)
*Selim Jerad,Anej Svete,Sophie Hao,Ryan Cotterell,William Merrill*

Main category: cs.LG

TL;DR: 循环Transformer通过O(log n)循环层和O(n^6)填充token可以识别所有上下文无关语言，但实际中可能不实用；对于自然子类如无歧义CFL，只需O(n^3)填充，更高效。


<details>
  <summary>Details</summary>
Motivation: Transformer在处理自然语言和代码等符合语法的输入时表现出色，但其处理语法结构的能力尚不清楚。已有研究表明标准Transformer无法识别上下文无关语言（CFL），甚至无法识别其子类正则语言。虽然已有工作证明O(log n)循环层可使Transformer识别正则语言，但CFL识别问题仍未解决。

Method: 提出循环Transformer架构，使用O(log n)循环层和O(n^6)填充token来实现CFL识别。对于无歧义CFL等自然子类，将填充需求降低到O(n^3)。通过理论分析和实验验证，证明循环机制在需要对数深度的语言上的有效性。

Result: 理论证明循环Transformer可以识别所有CFL，但需要大量填充token（O(n^6)）。对于无歧义CFL，填充需求降至O(n^3)。实验验证循环机制在需要对数深度的语言上确实有效。

Conclusion: Transformer识别CFL存在复杂性：通用识别可能需要不切实际的大量填充，但自然约束（如无歧义性）可实现高效识别。循环机制为Transformer处理语法结构提供了理论可能性，但在实际应用中需要考虑计算效率。

Abstract: Transformers excel on tasks that process well-formed inputs according to some grammar, such as natural language and code. However, it remains unclear how they can process grammatical syntax. In fact, under standard complexity conjectures, standard transformers cannot recognize context-free languages (CFLs), a canonical formalism to describe syntax, or even regular languages, a subclass of CFLs (Merrill et al., 2022). Merrill & Sabharwal (2024) show that $\mathcal{O}(\log n)$ looping layers (w.r.t. input length $n$) allows transformers to recognize regular languages, but the question of context-free recognition remained open. In this work, we show that looped transformers with $\mathcal{O}(\log n)$ looping layers and $\mathcal{O}(n^6)$ padding tokens can recognize all CFLs. However, training and inference with $\mathcal{O}(n^6)$ padding tokens is potentially impractical. Fortunately, we show that, for natural subclasses such as unambiguous CFLs, the recognition problem on transformers becomes more tractable, requiring $\mathcal{O}(n^3)$ padding. We empirically validate our results and show that looping helps on a language that provably requires logarithmic depth. Overall, our results shed light on the intricacy of CFL recognition by transformers: While general recognition may require an intractable amount of padding, natural constraints such as unambiguity yield efficient recognition algorithms.

</details>


### [248] [Learning Resilient Elections with Adversarial GNNs](https://arxiv.org/abs/2601.01653)
*Hao Xiang Li,Yash Shah,Lorenzo Giusti*

Main category: cs.LG

TL;DR: 提出一种基于图神经网络和对抗训练的投票规则学习方法，通过双图表示选举并提高抗策略投票能力


<details>
  <summary>Details</summary>
Motivation: 传统投票规则难以满足所有场景需求，现有学习方法在抗策略投票方面存在不足，需要更鲁棒的自动化投票机制设计

Method: 使用双图表示选举，采用图神经网络学习投票规则，结合对抗训练提高抗策略投票能力

Result: 在合成和真实数据集上验证了方法的有效性，解决了先前工作的关键限制

Conclusion: 该方法为机器学习应用于真实世界选举开辟了新前沿，提高了投票规则的社会福利和鲁棒性

Abstract: In the face of adverse motives, it is indispensable to achieve a consensus. Elections have been the canonical way by which modern democracy has operated since the 17th century. Nowadays, they regulate markets, provide an engine for modern recommender systems or peer-to-peer networks, and remain the main approach to represent democracy. However, a desirable universal voting rule that satisfies all hypothetical scenarios is still a challenging topic, and the design of these systems is at the forefront of mechanism design research. Automated mechanism design is a promising approach, and recent works have demonstrated that set-invariant architectures are uniquely suited to modelling electoral systems. However, various concerns prevent the direct application to real-world settings, such as robustness to strategic voting. In this paper, we generalise the expressive capability of learned voting rules, and combine improvements in neural network architecture with adversarial training to improve the resilience of voting rules while maximizing social welfare. We evaluate the effectiveness of our methods on both synthetic and real-world datasets. Our method resolves critical limitations of prior work regarding learning voting rules by representing elections using bipartite graphs, and learning such voting rules using graph neural networks. We believe this opens new frontiers for applying machine learning to real-world elections.

</details>


### [249] [HyperCLOVA X 8B Omni](https://arxiv.org/abs/2601.01792)
*NAVER Cloud HyperCLOVA X Team*

Main category: cs.LG

TL;DR: HyperCLOVA X 8B Omni是首个支持文本、音频和视觉任意输入输出的全模态模型，通过统一的多模态序列预测实现跨模态理解与生成。


<details>
  <summary>Details</summary>
Motivation: 构建实用的任意到任意全模态助手，避免使用分离的模态特定管道，而是将多模态理解和生成整合到单一模型中。

Method: 通过共享的下一个token预测接口统一模态，使用视觉和音频编码器注入连续嵌入以实现细粒度理解和接地，在交错的多模态序列上进行训练。

Result: 在韩语和英语的文本、音频和视觉的多种输入输出组合上，与类似规模模型相比表现出有竞争力的性能。

Conclusion: HyperCLOVA X 8B Omni作为8B规模的全模态探索点，其开放权重发布将支持广泛的研究和部署场景。

Abstract: In this report, we present HyperCLOVA X 8B Omni, the first any-to-any omnimodal model in the HyperCLOVA X family that supports text, audio, and vision as both inputs and outputs. By consolidating multimodal understanding and generation into a single model rather than separate modality-specific pipelines, HyperCLOVA X 8B Omni serves as an 8B-scale omni-pathfinding point toward practical any-to-any omni assistants. At a high level, the model unifies modalities through a shared next-token prediction interface over an interleaved multimodal sequence, while vision and audio encoders inject continuous embeddings for fine-grained understanding and grounding. Empirical evaluations demonstrate competitive performance against comparably sized models across diverse input-output combinations spanning text, audio, and vision, in both Korean and English. We anticipate that the open-weight release of HyperCLOVA X 8B Omni will support a wide range of research and deployment scenarios.

</details>


### [250] [Length-Aware Adversarial Training for Variable-Length Trajectories: Digital Twins for Mall Shopper Paths](https://arxiv.org/abs/2601.01663)
*He Sun,Jiwoong Shin,Ravi Dhar*

Main category: cs.LG

TL;DR: 提出长度感知采样(LAS)方法，通过按长度分组轨迹减少批次内长度异质性，改善生成模型对轨迹衍生统计量的分布匹配


<details>
  <summary>Details</summary>
Motivation: 轨迹生成建模中，标准小批量训练在轨迹长度高度异质时不稳定，导致轨迹衍生统计量的分布匹配效果下降

Method: 提出长度感知采样(LAS)：按轨迹长度分组，从单一长度桶中采样批次，减少批次内长度异质性；结合条件轨迹GAN和辅助时间对齐损失

Result: LAS在多商场购物者轨迹数据集和多种公共序列数据集(GPS、教育、电商、电影)上一致改善衍生变量分布的匹配，优于随机采样

Conclusion: LAS是一种简单有效的批处理策略，通过减少批次内长度异质性改善轨迹生成模型的分布匹配性能，具有理论保证和实证优势

Abstract: We study generative modeling of \emph{variable-length trajectories} -- sequences of visited locations/items with associated timestamps -- for downstream simulation and counterfactual analysis. A recurring practical issue is that standard mini-batch training can be unstable when trajectory lengths are highly heterogeneous, which in turn degrades \emph{distribution matching} for trajectory-derived statistics. We propose \textbf{length-aware sampling (LAS)}, a simple batching strategy that groups trajectories by length and samples batches from a single length bucket, reducing within-batch length heterogeneity (and making updates more consistent) without changing the model class. We integrate LAS into a conditional trajectory GAN with auxiliary time-alignment losses and provide (i) a distribution-level guarantee for derived variables under mild boundedness assumptions, and (ii) an IPM/Wasserstein mechanism explaining why LAS improves distribution matching by removing length-only shortcut critics and targeting within-bucket discrepancies. Empirically, LAS consistently improves matching of derived-variable distributions on a multi-mall dataset of shopper trajectories and on diverse public sequence datasets (GPS, education, e-commerce, and movies), outperforming random sampling across dataset-specific metrics.

</details>


### [251] [Output Embedding Centering for Stable LLM Pretraining](https://arxiv.org/abs/2601.02031)
*Felix Stollenwerk,Anna Lokrantz,Niclas Hertzberg*

Main category: cs.LG

TL;DR: 本文提出输出嵌入中心化（OEC）作为解决大型语言模型预训练中输出logit发散问题的新方法，优于现有的z-loss方法


<details>
  <summary>Details</summary>
Motivation: 大型语言模型预训练不仅昂贵，而且容易出现训练不稳定性。在训练后期使用较大学习率时经常出现输出logit发散问题。现有的z-loss方法只是治标不治本，需要从根本上解决这个问题。

Method: 从输出嵌入几何角度分析不稳定性原因，提出输出嵌入中心化（OEC）作为新的缓解策略。OEC有两种实现方式：确定性操作的μ-centering和正则化方法的μ-loss。

Result: 实验表明两种OEC变体在训练稳定性和学习率敏感性方面都优于z-loss。特别是在z-loss失败的大学习率情况下，OEC方法仍能确保训练收敛。μ-loss对正则化超参数调优的敏感性显著低于z-loss。

Conclusion: 输出嵌入中心化（OEC）是解决输出logit发散问题的有效方法，相比z-loss具有更好的稳定性和更低的超参数敏感性，为大型语言模型预训练提供了更可靠的解决方案。

Abstract: Pretraining of large language models is not only expensive but also prone to certain training instabilities. A specific instability that often occurs for large learning rates at the end of training is output logit divergence. The most widely used mitigation strategy, z-loss, merely addresses the symptoms rather than the underlying cause of the problem. In this paper, we analyze the instability from the perspective of the output embeddings' geometry and identify its cause. Based on this, we propose output embedding centering (OEC) as a new mitigation strategy, and prove that it suppresses output logit divergence. OEC can be implemented in two different ways, as a deterministic operation called μ-centering, or a regularization method called μ-loss. Our experiments show that both variants outperform z-loss in terms of training stability and learning rate sensitivity. In particular, they ensure that training converges even for large learning rates when z-loss fails. Furthermore, we find that μ-loss is significantly less sensitive to regularization hyperparameter tuning than z-loss.

</details>


### [252] [Who is the Winning Algorithm? Rank Aggregation for Comparative Studies](https://arxiv.org/abs/2601.01664)
*Amichai Painsky*

Main category: cs.LG

TL;DR: 提出新框架，利用算法在基准数据集上的完整排名信息（不仅是获胜次数，还包括第二、第三等名次）来估计每个算法在未来未见数据集上获胜的概率，相比现有方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只统计算法在基准数据集上的获胜次数（最大似然估计），但忽略了完整的排名信息（如第二、第三等名次）。这些额外信息可能对预测算法在未来数据集上的表现有重要价值。

Method: 引入新的概念框架，基于算法在基准数据集上的完整排名（不仅是第一名，还包括所有名次）来估计每个算法在未来未见数据集上获胜的概率。该方法充分利用了排名分布中的全部信息。

Result: 提出的框架在合成数据和真实世界示例中都显著优于当前已知的方法，能够更准确地预测算法在未来数据集上的获胜概率。

Conclusion: 利用算法在基准数据集上的完整排名信息（而不仅仅是获胜次数）可以显著提高预测算法未来表现的能力，为算法选择和评估提供了更有效的方法。

Abstract: Consider a collection of m competing machine learning algorithms. Given their performance on a benchmark of datasets, we would like to identify the best performing algorithm. Specifically, which algorithm is most likely to ``win'' (rank highest) on a future, unseen dataset. The standard maximum likelihood approach suggests counting the number of wins per each algorithm. In this work, we argue that there is much more information in the complete rankings. That is, the number of times that each algorithm finished second, third and so forth. Yet, it is not entirely clear how to effectively utilize this information for our purpose. In this work we introduce a novel conceptual framework for estimating the win probability for each of the m algorithms, given their complete rankings over a benchmark of datasets. Our proposed framework significantly improves upon currently known methods in synthetic and real-world examples.

</details>


### [253] [Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives](https://arxiv.org/abs/2601.01665)
*Wei Liu,Yaoxin Wu,Yingqian Zhang,Thomas Bäck,Yingjie Fan*

Main category: cs.LG

TL;DR: 提出一个面向鲁棒性的多目标组合优化DRL求解器框架，包含偏好对抗攻击生成困难实例和硬度感知偏好选择的防御策略，提升求解器的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管深度强化学习在多目标组合优化问题上表现出潜力，但基于学习的求解器鲁棒性研究不足，尤其是在多样化和复杂的问题分布上。需要探索如何评估和提升这些求解器在不同分布下的鲁棒性。

Method: 提出统一鲁棒性框架：1) 偏好对抗攻击方法，生成暴露求解器弱点的困难实例；2) 防御策略，将硬度感知偏好选择集成到对抗训练中，减少对受限偏好区域的过拟合；3) 通过帕累托前沿质量退化量化攻击影响。

Result: 实验在多目标旅行商问题、多目标容量车辆路径问题和多目标背包问题上验证：攻击方法成功为不同求解器生成困难实例；防御方法显著增强神经求解器的鲁棒性和泛化性，在困难或分布外实例上表现优异。

Conclusion: 提出的鲁棒性框架有效评估和提升多目标组合优化DRL求解器的鲁棒性，攻击方法能暴露求解器弱点，防御方法通过硬度感知偏好选择改善泛化性能，为学习型求解器的鲁棒性研究提供了系统方法。

Abstract: Deep reinforcement learning (DRL) has shown great promise in addressing multi-objective combinatorial optimization problems (MOCOPs). Nevertheless, the robustness of these learning-based solvers has remained insufficiently explored, especially across diverse and complex problem distributions. In this paper, we propose a unified robustness-oriented framework for preference-conditioned DRL solvers for MOCOPs. Within this framework, we develop a preference-based adversarial attack to generate hard instances that expose solver weaknesses, and quantify the attack impact by the resulting degradation on Pareto-front quality. We further introduce a defense strategy that integrates hardness-aware preference selection into adversarial training to reduce overfitting to restricted preference regions and improve out-of-distribution performance. The experimental results on multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle routing problem (MOCVRP), and multi-objective knapsack problem (MOKP) verify that our attack method successfully learns hard instances for different solvers. Furthermore, our defense method significantly strengthens the robustness and generalizability of neural solvers, delivering superior performance on hard or out-of-distribution instances.

</details>


### [254] [Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting](https://arxiv.org/abs/2601.02151)
*Muxi Diao,Lele Yang,Wuxuan Gong,Yutong Zhang,Zhonghao Yan,Yufei Han,Kongming Liang,Weiran Xu,Zhanyu Ma*

Main category: cs.LG

TL;DR: 论文提出EAFT方法，通过熵自适应门控机制区分知识冲突和认知不确定性，在保持下游性能的同时显著缓解SFT中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 监督微调(SFT)在领域适应中常导致灾难性遗忘，而强化学习(RL)能有效保持通用能力。研究发现这种差异源于分布差距：RL与模型内部信念对齐，而SFT强制模型拟合外部监督，导致"自信冲突"标记引发破坏性梯度更新。

Method: 提出熵自适应微调(EAFT)，利用标记级熵作为门控机制区分认知不确定性和知识冲突。模型从不确定样本中学习，同时抑制冲突数据的梯度更新，避免破坏性更新。

Result: 在Qwen和GLM系列模型(4B-32B参数)的数学、医疗和智能体领域实验中，EAFT在保持下游性能与标准SFT相当的同时，显著减轻了通用能力的退化。

Conclusion: EAFT通过熵自适应门控有效解决了SFT中的灾难性遗忘问题，为领域适应提供了更优的微调策略，平衡了专业化和通用能力保持。

Abstract: Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as "Confident Conflicts" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.

</details>


### [255] [HeurekaBench: A Benchmarking Framework for AI Co-scientist](https://arxiv.org/abs/2601.01678)
*Siba Smarak Panigrahi,Jovana Videnović,Maria Brbić*

Main category: cs.LG

TL;DR: HeurekaBench是一个用于评估科学智能体系统的基准框架，通过半自动化流程从真实科学研究和代码仓库中创建开放式的探索性研究问题，并在单细胞生物学领域实例化为sc-HeurekaBench基准。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的推理模型能够开发作为合作科学家的智能体系统，但评估这些系统具有挑战性，需要真实、端到端的研究场景来整合数据分析、解释和从实验数据中生成新见解。

Method: 提出HeurekaBench框架，使用半自动化流程创建基准：基于科学研究及其代码仓库，利用多个LLM提取见解并生成候选工作流程，然后与报告结果进行验证。在单细胞生物学领域实例化为sc-HeurekaBench基准。

Result: 使用sc-HeurekaBench基准比较最先进的单细胞智能体，发现添加批评模块可以将开源LLM智能体的不良响应改善高达22%，并缩小与闭源对应系统的差距。展示了基准在定量分析智能体系统设计选择方面的优势。

Conclusion: HeurekaBench为科学智能体的严格端到端评估设定了路径，将基准构建基于真实的科学工作流程，有助于推动科学智能体系统的评估和发展。

Abstract: LLM-based reasoning models have enabled the development of agentic systems that act as co-scientists, assisting in multi-step scientific analysis. However, evaluating these systems is challenging, as it requires realistic, end-to-end research scenarios that integrate data analysis, interpretation, and the generation of new insights from the experimental data. To address this limitation, we introduce HeurekaBench, a framework to create benchmarks with exploratory, open-ended research questions for experimental datasets. Each such question is grounded in a scientific study and its corresponding code repository, and is created using a semi-automated pipeline that leverages multiple LLMs to extract insights and generate candidate workflows, which are then verified against reported findings. We instantiate the framework in single-cell biology to obtain sc-HeurekaBench benchmark and use it to compare state-of-the-art single-cell agents. We further showcase the benefits of our benchmark for quantitatively analyzing current design choices in agentic systems. We find that the addition of a critic module can improve ill-formed responses for open-source LLM-based agents by up to 22% and close the gap with their closed-source counterparts. Overall, HeurekaBench sets a path toward rigorous, end-to-end evaluation of scientific agents, grounding benchmark construction in real scientific workflows.

</details>


### [256] [DiMEx: Breaking the Cold Start Barrier in Data-Free Model Extraction via Latent Diffusion Priors](https://arxiv.org/abs/2601.01688)
*Yash Thesia,Meera Suthar*

Main category: cs.LG

TL;DR: DiMEx利用预训练扩散模型的语义先验，通过潜在空间贝叶斯优化实现高效模型窃取；同时提出HSE防御方法，通过检测优化轨迹来对抗此类攻击。


<details>
  <summary>Details</summary>
Motivation: 针对机器学习即服务中的模型窃取攻击，特别是无数据模型提取面临的"冷启动"问题——基于GAN的攻击需要大量查询从随机噪声收敛到有意义数据，效率低下。

Method: 提出DiMEx框架：利用预训练潜在扩散模型的丰富语义先验，在生成器的潜在空间中使用随机嵌入贝叶斯优化，直接合成高质量查询，绕过初始化障碍。同时提出HSE防御：混合状态集成防御，通过检测潜在空间攻击的独特"优化轨迹"来识别攻击。

Result: DiMEx在SVHN数据集上仅用2000次查询就达到52.1%的协议率，比最先进的GAN基线高出16%以上。HSE防御能够将攻击成功率压制到21.6%，且延迟可忽略。

Conclusion: DiMEx展示了利用预训练扩散模型语义先验进行高效模型窃取的可行性，而HSE防御通过检测攻击的时间特征能够有效对抗此类语义攻击，为MLaaS安全提供了新的防御思路。

Abstract: Model stealing attacks pose an existential threat to Machine Learning as a Service (MLaaS), allowing adversaries to replicate proprietary models for a fraction of their training cost. While Data-Free Model Extraction (DFME) has emerged as a stealthy vector, it remains fundamentally constrained by the "Cold Start" problem: GAN-based adversaries waste thousands of queries converging from random noise to meaningful data. We propose DiMEx, a framework that weaponizes the rich semantic priors of pre-trained Latent Diffusion Models to bypass this initialization barrier entirely. By employing Random Embedding Bayesian Optimization (REMBO) within the generator's latent space, DiMEx synthesizes high-fidelity queries immediately, achieving 52.1 percent agreement on SVHN with just 2,000 queries - outperforming state-of-the-art GAN baselines by over 16 percent. To counter this highly semantic threat, we introduce the Hybrid Stateful Ensemble (HSE) defense, which identifies the unique "optimization trajectory" of latent-space attacks. Our results demonstrate that while DiMEx evades static distribution detectors, HSE exploits this temporal signature to suppress attack success rates to 21.6 percent with negligible latency.

</details>


### [257] [Enhanced Multi-model Online Conformal Prediction](https://arxiv.org/abs/2601.01692)
*Erfan Hajihashemi,Yanning Shen*

Main category: cs.LG

TL;DR: 提出一种新颖的多模型在线共形预测算法，通过二分图选择有效模型子集，降低计算复杂度并提高预测效率


<details>
  <summary>Details</summary>
Motivation: 传统共形预测依赖单一固定模型，在在线环境中可能表现不稳定；现有多模型方法计算成本高且可能包含低效模型影响性能

Method: 开发多模型在线共形预测算法，每个时间步生成二分图识别有效模型子集，从中选择模型构建预测集

Result: 实验表明该方法在预测集大小和计算效率方面优于现有多模型共形预测技术

Conclusion: 提出的算法通过智能模型选择机制，在保证覆盖率的同时显著提高了预测效率和计算性能

Abstract: Conformal prediction is a framework for uncertainty quantification that constructs prediction sets for previously unseen data, guaranteeing coverage of the true label with a specified probability. However, the efficiency of these prediction sets, measured by their size, depends on the choice of the underlying learning model. Relying on a single fixed model may lead to suboptimal performance in online environments, as a single model may not consistently perform well across all time steps. To mitigate this, prior work has explored selecting a model from a set of candidates. However, this approach becomes computationally expensive as the number of candidate models increases. Moreover, poorly performing models in the set may also hinder the effectiveness. To tackle this challenge, this work develops a novel multi-model online conformal prediction algorithm that reduces computational complexity and improves prediction efficiency. At each time step, a bipartite graph is generated to identify a subset of effective models, from which a model is selected to construct the prediction set. Experiments demonstrate that our method outperforms existing multi-model conformal prediction techniques in terms of both prediction set size and computational efficiency.

</details>


### [258] [Digital Twin-Driven Communication-Efficient Federated Anomaly Detection for Industrial IoT](https://arxiv.org/abs/2601.01701)
*Mohammed Ayalew Belay,Adil Rasheed,Pierluigi Salvo Rossi*

Main category: cs.LG

TL;DR: 提出数字孪生集成联邦学习(DTFL)方法解决工业异常检测中的隐私、数据稀缺和通信效率问题，包含五种新方法，在公开数据集上验证了通信效率提升和收敛加速。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测面临依赖真实传感器数据、标注数据有限、高误报率和隐私问题等挑战，需要既能保护隐私又能提高通信效率的解决方案。

Method: 提出五种数字孪生集成联邦学习方法：DTML（元学习）、FPF（参数融合）、LPE（分层参数交换）、CWA（循环权重适应）和DTKD（知识蒸馏），结合合成和真实世界知识，平衡泛化与通信开销。

Result: 在公开网络物理异常检测数据集上，CWA在33轮达到80%准确率目标，FPF需41轮，LPE需48轮，DTML需87轮，而标准FedAvg和DTKD在100轮内未达标，通信效率提升显著（比DTML少62%轮数，比LPE少31%）。

Conclusion: 将数字孪生知识集成到联邦学习中能显著加速收敛到工业物联网异常检测的操作性准确阈值，同时保护数据隐私并提高通信效率。

Abstract: Anomaly detection is increasingly becoming crucial for maintaining the safety, reliability, and efficiency of industrial systems. Recently, with the advent of digital twins and data-driven decision-making, several statistical and machine-learning methods have been proposed. However, these methods face several challenges, such as dependence on only real sensor datasets, limited labeled data, high false alarm rates, and privacy concerns. To address these problems, we propose a suite of digital twin-integrated federated learning (DTFL) methods that enhance global model performance while preserving data privacy and communication efficiency. Specifically, we present five novel approaches: Digital Twin-Based Meta-Learning (DTML), Federated Parameter Fusion (FPF), Layer-wise Parameter Exchange (LPE), Cyclic Weight Adaptation (CWA), and Digital Twin Knowledge Distillation (DTKD). Each method introduces a unique mechanism to combine synthetic and real-world knowledge, balancing generalization with communication overhead. We conduct an extensive experiment using a publicly available cyber-physical anomaly detection dataset. For a target accuracy of 80%, CWA reaches the target in 33 rounds, FPF in 41 rounds, LPE in 48 rounds, and DTML in 87 rounds, whereas the standard FedAvg baseline and DTKD do not reach the target within 100 rounds. These results highlight substantial communication-efficiency gains (up to 62% fewer rounds than DTML and 31% fewer than LPE) and demonstrate that integrating DT knowledge into FL accelerates convergence to operationally meaningful accuracy thresholds for IIoT anomaly detection.

</details>


### [259] [UnPII: Unlearning Personally Identifiable Information with Quantifiable Exposure Risk](https://arxiv.org/abs/2601.01786)
*Intae Jeon,Yujeong Kwon,Hyungjoon Koo*

Main category: cs.LG

TL;DR: UnPII：首个基于PII风险优先级的遗忘方法，通过PII风险指数（PRI）评估不同PII属性的隐私风险，实现差异化遗忘策略，提升大语言模型隐私合规性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在金融、医疗、政府等关键领域的广泛应用，处理敏感个人身份信息（PII）引发隐私担忧。GDPR等法规要求应请求删除PII，需要可靠且经济高效的数据移除方案。现有遗忘技术通常采用统一策略，未考虑不同PII属性的隐私风险差异和业务风险。

Method: 提出UnPII方法：1）引入PII风险指数（PRI），综合评估可识别性、敏感性、可用性、可链接性、持久性、可暴露性和合规性七个维度的风险；2）构建合成PII数据集（1700个实例）模拟真实暴露场景；3）与现有遗忘算法（梯度上升、负偏好优化、直接偏好优化）无缝集成，无需修改其底层原理。

Result: 实验结果显示，UnPII在遗忘过程中平均仅增加27.5%的微调开销，同时实现：准确性提升最高11.8%，效用提升最高6.3%，泛化能力提升最高12.4%。

Conclusion: UnPII是首个以PII为中心的遗忘方法，通过基于风险的优先级排序实现差异化遗忘，有效平衡隐私保护与模型性能，为组织隐私政策提供可定制化解决方案。

Abstract: The ever-increasing adoption of Large Language Models in critical sectors like finance, healthcare, and government raises privacy concerns regarding the handling of sensitive Personally Identifiable Information (PII) during training. In response, regulations such as European Union's General Data Protection Regulation (GDPR) mandate the deletion of PII upon requests, underscoring the need for reliable and cost-effective data removal solutions. Machine unlearning has emerged as a promising direction for selectively forgetting data points. However, existing unlearning techniques typically apply a uniform forgetting strategy that neither accounts for the varying privacy risks posed by different PII attributes nor reflects associated business risks. In this work, we propose UnPII, the first PII-centric unlearning approach that prioritizes forgetting based on the risk of individual or combined PII attributes. To this end, we introduce the PII risk index (PRI), a composite metric that incorporates multiple dimensions of risk factors: identifiability, sensitivity, usability, linkability, permanency, exposability, and compliancy. The PRI enables a nuanced evaluation of privacy risks associated with PII exposures and can be tailored to align with organizational privacy policies. To support realistic assessment, we systematically construct a synthetic PII dataset (e.g., 1,700 PII instances) that simulates realistic exposure scenarios. UnPII seamlessly integrates with established unlearning algorithms, such as Gradient Ascent, Negative Preference Optimization, and Direct Preference Optimization, without modifying their underlying principles. Our experimental results demonstrate that UnPII achieves the improvements of accuracy up to 11.8%, utility up to 6.3%, and generalizability up to 12.4%, respectively, while incurring a modest fine-tuning overhead of 27.5% on average during unlearning.

</details>


### [260] [Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving](https://arxiv.org/abs/2601.01800)
*Qi Wei,Junchao Fan,Zhao Yang,Jianhua Wang,Jingkai Mao,Xiaolin Chang*

Main category: cs.LG

TL;DR: CARRL是一种针对自动驾驶的鲁棒强化学习方法，通过风险暴露对手和风险目标鲁棒代理的博弈，专门处理稀疏的安全关键风险，显著降低碰撞率。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习在自动驾驶中存在脆弱性，传统对抗训练方法忽略了代理与对手之间的不对称性，未能反映安全关键风险的稀疏性，导致实际场景中的鲁棒性不足。

Method: 提出CARRL框架，包含风险暴露对手（REA）和风险目标鲁棒代理（RTRA）。将两者建模为一般和博弈，REA专注于暴露安全关键故障（如碰撞），RTRA学习平衡安全与驾驶效率。REA采用解耦优化机制识别稀疏安全关键时刻，RTRA通过双回放缓冲池联合利用良性对抗经验，并强制扰动下的策略一致性。

Result: 实验结果显示，与最先进的基线方法相比，该方法在所有情况下至少降低了22.66%的碰撞率。

Conclusion: CARRL通过专门处理稀疏安全关键风险的对抗训练方法，显著提高了自动驾驶策略的鲁棒性，为实际部署提供了更可靠的解决方案。

Abstract: Reinforcement learning (RL) has shown considerable potential in autonomous driving (AD), yet its vulnerability to perturbations remains a critical barrier to real-world deployment. As a primary countermeasure, adversarial training improves policy robustness by training the AD agent in the presence of an adversary that deliberately introduces perturbations. Existing approaches typically model the interaction as a zero-sum game with continuous attacks. However, such designs overlook the inherent asymmetry between the agent and the adversary and then fail to reflect the sparsity of safety-critical risks, rendering the achieved robustness inadequate for practical AD scenarios. To address these limitations, we introduce criticality-aware robust RL (CARRL), a novel adversarial training approach for handling sparse, safety-critical risks in autonomous driving. CARRL consists of two interacting components: a risk exposure adversary (REA) and a risk-targeted robust agent (RTRA). We model the interaction between the REA and RTRA as a general-sum game, allowing the REA to focus on exposing safety-critical failures (e.g., collisions) while the RTRA learns to balance safety with driving efficiency. The REA employs a decoupled optimization mechanism to better identify and exploit sparse safety-critical moments under a constrained budget. However, such focused attacks inevitably result in a scarcity of adversarial data. The RTRA copes with this scarcity by jointly leveraging benign and adversarial experiences via a dual replay buffer and enforces policy consistency under perturbations to stabilize behavior. Experimental results demonstrate that our approach reduces the collision rate by at least 22.66\% across all cases compared to state-of-the-art baseline methods.

</details>


### [261] [Moments Matter:Stabilizing Policy Optimization using Return Distributions](https://arxiv.org/abs/2601.01803)
*Dennis Jabs,Aditya Mohan,Marius Lindauer*

Main category: cs.LG

TL;DR: 提出一种基于分布评论家高阶矩（偏度和峰度）的PPO改进方法，通过惩罚极端尾部行为来减少策略更新引起的不稳定性，在Walker2D环境中稳定性提升达75%。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习代理即使获得相同回合奖励，其行为也可能因环境和算法因素而大不相同。在连续控制任务中，微小参数变化可能导致不稳定步态，这既影响算法比较也阻碍实际应用。现有方法通过约束后更新奖励分布R(θ)来提升稳定性，但直接估计R(θ)在高维设置中计算成本过高。

Method: 通过分布评论家建模状态-动作奖励分布，然后利用该分布的高阶矩（偏度和峰度）对PPO的优势函数进行偏置修正。通过惩罚极端尾部行为，阻止策略进入容易产生不稳定性的参数区域。

Result: 在Walker2D环境中，该方法将稳定性提升达75%，同时保持了可比较的评估奖励。当后更新评论家值与后更新奖励对齐不佳时，标准PPO难以产生狭窄的R(θ)，而本方法的矩基修正能有效缩小R(θ)分布。

Conclusion: 利用环境随机性来减轻更新引起的变异性是有效的。通过分布评论家的高阶矩修正PPO优势函数，能够显著提升策略稳定性，同时不牺牲性能，为连续控制中的稳定性问题提供了计算高效的解决方案。

Abstract: Deep Reinforcement Learning (RL) agents often learn policies that achieve the same episodic return yet behave very differently, due to a combination of environmental (random transitions, initial conditions, reward noise) and algorithmic (minibatch selection, exploration noise) factors. In continuous control tasks, even small parameter shifts can produce unstable gaits, complicating both algorithm comparison and real-world transfer. Previous work has shown that such instability arises when policy updates traverse noisy neighborhoods and that the spread of post-update return distribution $R(θ)$, obtained by repeatedly sampling minibatches, updating $θ$, and measuring final returns, is a useful indicator of this noise. Although explicitly constraining the policy to maintain a narrow $R(θ)$ can improve stability, directly estimating $R(θ)$ is computationally expensive in high-dimensional settings. We propose an alternative that takes advantage of environmental stochasticity to mitigate update-induced variability. Specifically, we model state-action return distribution through a distributional critic and then bias the advantage function of PPO using higher-order moments (skewness and kurtosis) of this distribution. By penalizing extreme tail behaviors, our method discourages policies from entering parameter regimes prone to instability. We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow $R(θ)$. In such cases, our moment-based correction narrows $R(θ)$, improving stability by up to 75% in Walker2D, while preserving comparable evaluation returns.

</details>


### [262] [RealPDEBench: A Benchmark for Complex Physical Systems with Real-World Data](https://arxiv.org/abs/2601.01829)
*Peiyan Hu,Haodong Feng,Hongyuan Liu,Tongtong Yan,Wenhao Deng,Tianrun Gao,Rong Zheng,Haoren Zheng,Chenglei Yu,Chuanrui Wang,Kaiwen Li,Zhi-Ming Ma,Dezhi Zhou,Xingcai Lu,Dixia Fan,Tailin Wu*

Main category: cs.LG

TL;DR: RealPDEBench是首个结合真实测量数据与配对数值模拟的科学机器学习基准，包含5个数据集、3个任务、8个指标和10个基线方法，旨在解决科学ML中真实数据稀缺和模拟-现实差距问题。


<details>
  <summary>Details</summary>
Motivation: 当前科学机器学习模型主要依赖模拟数据进行训练和验证，缺乏真实世界数据限制了模型发展和评估，也阻碍了模拟到现实迁移等关键任务的研究。

Method: 构建包含5个真实测量数据集及其配对模拟数据的基准，定义3个比较任务，设计8个评估指标（数据导向和物理导向），并评估10个代表性基线方法。

Result: 实验显示模拟数据与真实数据存在显著差异，但使用模拟数据进行预训练能持续提升模型的准确性和收敛速度。

Conclusion: RealPDEBench为科学机器学习提供了真实世界数据的洞察，有助于缩小模拟-现实差距并推动模型在实际部署中的应用。

Abstract: Predicting the evolution of complex physical systems remains a central problem in science and engineering. Despite rapid progress in scientific Machine Learning (ML) models, a critical bottleneck is the lack of expensive real-world data, resulting in most current models being trained and validated on simulated data. Beyond limiting the development and evaluation of scientific ML, this gap also hinders research into essential tasks such as sim-to-real transfer. We introduce RealPDEBench, the first benchmark for scientific ML that integrates real-world measurements with paired numerical simulations. RealPDEBench consists of five datasets, three tasks, eight metrics, and ten baselines. We first present five real-world measured datasets with paired simulated datasets across different complex physical systems. We further define three tasks, which allow comparisons between real-world and simulated data, and facilitate the development of methods to bridge the two. Moreover, we design eight evaluation metrics, spanning data-oriented and physics-oriented metrics, and finally benchmark ten representative baselines, including state-of-the-art models, pretrained PDE foundation models, and a traditional method. Experiments reveal significant discrepancies between simulated and real-world data, while showing that pretraining with simulated data consistently improves both accuracy and convergence. In this work, we hope to provide insights from real-world data, advancing scientific ML toward bridging the sim-to-real gap and real-world deployment. Our benchmark, datasets, and instructions are available at https://realpdebench.github.io/.

</details>


### [263] [FAROS: Robust Federated Learning with Adaptive Scaling against Backdoor Attacks](https://arxiv.org/abs/2601.01833)
*Chenyu Hu,Qiming Hu,Sinan Chen,Nianyu Li,Mingyue Zhang,Jialong Li*

Main category: cs.LG

TL;DR: FAROS：一个增强的联邦学习框架，通过自适应差分缩放和鲁棒核心集计算来防御后门攻击，相比现有防御方法在攻击成功率和主任务准确率上表现更优。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临后门攻击的严重威胁，现有防御方法依赖固定参数，存在单点故障风险，难以应对复杂攻击者策略。

Method: 提出FAROS框架，包含自适应差分缩放机制（根据客户端上传梯度的离散度动态调整防御敏感性）和鲁棒核心集计算（计算高置信度客户端核心集的质心以降低单点故障风险）。

Result: 在多种数据集、模型和攻击场景下的实验表明，该方法在攻击成功率和主任务准确率上均优于现有防御方法。

Conclusion: FAROS通过动态自适应防御机制有效应对复杂的后门攻击策略，提高了联邦学习系统的安全性和鲁棒性。

Abstract: Federated Learning (FL) enables multiple clients to collaboratively train a shared model without exposing local data. However, backdoor attacks pose a significant threat to FL. These attacks aim to implant a stealthy trigger into the global model, causing it to mislead on inputs that possess a specific trigger while functioning normally on benign data. Although pre-aggregation detection is a main defense direction, existing state-of-the-art defenses often rely on fixed defense parameters. This reliance makes them vulnerable to single-point-of-failure risks, rendering them less effective against sophisticated attackers. To address these limitations, we propose FAROS, an enhanced FL framework that incorporates Adaptive Differential Scaling (ADS) and Robust Core-set Computing (RCC). The ADS mechanism adjusts the defense's sensitivity dynamically, based on the dispersion of uploaded gradients by clients in each round. This allows it to counter attackers who strategically shift between stealthiness and effectiveness. Furthermore, the RCC effectively mitigates the risk of single-point failure by computing the centroid of a core set comprising clients with the highest confidence. We conducted extensive experiments across various datasets, models, and attack scenarios. The results demonstrate that our method outperforms current defenses in both attack success rate and main task accuracy.

</details>


### [264] [Tackling Resource-Constrained and Data-Heterogeneity in Federated Learning with Double-Weight Sparse Pack](https://arxiv.org/abs/2601.01840)
*Qiantao Yang,Liquan Chen,Mingfu Xue,Songze Li*

Main category: cs.LG

TL;DR: FedCSPACK：基于余弦稀疏化参数打包和双权重聚合的个性化联邦学习方法，有效解决数据异构性和客户端资源受限问题


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在处理数据异构性时，忽略了客户端通信带宽和计算能力有限的问题，无法在解决数据异构性和适应有限客户端资源之间取得有效平衡

Method: 提出FedCSPACK方法：1）客户端基于余弦相似性打包模型参数并选择贡献最大的参数包共享，减少带宽需求；2）客户端生成基于共享参数包的掩码矩阵，提高稀疏更新在服务器上的对齐和聚合效率；3）在掩码中嵌入方向和分布距离权重，实现加权引导聚合机制

Result: 在四个数据集上使用十种最先进方法的广泛实验表明，FedCSPACK在保持高模型准确性的同时，有效提高了通信和计算效率

Conclusion: FedCSPACK通过参数打包和双权重聚合机制，在有限客户端资源下有效缓解数据异构性对模型性能的影响，实现了通信、计算效率和模型准确性的良好平衡

Abstract: Federated learning has drawn widespread interest from researchers, yet the data heterogeneity across edge clients remains a key challenge, often degrading model performance. Existing methods enhance model compatibility with data heterogeneity by splitting models and knowledge distillation. However, they neglect the insufficient communication bandwidth and computing power on the client, failing to strike an effective balance between addressing data heterogeneity and accommodating limited client resources. To tackle this limitation, we propose a personalized federated learning method based on cosine sparsification parameter packing and dual-weighted aggregation (FedCSPACK), which effectively leverages the limited client resources and reduces the impact of data heterogeneity on model performance. In FedCSPACK, the client packages model parameters and selects the most contributing parameter packages for sharing based on cosine similarity, effectively reducing bandwidth requirements. The client then generates a mask matrix anchored to the shared parameter package to improve the alignment and aggregation efficiency of sparse updates on the server. Furthermore, directional and distribution distance weights are embedded in the mask to implement a weighted-guided aggregation mechanism, enhancing the robustness and generalization performance of the global model. Extensive experiments across four datasets using ten state-of-the-art methods demonstrate that FedCSPACK effectively improves communication and computational efficiency while maintaining high model accuracy.

</details>


### [265] [High-Order Epistasis Detection Using Factorization Machine with Quadratic Optimization Annealing and MDR-Based Evaluation](https://arxiv.org/abs/2601.01860)
*Shuta Kikuchi,Shu Tanaka*

Main category: cs.LG

TL;DR: 提出基于因子分解机和二次优化退火的FMQA方法，用于高效检测高阶上位性，解决了传统MDR方法在组合爆炸下的计算不可行问题。


<details>
  <summary>Details</summary>
Motivation: 高阶上位性检测在遗传关联研究中面临组合爆炸的挑战，传统多因子降维(MDR)方法在基因座数量或交互阶数增加时计算不可行。

Method: 将上位性检测定义为黑盒优化问题，使用因子分解机结合二次优化退火(FMQA)求解，以MDR计算的分类错误率作为黑盒目标函数。

Result: 在模拟病例对照数据集上的实验表明，该方法能在有限迭代次数内成功识别不同交互阶数和基因座数量的真实上位性。

Conclusion: 提出的FMQA方法对于高阶上位性检测既有效又计算高效。

Abstract: Detecting high-order epistasis is a fundamental challenge in genetic association studies due to the combinatorial explosion of candidate locus combinations. Although multifactor dimensionality reduction (MDR) is a widely used method for evaluating epistasis, exhaustive MDR-based searches become computationally infeasible as the number of loci or the interaction order increases. In this paper, we define the epistasis detection problem as a black-box optimization problem and solve it with a factorization machine with quadratic optimization annealing (FMQA). We propose an efficient epistasis detection method based on FMQA, in which the classification error rate (CER) computed by MDR is used as a black-box objective function. Experimental evaluations were conducted using simulated case-control datasets with predefined high-order epistasis. The results demonstrate that the proposed method successfully identified ground-truth epistasis across various interaction orders and the numbers of genetic loci within a limited number of iterations. These results indicate that the proposed method is effective and computationally efficient for high-order epistasis detection.

</details>


### [266] [Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance](https://arxiv.org/abs/2601.01887)
*Jiawen Zhang,Lipeng He,Kejia Chen,Jian Lou,Jian Liu,Xiaohu Yang,Ruoxi Jia*

Main category: cs.LG

TL;DR: 仅需单个安全示例即可完全恢复安全对齐大语言模型，无需牺牲实用性，成本极低


<details>
  <summary>Details</summary>
Motivation: 现有方法需要大量安全样本或校准集，导致计算开销大且模型实用性下降。研究发现安全对齐可以更高效地恢复

Method: 使用单个安全示例进行微调恢复，发现安全梯度具有低秩结构，只需几个epoch即可收敛

Result: 该方法在五个安全对齐LLM和多个数据集上验证有效，无论有害示例数量或模型大小，都能有效恢复安全对齐

Conclusion: 安全对齐可以通过极低成本高效恢复，安全梯度的低秩结构解释了这种高效修正的可能性

Abstract: Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.

</details>


### [267] [FedBiCross: A Bi-Level Optimization Framework to Tackle Non-IID Challenges in Data-Free One-Shot Federated Learning on Medical Data](https://arxiv.org/abs/2601.01901)
*Yuexuan Xia,Yinghao Zhang,Yalin Liu,Hong-Ning Dai,Yong Xia*

Main category: cs.LG

TL;DR: FedBiCross：一种个性化的单次联邦学习框架，通过聚类和双层跨集群优化解决非IID数据下的预测冲突问题


<details>
  <summary>Details</summary>
Motivation: 现有的单次联邦学习方法在非IID数据下，所有客户端的预测在平均时会相互抵消，产生接近均匀的软标签，导致蒸馏监督信号弱。需要解决预测冲突问题，同时保护医疗数据的隐私敏感性。

Method: 提出FedBiCross框架，包含三个阶段：1）基于模型输出相似性对客户端进行聚类，形成连贯的子集成；2）双层跨集群优化，学习自适应权重，选择性利用有益的跨集群知识同时抑制负迁移；3）个性化蒸馏进行客户端特定适应。

Result: 在四个医学图像数据集上的实验表明，FedBiCross在不同非IID程度下始终优于最先进的基线方法。

Conclusion: FedBiCross通过聚类和选择性知识转移有效解决了非IID数据下单次联邦学习中的预测冲突问题，为隐私敏感的医疗应用提供了有效的个性化解决方案。

Abstract: Data-free knowledge distillation-based one-shot federated learning (OSFL) trains a model in a single communication round without sharing raw data, making OSFL attractive for privacy-sensitive medical applications. However, existing methods aggregate predictions from all clients to form a global teacher. Under non-IID data, conflicting predictions cancel out during averaging, yielding near-uniform soft labels that provide weak supervision for distillation. We propose FedBiCross, a personalized OSFL framework with three stages: (1) clustering clients by model output similarity to form coherent sub-ensembles, (2) bi-level cross-cluster optimization that learns adaptive weights to selectively leverage beneficial cross-cluster knowledge while suppressing negative transfer, and (3) personalized distillation for client-specific adaptation. Experiments on four medical image datasets demonstrate that FedBiCross consistently outperforms state-of-the-art baselines across different non-IID degrees.

</details>


### [268] [TT-FSI: Scalable Faithful Shapley Interactions via Tensor-Train](https://arxiv.org/abs/2601.01903)
*Ungsik Kim,Suwon Lee*

Main category: cs.LG

TL;DR: TT-FSI：利用矩阵乘积算子（MPO）高效计算忠实Shapley交互指数（FSI），将时间复杂度从O(d^ℓ·2^d)降低到O(ℓ²d³·2^d)，内存使用从O(4^d)降低到O(ℓd²)，实现了指数级改进。


<details>
  <summary>Details</summary>
Motivation: FSI指数是唯一满足忠实公理的Shapley交互指数，但计算复杂度极高（O(d^ℓ·2^d)时间，O(4^d)内存），限制了其在实际高维问题中的应用。

Method: 利用FSI的代数结构，证明线性算子v↦FSI(v)具有TT秩为O(ℓd)的MPO表示，从而设计高效的扫描算法，实现时间和内存的指数级优化。

Result: 在6个数据集（d=8到d=20）上的实验显示：相比基线加速280倍，相比SHAP-IQ加速85倍，内存减少290倍。TT-FSI可扩展到d=20（100万个联盟），而所有竞争方法均失败。

Conclusion: TT-FSI通过MPO表示和扫描算法，首次实现了FSI指数的高效可扩展计算，为高维特征交互分析提供了实用工具。

Abstract: The Faithful Shapley Interaction (FSI) index uniquely satisfies the faithfulness axiom among Shapley interaction indices, but computing FSI requires $O(d^\ell \cdot 2^d)$ time and existing implementations use $O(4^d)$ memory. We present TT-FSI, which exploits FSI's algebraic structure via Matrix Product Operators (MPO). Our main theoretical contribution is proving that the linear operator $v \mapsto \text{FSI}(v)$ admits an MPO representation with TT-rank $O(\ell d)$, enabling an efficient sweep algorithm with $O(\ell^2 d^3 \cdot 2^d)$ time and $O(\ell d^2)$ core storage an exponential improvement over existing methods. Experiments on six datasets ($d=8$ to $d=20$) demonstrate up to 280$\times$ speedup over baseline, 85$\times$ over SHAP-IQ, and 290$\times$ memory reduction. TT-FSI scales to $d=20$ (1M coalitions) where all competing methods fail.

</details>


### [269] [Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning](https://arxiv.org/abs/2601.01904)
*Yuxuan Li,Harshith Reddy Kethireddy,Srijita Das*

Main category: cs.LG

TL;DR: 该论文研究了偏好强化学习中特征依赖噪声的问题，发现现有噪声鲁棒方法在特征依赖噪声下性能显著下降，而无显式去噪的方法反而表现更好，语言模型噪声也表现出类似特征依赖噪声的特性。


<details>
  <summary>Details</summary>
Motivation: 偏好强化学习（PbRL）在奖励函数不易获得的复杂任务中很有用，但偏好数据常包含不确定性和噪声。现有研究大多关注均匀分布的噪声检测，而忽略了与观测特征相关的噪声类型。

Method: 提出了特征依赖噪声的形式化概念，包括轨迹特征噪声、轨迹相似性噪声、不确定性感知噪声和语言模型噪声等变体。在DMControl和Meta-world的复杂连续控制任务中评估特征依赖噪声。

Result: 实验表明，在某些特征依赖噪声设置下，最先进的噪声鲁棒PbRL方法的学习性能显著恶化，而无显式去噪的PbRL方法在多数设置中反而优于噪声鲁棒方法。语言模型噪声也表现出类似特征依赖噪声的特性。

Conclusion: 特征依赖噪声对现有PbRL方法构成挑战，需要进一步研究如何鲁棒地学习具有特征依赖噪声的偏好数据。语言模型噪声模拟了真实人类偏好噪声的特性，值得深入研究。

Abstract: Learning from Preferences in Reinforcement Learning (PbRL) has gained attention recently, as it serves as a natural fit for complicated tasks where the reward function is not easily available. However, preferences often come with uncertainty and noise if they are not from perfect teachers. Much prior literature aimed to detect noise, but with limited types of noise and most being uniformly distributed with no connection to observations. In this work, we formalize the notion of targeted feature-dependent noise and propose several variants like trajectory feature noise, trajectory similarity noise, uncertainty-aware noise, and Language Model noise.
  We evaluate feature-dependent noise, where noise is correlated with certain features in complex continuous control tasks from DMControl and Meta-world. Our experiments show that in some feature-dependent noise settings, the state-of-the-art noise-robust PbRL method's learning performance is significantly deteriorated, while PbRL method with no explicit denoising can surprisingly outperform noise-robust PbRL in majority settings.
  We also find language model's noise exhibits similar characteristics to feature-dependent noise, thereby simulating realistic humans and call for further study in learning with feature-dependent noise robustly.

</details>


### [270] [Distorted Distributional Policy Evaluation for Offline Reinforcement Learning](https://arxiv.org/abs/2601.01917)
*Ryo Iwaki,Takayuki Osogami*

Main category: cs.LG

TL;DR: 提出一种新的分位数扭曲方法，通过非均匀悲观主义改进离线分布强化学习性能


<details>
  <summary>Details</summary>
Motivation: 现有离线分布强化学习方法采用均匀低估回报分位数的策略，导致过于保守的价值估计，限制了泛化能力和性能表现

Method: 引入分位数扭曲概念，根据支持数据的可用性调整保守程度，实现非均匀悲观主义

Result: 理论分析和实证验证表明，该方法相比均匀悲观主义能显著提升性能

Conclusion: 分位数扭曲为离线分布强化学习提供了一种更有效的非均匀悲观主义方法，解决了现有方法过于保守的问题

Abstract: While Distributional Reinforcement Learning (DRL) methods have demonstrated strong performance in online settings, its success in offline scenarios remains limited. We hypothesize that a key limitation of existing offline DRL methods lies in their approach to uniformly underestimate return quantiles. This uniform pessimism can lead to overly conservative value estimates, ultimately hindering generalization and performance. To address this, we introduce a novel concept called quantile distortion, which enables non-uniform pessimism by adjusting the degree of conservatism based on the availability of supporting data. Our approach is grounded in theoretical analysis and empirically validated, demonstrating improved performance over uniform pessimism.

</details>


### [271] [Theoretical Convergence of SMOTE-Generated Samples](https://arxiv.org/abs/2601.01927)
*Firuz Kamalov,Hana Sulieman,Witold Pedrycz*

Main category: cs.LG

TL;DR: 本文对SMOTE算法进行了严格的理论分析，证明了其合成变量Z依概率收敛于原始变量X，并在X紧致时证明了更强的均值收敛，同时发现较小的最近邻秩值能带来更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 不平衡数据广泛影响机器学习应用，SMOTE作为最流行的解决方法之一，不仅需要经验验证，更需要理论分析来建立其理论基础。

Method: 对SMOTE算法进行严格的数学理论分析，证明其收敛性质，包括依概率收敛和均值收敛，并通过数值实验使用真实和合成数据进行验证。

Result: 证明了SMOTE合成变量Z依概率收敛于原始变量X；当X紧致时证明了更强的均值收敛；发现较小的最近邻秩值能带来更快的收敛速度；数值实验支持了理论结果。

Conclusion: 本文为SMOTE算法提供了理论基础，不仅增强了不平衡数据处理技术，还为更广泛的数据增强方法提供了理论支持。

Abstract: Imbalanced data affects a wide range of machine learning applications, from healthcare to network security. As SMOTE is one of the most popular approaches to addressing this issue, it is imperative to validate it not only empirically but also theoretically. In this paper, we provide a rigorous theoretical analysis of SMOTE's convergence properties. Concretely, we prove that the synthetic random variable Z converges in probability to the underlying random variable X. We further prove a stronger convergence in mean when X is compact. Finally, we show that lower values of the nearest neighbor rank lead to faster convergence offering actionable guidance to practitioners. The theoretical results are supported by numerical experiments using both real-life and synthetic data. Our work provides a foundational understanding that enhances data augmentation techniques beyond imbalanced data scenarios.

</details>


### [272] [DéjàQ: Open-Ended Evolution of Diverse, Learnable and Verifiable Problems](https://arxiv.org/abs/2601.01931)
*Willem Röpke,Samuel Coward,Andrei Lupu,Thomas Foster,Tim Rocktäschel,Jakob Foerster*

Main category: cs.LG

TL;DR: DéjàQ是一个通过进化合成数学问题来增强模型推理能力的框架，它让模型在训练过程中动态生成和修改问题，而不是依赖静态数据集。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型主要依赖静态数据集，这可能导致模型记忆而非真正理解，限制了泛化能力。需要一种能够动态适应模型学习进度的训练方法。

Method: 提出DéjàQ框架，通过进化过程联合生成多样化的合成数学问题。采用两种LLM驱动的变异策略：1）改变上下文细节；2）直接修改问题结构。模型在训练过程中自行变异训练数据。

Result: 模型能够生成新颖且有意义的数学问题，LLM驱动的变异策略改善了强化学习训练效果。生成的问題有效，计算开销可控。

Conclusion: 动态进化训练数据能有效增强数学推理能力，具有广泛适用性。作者将开源代码以支持进一步研究。

Abstract: Recent advances in reasoning models have yielded impressive results in mathematics and coding. However, most approaches rely on static datasets, which have been suggested to encourage memorisation and limit generalisation. We introduce DéjàQ, a framework that departs from this paradigm by jointly evolving a diverse set of synthetic mathematical problems alongside model training. This evolutionary process adapts to the model's ability throughout training, optimising problems for learnability. We propose two LLM-driven mutation strategies in which the model itself mutates the training data, either by altering contextual details or by directly modifying problem structure. We find that the model can generate novel and meaningful problems, and that these LLM-driven mutations improve RL training. We analyse key aspects of DéjàQ, including the validity of generated problems and computational overhead. Our results underscore the potential of dynamically evolving training data to enhance mathematical reasoning and indicate broader applicability, which we will support by open-sourcing our code.

</details>


### [273] [SynRXN: An Open Benchmark and Curated Dataset for Computational Reaction Modeling](https://arxiv.org/abs/2601.01943)
*Tieu-Long Phan,Nhu-Ngoc Nguyen Song,Peter F. Stadler*

Main category: cs.LG

TL;DR: SynRXN是一个用于计算机辅助合成规划的统一基准测试框架和开放数据资源，将端到端合成规划分解为五个任务族，提供标准化数据集和评估流程。


<details>
  <summary>Details</summary>
Motivation: 当前计算机辅助合成规划领域缺乏统一、透明的基准测试框架，数据集异构且评估标准不一致，难以进行公平的方法比较和性能评估。

Method: 将合成规划分解为五个任务族：反应平衡、原子映射、反应分类、反应性质预测和合成路线设计。从异构公共来源收集反应数据，进行标准化处理，提供防泄漏的数据分割、标准化评估工作流和指标套件。

Result: 创建了一个包含版本化数据集、透明数据分割、标准化评估流程的完整基准测试框架，支持可重复的数据集重建，所有资源均在开放许可下发布。

Conclusion: SynRXN通过消除数据集异质性并提供透明、可重复的评估框架，实现了CASP方法的公平纵向比较，支持完整的反应信息学管道测试，降低了实际合成规划工作负载中稳健性能评估的门槛。

Abstract: We present SynRXN, a unified benchmarking framework and open-data resource for computer-aided synthesis planning (CASP). SynRXN decomposes end-to-end synthesis planning into five task families, covering reaction rebalancing, atom-to-atom mapping, reaction classification, reaction property prediction, and synthesis route design. Curated, provenance-tracked reaction corpora are assembled from heterogeneous public sources into a harmonized representation and packaged as versioned datasets for each task family, with explicit source metadata, licence tags, and machine-readable manifests that record checksums, and row counts. For every task, SynRXN provides transparent splitting functions that generate leakage-aware train, validation, and test partitions, together with standardized evaluation workflows and metric suites tailored to classification, regression, and structured prediction settings. For sensitive benchmarking, we combine public training and validation data with held-out gold-standard test sets, and contamination-prone tasks such as reaction rebalancing and atom-to-atom mapping are distributed only as evaluation sets and are explicitly not intended for model training. Scripted build recipes enable bitwise-reproducible regeneration of all corpora across machines and over time, and the entire resource is released under permissive open licences to support reuse and extension. By removing dataset heterogeneity and packaging transparent, reusable evaluation scaffolding, SynRXN enables fair longitudinal comparison of CASP methods, supports rigorous ablations and stress tests along the full reaction-informatics pipeline, and lowers the barrier for practitioners who seek robust and comparable performance estimates for real-world synthesis planning workloads.

</details>


### [274] [Refinement Provenance Inference: Detecting LLM-Refined Training Prompts from Model Behavior](https://arxiv.org/abs/2601.01966)
*Bo Yin,Qi Li,Runpeng Yu,Xinchao Wang*

Main category: cs.LG

TL;DR: 提出Refinement Provenance Inference (RPI)任务，开发RePro框架通过教师强制似然特征和logit排序信号推断训练数据中提示是否经过LLM精炼


<details>
  <summary>Details</summary>
Motivation: 指令调优越来越多依赖LLM提示精炼，需要实例级审计来推断模型训练时使用的是原始提示还是精炼版本，这对数据集治理和争议解决很重要

Method: 提出RePro框架，利用提示精炼导致的教师强制token分布稳定偏移，融合教师强制似然特征和logit排序信号，通过影子微调学习可迁移表示，使用轻量级线性头进行推断

Result: RePro在不同精炼器上表现稳定且可迁移性好，表明其利用了精炼器无关的分布偏移而非改写风格伪影

Conclusion: 提示精炼会产生可检测的分布偏移，RePro框架能有效解决RPI任务，为训练数据来源推断提供实用方法

Abstract: Instruction tuning increasingly relies on LLM-based prompt refinement, where prompts in the training corpus are selectively rewritten by an external refiner to improve clarity and instruction alignment. This motivates an instance-level audit problem: for a fine-tuned model and a training prompt-response pair, can we infer whether the model was trained on the original prompt or its LLM-refined version within a mixed corpus? This matters for dataset governance and dispute resolution when training data are contested. However, it is non-trivial in practice: refined and raw instances are interleaved in the training corpus with unknown, source-dependent mixture ratios, making it harder to develop provenance methods that generalize across models and training setups. In this paper, we formalize this audit task as Refinement Provenance Inference (RPI) and show that prompt refinement yields stable, detectable shifts in teacher-forced token distributions, even when semantic differences are not obvious. Building on this phenomenon, we propose RePro, a logit-based provenance framework that fuses teacher-forced likelihood features with logit-ranking signals. During training, RePro learns a transferable representation via shadow fine-tuning, and uses a lightweight linear head to infer provenance on unseen victims without training-data access. Empirically, RePro consistently attains strong performance and transfers well across refiners, suggesting that it exploits refiner-agnostic distribution shifts rather than rewrite-style artifacts.

</details>


### [275] [SerpentFlow: Generative Unpaired Domain Alignment via Shared-Structure Decomposition](https://arxiv.org/abs/2601.01979)
*Julie Keisler,Anastase Alexandre Charantonis,Yannig Goude,Boutheina Oueslati,Claire Monteleoni*

Main category: cs.LG

TL;DR: SerpentFlow：一种用于无配对域对齐的生成框架，通过将数据分解为共享结构和域特定成分，生成合成训练对，使传统需要配对数据的条件生成模型能用于无配对场景。


<details>
  <summary>Details</summary>
Motivation: 在无配对观测数据的情况下进行域对齐具有挑战性，因为缺乏跨域的直接监督。现有方法通常需要配对数据，限制了在真实场景中的应用。

Method: 提出SerpentFlow框架，在潜在空间中将数据分解为共享成分（跨域共有）和域特定成分。通过用随机噪声替换域特定成分，构建共享表示与目标域样本之间的合成训练对，从而使用条件生成模型。在超分辨率任务中，共享成分对应低频内容，高频细节对应域特定变异性，使用分类器准则自动确定分割频率。

Result: 在合成图像、物理过程模拟和气候降尺度任务上的实验表明，该方法能有效重建与底层低频模式一致的高频结构，支持共享结构分解作为无配对域对齐的有效策略。

Conclusion: 共享结构分解是解决无配对域对齐问题的有效方法，SerpentFlow框架通过生成合成训练对，使条件生成模型能应用于无配对场景，在多个任务上表现出色。

Abstract: Domain alignment refers broadly to learning correspondences between data distributions from distinct domains. In this work, we focus on a setting where domains share underlying structural patterns despite differences in their specific realizations. The task is particularly challenging in the absence of paired observations, which removes direct supervision across domains. We introduce a generative framework, called SerpentFlow (SharEd-structuRe decomPosition for gEnerative domaiN adapTation), for unpaired domain alignment. SerpentFlow decomposes data within a latent space into a shared component common to both domains and a domain-specific one. By isolating the shared structure and replacing the domain-specific component with stochastic noise, we construct synthetic training pairs between shared representations and target-domain samples, thereby enabling the use of conditional generative models that are traditionally restricted to paired settings. We apply this approach to super-resolution tasks, where the shared component naturally corresponds to low-frequency content while high-frequency details capture domain-specific variability. The cutoff frequency separating low- and high-frequency components is determined automatically using a classifier-based criterion, ensuring a data-driven and domain-adaptive decomposition. By generating pseudo-pairs that preserve low-frequency structures while injecting stochastic high-frequency realizations, we learn the conditional distribution of the target domain given the shared representation. We implement SerpentFlow using Flow Matching as the generative pipeline, although the framework is compatible with other conditional generative approaches. Experiments on synthetic images, physical process simulations, and a climate downscaling task demonstrate that the method effectively reconstructs high-frequency structures consistent with underlying low-frequency patterns, supporting shared-structure decomposition as an effective strategy for unpaired domain alignment.

</details>


### [276] [Prior Diffusiveness and Regret in the Linear-Gaussian Bandit](https://arxiv.org/abs/2601.02022)
*Yifan Zhu,John C. Duchi,Benjamin Van Roy*

Main category: cs.LG

TL;DR: Thompson采样在线性高斯bandit中实现了$\tilde{O}(σd \sqrt{T} + d r \sqrt{\mathrm{Tr}(Σ_0)})$的贝叶斯遗憾，其中burn-in项与minimax遗憾项呈加法关系而非乘法关系。


<details>
  <summary>Details</summary>
Motivation: 现有Thompson采样在线性高斯bandit中的遗憾界中，先验依赖的"burn-in"项$d r \sqrt{\mathrm{Tr}(Σ_0)}$与minimax遗憾项$σd \sqrt{T}$呈乘法关系。本文旨在证明这两个项实际上可以解耦为加法关系，从而提供更紧的遗憾界。

Method: 通过新的"椭圆势能"引理来分析Thompson采样在线性高斯bandit中的性能。该引理帮助分离先验信息对遗憾的影响，并提供了理论证明框架。

Result: 证明了Thompson采样具有$\tilde{O}(σd \sqrt{T} + d r \sqrt{\mathrm{Tr}(Σ_0)})$的贝叶斯遗憾上界，其中$\tilde{O}$隐藏了对数因子。同时提供了下界证明，表明burn-in项是不可避免的。

Conclusion: 在线性高斯bandit中，Thompson采样的先验依赖burn-in项与长期minimax遗憾项可以解耦为加法关系，这改进了现有乘法依赖的遗憾界，并通过椭圆势能引理和匹配下界提供了完整的理论分析。

Abstract: We prove that Thompson sampling exhibits $\tilde{O}(σd \sqrt{T} + d r \sqrt{\mathrm{Tr}(Σ_0)})$ Bayesian regret in the linear-Gaussian bandit with a $\mathcal{N}(μ_0, Σ_0)$ prior distribution on the coefficients, where $d$ is the dimension, $T$ is the time horizon, $r$ is the maximum $\ell_2$ norm of the actions, and $σ^2$ is the noise variance. In contrast to existing regret bounds, this shows that to within logarithmic factors, the prior-dependent ``burn-in'' term $d r \sqrt{\mathrm{Tr}(Σ_0)}$ decouples additively from the minimax (long run) regret $σd \sqrt{T}$. Previous regret bounds exhibit a multiplicative dependence on these terms. We establish these results via a new ``elliptical potential'' lemma, and also provide a lower bound indicating that the burn-in term is unavoidable.

</details>


### [277] [GDRO: Group-level Reward Post-training Suitable for Diffusion Models](https://arxiv.org/abs/2601.02036)
*Yiyang Wang,Xi Chen,Xiaogang Xu,Yu Liu,Hengshuang Zhao*

Main category: cs.LG

TL;DR: 提出GDRO方法，针对文本到图像整流流扩散模型的奖励对齐问题，通过群体级离线优化解决效率低、依赖随机采样器和奖励黑客问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用在线强化学习进行奖励对齐，但面临效率低、依赖随机采样器和奖励黑客问题。整流流模型与LLMs有本质差异：1) 在线图像采样耗时巨大；2) 整流流是确定性的（一旦初始噪声固定）。

Method: 设计Group-level Direct Reward Optimization (GDRO)，结合整流流模型特性的群体级奖励对齐后训练范式。支持完全离线训练，无需图像采样；扩散采样器独立，无需ODE-to-SDE近似；引入修正评分考虑奖励黑客趋势。

Result: GDRO在OCR和GenEval任务中有效提升扩散模型的奖励分数，同时展示出强大的稳定性和鲁棒性，能够缓解奖励黑客问题。

Conclusion: GDRO为文本到图像整流流扩散模型提供了一种高效、稳定且鲁棒的群体级奖励对齐方法，解决了现有在线RL方法的局限性。

Abstract: Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking.

</details>


### [278] [Multivariate Time-series Anomaly Detection via Dynamic Model Pool & Ensembling](https://arxiv.org/abs/2601.02037)
*Wei Hu,Zewei Yu,Jianqiu Xu*

Main category: cs.LG

TL;DR: DMPEAD：一种用于多元时间序列异常检测的动态模型池集成框架，通过构建多样化模型池、动态更新和集成排名靠前的模型，解决了现有多模型方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有多元时间序列异常检测的多模型方法存在三个主要问题：1）选择方法依赖单一模型且对策略敏感；2）集成方法通常组合所有模型或仅限于单变量数据；3）大多数方法依赖固定数据维度，限制了可扩展性。

Method: 提出DMPEAD框架，包含三个步骤：1）通过参数传递和多样性度量构建多样化模型池；2）使用元模型和基于相似性的策略动态更新模型池，实现自适应扩展、子集选择和池合并；3）通过代理指标排名和top-k聚合在选定子集中集成排名靠前的模型，输出最终异常检测结果。

Result: 在8个真实世界数据集上的广泛实验表明，DMPEAD模型优于所有基线方法，展现出卓越的适应性和可扩展性。

Conclusion: DMPEAD框架有效解决了现有多模型方法在多元时间序列异常检测中的局限性，通过动态模型池构建和集成策略实现了更好的性能和可扩展性。

Abstract: Multivariate time-series (MTS) anomaly detection is critical in domains such as service monitor, IoT, and network security. While multi-model methods based on selection or ensembling outperform single-model ones, they still face limitations: (i) selection methods rely on a single chosen model and are sensitive to the strategy; (ii) ensembling methods often combine all models or are restricted to univariate data; and (iii) most methods depend on fixed data dimensionality, limiting scalability. To address these, we propose DMPEAD, a Dynamic Model Pool and Ensembling framework for MTS Anomaly Detection. The framework first (i) constructs a diverse model pool via parameter transfer and diversity metric, then (ii) updates it with a meta-model and similarity-based strategy for adaptive pool expansion, subset selection, and pool merging, finally (iii) ensembles top-ranked models through proxy metric ranking and top-k aggregation in the selected subset, outputting the final anomaly detection result. Extensive experiments on 8 real-world datasets show that our model outperforms all baselines, demonstrating superior adaptability and scalability.

</details>


### [279] [Explore the Ideology of Deep Learning in ENSO Forecasts](https://arxiv.org/abs/2601.02050)
*Yanhai Gan,Yipeng Chen,Ning Li,Xingguo Liu,Junyu Dong,Xianyao Chen*

Main category: cs.LG

TL;DR: 本文提出基于有界变差函数的可解释性框架，通过激活函数饱和区"复活"神经元，增强模型表达能力，揭示ENSO可预测性主要来自热带太平洋，印度洋和大西洋也有贡献，与物理理解一致。


<details>
  <summary>Details</summary>
Motivation: ENSO对全球气候变率有深远影响，但其预测仍是重大挑战。深度学习虽显著提高了预测技能，但模型的不透明性阻碍了科学信任和业务部署，需要建立数学基础的可解释性框架。

Method: 引入基于有界变差函数的数学基础可解释性框架，通过从激活函数饱和区"复活"神经元来增强模型表达能力，进行控制实验验证方法稳健性。

Result: 分析显示ENSO可预测性主要来自热带太平洋，印度洋和大西洋也有贡献，与物理理解一致。控制实验证实方法稳健且与已知预测因子一致。发现春季可预测性障碍期间敏感性扩大但预测性能下降，可能源于次优变量选择。

Conclusion: 结果表明，纳入额外的海洋-大气变量可能有助于超越春季可预测性障碍限制，推进ENSO长期预测。该方法为深度学习模型提供了数学基础的可解释性框架，增强了科学信任。

Abstract: The El Ni{~n}o-Southern Oscillation (ENSO) exerts profound influence on global climate variability, yet its prediction remains a grand challenge. Recent advances in deep learning have significantly improved forecasting skill, but the opacity of these models hampers scientific trust and operational deployment. Here, we introduce a mathematically grounded interpretability framework based on bounded variation function. By rescuing the "dead" neurons from the saturation zone of the activation function, we enhance the model's expressive capacity. Our analysis reveals that ENSO predictability emerges dominantly from the tropical Pacific, with contributions from the Indian and Atlantic Oceans, consistent with physical understanding. Controlled experiments affirm the robustness of our method and its alignment with established predictors. Notably, we probe the persistent Spring Predictability Barrier (SPB), finding that despite expanded sensitivity during spring, predictive performance declines-likely due to suboptimal variable selection. These results suggest that incorporating additional ocean-atmosphere variables may help transcend SPB limitations and advance long-range ENSO prediction.

</details>


### [280] [The Homogeneity Trap: Spectral Collapse in Doubly-Stochastic Deep Networks](https://arxiv.org/abs/2601.02080)
*Yizhi Liu*

Main category: cs.LG

TL;DR: 本文发现双随机矩阵约束在深度架构中存在"同质性陷阱"：最大熵偏置导致混合算子趋向均匀重心，抑制次主导奇异值，限制网络的有效感受野，且层归一化无法缓解噪声主导下的正交崩溃。


<details>
  <summary>Details</summary>
Motivation: 双随机矩阵在结构保持的深度架构中应用日益广泛，但作者发现这些约束存在关键的谱退化现象，称为"同质性陷阱"，需要深入分析其理论影响。

Method: 通过理论分析双随机矩阵的谱特性，推导了次主导奇异值σ_2与网络有效深度的谱界，并形式化证明了层归一化在噪声主导机制下的失效条件。

Result: 发现Sinkhorn投影的最大熵偏置使混合算子趋向均匀重心，抑制σ_2并过滤高频特征分量；证明了当信噪比低于临界阈值时，几何结构会不可逆地丢失到噪声诱导的正交崩溃中。

Conclusion: 双随机矩阵约束的网络存在熵稳定性与谱表达性之间的基本权衡，需要重新思考如何在保持数值稳定性和概率可解释性的同时维持足够的谱表达能力。

Abstract: Doubly-stochastic matrices (DSM) are increasingly utilized in structure-preserving deep architectures -- such as Optimal Transport layers and Sinkhorn-based attention -- to enforce numerical stability and probabilistic interpretability. In this work, we identify a critical spectral degradation phenomenon inherent to these constraints, termed the Homogeneity Trap. We demonstrate that the maximum-entropy bias, typical of Sinkhorn-based projections, drives the mixing operator towards the uniform barycenter, thereby suppressing the subdominant singular value σ_2 and filtering out high-frequency feature components. We derive a spectral bound linking σ_2 to the network's effective depth, showing that high-entropy constraints restrict feature transformation to a shallow effective receptive field. Furthermore, we formally demonstrate that Layer Normalization fails to mitigate this collapse in noise-dominated regimes; specifically, when spectral filtering degrades the Signal-to-Noise Ratio (SNR) below a critical threshold, geometric structure is irreversibly lost to noise-induced orthogonal collapse. Our findings highlight a fundamental trade-off between entropic stability and spectral expressivity in DSM-constrained networks.

</details>


### [281] [A Differentiable Adversarial Framework for Task-Aware Data Subsampling](https://arxiv.org/abs/2601.02081)
*Jiacheng Lyu,Bihua Bao*

Main category: cs.LG

TL;DR: 提出对抗性软选择下采样(ASSS)框架，将数据缩减重构为可微分的端到端学习问题，通过选择器网络与任务网络的对抗博弈学习样本重要性权重，实现任务感知的智能数据降采样。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集对模型训练带来计算挑战，传统数据下采样方法作为静态、任务无关的预处理步骤通常会丢弃对下游预测至关重要的信息，需要一种能够保留任务相关信息的智能数据缩减方法。

Method: 提出对抗性软选择下采样(ASSS)框架，包含选择器网络和任务网络。选择器网络学习为样本分配连续重要性权重，通过Gumbel-Softmax松弛实现直接优化，在平衡预测保真度和稀疏性的损失函数指导下，识别并保留对特定任务目标信息量最大的样本。

Result: 在四个大规模真实世界数据集上的综合实验表明，ASSS始终优于聚类和最近邻稀疏化等启发式下采样基线方法，在保持模型性能方面表现优异。值得注意的是，ASSS不仅能匹配，有时甚至能超过使用整个数据集的训练性能，展示了智能去噪的效果。

Conclusion: 该工作将任务感知数据下采样确立为可学习组件，为有效的大规模数据学习提供了原则性解决方案，建立了数据缩减与信息瓶颈原理的理论联系。

Abstract: The proliferation of large-scale datasets poses a major computational challenge to model training. The traditional data subsampling method works as a static, task independent preprocessing step which usually discards information that is critical to downstream prediction. In this paper, we introduces the antagonistic soft selection subsampling (ASSS) framework as is a novel paradigm that reconstructs data reduction into a differentiable end-to-end learning problem. ASSS uses the adversarial game between selector network and task network, and selector network learning assigns continuous importance weights to samples. This direct optimization implemented by Gumbel-Softmax relaxation allows the selector to identify and retain samples with the maximum amount of information for a specific task target under the guidance of the loss function that balances the fidelity and sparsity of the prediction. Theoretical analysis links this framework with the information bottleneck principle. Comprehensive experiments on four large-scale real world datasets show that ASSS has always been better than heuristic subsampling baselines such as clustering and nearest neighbor thinning in maintaining model performance. It is worth noting that ASSS can not only match, but also sometimes exceed the training performance of the entire dataset, showcasing the effect of intelligent denoising. This work establishes task aware data subsampling as a learnable component, providing a principled solution for effective large-scale data learning.

</details>


### [282] [Horizon Activation Mapping for Neural Networks in Time Series Forecasting](https://arxiv.org/abs/2601.02094)
*Hans Krupakar,V A Kandappan*

Main category: cs.LG

TL;DR: 提出HAM（Horizon Activation Mapping）方法，用于跨不同神经网络家族的时序预测模型的可视化解释，通过梯度范数平均分析子序列重要性。


<details>
  <summary>Details</summary>
Motivation: 现有时序预测模型依赖误差指标和特定架构的解释方法，无法跨不同模型家族进行比较。需要一种模型无关的可视化解释技术来理解不同神经网络模型在时序预测中的行为。

Method: HAM受grad-CAM启发，使用梯度范数平均来研究时间序列的子序列重要性。引入因果和反因果模式计算每个时间步的梯度更新范数平均，并分析比例线表示范数平均的均匀分布。研究了批量大小、早停、数据分割、单变量预测和dropout等优化景观对HAM的影响。

Result: 批量大小差异显示存在指数近似关系。在ETTm2数据集上测试了多种模型（CycleNet、N-Linear、N-HITS、FEDformer、Pyraformer、SpaceTime、Multi-Resolution DDPM），发现NHITS的神经近似定理和SpaceTime的指数自回归活动与HAM图中的趋势相关。

Conclusion: HAM可用于细粒度模型选择、验证集选择以及跨不同神经网络模型家族的比较，为时序预测模型提供统一的解释框架。

Abstract: Neural networks for time series forecasting have relied on error metrics and architecture-specific interpretability approaches for model selection that don't apply across models of different families. To interpret forecasting models agnostic to the types of layers across state-of-the-art model families, we introduce Horizon Activation Mapping (HAM), a visual interpretability technique inspired by grad-CAM that uses gradient norm averages to study the horizon's subseries where grad-CAM studies attention maps over image data. We introduce causal and anti-causal modes to calculate gradient update norm averages across subseries at every timestep and lines of proportionality signifying uniform distributions of the norm averages. Optimization landscape studies with respect to changes in batch sizes, early stopping, train-val-test splits, univariate forecasting and dropouts are studied with respect to performances and subseries in HAM. Interestingly, batch size based differences in activities seem to indicate potential for existence of an exponential approximation across them per epoch relative to each other. Multivariate forecasting models including MLP-based CycleNet, N-Linear, N-HITS, self attention-based FEDformer, Pyraformer, SSM-based SpaceTime and diffusion-based Multi-Resolution DDPM over different horizon sizes trained over the ETTm2 dataset are used for HAM plots in this study. NHITS' neural approximation theorem and SpaceTime's exponential autoregressive activities have been attributed to trends in HAM plots over their training, validation and test sets. In general, HAM can be used for granular model selection, validation set choices and comparisons across different neural network model families.

</details>


### [283] [LION-DG: Layer-Informed Initialization with Deep Gradient Protocols for Accelerated Neural Network Training](https://arxiv.org/abs/2601.02105)
*Hyunjun Kim*

Main category: cs.LG

TL;DR: 提出LION-DG初始化方法，针对深度监督架构的辅助分类器进行层感知初始化，通过零初始化辅助头实现梯度唤醒，无需超参数即可加速收敛。


<details>
  <summary>Details</summary>
Motivation: 现有权重初始化方法大多是层无关的，而深度监督架构中的未训练辅助分类器头会通过梯度干扰破坏早期训练的稳定性，需要专门的初始化策略来解决这个问题。

Method: 提出LION-DG初始化方法：对主干网络应用标准的He初始化，而对辅助分类器头进行零初始化。这实现了"梯度唤醒"机制——辅助梯度在初始化时为零，随着权重增长自然引入，提供隐式的预热效果。

Result: 在CIFAR-10和CIFAR-100上的实验表明：DenseNet-DS在CIFAR-10上收敛速度提升8.3%；LSUV与LION-DG结合在CIFAR-10上达到81.92%的最佳准确率；ResNet-DS在CIFAR-100上获得11.3%的加速。

Conclusion: LION-DG方法简单、无需超参数、无计算开销，为深度监督架构提供了有效的初始化解决方案，并给出了针对不同架构的实践指导。

Abstract: Weight initialization remains decisive for neural network optimization, yet existing methods are largely layer-agnostic. We study initialization for deeply-supervised architectures with auxiliary classifiers, where untrained auxiliary heads can destabilize early training through gradient interference.
  We propose LION-DG, a layer-informed initialization that zero-initializes auxiliary classifier heads while applying standard He-initialization to the backbone. We prove that this implements Gradient Awakening: auxiliary gradients are exactly zero at initialization, then phase in naturally as weights grow -- providing an implicit warmup without hyperparameters.
  Experiments on CIFAR-10 and CIFAR-100 with DenseNet-DS and ResNet-DS architectures demonstrate: (1) DenseNet-DS: +8.3% faster convergence on CIFAR-10 with comparable accuracy, (2) Hybrid approach: Combining LSUV with LION-DG achieves best accuracy (81.92% on CIFAR-10), (3) ResNet-DS: Positive speedup on CIFAR-100 (+11.3%) with side-tap auxiliary design.
  We identify architecture-specific trade-offs and provide clear guidelines for practitioners. LION-DG is simple, requires zero hyperparameters, and adds no computational overhead.

</details>


### [284] [Prototype-Based Learning for Healthcare: A Demonstration of Interpretable AI](https://arxiv.org/abs/2601.02106)
*Ashish Rana,Ammar Shaker,Sascha Saralajew,Takashi Suzuki,Kosuke Yasuda,Shintaro Kato,Toshikazu Wada,Toshiyuki Fujikawa,Toru Kikutsuji*

Main category: cs.LG

TL;DR: ProtoPal框架结合原型学习和可解释AI，为个性化预防医疗提供可理解、可验证的预测和干预方案


<details>
  <summary>Details</summary>
Motivation: 当前机器学习在医疗领域存在可解释性不足的问题，个性化预防医疗需要让所有利益相关者都能理解和验证预测、干预和推荐结果

Method: 采用原型学习框架ProtoPal，包含前端和后端模式，通过原型表示提供直观的干预展示和模拟结果

Result: 在保持优异定量性能的同时，能够直观呈现干预措施及其模拟结果

Conclusion: 原型学习能够有效弥合医疗AI的可解释性差距，为个性化预防医疗提供既准确又易于理解的解决方案

Abstract: Despite recent advances in machine learning and explainable AI, a gap remains in personalized preventive healthcare: predictions, interventions, and recommendations should be both understandable and verifiable for all stakeholders in the healthcare sector. We present a demonstration of how prototype-based learning can address these needs. Our proposed framework, ProtoPal, features both front- and back-end modes; it achieves superior quantitative performance while also providing an intuitive presentation of interventions and their simulated outcomes.

</details>


### [285] [Edge-aware GAT-based protein binding site prediction](https://arxiv.org/abs/2601.02138)
*Weisen Yang,Hanqing Zhang,Wangren Qiu,Xuan Xiao,Weizhong Lin*

Main category: cs.LG

TL;DR: 提出Edge-aware GAT模型，用于精细预测蛋白质结合位点，通过原子级图结构和多维特征整合，在基准数据集上达到0.93 ROC-AUC，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确识别蛋白质结合位点对理解生物分子相互作用机制和药物靶点设计至关重要。传统方法在平衡预测准确性和计算效率方面存在困难，特别是在捕捉复杂空间构象时。

Method: 提出Edge-aware Graph Attention Network模型，构建原子级图并整合几何描述符、DSSP二级结构和相对溶剂可及性等多维结构特征。通过将原子间距离和方向向量作为注意力机制中的边特征，增强模型表示能力。使用方向张量传播和残基级注意力池化进一步改进结合位点定位和局部结构细节捕捉。

Result: 在基准数据集上，模型在蛋白质-蛋白质结合位点预测中达到0.93 ROC-AUC，优于多个最先进方法。PyMOL可视化证实了模型的实用性和可解释性。已部署公开可访问的Web服务器。

Conclusion: 该方法为识别蛋白质功能位点提供了一种新颖高效的解决方案，平衡了预测准确性、泛化能力和可解释性。

Abstract: Accurate identification of protein binding sites is crucial for understanding biomolecular interaction mechanisms and for the rational design of drug targets. Traditional predictive methods often struggle to balance prediction accuracy with computational efficiency when capturing complex spatial conformations. To address this challenge, we propose an Edge-aware Graph Attention Network (Edge-aware GAT) model for the fine-grained prediction of binding sites across various biomolecules, including proteins, DNA/RNA, ions, ligands, and lipids. Our method constructs atom-level graphs and integrates multidimensional structural features, including geometric descriptors, DSSP-derived secondary structure, and relative solvent accessibility (RSA), to generate spatially aware embedding vectors. By incorporating interatomic distances and directional vectors as edge features within the attention mechanism, the model significantly enhances its representation capacity. On benchmark datasets, our model achieves an ROC-AUC of 0.93 for protein-protein binding site prediction, outperforming several state-of-the-art methods. The use of directional tensor propagation and residue-level attention pooling further improves both binding site localization and the capture of local structural details. Visualizations using PyMOL confirm the model's practical utility and interpretability. To facilitate community access and application, we have deployed a publicly accessible web server at http://119.45.201.89:5000/. In summary, our approach offers a novel and efficient solution that balances prediction accuracy, generalization, and interpretability for identifying functional sites in proteins.

</details>


### [286] [ACDZero: Graph-Embedding-Based Tree Search for Mastering Automated Cyber Defense](https://arxiv.org/abs/2601.02196)
*Yu Li,Sizhe Tang,Rongqian Chen,Fei Xu Yu,Guangyu Jiang,Mahdi Imani,Nathaniel D. Bastian,Tian Lan*

Main category: cs.LG

TL;DR: 提出基于蒙特卡洛树搜索和图神经网络的自动化网络防御方法，在复杂网络环境中实现样本高效的防御策略


<details>
  <summary>Details</summary>
Motivation: 现有基于深度强化学习的自动化网络防御方法在复杂网络的大决策/状态空间中面临探索困难，需要大量样本，需要学习样本高效的防御策略

Method: 将CAGE-4挑战中的自动化网络防御建模为基于上下文的部分可观测马尔可夫决策问题，提出基于蒙特卡洛树搜索的规划中心防御策略，使用图神经网络嵌入网络观测作为属性图，结合学习到的图嵌入和图编辑动作先验来指导搜索

Result: 在CC4场景中评估，涉及不同网络结构和对手行为，显示搜索引导的、基于图嵌入的规划相比最先进的强化学习基线提高了防御奖励和鲁棒性

Conclusion: 提出的结合模型无关泛化、策略蒸馏和前向规划的MCTS方法在复杂搜索空间中实现了实用的自动化网络防御解决方案

Abstract: Automated cyber defense (ACD) seeks to protect computer networks with minimal or no human intervention, reacting to intrusions by taking corrective actions such as isolating hosts, resetting services, deploying decoys, or updating access controls. However, existing approaches for ACD, such as deep reinforcement learning (RL), often face difficult exploration in complex networks with large decision/state spaces and thus require an expensive amount of samples. Inspired by the need to learn sample-efficient defense policies, we frame ACD in CAGE Challenge 4 (CAGE-4 / CC4) as a context-based partially observable Markov decision problem and propose a planning-centric defense policy based on Monte Carlo Tree Search (MCTS). It explicitly models the exploration-exploitation tradeoff in ACD and uses statistical sampling to guide exploration and decision making. We make novel use of graph neural networks (GNNs) to embed observations from the network as attributed graphs, to enable permutation-invariant reasoning over hosts and their relationships. To make our solution practical in complex search spaces, we guide MCTS with learned graph embeddings and priors over graph-edit actions, combining model-free generalization and policy distillation with look-ahead planning. We evaluate the resulting agent on CC4 scenarios involving diverse network structures and adversary behaviors, and show that our search-guided, graph-embedding-based planning improves defense reward and robustness relative to state-of-the-art RL baselines.

</details>


### [287] [CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents](https://arxiv.org/abs/2601.02201)
*Keyu Wang,Bingchen Miao,Wendong Bu,Yu Wu,Juncheng Li,Shengyu Zhang,Wenqiao Zhang,Siliang Tang,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: CORE是一个基于代码的逆自训练框架，通过图扩展桥接模仿与探索，自动从专家演示中推断奖励函数，提升虚拟代理的行为多样性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态虚拟代理训练面临两大挑战：行为克隆方法简单有效但行为多样性低；强化学习方法能发现新策略但严重依赖手动设计的奖励函数。需要解决这两种方法之间的冲突。

Method: 1. 语义代码抽象：自动从专家演示中推断奖励函数（标签函数），无需手动设计；2. 策略图扩展：构建多路径策略图，增强域内行为多样性；3. 轨迹引导外推：利用成功和失败轨迹扩展任务空间，丰富域外行为多样性。

Result: 在Web和Android平台上的实验表明，CORE显著提高了整体性能和泛化能力，展示了其作为构建强大虚拟代理的鲁棒且可泛化训练范式的潜力。

Conclusion: CORE通过桥接模仿与探索，解决了行为克隆和强化学习之间的冲突，提供了一种既能提升行为多样性又无需手动设计奖励函数的训练框架，为构建强大的多模态虚拟代理提供了新范式。

Abstract: The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.

</details>


### [288] [Quantized SO(3)-Equivariant Graph Neural Networks for Efficient Molecular Property Prediction](https://arxiv.org/abs/2601.02213)
*Haoyu Zhou,Ping Xue,Tianfan Fu,Hao Zhang*

Main category: cs.LG

TL;DR: 提出三种创新技术压缩SO(3)-等变图神经网络，通过量化实现2.37-2.73倍推理加速和4倍模型压缩，在分子基准测试中保持精度和等变性。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署3D等变图神经网络面临高计算成本挑战，需要压缩和加速SO(3)-等变GNN以适用于实际化学应用。

Method: 提出三种量化等变变换器创新：1) 幅值-方向解耦量化方案，分别量化等变特征的范数和方向；2) 分支分离量化感知训练策略，在注意力机制中区别处理不变和等变特征通道；3) 鲁棒性增强的注意力归一化机制，稳定低精度注意力计算。

Result: 在QM9和rMD17分子基准测试中，8位模型在能量和力预测方面达到与全精度基线相当的精度，推理速度提升2.37-2.73倍，模型大小减小4倍，同时保持等变性。

Conclusion: 提出的量化技术使对称感知GNN能够部署在实际化学应用中，在不牺牲精度或物理对称性的前提下显著提升效率。

Abstract: Deploying 3D graph neural networks (GNNs) that are equivariant to 3D rotations (the group SO(3)) on edge devices is challenging due to their high computational cost. This paper addresses the problem by compressing and accelerating an SO(3)-equivariant GNN using low-bit quantization techniques. Specifically, we introduce three innovations for quantized equivariant transformers: (1) a magnitude-direction decoupled quantization scheme that separately quantizes the norm and orientation of equivariant (vector) features, (2) a branch-separated quantization-aware training strategy that treats invariant and equivariant feature channels differently in an attention-based $SO(3)$-GNN, and (3) a robustness-enhancing attention normalization mechanism that stabilizes low-precision attention computations. Experiments on the QM9 and rMD17 molecular benchmarks demonstrate that our 8-bit models achieve accuracy on energy and force predictions comparable to full-precision baselines with markedly improved efficiency. We also conduct ablation studies to quantify the contribution of each component to maintain accuracy and equivariance under quantization, using the Local error of equivariance (LEE) metric. The proposed techniques enable the deployment of symmetry-aware GNNs in practical chemistry applications with 2.37--2.73x faster inference and 4x smaller model size, without sacrificing accuracy or physical symmetry.

</details>


### [289] [ELLA: Efficient Lifelong Learning for Adapters in Large Language Models](https://arxiv.org/abs/2601.02232)
*Shristi Das Biswas,Yue Zhang,Anwesan Pal,Radhika Bhargava,Kaushik Roy*

Main category: cs.LG

TL;DR: ELLA：一种基于选择性子空间去相关原则的持续学习框架，通过惩罚任务特定方向的对齐来减少灾难性遗忘，同时保留低能量子空间的自由度以实现正向迁移。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在持续学习设置中面临严重的灾难性遗忘问题。现有方法存在根本性限制：基于重放的方法不切实际且侵犯隐私，而严格正交性方法在规模扩展时会失效，因为每个新任务都被投影到正交补空间，逐渐减少剩余自由度并禁止共享表示中的重叠，从而消除正向迁移。

Method: ELLA基于选择性子空间去相关原则，明确表征过去更新的结构，惩罚沿其高能量、任务特定方向的对齐，同时保留低能量残差子空间的自由度以实现迁移。这通过一个轻量级正则化器在单个聚合更新矩阵上实现，对应一个各向异性收缩算子来限制干扰。

Result: 在三个流行基准测试中实现最先进的持续学习性能，相对准确率提升高达9.6%，内存占用减少35倍。无需数据重放、无需架构扩展、存储需求极小，且能稳健扩展到不同架构，并主动增强模型在未见任务上的零样本泛化性能。

Conclusion: ELLA为构建性终身LLM适应提供了一个原则性且可扩展的解决方案，通过选择性子空间去相关有效平衡了灾难性遗忘和正向迁移之间的权衡。

Abstract: Large Language Models (LLMs) suffer severe catastrophic forgetting when adapted sequentially to new tasks in a continual learning (CL) setting. Existing approaches are fundamentally limited: replay-based methods are impractical and privacy-violating, while strict orthogonality-based methods collapse under scale: each new task is projected onto an orthogonal complement, progressively reducing the residual degrees of freedom and eliminating forward transfer by forbidding overlap in shared representations. In this work, we introduce ELLA, a training framework built on the principle of selective subspace de-correlation. Rather than forbidding all overlap, ELLA explicitly characterizes the structure of past updates and penalizes alignments along their high-energy, task-specific directions, while preserving freedom in the low-energy residual subspaces to enable transfer. Formally, this is realized via a lightweight regularizer on a single aggregated update matrix. We prove this mechanism corresponds to an anisotropic shrinkage operator that bounds interference, yielding a penalty that is both memory- and compute-constant regardless of task sequence length. ELLA requires no data replay, no architectural expansion, and negligible storage. Empirically, it achieves state-of-the-art CL performance on three popular benchmarks, with relative accuracy gains of up to $9.6\%$ and a $35\times$ smaller memory footprint. Further, ELLA scales robustly across architectures and actively enhances the model's zero-shot generalization performance on unseen tasks, establishing a principled and scalable solution for constructive lifelong LLM adaptation.

</details>


### [290] [Neuro-Channel Networks: A Multiplication-Free Architecture by Biological Signal Transmission](https://arxiv.org/abs/2601.02253)
*Emrah Mete,Emin Erkan Korkmaz*

Main category: cs.LG

TL;DR: 提出Neuro-Channel Networks (NCN)乘法自由架构，用通道宽度和神经递质参数替代传统权重，仅使用加法、减法和位运算实现前向传播，降低对高性能GPU硬件的依赖。


<details>
  <summary>Details</summary>
Motivation: 深度学习严重依赖昂贵、高能耗且供应稀缺的GPU硬件，限制了AI在边缘设备的普及。传统人工感知器依赖密集矩阵乘法，而生物神经系统通过物理离子通道限制和化学神经递质调节信号传输，无需算术乘法即可实现高效计算。

Method: 提出Neuro-Channel Networks (NCN)架构：1) 用通道宽度物理限制信号幅度；2) 引入神经递质参数基于符号逻辑调节信号传输；3) 前向传播仅使用加法、减法和位运算（最小值、符号），完全消除浮点乘法；4) 使用标准反向传播训练。

Result: 在概念验证研究中，NCN能够以100%准确率解决非线性可分问题（如XOR和多数函数），证明其无需乘法权重即可形成复杂决策边界的能力。

Conclusion: NCN架构为下一代神经形态硬件提供了高效替代方案，使复杂模型能够在商用CPU或超低功耗芯片上运行，无需依赖昂贵的GPU集群，有望推动AI在边缘设备的广泛部署。

Abstract: The rapid proliferation of Deep Learning is increasingly constrained by its heavy reliance on high-performance hardware, particularly Graphics Processing Units (GPUs). These specialized accelerators are not only prohibitively expensive and energy-intensive but also suffer from significant supply scarcity, limiting the ubiquity of Artificial Intelligence (AI) deployment on edge devices. The core of this inefficiency stems from the standard artificial perceptron's dependence on intensive matrix multiplications. However, biological nervous systems achieve unparalleled efficiency without such arithmetic intensity; synaptic signal transmission is regulated by physical ion channel limits and chemical neurotransmitter levels rather than a process that can be analogous to arithmetic multiplication. Inspired by this biological mechanism, we propose Neuro-Channel Networks (NCN), a novel multiplication-free architecture designed to decouple AI from expensive hardware dependencies. In our model, weights are replaced with Channel Widths that physically limit the signal magnitude, while a secondary parameter acts as a Neurotransmitter to regulate Signal Transmission based on sign logic. The forward pass relies exclusively on addition, subtraction, and bitwise operations (minimum, sign), eliminating floating-point multiplication entirely. In this proof-of-concept study, we demonstrate that NCNs can solve non-linearly separable problems like XOR and the Majority function with 100% accuracy using standard backpropagation, proving their capability to form complex decision boundaries without multiplicative weights. This architecture offers a highly efficient alternative for next-generation neuromorphic hardware, paving the way for running complex models on commodity CPUs or ultra-low-power chips without relying on costly GPU clusters.

</details>


### [291] [POSEIDON: Physics-Optimized Seismic Energy Inference and Detection Operating Network](https://arxiv.org/abs/2601.02264)
*Boris Kriuk,Fedor Kriuk*

Main category: cs.LG

TL;DR: POSEIDON是一个物理信息能量模型，用于统一多任务地震事件预测，结合了Gutenberg-Richter定律和Omori-Utsu余震衰减定律作为可学习约束，在三个任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法通常作为黑箱运行，忽略了已建立的物理定律。地震预测和地震危险性评估仍然是地球物理学中的基本挑战，需要将物理原理整合到预测模型中。

Method: 提出了POSEIDON（物理优化地震能量推断和检测操作网络），这是一个物理信息能量模型，将Gutenberg-Richter震级-频率关系和Omori-Utsu余震衰减定律作为可学习约束嵌入到能量建模框架中。同时处理三个相互关联的预测任务：余震序列识别、海啸生成潜力和前震检测。

Result: POSEIDON在所有任务上实现了最先进的性能，优于梯度提升、随机森林和CNN基线，在所有比较方法中获得了最高的平均F1分数。学习到的物理参数收敛到科学可解释的值：Gutenberg-Richter b值为0.752，Omori-Utsu参数p=0.835，c=0.1948天，这些值落在已建立的地震学范围内，同时增强了预测准确性。

Conclusion: POSEIDON成功地将物理定律整合到机器学习模型中，实现了更好的预测性能和科学可解释性。同时发布的Poseidon数据集（包含280万个事件，跨越30年）将促进物理信息地震研究的发展。

Abstract: Earthquake prediction and seismic hazard assessment remain fundamental challenges in geophysics, with existing machine learning approaches often operating as black boxes that ignore established physical laws. We introduce POSEIDON (Physics-Optimized Seismic Energy Inference and Detection Operating Network), a physics-informed energy-based model for unified multi-task seismic event prediction, alongside the Poseidon dataset -- the largest open-source global earthquake catalog comprising 2.8 million events spanning 30 years. POSEIDON embeds fundamental seismological principles, including the Gutenberg-Richter magnitude-frequency relationship and Omori-Utsu aftershock decay law, as learnable constraints within an energy-based modeling framework. The architecture simultaneously addresses three interconnected prediction tasks: aftershock sequence identification, tsunami generation potential, and foreshock detection. Extensive experiments demonstrate that POSEIDON achieves state-of-the-art performance across all tasks, outperforming gradient boosting, random forest, and CNN baselines with the highest average F1 score among all compared methods. Crucially, the learned physics parameters converge to scientifically interpretable values -- Gutenberg-Richter b-value of 0.752 and Omori-Utsu parameters p=0.835, c=0.1948 days -- falling within established seismological ranges while enhancing rather than compromising predictive accuracy. The Poseidon dataset is publicly available at https://huggingface.co/datasets/BorisKriuk/Poseidon, providing pre-computed energy features, spatial grid indices, and standardized quality metrics to advance physics-informed seismic research.

</details>


### [292] [Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck](https://arxiv.org/abs/2601.02307)
*Dina El Zein,James Henderson*

Main category: cs.LG

TL;DR: 提出NVDP方法，通过向transformer嵌入添加噪声实现隐私保护文本数据共享，在GLUE基准上展示隐私与准确性的权衡


<details>
  <summary>Details</summary>
Motivation: transformer嵌入包含多个向量（每个token一个），容易编码敏感信息，使攻击者能够以相当高的准确率恢复输入数据，需要隐私保护的数据共享方法

Method: 提出非参数变分差分隐私(NVDP)，将非参数变分信息瓶颈(NVIB)层集成到transformer架构中，向多向量嵌入注入噪声以隐藏信息，使用Rényi散度及其对应的贝叶斯差分隐私(BDP)保证来测量隐私保护

Result: 在GLUE基准测试中，通过调整噪声水平实现了隐私与准确性的有用权衡；在较低噪声水平下，模型保持高准确性同时提供强隐私保证，有效平衡隐私与效用

Conclusion: NVDP方法通过向transformer嵌入添加噪声，实现了隐私保护文本数据共享，在保持实用性的同时提供强隐私保护，为隐私与效用平衡提供了有效解决方案

Abstract: We propose a privacy-preserving method for sharing text data by sharing noisy versions of their transformer embeddings. It has been shown that hidden representations learned by deep models can encode sensitive information from the input, making it possible for adversaries to recover the input data with considerable accuracy. This problem is exacerbated in transformer embeddings because they consist of multiple vectors, one per token. To mitigate this risk, we propose Nonparametric Variational Differential Privacy (NVDP), which ensures both useful data sharing and strong privacy protection. We take a differential privacy approach, integrating a Nonparametric Variational Information Bottleneck (NVIB) layer into the transformer architecture to inject noise into its multi-vector embeddings and thereby hide information, and measuring privacy protection with Rényi divergence and its corresponding Bayesian Differential Privacy (BDP) guarantee. Training the NVIB layer calibrates the noise level according to utility. We test NVDP on the GLUE benchmark and show that varying the noise level gives us a useful tradeoff between privacy and accuracy. With lower noise levels, our model maintains high accuracy while offering strong privacy guarantees, effectively balancing privacy and utility.

</details>


### [293] [Temporal Kolmogorov-Arnold Networks (T-KAN) for High-Frequency Limit Order Book Forecasting: Efficiency, Interpretability, and Alpha Decay](https://arxiv.org/abs/2601.02310)
*Ahmad Makinde*

Main category: cs.LG

TL;DR: T-KAN模型通过可学习的B样条激活函数替代传统LSTM的固定线性权重，在高频交易环境中显著提升了预测性能，相比DeepLOB在k=100时F1分数相对提升19.1%，在1.0bps交易成本下获得132.48%回报。


<details>
  <summary>Details</summary>
Motivation: 高频交易环境中的限价订单簿数据噪声大、非线性强，传统模型如DeepLOB随着时间跨度增加预测能力衰减严重（alpha衰减问题），需要更有效的模型来捕捉市场信号特征。

Method: 提出Temporal Kolmogorov-Arnold Networks (T-KAN)，用可学习的B样条激活函数替代标准LSTM中的固定线性权重，使模型能够学习市场信号的"形状"而不仅仅是幅度，并针对FPGA实现进行低延迟优化。

Result: 在FI-2010数据集上，T-KAN在k=100时间跨度上F1分数相对提升19.1%；在1.0bps交易成本下获得132.48%回报，而DeepLOB亏损82.76%；模型具有良好可解释性，能清晰显示样条中的"死区"。

Conclusion: T-KAN通过可学习的B样条激活函数有效解决了高频交易中的alpha衰减问题，显著提升了预测性能和交易回报，同时具有良好的可解释性和硬件优化潜力。

Abstract: High-Frequency trading (HFT) environments are characterised by large volumes of limit order book (LOB) data, which is notoriously noisy and non-linear. Alpha decay represents a significant challenge, with traditional models such as DeepLOB losing predictive power as the time horizon (k) increases. In this paper, using data from the FI-2010 dataset, we introduce Temporal Kolmogorov-Arnold Networks (T-KAN) to replace the fixed, linear weights of standard LSTMs with learnable B-spline activation functions. This allows the model to learn the 'shape' of market signals as opposed to just their magnitude. This resulted in a 19.1% relative improvement in the F1-score at the k = 100 horizon. The efficacy of T-KAN networks cannot be understated, producing a 132.48% return compared to the -82.76% DeepLOB drawdown under 1.0 bps transaction costs. In addition to this, the T-KAN model proves quite interpretable, with the 'dead-zones' being clearly visible in the splines. The T-KAN architecture is also uniquely optimized for low-latency FPGA implementation via High level Synthesis (HLS). The code for the experiments in this project can be found at https://github.com/AhmadMak/Temporal-Kolmogorov-Arnold-Networks-T-KAN-for-High-Frequency-Limit-Order-Book-Forecasting.

</details>


### [294] [Game of Coding: Coding Theory in the Presence of Rational Adversaries, Motivated by Decentralized Machine Learning](https://arxiv.org/abs/2601.02313)
*Hanzaleh Akbari Nodehi,Viveck R. Cadambe,Mohammad Ali Maddah-Ali*

Main category: cs.LG

TL;DR: 论文提出了一种基于博弈论的编码框架，用于处理去中心化系统中理性对手（而非纯粹恶意对手）的编码问题，即使在诚实节点不占多数的情况下也能实现非零概率的数据恢复。


<details>
  <summary>Details</summary>
Motivation: 在去中心化机器学习等新兴应用中，参与节点因贡献获得奖励，这催生了理性对手而非纯粹恶意对手。传统编码理论假设最坏情况对抗模型，要求诚实节点数量超过恶意节点，但在去中心化系统中诚实节点可能不占多数，因此需要新的编码框架。

Method: 提出"编码博弈"这一新颖的博弈论框架，将编码理论扩展到信任最小化的设置中。重点关注重复编码，展示了该框架的两个关键特性：1) 即使对手节点占多数也能实现非零概率的数据恢复；2) 具有Sybil抵抗性，即均衡状态不随对手节点数量增加而改变。

Result: 该框架能够在诚实节点不占多数的情况下实现数据恢复，并且具有Sybil抵抗性。当对手策略未知时，该框架仍能提供解决方案，为去中心化系统中的编码问题提供了新的理论工具。

Conclusion: 论文提出了一个将编码理论与博弈论相结合的新框架，用于处理去中心化系统中的理性对手问题。该框架突破了传统编码理论对诚实节点占多数的要求，为去中心化应用中的可靠通信、存储和计算提供了新的解决方案，并指出了未来研究的开放性问题。

Abstract: Coding theory plays a crucial role in enabling reliable communication, storage, and computation. Classical approaches assume a worst-case adversarial model and ensure error correction and data recovery only when the number of honest nodes exceeds the number of adversarial ones by some margin. However, in some emerging decentralized applications, particularly in decentralized machine learning (DeML), participating nodes are rewarded for accepted contributions. This incentive structure naturally gives rise to rational adversaries who act strategically rather than behaving in purely malicious ways.
  In this paper, we first motivate the need for coding in the presence of rational adversaries, particularly in the context of outsourced computation in decentralized systems. We contrast this need with existing approaches and highlight their limitations. We then introduce the game of coding, a novel game-theoretic framework that extends coding theory to trust-minimized settings where honest nodes are not in the majority. Focusing on repetition coding, we highlight two key features of this framework: (1) the ability to achieve a non-zero probability of data recovery even when adversarial nodes are in the majority, and (2) Sybil resistance, i.e., the equilibrium remains unchanged even as the number of adversarial nodes increases. Finally, we explore scenarios in which the adversary's strategy is unknown and outline several open problems for future research.

</details>


### [295] [DatBench: Discriminative, Faithful, and Efficient VLM Evaluations](https://arxiv.org/abs/2601.02316)
*Siddharth Joshi,Haoli Yin,Rishabh Adiga,Ricardo Monti,Aldo Carranza,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Darren Teh,David Schwab,Fan Pan,Haakon Mongstad,Jack Urbanek,Jason Lee,Jason Telanoff,Josh Wills,Kaleigh Mentzer,Luke Merrick,Parth Doshi,Paul Burstein,Pratyush Maini,Scott Loftin,Spandan Das,Tony Jiang,Vineeth Dorna,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: 论文提出评估视觉语言模型的三个标准（忠实性、区分性、效率），发现现有评估存在多个缺陷，并通过数据清洗和任务转换创建了更可靠的评估套件DatBench


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的评估方法尚不成熟，存在多种缺陷：多项选择题鼓励猜测、与下游应用脱节、存在大量无需图像即可回答的问题、数据标签错误等，且评估计算成本过高（占开发算力的20%）

Method: 提出评估的三个标准（忠实性、区分性、效率），识别现有评估的失败模式，通过将多项选择题转换为生成任务、过滤"盲目可解"问题和错误标签样本，创建清洗后的评估套件DatBench

Result: 多项选择题转换为生成任务后，模型能力下降高达35%；过滤问题后提高了区分能力并降低了计算成本；发布的DatBench-Full包含33个数据集，DatBench子集实现13倍平均加速（最高50倍）

Conclusion: 为视觉语言模型评估提供了更严谨和可持续的路径，通过数据清洗和任务转换显著提高了评估的忠实性和区分性，同时大幅降低了计算成本

Abstract: Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.

</details>


### [296] [Heterogeneous Low-Bandwidth Pre-Training of LLMs](https://arxiv.org/abs/2601.02360)
*Yazan Obeidi,Amir Sarfi,Joel Lidin,Paul Janson,Eugene Belilovsky*

Main category: cs.LG

TL;DR: SparseLoCo（低通信数据并行）与低带宽流水线模型并行相结合，通过激活和激活梯度压缩实现异构分布式训练，在保持模型质量的同时显著减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型预训练需要分布式计算，但带宽限制使得在数据中心之外难以扩展，特别是当模型并行需要频繁的大规模设备间通信时。需要研究如何将低通信数据并行方法与低带宽流水线模型并行相结合。

Method: 提出异构分布式训练框架：高带宽参与者托管完整副本，资源有限参与者分组使用流水线并行和子空间投影的级间通信。将子空间流水线压缩与SparseLoCo结合，研究多种适配方法。

Result: 在178M-1B参数的大规模语言建模实验中，激活压缩与SparseLoCo结合成本适中，选择性（异构）压缩相比压缩所有副本能持续改善损失-通信权衡，特别是在高压缩比下。

Conclusion: 这些结果表明了一条实用的路径，可以将低带宽模型并行和异构参与者纳入LLM预训练中，为资源受限环境下的分布式训练提供了可行方案。

Abstract: Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [297] [European Options in Market Models with Multiple Defaults: the BSDE approach](https://arxiv.org/abs/2601.01250)
*Miryana Grigorova,James Wheeldon*

Main category: q-fin.MF

TL;DR: 该论文研究由布朗运动和p个违约鞅驱动的非线性倒向随机微分方程，建立了存在唯一性、比较定理，并在线性情形给出显式解，应用于含违约资产的市场定价与对冲问题。


<details>
  <summary>Details</summary>
Motivation: 研究多违约跳跃环境下非线性BSDE的理论性质，为含违约风险的金融市场中衍生品定价与对冲问题提供数学工具，特别是处理大投资者影响违约概率的情形。

Method: 首先证明非线性BSDE的存在唯一性，然后建立比较定理和严格比较定理。在线性驱动情形，利用伴随指数半鞅推导显式公式，区分可料和可选有限变差过程。最后将理论应用于违约资产市场的定价与对冲问题。

Result: 建立了多违约跳跃BSDE的存在唯一性定理和比较定理；在线性驱动情形给出了显式解公式；成功应用于含违约资产的线性/非线性完全市场中欧式期权的定价与对冲，包括大投资者影响违约概率的两种市场模型。

Conclusion: 该研究为多违约环境下的非线性BSDE建立了完整的理论框架，提供了处理违约风险金融问题的有效数学工具，特别适用于大投资者策略影响违约概率的市场情形。

Abstract: We study non-linear Backward Stochastic Differential Equations (BSDEs) driven by a Brownian motion and p default martingales. The driver of the BSDE with multiple default jumps can take a generalized form involving an optional finite variation process. We first show existence and uniqueness. We then establish comparison and strict comparison results for these BSDEs, under a suitable assumption on the driver. In the case of a linear driver, we derive an explicit formula for the first component of the BSDE using an adjoint exponential semimartingale. The representation depends on whether the finite variation process is predictable or only optional. We apply our results to the problem of pricing and hedging a European option in a linear complete market with two defaultable assets and in a non-linear complete market with p defaultable assets. Two examples of the latter market model are provided: an example where the seller of the option is a large investor influencing the probability of default of a single asset and an example where the large seller's strategy affects the default probabilities of all p assets.

</details>


### [298] [Critical volatility threshold for log-normal to power-law transition](https://arxiv.org/abs/2601.01269)
*Valerii Kremnev*

Main category: q-fin.MF

TL;DR: 随机游走模型在局部市场观测中表现良好，但递归结构会产生幂律分布事件。研究发现，从对数正态到幂律动态的转变只需要三个条件：基础过程的随机性、收益的整流以及期望值的迭代前馈。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解释为什么在金融市场中，尽管局部观测符合对数正态分布，但递归结构（如分层衍生品、杠杆头寸、迭代融资轮次）会周期性地产生幂律分布事件。作者希望理解从对数正态到幂律动态转变的机制。

Method: 使用无限期权链作为说明模型，推导出无条件情况下的临界波动率阈值σ* = √(2π) ≈ 250.66%。考虑选择性生存条件（参与者需要最低回报才能继续）时，临界阈值不连续地下降到σ_th* = √(π/2) ≈ 125.3%。

Result: 发现了从对数正态到幂律分布的转变机制，推导出临界波动率分布（V*分布）——一种幂律分布，其指数可以用生存压力和条件期望增长率的闭式表达式表示。结果表明，厚尾可能是具有选择性的迭代对数正态过程的内生属性，而非外生特征。

Conclusion: 幂律分布可能是迭代对数正态过程在选择性生存压力下自然涌现的特性，而非金融市场的外生特征。这一发现为理解金融市场的极端事件提供了新的理论框架。

Abstract: Random walk models with log-normal outcomes fit local market observations remarkably well. Yet interconnected or recursive structures - layered derivatives, leveraged positions, iterative funding rounds - periodically produce power-law distributed events. We show that the transition from log-normal to power-law dynamics requires only three conditions: randomness in the underlying process, rectification of payouts, and iterative feed-forward of expected values. Using an infinite option-on-option chain as an illustrative model, we derive a critical volatility threshold at $σ^* = \sqrt{2π} \approx 250.66\%$ for the unconditional case. With selective survival - where participants require minimum returns to continue - the critical threshold drops discontinuously to $σ_{\text{th}}^{*} = \sqrt{π/2} \approx 125.3\%$, and can decrease further with higher survival thresholds. The resulting outcomes follow what we term the Critical Volatility ($V^*$) Distribution - a power-law whose exponent admits closed-form expression in terms of survival pressure and conditional expected growth. The result suggests that fat tails may be an emergent property of iterative log-normal processes with selection rather than an exogenous feature.

</details>


### [299] [Forward Performance Processes under Multiple Default Risks](https://arxiv.org/abs/2601.02276)
*Wing Fung Chong,Roxana Dumitrescu,Gechun Liang,Kenneth Tsz Hin Ng*

Main category: q-fin.MF

TL;DR: 构建具有多个可违约风险市场中的前向指数效用，通过递归定义的无限时域BSDE系统，并扩展到具有遍历动态的随机因子模型。


<details>
  <summary>Details</summary>
Motivation: 在多可违约风险市场中，需要构建前向效用过程来评估投资绩效。传统方法难以处理违约风险和一般过滤下的性能过程特性。

Method: 使用Jacod-Pham分解描述随机场，在无违约过滤下刻画前向性能过程。通过递归定义的无限时域BSDE系统构建前向效用，并扩展到具有遍历动态的随机因子模型。

Result: 建立了BSDE解的存在性、唯一性和有界性，开发了性能过程在一般过滤下的严格刻画方法，在遍历极限下识别了极限BSDE及其与风险敏感长期增长率的关系。

Conclusion: 成功构建了多可违约风险市场中的前向指数效用框架，解决了技术挑战，为风险敏感长期绩效评估提供了理论基础。

Abstract: This article constructs a forward exponential utility in a market with multiple defaultable risks. Using the Jacod-Pham decomposition for random fields, we first characterize forward performance processes in a defaultable market under the default-free filtration. We then construct a forward utility via a system of recursively defined, indexed infinite-horizon backward stochastic differential equations (BSDEs) with discounting, and establish the existence, uniqueness, and boundedness of their solutions. To verify the required (super)martingale property of the performance process, we develop a rigorous characterization of this property with respect to the general filtration in terms of a set of (in)equalities relative to the default-free filtration. We further extend the analysis to a stochastic factor model with ergodic dynamics. In this setting, we derive uniform bounds for the Markovian solutions of the infinite-horizon BSDEs, overcoming technical challenges arising from the special structure of the system of BSDEs in the defaultable setting. Passing to the ergodic limit, we identify the limiting BSDE and relate its constant to the risk-sensitive long-run growth rate of the optimal wealth process.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [300] [Distribution-Matching Posterior Inference for Incomplete Structural Models](https://arxiv.org/abs/2601.01077)
*Takashi Kano*

Main category: econ.EM

TL;DR: 提出基于分布匹配的后验推断框架DMPI，通过Jensen-Shannon散度构建准似然，处理不完全结构模型中的模型误设和随机奇异性问题。


<details>
  <summary>Details</summary>
Motivation: 现有不完全结构模型的贝叶斯推断方法在处理模型误设和随机奇异性时存在局限，需要开发更稳健的推断框架。

Method: 基于最小经济解释(MEI)扩展，使用Jensen-Shannon散度构建理论矩分布与经验矩分布之间的准似然，采用狄利克雷-多项分布结构和加性平滑，通过带Metropolis-Hastings突变的序列蒙特卡洛算法联合采样结构参数和理论矩分布。

Result: 蒙特卡洛实验显示DMPI在新凯恩斯模型误设情况下提供稳健推断，通过概率性降权改善分布匹配一致性；实证应用表明简约的随机奇异NK模型比过度参数化的满秩模型能更好拟合美国商业周期矩。

Conclusion: DMPI框架为不完全结构模型提供了有效的贝叶斯推断方法，能够处理模型误设和随机奇异性，在理论和实证应用中均表现出良好性能。

Abstract: This paper introduces a Bayesian inference framework for incomplete structural models, termed distribution-matching posterior inference (DMPI). Extending the minimal econometric interpretation (MEI), DMPI constructs a divergence-based quasi-likelihood using the Jensen-Shannon divergence between theoretical and empirical population-moment distributions, based on a Dirichlet-multinomial structure with additive smoothing. The framework accommodates model misspecification and stochastic singularity. Posterior inference is implemented via a sequential Monte Carlo algorithm with Metropolis-Hastings mutation that jointly samples structural parameters and theoretical moment distributions. Monte Carlo experiments using misspecified New Keynesian (NK) models demonstrate that DMPI yields robust inference and improves distribution-matching coherence by probabilistically down-weighting moment distributions inconsistent with the structural model. An empirical application to U.S. data shows that a parsimonious stochastic singular NK model provides a better fit to business-cycle moments than an overparameterized full-rank counterpart.

</details>


### [301] [Optimizing Patient Placement in Normal Care Units: An Instrumental Causal Forest Approach Minimizing Mortality](https://arxiv.org/abs/2601.01149)
*Johannes Cordier*

Main category: econ.EM

TL;DR: 使用工具变量因果森林分析NCU安置对健康结果的影响，发现专业化与利用率之间的权衡，设计最小化后悔安置政策降低死亡率


<details>
  <summary>Details</summary>
Motivation: 医院正常护理单元(NCU)的安置影响健康结果，不同NCU有不同专业特长，患者可能适合多个NCU，且NCU利用率水平也影响结果。需要理解NCU安置效果如何随患者特征和利用率变化，以优化患者分配

Method: 使用工具变量因果森林方法，以急诊入院作为工具变量，估计NCU安置效果在患者和利用率水平上的异质性。基于结果设计最小化后悔安置政策，使用频率主义、Balke-Pearl和Manski界限

Result: 结果显示专业化与利用率之间存在明显权衡关系。设计的安置政策通过根据个体化平均处理效应重新分配患者，能够在不增加容量的情况下降低死亡率

Conclusion: 数据驱动的患者安置可以通过更有效地利用现有资源改善健康结果，最小化后悔安置政策展示了在不扩张容量的情况下优化患者分配的可行性

Abstract: Normal care units (NCU) placement affects health outcomes. NCUs in a hospital have different specialisations. There are patients that can potentially stay in multiple different NCUs. On a given day the NCUs are on different utilisation levels, which also affects health outcomes. Our approach uses instrumental variable causal forests, with emergency admission as an instrument, to estimate how the effect of NCU placement varies across patients and utilisation levels. The results show a clear trade-off between specialisation and utilization. Based on these findings, we design a minimax regret placement policy, using frequentist, Balke-Pearl and Manski bounds, that lowers mortality without capacity expansion. The policy reallocates patients according to their individualized average treatment effects, showing that data-driven patient placement can improve outcomes by using existing resources more efficiently.

</details>


### [302] [Dynamic Risk in the U.S. Banking System: An Analysis of Sentiment, Policy Shocks, and Spillover Effects](https://arxiv.org/abs/2601.01783)
*Haibo Wang,Jun Huang,Lutfu S Sua,Jaime Ortiz,Jinshyang Roan,Bahram Alidaee*

Main category: econ.EM

TL;DR: 该研究通过时变参数VAR模型发现，2023年美国银行业危机主要通过信息传染渠道传播，而非直接金融关联，验证了"太相似而无法生存"假说。


<details>
  <summary>Details</summary>
Motivation: 研究旨在超越探索性分析，检验"太相似而无法生存"假说，即银行危机通过业务模式相似性在利率压力下传播，而非传统金融关联渠道。

Method: 使用时变参数向量自回归模型，采用30天滚动窗口分析2022年3月18日至2023年3月15日期间四家倒闭银行和一组幸存同行银行的每日股票收益率。

Result: 发现系统连通性在危机高峰期急剧上升，SIVB、FRC和WAL是主要风险净传播者，其相似同行成为显著风险净接收者；市场情绪和政策不确定性显著放大了风险溢出效应。

Conclusion: 研究证实了银行网络中系统性风险的持续性，强调了实时监控对加强金融稳定的重要性，为信息传染渠道提供了概念框架和实证验证。

Abstract: The 2023 U.S. banking crisis propagated not through direct financial linkages but through a high-frequency, information-based contagion channel. This paper moves beyond exploration analysis to test the "too-similar-to-fail" hypothesis, arguing that risk spillovers were driven by perceived similarities in bank business models under acute interest rate pressure. Employing a Time-Varying Parameter Vector Autoregression (TVP-VAR) model with 30-day rolling windows, a method uniquely suited for capturing the rapid network shifts inherent in a panic, we analyze daily stock returns for the four failed institutions and a systematically selected peer group of surviving banks vulnerable to the same risks from March 18, 2022, to March 15, 2023. Our results provide strong evidence for this contagion channel: total system connectedness surged dramatically during the crisis peak, and we identify SIVB, FRC, and WAL as primary net transmitters of risk while their perceived peers became significant net receivers, a key dynamic indicator of systemic vulnerability that cannot be captured by asset-by-asset analysis. We further demonstrate that these spillovers were significantly amplified by market sentiment (as measured by the VIX) and economic policy uncertainty (EPU). By providing a clear conceptual framework and robust empirical validation, our findings confirm the persistence of systemic risks within the banking network and highlight the importance of real-time monitoring in strengthening financial stability.

</details>


### [303] [When and Why State-Dependent Local Projections Work](https://arxiv.org/abs/2601.01622)
*Valentin Winkler*

Main category: econ.EM

TL;DR: 该论文研究了状态依赖局部投影(LPs)，建立了其估计量的一般特征，表明它们恢复因果效应的加权平均，阐明了LPs与VARs的区别，并提出了使VAR估计量等于LP估计量的方法，同时揭示了LP-IV设置中状态依赖加权可能导致的误解。


<details>
  <summary>Details</summary>
Motivation: 状态依赖局部投影(LPs)在实证宏观经济学中被广泛使用，但对其估计量的解释存在模糊性。论文旨在澄清状态依赖LPs的正确解释，明确它们与向量自回归(VARs)的关系，并揭示工具变量(LP-IV)设置中可能出现的误解来源。

Method: 论文首先在最小假设下建立了状态依赖LPs估计量的一般特征化，证明其恢复因果效应的加权平均。然后通过理论分析比较LPs和VARs的目标估计量，并提出一个简单的基于VAR的估计器，使其概率极限等于LP估计量。最后在工具变量(LP-IV)设置中分析状态依赖加权的影响。

Result: 1) 状态依赖LPs在几乎所有实际使用的设定下都恢复因果效应的加权平均；2) LPs和VARs针对不同的估计量，但可以通过提出的VAR-based估计器使两者一致；3) 在LP-IV设置中，即使效应不是状态依赖的，状态依赖加权也可能产生非零的交互项，这是LP-IV误解的关键来源。

Conclusion: 论文为正确解释状态依赖LPs提供了理论框架，澄清了LPs与VARs的关系，并强调了LP-IV设置中状态依赖加权可能导致误解的重要机制，对实证宏观经济学研究具有重要指导意义。

Abstract: This paper studies state-dependent local projections (LPs). First, I establish a general characterization of their estimand: under minimal assumptions, state-dependent LPs recover weighted averages of causal effects. This holds for essentially all specifications used in practice. Second, I show that state-dependent LPs and VARs target different estimands and propose a simple VAR-based estimator whose probability limit equals the LP estimand. Third, in instrumental variable (LP-IV) settings, state-dependent weighting can generate nonzero interaction terms, even when the effects are not state-dependent. Overall, this paper shows how to correctly interpret state-dependent LPs, clarifying their connection to VARs and highlighting a key source of LP-IV misinterpretation.

</details>


### [304] [Reinforcement Learning Based Computationally Efficient Conditional Choice Simulation Estimation of Dynamic Discrete Choice Models](https://arxiv.org/abs/2601.02069)
*Ahmed Khwaja,Sonal Srivastava*

Main category: econ.EM

TL;DR: 论文提出了一种基于强化学习的条件选择模拟（CCS）两步估计方法，用于解决高维状态-动作空间下动态离散选择（DDC）模型的估计挑战，结合了机器学习可扩展性和结构模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 在市场营销中，动态离散选择模型面临"大数据"环境下高维状态-动作空间的估计挑战，传统方法计算量大，需要一种既能保持结构模型解释性又能实现计算效率的新方法。

Method: 将CCS估计框架重构为强化学习问题，利用RL算法的可扩展性优势，但保持CCS的"前向模拟"特性。关键洞察包括：CCS是RL算法的特例；CCS只在模拟路径结束后更新值函数，而RL可沿路径更新；RL关注已知奖励函数下的最优策略，DDC关注已知最优策略下的奖励函数估计。

Result: 通过蒙特卡洛模拟，在经典的机器更换模型和消费者食品购买模型上验证了该方法相比传统CCS估计的计算效率优势。方法能够扩展到高维营销问题，同时保持可解释性和可处理性。

Conclusion: 将DDC模型的CCS估计框架化为RL问题，显著提高了方法在高维营销问题中的适用性和可扩展性，同时保留了结构模型的可解释性和可处理性，为反事实政策分析提供了有价值的工具。

Abstract: Dynamic discrete choice (DDC) models have found widespread application in marketing. However, estimating these becomes challenging in "big data" settings with high-dimensional state-action spaces. To address this challenge, this paper develops a Reinforcement Learning (RL)-based two-step ("computationally light") Conditional Choice Simulation (CCS) estimation approach that combines the scalability of machine learning with the transparency, explainability, and interpretability of structural models, which is particularly valuable for counterfactual policy analysis. The method is premised on three insights: (1) the CCS ("forward simulation") approach is a special case of RL algorithms, (2) starting from an initial state-action pair, CCS updates the corresponding value function only after each simulation path has terminated, whereas RL algorithms may update for all the state-action pairs visited along a simulated path, and (3) RL focuses on inferring an agent's optimal policy with known reward functions, whereas DDC models focus on estimating the reward functions presupposing optimal policies. The procedure's computational efficiency over CCS estimation is demonstrated using Monte Carlo simulations with a canonical machine replacement and a consumer food purchase model. Framing CCS estimation of DDC models as an RL problem increases their applicability and scalability to high-dimensional marketing problems while retaining both interpretability and tractability.

</details>


### [305] [Fare-Free Bus Service and CO2 Reductions: Evidence from a Natural Experiment](https://arxiv.org/abs/2601.02190)
*Anna Alberini,Javier Bas,Cinzia Cirillo*

Main category: econ.EM

TL;DR: 研究评估华盛顿特区地铁区Alexandria免费公交服务的影响，发现公交使用率最多增加6%，对地面臭氧和交通事故无影响，但能减少0.294-0.494吨CO2排放，成本为每吨70-120美元。


<details>
  <summary>Details</summary>
Motivation: 评估免费公交服务政策对居民出行行为、环境（CO2排放）和交通安全的影响，为城市交通政策提供实证依据。

Method: 采用双重差分法研究设计，对比Alexandria（实验组）与其他控制区域的公交使用变化，通过问卷调查收集数据，分析公交使用频率、汽车里程减少量及CO2排放变化。

Result: 免费公交服务对公交使用率影响有限（最多增加6%），对地面臭氧和交通事故无显著影响；但能减少0.294-0.494吨CO2/年（相当于美国汽车年均排放的5-9%），成本效益为每吨CO2减排成本70-120美元。

Conclusion: 免费公交服务对改变居民出行模式和环境效益有限，但具有一定CO2减排效果；政策推广至整个研究区域预计每年可减少0.454吨CO2（相当于8%的美国汽车年均排放）。

Abstract: We devise a difference-in-difference study design to assess the impact of fare-free bus service in Alexandria, located in the Washington, DC metro area. Our surveys show modest to no effect, with at most 6% more residents in Alexandria increasing their bus usage compared to control locations. We find no effect on ground-level ozone or road crashes, suggesting little to no impact on road traffic.
  One-third of respondents in control locations indicated they would use buses more frequently if fare-free service were available in their areas. Based on the respondent-reported reductions in car miles, the program led to a reduction of 0.294 to 0.494 tons of CO2 per year, or 5% to 9% of the average annual emissions from a US car, at a cost of $70-$120 per ton of CO2. We predict a CO2 reduction of 0.454 tons per year, equivalent to 8% of the average US car's annual emissions if the fare-free bus covered all of the study areas.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [306] [VEAT Quantifies Implicit Associations in Text-to-Video Generator Sora and Reveals Challenges in Bias Mitigation](https://arxiv.org/abs/2601.00996)
*Yongxu Sun,Michael Saxon,Ian Yang,Anna-Maria Gueorguieva,Aylin Caliskan*

Main category: cs.CY

TL;DR: 研究提出视频嵌入关联测试(VEAT)方法，发现Sora等文本到视频生成器存在种族和性别偏见，将欧洲裔美国人和女性与愉悦性关联更强，且偏见程度与现实世界人口分布高度相关。


<details>
  <summary>Details</summary>
Motivation: 随着Sora等文本到视频生成器的出现，需要评估生成内容是否反映社会偏见。现有偏见测试主要针对文字和图像，缺乏对视频内容的系统性评估方法。

Method: 提出视频嵌入关联测试(VEAT)和单类别VEAT(SC-VEAT)，将嵌入关联测试从文字和图像扩展到视频。使用Sora生成器，量化17种职业和7种奖项中种族（非裔vs欧裔美国人）和性别（女性vs男性）与愉悦性（愉快vs不愉快）的关联。

Result: Sora视频显示：欧裔美国人和女性与愉悦性关联更强（效应量d>0.8）。效应大小与现实人口分布高度相关：职业中男性比例和白人比例（r=0.93, r=0.83），奖项中男性比例和非黑人比例（r=0.88, r=0.99）。显式去偏提示通常能减少效应量，但可能适得其反：两个黑人关联职业（清洁工、邮政服务）在去偏后黑人关联更强。

Conclusion: 易获取的文本到视频生成器可能放大代表性伤害，除非经过严格评估和负责任部署。需要系统性评估方法来检测和缓解视频生成中的社会偏见。

Abstract: Text-to-Video (T2V) generators such as Sora raise concerns about whether generated content reflects societal bias. We extend embedding-association tests from words and images to video by introducing the Video Embedding Association Test (VEAT) and Single-Category VEAT (SC-VEAT). We validate these methods by reproducing the direction and magnitude of associations from widely used baselines, including Implicit Association Test (IAT) scenarios and OASIS image categories. We then quantify race (African American vs. European American) and gender (women vs. men) associations with valence (pleasant vs. unpleasant) across 17 occupations and 7 awards. Sora videos associate European Americans and women more with pleasantness (both d>0.8). Effect sizes correlate with real-world demographic distributions: percent men and White in occupations (r=0.93, r=0.83) and percent male and non-Black among award recipients (r=0.88, r=0.99). Applying explicit debiasing prompts generally reduces effect-size magnitudes, but can backfire: two Black-associated occupations (janitor, postal service) become more Black-associated after debiasing. Together, these results reveal that easily accessible T2V generators can actually amplify representational harms if not rigorously evaluated and responsibly deployed.

</details>


### [307] [An Agentic Software Framework for Data Governance under DPDP](https://arxiv.org/abs/2601.01101)
*Apurva Kulkarni,Chandrashekar Ramanathan*

Main category: cs.CY

TL;DR: 提出基于智能体框架的印度DPDP法案合规解决方案，通过KYU Agent和Compliance Agent实现动态、可解释的数据治理


<details>
  <summary>Details</summary>
Motivation: 印度DPDP法案要求严格的数据隐私合规，传统合规工具基于硬编码规则，缺乏灵活性、透明度，难以适应动态政策变化，需要透明、可追溯、自适应的合规机制

Method: 引入智能体框架，集成KYU Agent（语义理解、用户可信度建模）和Compliance Agent（数据敏感性推理），基于开源智能体框架构建，采用目标驱动的智能体管道

Result: 在医疗、教育、电商等10个领域评估，通过匿名化评分衡量DPDP合规效果，展示可扩展的合规数据治理，支持掩码、假名化、泛化等策略

Conclusion: 该框架通过协作智能体、动态策略执行和领域感知匿名化，实现了可扩展、透明、合规的数据治理，为负责任AI软件提供解决方案

Abstract: Despite the rise of data-driven software systems in the modern digital landscape, data governance under a legal framework remains a critical challenge. In India, the Digital Personal Data Protection (DPDP) Act mandates rigorous data privacy and compliance requirements, necessitating software frameworks that are both ethical and regulation-aware. From a software development perspective, traditional compliance tools often rely on hard-coded rules and static configurations, making them inflexible to dynamic policy updates or evolving legal contexts. Additionally, their monolithic architectures obscure decision-making processes, creating black-box behavior in critical governance workflows. Developing responsible AI software demands transparency, traceability, and adaptive enforcement mechanisms that make ethical decisions explainable. To address this challenge, a novel agentic framework is introduced to embed compliance logic directly into software agents that govern and adapt data policies. In this paper, the implementation focuses on the DPDP Act. The framework integrates KYU Agent and Compliance Agent for this purpose. KYU (Know-YourUser) Agent supports semantic understanding, user trustworthiness modelling and Compliance Agent uses data sensitivity reasoning within a goal-driven, agentic pipeline. The proposed framework, built using an open-sourced agentic framework and has been evaluated across ten diverse domains, including healthcare, education, and e-commerce. Its effectiveness under DPDP, measured via an Anonymization Score, demonstrates scalable, compliant data governance through masking, pseudonymization, and generalization strategies tailored to domain-specific needs. The proposed framework delivers scalable, transparent, and compliant data governance through collaborative agents, dynamic policy enforcement, and domain-aware anonymization.

</details>


### [308] [Inconsistencies in Classification of Online News Articles: A Call for Common Standards in Brand Safety Services](https://arxiv.org/abs/2601.01303)
*Michael Smith,Riley Grossman,Antonio Torres-Aguero,Pritam Sen,Cristian Borcea,Yi Chen*

Main category: cs.CY

TL;DR: 研究发现三大品牌安全服务商对新闻文章的分类存在显著不一致，导致广告支出错配和收入损失，呼吁建立标准化透明系统


<details>
  <summary>Details</summary>
Motivation: 新闻内容在公共话语中具有核心作用，且数字广告支出已经不足，品牌安全分类的不一致会对广告商和出版商造成重大财务影响

Method: 收集51个域名上的4,352篇新闻文章数据，分析DoubleVerify、Integral Ad Science和Oracle三家领先品牌安全提供商的分类评级

Result: 品牌安全服务经常产生冲突的分类结果，不同提供商之间存在显著差异，这些不一致可能导致广告支出错配和收入损失

Conclusion: 当前品牌安全系统存在缺陷，需要建立标准化和透明的品牌安全系统，以减轻对数字广告生态系统的有害影响

Abstract: This study examines inconsistencies in the brand safety classifications of online news articles by analyzing ratings from three leading brand safety providers, DoubleVerify, Integral Ad Science, and Oracle. We focus on news content because of its central role in public discourse and the significant financial consequences of unsafe classifications in a sector that is already underserved by digital ad spending. By collecting data from 4,352 news articles on 51 domains, our analysis shows that brand safety services often produce conflicting classifications, with significant discrepancies between providers. These inconsistencies can have harmful consequences for both advertisers and publishers, leading to misplaced advertising spending and revenue losses. This research provides critical insights into the shortcomings of the current brand safety landscape. We argue for a standardized and transparent brand safety system to mitigate the harmful effects of the current system on the digital advertising ecosystem.

</details>


### [309] [AppellateGen: A Benchmark for Appellate Legal Judgment Generation](https://arxiv.org/abs/2601.01331)
*Hongkun Yang,Lionel Z. Wang,Wei Fan,Yiran Hu,Lixu Wang,Chenyu Liu,Shenghong Fu,Haoyang Li,Xin Xu,Jiexin Zheng,Wei Dong*

Main category: cs.CY

TL;DR: 提出了AppellateGen基准，用于上诉审法律判决生成，包含7,351个案件对，并设计了基于司法标准操作程序的法律多智能体系统(SLMAS)来模拟司法工作流程。


<details>
  <summary>Details</summary>
Motivation: 现有法律判决生成研究主要关注一审审判，依赖静态的事实到判决映射，忽略了上诉审的辩证性质。需要解决上诉审法律判决生成中考虑初始判决和证据更新的因果依赖关系。

Method: 提出基于司法标准操作程序(SOP)的法律多智能体系统(SLMAS)，将生成过程分解为问题识别、检索和起草三个离散阶段，模拟司法工作流程。

Result: 实验结果表明，虽然SLMAS提高了逻辑一致性，但上诉推理的复杂性对当前大语言模型仍然构成重大挑战。数据集和代码已公开。

Conclusion: AppellateGen基准填补了上诉审法律判决生成的空白，SLMAS方法通过模拟司法工作流程提高了逻辑一致性，但上诉推理的复杂性仍需进一步研究。

Abstract: Legal judgment generation is a critical task in legal intelligence. However, existing research in legal judgment generation has predominantly focused on first-instance trials, relying on static fact-to-verdict mappings while neglecting the dialectical nature of appellate (second-instance) review. To address this, we introduce AppellateGen, a benchmark for second-instance legal judgment generation comprising 7,351 case pairs. The task requires models to draft legally binding judgments by reasoning over the initial verdict and evidentiary updates, thereby modeling the causal dependency between trial stages. We further propose a judicial Standard Operating Procedure (SOP)-based Legal Multi-Agent System (SLMAS) to simulate judicial workflows, which decomposes the generation process into discrete stages of issue identification, retrieval, and drafting. Experimental results indicate that while SLMAS improves logical consistency, the complexity of appellate reasoning remains a substantial challenge for current LLMs. The dataset and code are publicly available at: https://anonymous.4open.science/r/AppellateGen-5763.

</details>


### [310] [The Gray Area: Characterizing Moderator Disagreement on Reddit](https://arxiv.org/abs/2601.01620)
*Shayan Alipour,Shruti Phadke,Seyed Shahabeddin Mousavi,Amirhossein Afsharrad,Morteza Zihayat,Mattia Samory*

Main category: cs.CY

TL;DR: 该研究分析Reddit论坛中志愿者版主之间的内容审核分歧，发现七分之一的审核案例存在争议，这些"灰色地带"案例主要涉及难以判断用户意图的行为（如钓鱼、刷屏）和社区治理冲突，且近半数涉及自动化审核决策。研究表明灰色地带案例本质上更难裁决，当前最先进的语言模型也难以处理。


<details>
  <summary>Details</summary>
Motivation: 志愿者版主对在线内容审核至关重要，但他们经常在什么内容应该被允许或禁止上存在分歧。研究者希望理解这种审核"灰色地带"的复杂性，特别是版主之间的分歧如何影响内容审核过程。

Method: 研究分析了24个不同主题和规模的subreddit中5年间的430万条审核日志条目。通过比较争议案例（灰色地带）和无争议案例，使用信息论评估方法分析这些案例的差异，并测试了最先进的语言模型在处理这些争议案例时的表现。

Result: 研究发现：1）七分之一的审核案例存在版主争议；2）灰色地带案例主要涉及用户意图难以直接判断的违规行为（如钓鱼、刷屏）以及社区治理冲突；3）近半数灰色地带案例涉及自动化审核决策；4）信息论评估显示灰色地带案例本质上比无争议案例更难裁决；5）当前最先进的语言模型在处理这些争议案例时表现不佳。

Conclusion: 研究强调了专家人类版主在监督审核过程中的关键作用，揭示了当前审核流程和工具面临的挑战。灰色地带案例的复杂性表明，完全依赖自动化系统进行内容审核存在局限性，需要人类专家的判断和监督。

Abstract: Volunteer moderators play a crucial role in sustaining online dialogue, but they often disagree about what should or should not be allowed. In this paper, we study the complexity of content moderation with a focus on disagreements between moderators, which we term the ``gray area'' of moderation. Leveraging 5 years and 4.3 million moderation log entries from 24 subreddits of different topics and sizes, we characterize how gray area, or disputed cases, differ from undisputed cases. We show that one-in-seven moderation cases are disputed among moderators, often addressing transgressions where users' intent is not directly legible, such as in trolling and brigading, as well as tensions around community governance. This is concerning, as almost half of all gray area cases involved automated moderation decisions. Through information-theoretic evaluations, we demonstrate that gray area cases are inherently harder to adjudicate than undisputed cases and show that state-of-the-art language models struggle to adjudicate them. We highlight the key role of expert human moderators in overseeing the moderation process and provide insights about the challenges of current moderation processes and tools.

</details>


### [311] [From Chat Control to Robot Control: The Backdoors Left Open for the Sake of Safety](https://arxiv.org/abs/2601.02205)
*Neziha Akalin,Alberto Giaretta*

Main category: cs.CY

TL;DR: 欧盟Chat Control法律提案将数字监控扩展到机器人交互领域，可能将日常机器人转变为监控工具，在保护与监控之间制造矛盾，并因削弱加密而增加安全风险。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在护理、教育和远程呈现等领域成为人际沟通渠道，欧盟的Chat Control法律提案可能将数字监控扩展到这些具身系统，从而改变人机交互的基础。作者旨在提高对这一潜在转变的认识，防止某些未来成为现实。

Method: 本文采用批判性分析的方法，探讨欧盟Chat Control法律提案如何通过激励提供商实施内容检测和通信扫描，将数字监控扩展到机器人交互领域。分析聚焦于该法规对机器人作为沟通渠道的影响，特别是对隐私、自主权和信任的潜在威胁。

Result: 研究发现，将数字监控法律扩展到具身机器人系统会导致持续监控，模糊保护与控制之间的界限，将伴侣机器人转变为潜在告密者。同时，削弱端到端加密的监控机制实际上成为后门，扩大了攻击面，使对手能够利用法律强制的监控基础设施。这创造了"通过不安全实现安全"的悖论。

Conclusion: 旨在保护用户的系统可能反而损害用户的隐私、自主权和信任。本文不是要预测未来，而是要提高认识，帮助防止某些未来成为现实。需要警惕将数字监控扩展到机器人交互领域可能带来的深远影响。

Abstract: This paper explores how a recent European Union proposal, the so-called Chat Control law, which creates regulatory incentives for providers to implement content detection and communication scanning, could transform the foundations of human-robot interaction (HRI). As robots increasingly act as interpersonal communication channels in care, education, and telepresence, they convey not only speech but also gesture, emotion, and contextual cues. We argue that extending digital surveillance laws to such embodied systems would entail continuous monitoring, embedding observation into the very design of everyday robots. This regulation blurs the line between protection and control, turning companions into potential informants. At the same time, monitoring mechanisms that undermine end-to-end encryption function as de facto backdoors, expanding the attack surface and allowing adversaries to exploit legally induced monitoring infrastructures. This creates a paradox of safety through insecurity: systems introduced to protect users may instead compromise their privacy, autonomy, and trust. This work does not aim to predict the future, but to raise awareness and help prevent certain futures from materialising.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [312] [Beyond Demand Estimation: Consumer Surplus Evaluation via Cumulative Propensity Weights](https://arxiv.org/abs/2601.01029)
*Zeyu Bian,Max Biggs,Ruijiang Gao,Zhengling Qi*

Main category: stat.ML

TL;DR: 提出使用观测数据审计AI决策消费者剩余影响的实用框架，通过随机价格和累积倾向权重避免传统需求函数估计的困难


<details>
  <summary>Details</summary>
Motivation: 传统方法通过估计需求函数计算消费者剩余存在实践困难：参数模型易错设，非参数方法数据需求大且收敛慢。需要更实用的框架来审计AI定价和算法贷款的消费者剩余影响

Method: 利用算法定价固有的探索-利用权衡产生的随机性，通过累积倾向权重(CPW)重新加权购买结果来重建积分，避免显式估计需求函数。提出增强型累积倾向权重(ACPW)估计器，只需需求模型或历史定价策略分布之一正确设定即可

Result: 方法能够使用灵活机器学习方法估计消费者剩余，即使机器学习估计收敛较慢也能获得快速收敛率。框架可扩展到不平等感知的剩余度量，量化利润-公平权衡

Conclusion: 开发了实用的消费者剩余审计框架，克服传统方法局限，支持灵活机器学习应用，并扩展到公平性考量，通过数值研究验证了有效性

Abstract: This paper develops a practical framework for using observational data to audit the consumer surplus effects of AI-driven decisions, specifically in targeted pricing and algorithmic lending. Traditional approaches first estimate demand functions and then integrate to compute consumer surplus, but these methods can be challenging to implement in practice due to model misspecification in parametric demand forms and the large data requirements and slow convergence of flexible nonparametric or machine learning approaches. Instead, we exploit the randomness inherent in modern algorithmic pricing, arising from the need to balance exploration and exploitation, and introduce an estimator that avoids explicit estimation and numerical integration of the demand function. Each observed purchase outcome at a randomized price is an unbiased estimate of demand and by carefully reweighting purchase outcomes using novel cumulative propensity weights (CPW), we are able to reconstruct the integral. Building on this idea, we introduce a doubly robust variant named the augmented cumulative propensity weighting (ACPW) estimator that only requires one of either the demand model or the historical pricing policy distribution to be correctly specified. Furthermore, this approach facilitates the use of flexible machine learning methods for estimating consumer surplus, since it achieves fast convergence rates by incorporating an estimate of demand, even when the machine learning estimate has slower convergence rates. Neither of these estimators is a standard application of off-policy evaluation techniques as the target estimand, consumer surplus, is unobserved. To address fairness, we extend this framework to an inequality-aware surplus measure, allowing regulators and firms to quantify the profit-equity trade-off. Finally, we validate our methods through comprehensive numerical studies.

</details>


### [313] [Fibonacci-Driven Recursive Ensembles: Algorithms, Convergence, and Learning Dynamics](https://arxiv.org/abs/2601.01055)
*Ernest Fokoué*

Main category: stat.ML

TL;DR: 该论文提出了基于斐波那契型更新流的递归集成学习算法与动力学基础，相比传统一阶提升方法，采用二阶递归架构，每个预测器依赖前两个预测器，形成具有记忆的学习动态。


<details>
  <summary>Details</summary>
Motivation: 传统提升方法（如AdaBoost）使用一阶加法更新，缺乏记忆机制。本文旨在开发具有记忆能力的递归集成学习框架，通过斐波那契型更新流整合历史结构并适应新残差信息，提升集成学习的表达能力。

Method: 提出递归权重更新算法家族，涵盖斐波那契、三波那契及高阶递归；建立连续时间极限得到控制集成演化的微分方程系统；提供全局收敛条件、谱稳定性准则和非渐近泛化界分析。

Result: 递归流在核岭回归、样条平滑器和随机傅里叶特征模型中一致改进近似和泛化能力，超越静态加权方法；建立了递归集成、结构化加权和动力系统视角的统一理论框架。

Conclusion: 该研究完成了从斐波那契加权、几何加权理论到完全动态递归集成学习系统的三部曲，为统计学习提供了新的递归集成学习范式，统一了多个理论视角。

Abstract: This paper develops the algorithmic and dynamical foundations of recursive ensemble learning driven by Fibonacci-type update flows. In contrast with classical boosting  Freund and Schapire (1997); Friedman (2001), where the ensemble evolves through first-order additive updates, we study second-order recursive architectures in which each predictor depends on its two immediate predecessors. These Fibonacci flows induce a learning dynamic with memory, allowing ensembles to integrate past structure while adapting to new residual information. We introduce a general family of recursive weight-update algorithms encompassing Fibonacci, tribonacci, and higher-order recursions, together with continuous-time limits that yield systems of differential equations governing ensemble evolution. We establish global convergence conditions, spectral stability criteria, and non-asymptotic generalization bounds under Rademacher Bartlett and Mendelson (2002) and algorithmic stability analyses. The resulting theory unifies recursive ensembles, structured weighting, and dynamical systems viewpoints in statistical learning. Experiments with kernel ridge regression Rasmussen and Williams (2006), spline smoothers Wahba (1990), and random Fourier feature models Rahimi and Recht (2007) demonstrate that recursive flows consistently improve approximation and generalization beyond static weighting. These results complete the trilogy begun in Papers I and II: from Fibonacci weighting, through geometric weighting theory, to fully dynamical recursive ensemble learning systems.

</details>


### [314] [Neural Networks on Symmetric Spaces of Noncompact Type](https://arxiv.org/abs/2601.01097)
*Xuan Son Nguyen,Shuo Yang,Aymeric Histace*

Main category: stat.ML

TL;DR: 提出了一种在非紧型对称空间上构建神经网络的新方法，基于统一的点到超平面距离公式，并设计了全连接层和注意力机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究展示了神经网络在双曲空间和SPD流形上的良好性能，这些空间都属于非紧型对称空间。需要开发一个统一的方法在这些空间上构建神经网络。

Method: 提出统一的点到超平面距离公式，推导出高阶非紧型对称空间中G不变黎曼度量下的闭式距离表达式，基于此设计全连接层和注意力机制。

Result: 方法在图像分类、EEG信号分类、图像生成和自然语言推理等挑战性基准测试中得到验证。

Conclusion: 提出了一种在非紧型对称空间上构建神经网络的统一方法，通过点到超平面距离公式实现了有效的网络架构设计，并在多个任务上验证了其有效性。

Abstract: Recent works have demonstrated promising performances of neural networks on hyperbolic spaces and symmetric positive definite (SPD) manifolds. These spaces belong to a family of Riemannian manifolds referred to as symmetric spaces of noncompact type. In this paper, we propose a novel approach for developing neural networks on such spaces. Our approach relies on a unified formulation of the distance from a point to a hyperplane on the considered spaces. We show that some existing formulations of the point-to-hyperplane distance can be recovered by our approach under specific settings. Furthermore, we derive a closed-form expression for the point-to-hyperplane distance in higher-rank symmetric spaces of noncompact type equipped with G-invariant Riemannian metrics. The derived distance then serves as a tool to design fully-connected (FC) layers and an attention mechanism for neural networks on the considered spaces. Our approach is validated on challenging benchmarks for image classification, electroencephalogram (EEG) signal classification, image generation, and natural language inference.

</details>


### [315] [Conformal Blindness: A Note on $A$-Cryptic change-points](https://arxiv.org/abs/2601.01147)
*Johan Hallberg Szabadváry*

Main category: stat.ML

TL;DR: 本文揭示了共形测试鞅(CTMs)的一个根本局限性——共形盲区现象，即即使数据交换性发生显著破坏，p值序列仍可能保持均匀分布，导致CTMs无法检测到这种变化。


<details>
  <summary>Details</summary>
Motivation: 共形测试鞅(CTMs)是共形预测框架中用于检验数据交换性假设的标准方法，通过监测p值序列偏离均匀分布的程度来检测交换性破坏。然而，交换性意味着均匀p值，但均匀p值不一定意味着交换性。这引发了一个关键问题：是否存在交换性的显著破坏，但p值仍保持均匀分布，从而使CTMs失效？

Method: 1. 通过理论构造，针对理论理想的"预言机"一致性度量（由真实条件密度给出），证明了A-隐秘变化点的可能性；2. 使用二元高斯分布，识别出一条边际均值变化但不改变一致性得分分布的直线，从而产生完全均匀的p值；3. 通过模拟验证即使大规模分布偏移也可能对CTM完全隐秘。

Result: 1. 确认了共形盲区现象的存在——即使数据交换性发生显著破坏，p值序列仍可能保持均匀分布；2. 通过二元高斯分布的构造，展示了边际均值变化但不影响一致性得分分布的具体情况；3. 模拟结果表明，即使大规模分布偏移也可能对CTM完全隐秘，揭示了CTMs的根本局限性。

Conclusion: 共形测试鞅存在根本局限性，即共形盲区现象，使得某些类型的分布变化无法被检测。这强调了在应用CTMs时，一致性度量与潜在分布偏移的对齐至关重要，需要仔细选择一致性度量以避免盲区。

Abstract: Conformal Test Martingales (CTMs) are a standard method within the Conformal Prediction framework for testing the crucial assumption of data exchangeability by monitoring deviations from uniformity in the p-value sequence. Although exchangeability implies uniform p-values, the converse does not hold. This raises the question of whether a significant break in exchangeability can occur, such that the p-values remain uniform, rendering CTMs blind. We answer this affirmatively, demonstrating the phenomenon of \emph{conformal blindness}.
  Through explicit construction, for the theoretically ideal ``oracle'' conformity measure (given by the true conditional density), we demonstrate the possibility of an \emph{$A$-cryptic change-point} (where $A$ refers to the conformity measure). Using bivariate Gaussian distributions, we identify a line along which a change in the marginal means does not alter the distribution of the conformity scores, thereby producing perfectly uniform p-values.
  Simulations confirm that even a massive distribution shift can be perfectly cryptic to the CTM, highlighting a fundamental limitation and emphasising the critical role of the alignment of the conformity measure with potential shifts.

</details>


### [316] [Evidence Slopes and Effective Dimension in Singular Linear Models](https://arxiv.org/abs/2601.01238)
*Kalyaan Rao*

Main category: stat.ML

TL;DR: 论文研究贝叶斯模型选择中Laplace近似和BIC在奇异模型中的失效问题，提出基于实对数典范阈值(RLCT)的修正方法，在线性高斯秩模型和字典模型中验证了RLCT作为有效维度的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯模型选择方法（Laplace近似和BIC）假设有效模型维度等于参数数量，但在过参数化或秩不足的奇异模型中，这一假设不成立。需要研究奇异学习理论中的实对数典范阈值(RLCT)如何提供更准确的有效维度估计。

Method: 在线性高斯秩模型和线性子空间（字典）模型中，利用精确边缘似然闭式解和RLCT解析可处理的特点，理论分析和实证研究Laplace/BIC误差与RLCT的关系，提出RLCT感知的修正方法。

Result: Laplace/BIC误差随(d/2 - λ)log n线性增长，其中d是环境参数维度，λ是RLCT。RLCT感知修正恢复了正确的证据斜率，且对表示相同数据子空间的过完备重参数化具有不变性。

Conclusion: 研究为奇异模型中Laplace失效提供了具体的有限样本特征，证明证据斜率在简单线性设置中可作为有效维度的实用估计器，RLCT修正方法能提高模型选择的准确性。

Abstract: Bayesian model selection commonly relies on Laplace approximation or the Bayesian Information Criterion (BIC), which assume that the effective model dimension equals the number of parameters. Singular learning theory replaces this assumption with the real log canonical threshold (RLCT), an effective dimension that can be strictly smaller in overparameterized or rank-deficient models.
  We study linear-Gaussian rank models and linear subspace (dictionary) models in which the exact marginal likelihood is available in closed form and the RLCT is analytically tractable. In this setting, we show theoretically and empirically that the error of Laplace/BIC grows linearly with (d/2 minus lambda) times log n, where d is the ambient parameter dimension and lambda is the RLCT. An RLCT-aware correction recovers the correct evidence slope and is invariant to overcomplete reparameterizations that represent the same data subspace.
  Our results provide a concrete finite-sample characterization of Laplace failure in singular models and demonstrate that evidence slopes can be used as a practical estimator of effective dimension in simple linear settings.

</details>


### [317] [Fast Gibbs Sampling on Bayesian Hidden Markov Model with Missing Observations](https://arxiv.org/abs/2601.01442)
*Dongrong Li,Tianwei Yu,Xiaodan Fan*

Main category: stat.ML

TL;DR: 提出一种针对含缺失观测的隐马尔可夫模型的折叠吉布斯采样器，通过同时积分掉缺失观测和对应隐状态，在计算效率和采样效率上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现实世界数据中普遍存在缺失观测，这给隐马尔可夫模型的应用带来困难。现有的EM算法和吉布斯采样器存在非凸性、计算复杂度高、混合速度慢等问题

Method: 提出一种折叠吉布斯采样器，通过同时积分掉缺失观测和对应的隐状态，直接从后验分布中采样。该方法具有三个优势：估计精度与现有方法相当；每次迭代能产生更大的有效样本量；当缺失条目多时，每次迭代的计算复杂度显著降低

Result: 数值模拟和实际数据分析表明，该算法在时间复杂度和采样效率（以有效样本量衡量）方面一致优于现有算法

Conclusion: 提出的采样算法在计算和理论上都更快，特别在存在大量缺失条目时优势明显，为处理含缺失观测的隐马尔可夫模型提供了一种高效解决方案

Abstract: The Hidden Markov Model (HMM) is a widely-used statistical model for handling sequential data. However, the presence of missing observations in real-world datasets often complicates the application of the model. The EM algorithm and Gibbs samplers can be used to estimate the model, yet suffering from various problems including non-convexity, high computational complexity and slow mixing. In this paper, we propose a collapsed Gibbs sampler that efficiently samples from HMMs' posterior by integrating out both the missing observations and the corresponding latent states. The proposed sampler is fast due to its three advantages. First, it achieves an estimation accuracy that is comparable to existing methods. Second, it can produce a larger Effective Sample Size (ESS) per iteration, which can be justified theoretically and numerically. Third, when the number of missing entries is large, the sampler has a significant smaller computational complexity per iteration compared to other methods, thus is faster computationally. In summary, the proposed sampling algorithm is fast both computationally and theoretically and is particularly advantageous when there are a lot of missing entries. Finally, empirical evaluations based on numerical simulations and real data analysis demonstrate that the proposed algorithm consistently outperforms existing algorithms in terms of time complexity and sampling efficiency (measured in ESS).

</details>


### [318] [Modeling Information Blackouts in Missing Not-At-Random Time Series Data](https://arxiv.org/abs/2601.01480)
*Aman Sunesh,Allan Ma,Siddarth Nilol*

Main category: stat.ML

TL;DR: 提出一个联合建模交通动态和传感器丢失的潜状态空间框架，通过EM算法学习参数，在真实数据上显示时间动态主导性能，MNAR建模提供原则性改进


<details>
  <summary>Details</summary>
Motivation: 大规模交通预测中固定传感器网络常出现黑屏（连续缺失测量），传统方法假设缺失随机（MAR），但黑屏事件可能与未观测的交通状况相关（如拥堵），需要缺失非随机（MNAR）处理

Method: 提出潜状态空间框架：1）通过线性动态系统建模交通动态；2）通过伯努利观测通道建模传感器丢失，其概率取决于潜交通状态。使用扩展卡尔曼滤波进行推断，通过近似EM算法学习参数

Result: 在西雅图感应线圈数据上，引入潜动态相比基线大幅改进：黑屏插补RMSE从7.02（LOCF）和5.02（线性插值+季节朴素）降至4.23（MAR LDS），MSE相对LOCF减少约64%。MNAR建模提供一致但较小的额外改进（RMSE 4.20；相对MAR减少0.8%）

Conclusion: 时间动态主导性能，MNAR建模提供原则性改进，当缺失确实具有信息性时最有价值。在合成实验中，MNAR优势随缺失对潜状态依赖增强而增加

Abstract: Large-scale traffic forecasting relies on fixed sensor networks that often exhibit blackouts: contiguous intervals of missing measurements caused by detector or communication failures. These outages are typically handled under a Missing At Random (MAR) assumption, even though blackout events may correlate with unobserved traffic conditions (e.g., congestion or anomalous flow), motivating a Missing Not At Random (MNAR) treatment. We propose a latent state-space framework that jointly models (i) traffic dynamics via a linear dynamical system and (ii) sensor dropout via a Bernoulli observation channel whose probability depends on the latent traffic state. Inference uses an Extended Kalman Filter with Rauch-Tung-Striebel smoothing, and parameters are learned via an approximate EM procedure with a dedicated update for detector-specific missingness parameters. On the Seattle inductive loop detector data, introducing latent dynamics yields large gains over naive baselines, reducing blackout imputation RMSE from 7.02 (LOCF) and 5.02 (linear interpolation + seasonal naive) to 4.23 (MAR LDS), corresponding to about a 64% reduction in MSE relative to LOCF. Explicit MNAR modeling provides a consistent but smaller additional improvement on real data (imputation RMSE 4.20; 0.8% RMSE reduction relative to MAR), with similar modest gains for short-horizon post-blackout forecasts (evaluated at 1, 3, and 6 steps). In controlled synthetic experiments, the MNAR advantage increases as the true missingness dependence on latent state strengthens. Overall, temporal dynamics dominate performance, while MNAR modeling offers a principled refinement that becomes most valuable when missingness is genuinely informative.

</details>


### [319] [Variance-Reduced Diffusion Sampling via Conditional Score Expectation Identity](https://arxiv.org/abs/2601.01594)
*Alois Duston,Tan Bui-Thanh*

Main category: stat.ML

TL;DR: 提出条件分数期望(CSE)恒等式，基于此开发SNIS分数估计器，证明与Tweedie估计器的反相关性，推导方差最小化混合估计器，并扩展到贝叶斯逆问题。


<details>
  <summary>Details</summary>
Motivation: 针对仿射扩散过程的分数估计问题，传统方法存在局限性，需要更有效的分数估计器来改善采样质量和计算效率。

Method: 1) 证明CSE恒等式；2) 提出基于SNIS的CSE分数估计器；3) 分析CSE与Tweedie估计器的关系；4) 推导方差最小化的混合估计器；5) 扩展到贝叶斯逆问题。

Result: 混合估计器相比基线方法降低了方差，在固定计算预算下提高了采样质量，在图像重建和PDE逆问题中表现出更好的重建质量和样本多样性。

Conclusion: CSE框架为分数估计提供了理论基础和实用工具，混合估计器在多种应用中展现出优越性能，为扩散模型和逆问题求解提供了新方法。

Abstract: We introduce and prove a \textbf{Conditional Score Expectation (CSE)} identity: an exact relation for the marginal score of affine diffusion processes that links scores across time via a conditional expectation under the forward dynamics. Motivated by this identity, we propose a CSE-based statistical estimator for the score using a Self-Normalized Importance Sampling (SNIS) procedure with prior samples and forward noise. We analyze its relationship to the standard Tweedie estimator, proving anti-correlation for Gaussian targets and establishing the same behavior for general targets in the small time-step regime. Exploiting this structure, we derive a variance-minimizing blended score estimator given by a state--time dependent convex combination of the CSE and Tweedie estimators. Numerical experiments show that this optimal-blending estimator reduces variance and improves sample quality for a fixed computational budget compared to either baseline. We further extend the framework to Bayesian inverse problems via likelihood-informed SNIS weights, and demonstrate improved reconstruction quality and sample diversity on high-dimensional image reconstruction tasks and PDE-governed inverse problems.

</details>


### [320] [Deep Linear Discriminant Analysis Revisited](https://arxiv.org/abs/2601.01619)
*Maxat Tezekbayev,Rustem Takhanov,Arman Bolatov,Zhenisbek Assylbekov*

Main category: stat.ML

TL;DR: 论文发现传统深度线性判别分析（LDA）训练存在病态解问题，而交叉熵训练虽准确但破坏了生成模型结构。作者提出判别式负对数似然（DNLL）损失函数，在保持判别性能的同时恢复生成模型的一致性。


<details>
  <summary>Details</summary>
Motivation: 传统深度LDA的最大似然训练会导致类别均值漂移、协方差坍缩等病态解，而交叉熵训练虽然准确但破坏了生成模型的结构一致性。需要一种既能保持判别性能又能维持生成模型解释性的方法。

Method: 提出判别式负对数似然（DNLL）损失函数，在标准LDA负对数似然基础上增加一个惩罚项，该惩罚项明确阻止多个类别同时具有高概率的区域。DNLL可以解释为标准LDA负对数似然加上一个对混合密度的简单惩罚。

Result: 使用DNLL训练的深度LDA能够产生干净、良好分离的潜在空间，在合成数据和标准图像基准测试中达到与softmax分类器相当的测试准确率，并且产生显著更好的校准预测概率，恢复了深度判别模型的概率解释性。

Conclusion: DNLL损失函数成功调和了生成模型结构与判别性能之间的矛盾，为深度判别模型提供了既准确又具有一致概率解释的解决方案。

Abstract: We show that for unconstrained Deep Linear Discriminant Analysis (LDA) classifiers, maximum-likelihood training admits pathological solutions in which class means drift together, covariances collapse, and the learned representation becomes almost non-discriminative. Conversely, cross-entropy training yields excellent accuracy but decouples the head from the underlying generative model, leading to highly inconsistent parameter estimates. To reconcile generative structure with discriminative performance, we introduce the \emph{Discriminative Negative Log-Likelihood} (DNLL) loss, which augments the LDA log-likelihood with a simple penalty on the mixture density. DNLL can be interpreted as standard LDA NLL plus a term that explicitly discourages regions where several classes are simultaneously likely. Deep LDA trained with DNLL produces clean, well-separated latent spaces, matches the test accuracy of softmax classifiers on synthetic data and standard image benchmarks, and yields substantially better calibrated predictive probabilities, restoring a coherent probabilistic interpretation to deep discriminant models.

</details>


### [321] [Simplex Deep Linear Discriminant Analysis](https://arxiv.org/abs/2601.01679)
*Maxat Tezekbayev,Arman Bolatov,Zhenisbek Assylbekov*

Main category: stat.ML

TL;DR: 重新审视深度线性判别分析，从似然角度出发，发现无约束的深度LDA最大似然训练会导致类别重叠或坍缩，提出几何约束的深度LDA，在潜空间固定类别均值为正单纯形顶点，限制协方差为球形，实现稳定训练和良好类别分离。


<details>
  <summary>Details</summary>
Motivation: 传统LDA是简单的高斯模型，但将LDA头连接到神经编码器后，如何通过最大似然估计训练深度分类器存在疑问。研究发现无约束的深度LDA MLE训练会忽略判别性，导致类别重叠或坍缩，分类性能下降。

Method: 提出约束的深度LDA公式：1) 固定类别均值为潜空间中正单纯形的顶点；2) 限制共享协方差为球形；3) 仅学习先验和单个方差参数以及编码器。在这些几何约束下，MLE变得稳定。

Result: 在图像数据集（Fashion-MNIST、CIFAR-10、CIFAR-100）上，约束深度LDA模型达到与softmax基线相当的准确率，同时提供简单可解释的潜空间几何结构，在二维投影中清晰可见。

Conclusion: 通过几何约束的深度LDA，可以在保持与softmax基线竞争性能的同时，获得结构清晰、可解释的潜空间表示，解决了无约束深度LDA MLE训练中的退化问题。

Abstract: We revisit Deep Linear Discriminant Analysis (Deep LDA) from a likelihood-based perspective. While classical LDA is a simple Gaussian model with linear decision boundaries, attaching an LDA head to a neural encoder raises the question of how to train the resulting deep classifier by maximum likelihood estimation (MLE). We first show that end-to-end MLE training of an unconstrained Deep LDA model ignores discrimination: when both the LDA parameters and the encoder parameters are learned jointly, the likelihood admits a degenerate solution in which some of the class clusters may heavily overlap or even collapse, and classification performance deteriorates. Batchwise moment re-estimation of the LDA parameters does not remove this failure mode. We then propose a constrained Deep LDA formulation that fixes the class means to the vertices of a regular simplex in the latent space and restricts the shared covariance to be spherical, leaving only the priors and a single variance parameter to be learned along with the encoder. Under these geometric constraints, MLE becomes stable and yields well-separated class clusters in the latent space. On images (Fashion-MNIST, CIFAR-10, CIFAR-100), the resulting Deep LDA models achieve accuracy competitive with softmax baselines while offering a simple, interpretable latent geometry that is clearly visible in two-dimensional projections.

</details>


### [322] [Sparse Convex Biclustering](https://arxiv.org/abs/2601.01757)
*Jiakun Jiang,Dewei Xiang,Chenliang Gu,Wei Liu,Binhuan Wang*

Main category: stat.ML

TL;DR: SpaCoBi是一种新的稀疏凸双聚类方法，通过惩罚噪声和采用凸优化框架，显著提高了高维大规模数据集双聚类的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有双聚类方法在处理现代大规模数据集时面临挑战：高维特征中的噪声累积、非凸优化公式的限制以及计算复杂性，导致随着数据集增大而准确性和稳定性下降。

Method: 提出Sparse Convex Biclustering (SpaCoBi)，在双聚类过程中惩罚噪声，采用凸优化框架，并引入基于稳定性的调优准则，实现聚类保真度和稀疏性之间的最优平衡。

Result: 综合数值研究（包括模拟实验和对小鼠嗅球数据的应用）表明，SpaCoBi在准确性方面显著优于最先进的方法。

Conclusion: SpaCoBi为高维大规模数据集的双聚类提供了一个鲁棒且高效的解决方案。

Abstract: Biclustering is an essential unsupervised machine learning technique for simultaneously clustering rows and columns of a data matrix, with widespread applications in genomics, transcriptomics, and other high-dimensional omics data. Despite its importance, existing biclustering methods struggle to meet the demands of modern large-scale datasets. The challenges stem from the accumulation of noise in high-dimensional features, the limitations of non-convex optimization formulations, and the computational complexity of identifying meaningful biclusters. These issues often result in reduced accuracy and stability as the size of the dataset increases. To overcome these challenges, we propose Sparse Convex Biclustering (SpaCoBi), a novel method that penalizes noise during the biclustering process to improve both accuracy and robustness. By adopting a convex optimization framework and introducing a stability-based tuning criterion, SpaCoBi achieves an optimal balance between cluster fidelity and sparsity. Comprehensive numerical studies, including simulations and an application to mouse olfactory bulb data, demonstrate that SpaCoBi significantly outperforms state-of-the-art methods in accuracy. These results highlight SpaCoBi as a robust and efficient solution for biclustering in high-dimensional and large-scale datasets.

</details>


### [323] [A Multilayered Approach to Classifying Customer Responsiveness and Credit Risk](https://arxiv.org/abs/2601.01970)
*Ayomide Afolabi,Ebere Ogburu,Symon Kimitei*

Main category: stat.ML

TL;DR: 该研究评估了三种模型（响应、风险、响应-风险）中不同分类器在信用卡邮件营销和违约预测中的性能，发现Extra Trees在响应模型中召回率最高，Random Forest在风险模型中特异性最佳，在响应-风险多分类模型中准确率最高。


<details>
  <summary>Details</summary>
Motivation: 解决信用卡业务中的两个关键问题：1）识别对邮件营销活动可能响应的客户（响应模型）；2）预测客户违约风险（风险模型）；3）同时考虑响应性和风险性的综合评估（响应-风险模型），以优化信用卡营销策略和风险管理。

Method: 使用多种分类器（包括Extra Trees和Random Forest等）在三个不同模型中进行评估：响应模型（预测客户对信用卡邮件营销的响应）、风险模型（预测客户违约风险）、响应-风险多分类模型（同时考虑响应性和风险性）。通过优化各种性能指标来解决具体的信用风险和邮件响应业务问题。

Result: 1）响应模型中：Extra Trees分类器获得最高召回率79.1%，能有效识别潜在响应者；2）风险模型中：Random Forest分类器特异性达84.1%，能准确识别低违约风险客户；3）响应-风险多分类模型中：Random Forest分类器准确率最高达83.2%，能同时有效识别潜在响应者和低风险用户。

Conclusion: 不同分类器在不同业务场景下表现各异：Extra Trees在识别邮件营销响应者方面表现最佳，Random Forest在风险识别和综合评估中表现最优。研究为信用卡业务提供了针对性的机器学习解决方案，可根据具体业务需求选择合适的分类器模型。

Abstract: This study evaluates the performance of various classifiers in three distinct models: response, risk, and response-risk, concerning credit card mail campaigns and default prediction. In the response model, the Extra Trees classifier demonstrates the highest recall level (79.1%), emphasizing its effectiveness in identifying potential responders to targeted credit card offers. Conversely, in the risk model, the Random Forest classifier exhibits remarkable specificity of 84.1%, crucial for identifying customers least likely to default. Furthermore, in the multi-class response-risk model, the Random Forest classifier achieves the highest accuracy (83.2%), indicating its efficacy in discerning both potential responders to credit card mail campaign and low-risk credit card users. In this study, we optimized various performance metrics to solve a specific credit risk and mail responsiveness business problem.

</details>


### [324] [From Mice to Trains: Amortized Bayesian Inference on Graph Data](https://arxiv.org/abs/2601.02241)
*Svenja Jedhoff,Elizaveta Semenova,Aura Raulo,Anne Meyer,Paul-Christian Bürkner*

Main category: stat.ML

TL;DR: 将摊销贝叶斯推断(ABI)应用于图数据，使用置换不变的图编码器和神经后验估计器进行节点、边和图级参数推断


<details>
  <summary>Details</summary>
Motivation: 图数据在多个领域普遍存在，但图参数的后验估计面临置换不变性、可扩展性和捕获长程依赖等挑战，需要新的推断方法

Method: 采用两模块流水线：1)摘要网络将属性图映射为固定长度表示；2)推断网络近似参数后验分布。评估多种神经架构作为摘要网络

Result: 在受控合成设置和两个真实世界领域（生物学和物流）中评估了多种架构的性能，包括恢复能力和校准度

Conclusion: 将摊销贝叶斯推断适配到图数据是可行的，能够有效处理图参数推断的挑战，为图结构数据的后验估计提供了新方法

Abstract: Graphs arise across diverse domains, from biology and chemistry to social and information networks, as well as in transportation and logistics. Inference on graph-structured data requires methods that are permutation-invariant, scalable across varying sizes and sparsities, and capable of capturing complex long-range dependencies, making posterior estimation on graph parameters particularly challenging. Amortized Bayesian Inference (ABI) is a simulation-based framework that employs generative neural networks to enable fast, likelihood-free posterior inference. We adapt ABI to graph data to address these challenges to perform inference on node-, edge-, and graph-level parameters. Our approach couples permutation-invariant graph encoders with flexible neural posterior estimators in a two-module pipeline: a summary network maps attributed graphs to fixed-length representations, and an inference network approximates the posterior over parameters. In this setting, several neural architectures can serve as the summary network. In this work we evaluate multiple architectures and assess their performance on controlled synthetic settings and two real-world domains - biology and logistics - in terms of recovery and calibration.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [325] [Can Large Language Models Improve Venture Capital Exit Timing After IPO?](https://arxiv.org/abs/2601.00810)
*Mohammadhossien Rashidi*

Main category: q-fin.PM

TL;DR: 使用大语言模型分析IPO后信息，为风险投资提供最优退出时机建议，并与实际退出决策比较收益差异


<details>
  <summary>Details</summary>
Motivation: 现有研究主要描述VC何时退出，而非评估这些选择是否经济最优；同时LLMs在分析复杂金融数据和文本信息方面有潜力，但尚未应用于IPO后退出决策

Method: 引入LLMs框架，分析IPO后月度信息（财务表现、申报文件、新闻、市场信号），生成最优退出时机建议（卖出或继续持有），并与VC实际退出日期比较

Result: 计算LLM建议策略与实际退出策略的收益差异，量化遵循AI指导的收益或损失

Conclusion: 研究评估AI驱动指导是否能改善退出时机，为传统风险投资研究中的风险模型和实物期权模型提供补充

Abstract: Exit timing after an IPO is one of the most consequential decisions for venture capital (VC) investors, yet existing research focuses mainly on describing when VCs exit rather than evaluating whether those choices are economically optimal. Meanwhile, large language models (LLMs) have shown promise in synthesizing complex financial data and textual information but have not been applied to post-IPO exit decisions. This study introduces a framework that uses LLMs to estimate the optimal time for VC exit by analyzing monthly post IPO information financial performance, filings, news, and market signals and recommending whether to sell or continue holding. We compare these LLM generated recommendations with the actual exit dates observed for VCs and compute the return differences between the two strategies. By quantifying gains or losses associated with following the LLM, this study provides evidence on whether AI-driven guidance can improve exit timing and complements traditional hazard and real-options models in venture capital research.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [326] [Gradient-Free Approaches is a Key to an Efficient Interaction with Markovian Stochasticity](https://arxiv.org/abs/2601.01160)
*Boris Prokhorov,Semyon Chebykin,Alexander Gasnikov,Aleksandr Beznosikov*

Main category: math.OC

TL;DR: 提出一种针对马尔可夫噪声随机优化问题的零阶方法，在混合时间小于问题维度时，收敛估计与混合时间无关，证明了零阶方法优于一阶方法


<details>
  <summary>Details</summary>
Motivation: 解决马尔可夫噪声下的随机优化问题，传统一阶方法昂贵，探索零阶方法在特定条件下的优势

Method: 提出新颖的零阶导数自由方法，采用随机批处理方案，适用于强凸光滑和非光滑设置，支持单点和两点反馈

Result: 当噪声序列混合时间小于问题维度时，方法收敛估计与混合时间无关，证明零阶方法优于一阶方法

Conclusion: 在马尔可夫噪声环境下，当混合时间小于维度时，使用零阶方法比一阶方法更高效，并通过下界证明结果最优性

Abstract: This paper deals with stochastic optimization problems involving Markovian noise with a zero-order oracle. We present and analyze a novel derivative-free method for solving such problems in strongly convex smooth and non-smooth settings with both one-point and two-point feedback oracles. Using a randomized batching scheme, we show that when mixing time $τ$ of the underlying noise sequence is less than the dimension of the problem $d$, the convergence estimates of our method do not depend on $τ$. This observation provides an efficient way to interact with Markovian stochasticity: instead of invoking the expensive first-order oracle, one should use the zero-order oracle. Finally, we complement our upper bounds with the corresponding lower bounds. This confirms the optimality of our results.

</details>


### [327] [Scalable Method for Mean Field Control with Kernel Interactions via Random Fourier Features](https://arxiv.org/abs/2601.01175)
*Zhongyuan Cao,Kaustav Das,Nicolas Langrené,Mathieu Laurière*

Main category: math.OC

TL;DR: 提出结合粒子系统模拟与随机傅里叶特征逼近的可扩展均值场控制算法，显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 解决具有核交互的均值场控制问题在大规模群体中的计算复杂度问题，传统方法计算成本过高

Method: 结合粒子系统模拟与随机傅里叶特征逼近，用线性时间估计替代二次成本核评估，实现高效随机梯度下降训练反馈控制

Result: 理论复杂度分析显示计算效率提升，在人群运动和集群行为示例中保持控制性能的同时大幅降低计算成本

Conclusion: 随机特征逼近为高维大规模均值场控制提供了有效实用的工具

Abstract: We develop a scalable algorithm for mean field control problems with kernel interactions by combining particle system simulations with random Fourier feature approximations. The method replaces the quadratic-cost kernel evaluations by linear-time estimates, enabling efficient stochastic gradient descent for training feedback controls in large populations. We provide theoretical complexity bounds and demonstrate through crowd motion and flocking examples that the approach preserves control performance while substantially reducing computational cost. The results indicate that random feature approximations offer an effective and practical tool for high dimensional and large scale mean field control.

</details>


### [328] [Stochastic Control Methods for Optimization](https://arxiv.org/abs/2601.01248)
*Jinniao Qiu*

Main category: math.OC

TL;DR: 该论文提出了一种基于随机控制的全局优化框架，适用于欧几里得空间和Wasserstein概率测度空间，通过正则化控制问题逼近原优化问题，并建立了收敛性理论和数值算法。


<details>
  <summary>Details</summary>
Motivation: 传统全局优化方法在高维或复杂空间（如概率测度空间）中面临困难，需要开发能够处理非凸目标函数且具有理论保证的新方法。

Method: 1. 欧几里得空间：将原最小化问题转化为正则化随机控制问题，利用动态规划分析HJB方程，通过Cole-Hopf变换和Feynman-Kac公式获得可处理表示；2. 概率测度空间：构建正则化平均场控制问题，通过受控N粒子系统近似，建立收敛性理论。

Result: 证明了当正则化参数趋于零时（对于概率测度优化，当粒子数趋于无穷时），控制问题的值收敛到原目标的全局最小值。基于概率表示提出了蒙特卡洛数值方案，数值实验验证了方法的实际性能和理论收敛率。

Conclusion: 该随机控制框架为欧几里得空间和Wasserstein空间中的全局优化问题提供了统一的、理论严谨的解决方案，具有收敛性保证和可行的数值实现。

Abstract: In this work, we investigate a stochastic control framework for global optimization over both finite-dimensional Euclidean spaces and the Wasserstein space of probability measures. In the Euclidean setting, the original minimization problem is approximated by a family of regularized stochastic control problems; using dynamic programming, we analyze the associated Hamilton--Jacobi--Bellman equations and obtain tractable representations via the Cole--Hopf transform and the Feynman--Kac formula. For optimization over probability measures, we formulate a regularized mean-field control problem characterized by a master equation, and further approximate it by controlled $N$-particle systems. We establish that, as the regularization parameter tends to zero (and as the particle number tends to infinity for the optimization over probability measures), the value of the control problem converges to the global minimum of the original objective. Building on the resulting probabilistic representations, Monte Carlo-based numerical schemes are proposed and numerical experiments are reported to illustrate the practical performance of the methods and to support the theoretical convergence rates.

</details>


### [329] [Concave Certificates: Geometric Framework for Distributionally Robust Risk and Complexity Analysis](https://arxiv.org/abs/2601.01311)
*Hong T. M. Chu*

Main category: math.OC

TL;DR: 本文提出了一种基于增长速率函数最小凹主函数的几何框架，用于建立分布鲁棒优化的紧致边界，适用于非Lipschitz和非可微损失函数，并应用于深度学习中的对抗性分析。


<details>
  <summary>Details</summary>
Motivation: 当前分布鲁棒优化的边界认证方法存在局限性：基于全局Lipschitz边界的方法通常过于保守，而基于局部梯度信息的方法仅提供一阶近似。需要一种更精确且适用于非Lipschitz和非可微损失函数的认证框架。

Method: 提出基于增长速率函数最小凹主函数的几何框架，建立凹证书来获得分布鲁棒风险的紧致边界。将该框架扩展到复杂度分析，引入确定性边界补充统计泛化边界。提出对抗性分数作为凹证书的可处理松弛，实现神经网络的高效分层分析。

Result: 凹证书建立了适用于非Lipschitz和非可微损失的紧致DR风险边界。确定性边界补充了统计泛化边界。证明了对抗性和经验Rademacher复杂度之间的间隙可以消除对输入直径、网络宽度和深度的依赖。对抗性分数实现了神经网络的高效分层分析。

Conclusion: 提出的几何框架为分布鲁棒优化提供了紧致且通用的认证方法，特别适用于深度学习中的对抗性分析。该方法超越了传统Lipschitz和梯度方法的局限性，在真实世界数据的分类和回归任务中得到了验证。

Abstract: Distributionally Robust (DR) optimization aims to certify worst-case risk within a Wasserstein uncertainty set. Current certifications typically rely either on global Lipschitz bounds, which are often conservative, or on local gradient information, which provides only a first-order approximation. This paper introduces a novel geometric framework based on the least concave majorants of the growth rate function. Our proposed concave certificate establishes a tight bound of DR risk that remains applicable to non-Lipschitz and non-differentiable losses. We extend this framework to complexity analysis, introducing a deterministic bound that complements standard statistical generalization bound. Furthermore, we utilize this certificate to bound the gap between adversarial and empirical Rademacher complexity, demonstrating that dependencies on input diameter, network width, and depth can be eliminated. For practical application in deep learning, we introduce the adversarial score as a tractable relaxation of the concave certificate that enables efficient and layer-wise analysis of neural networks. We validate our theoretical results in various numerical experiments on classification and regression tasks on real-world data.

</details>


### [330] [On IDA-PBC with Maximum Energy Shapeability](https://arxiv.org/abs/2601.01385)
*Ziheng Jiao,Chengshuai Wu,Bo Fan,Meng Zhang,Romeo Ortega*

Main category: math.OC

TL;DR: 本文提出"最大能量可塑性"概念，用于简化IDA-PBC控制设计中的匹配方程求解问题，使原本复杂的偏微分方程组转化为更易求解的形式。


<details>
  <summary>Details</summary>
Motivation: IDA-PBC是一种成熟的非线性系统稳定化技术，但其应用受到匹配方程求解困难的限制。匹配方程是一组偏微分方程，求解复杂度高，阻碍了IDA-PBC的实际应用。

Method: 引入"最大能量可塑性"概念，当匹配方程的齐次部分具有m个独立解时（m为控制输入维度），可以将匹配方程转化为更易求解的偏微分方程组。提供了最大能量可塑性的充分条件。

Result: 证明了最大能量可塑性能够为IDA-PBC设计提供系统化程序，将匹配方程转化为更易求解的形式。展示了现有的一些构造性IDA-PBC设计实际上隐含地利用了最大能量可塑性。通过磁悬浮系统验证了所提方法的有效性。

Conclusion: 最大能量可塑性概念为IDA-PBC设计提供了新的系统化方法，简化了匹配方程的求解过程，有助于推动IDA-PBC在实际非线性控制系统中的应用。

Abstract: Interconnection and Damping Assignment Passivity-Based Control (IDA-PBC) is a well-established stabilization technique for affine nonlinear systems. However, its application is generally hindered by the requirement of solving a set of partial differential equations (PDEs), i.e., the so-called matching equation. This paper introduces the notion of \emph{maximum energy shapeability} which describes the scenario that the homogeneous part of the matching equation admits $m$ independent solutions with $m$ the dimension of the control input. We demonstrate that the maximum energy shapeability enables a systematic procedure for the IDA-PBC design by transforming the matching equation into a set of easier-to-solve PDEs. Sufficient conditions for maximum energy shapeability are also provided. It is shown that some existing constructive IDA-PBC designs actually implicitly exploit the maximum energy shapeability. The proposed procedure for the IDA-PBC design is illustrated with the magnetic levitation system.

</details>


### [331] [Multiscale replay: A robust algorithm for stochastic variational inequalities with a Markovian buffer](https://arxiv.org/abs/2601.01502)
*Milind Nakul,Tianjiao Li,Ashwin Pananjady*

Main category: math.OC

TL;DR: 提出MER算法，用于解决马尔可夫链生成样本的随机变分不等式问题，通过多尺度采样克服序列采样偏差，加速收敛且无需知道链的混合时间。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，数据通常来自马尔可夫链而非独立同分布，现有方法在序列采样时存在偏差，收敛速度慢，且需要知道链的混合时间参数。

Method: MER算法使用记忆缓冲区存储样本，采用多尺度采样方案而非均匀采样，模拟独立同分布样本下VI算法的行为，克服序列采样偏差。

Result: MER算法在迭代复杂度上实现加速收敛，无需知道马尔可夫链的混合时间，比标准样本跳过方法更鲁棒，适用于策略评估和广义线性模型训练。

Conclusion: MER算法有效解决了依赖数据下的随机变分不等式问题，通过多尺度采样克服序列采样偏差，在无需混合时间知识的情况下加速收敛，具有实际应用价值。

Abstract: We introduce the Multiscale Experience Replay (MER) algorithm for solving a class of stochastic variational inequalities (VIs) in settings where samples are generated from a Markov chain and we have access to a memory buffer to store them. Rather than uniformly sampling from the buffer, MER utilizes a multi-scale sampling scheme to emulate the behavior of VI algorithms designed for independent and identically distributed samples, overcoming bias in the de facto serial scheme and thereby accelerating convergence. Notably, unlike standard sample-skipping variants of serial algorithms, MER is robust in that it achieves this acceleration in iteration complexity whenever possible, and without requiring knowledge of the mixing time of the Markov chain. We also discuss applications of MER, particularly in policy evaluation with temporal difference learning and in training generalized linear models with dependent data.

</details>


### [332] [Lyapunov Functions can Exactly Quantify Rate Performance of Nonlinear Differential Equations](https://arxiv.org/abs/2601.01538)
*Declan S. Jagt,Matthew M. Peet*

Main category: math.OC

TL;DR: 论文提出了ODE系统点对点稳定性性能的广义概念，建立了与各种衰减率（指数、有理、有限时间）对应的Lyapunov条件，并通过SOS优化实现紧致性能边界估计。


<details>
  <summary>Details</summary>
Motivation: 现有Lyapunov方法虽然能检验指数、有理和有限时间稳定性，但无法确定这些方法是否能提供准确的系统性能边界。需要建立更精确的性能量化框架。

Method: 提出广义衰减率性能概念，建立与各种衰减率对应的必要充分Lyapunov条件，并通过Sum-of-Squares（SOS）编程实现指数、有理和有限时间稳定性的条件验证。

Result: 数值实验表明，所提出的SOS测试能够获得紧致的性能率边界，并在相关性能区域内提供准确的内界估计。

Conclusion: 该框架为ODE系统性能量化提供了统一的Lyapunov方法，能够准确估计各种衰减率下的系统性能边界，具有理论和实用价值。

Abstract: Pointwise-in-time stability notions for Ordinary Differential Equations (ODEs) provide quantitative metrics for system performance by establishing bounds on the rate of decay of the system state in terms of initial condition -- allowing stability to be quantified by e.g. the maximum provable decay rate. Such bounds may be obtained by finding suitable Lyapunov functions using, e.g. Sum-of-Squares (SOS) optimization. While Lyapunov tests have been proposed for numerous pointwise-in-time stability notions, including exponential, rational, and finite-time stability, it is unclear whether these characterizations are able to provide accurate bounds on system performance.
  In this paper, we start by proposing a generalized notion of rate performance -- with exponential, rational, and finite-time decay rates being special cases. Then, for any such notion and rate, we associate a Lyapunov condition which is shown to be necessary and sufficient for a system to achieve that rate. Finally, we show how the proposed conditions can be enforced using SOS programming in the case of exponential, rational, and finite-time stability. Numerical examples in each case demonstrate that the corresponding SOS test can achieve tight bounds on the rate performance with accurate inner bounds on the associated regions of performance.

</details>


### [333] [A MINRES-based Linesearch Algorithm for Nonconvex Optimization with Non-positive Curvature Detection](https://arxiv.org/abs/2601.01575)
*Hanfeng Zeng,Yang Liu,Wenqing Ouyang,Andre Milzarek*

Main category: math.OC

TL;DR: 提出基于MINRES的牛顿型算法求解非凸优化问题，利用MINRES计算包含二阶和非正曲率信息的下降方向，能够避开鞍点收敛到二阶临界点，在Polyak-Łojasiewicz条件下实现超线性收敛。


<details>
  <summary>Details</summary>
Motivation: 传统优化算法在处理非凸问题时可能陷入鞍点，需要开发能够利用二阶曲率信息并避开鞍点的高效算法。

Method: 使用最小残差法(MINRES)求解不定对称线性系统，计算包含二阶和非正曲率信息的下降方向，结合正则化技术和前向线搜索机制。

Result: 在Kurdyka-Łojasiewicz不等式和NPC可检测条件下，算法能避开严格鞍点收敛到二阶临界点；在Polyak-Łojasiewicz条件下实现局部超线性收敛；在CUTEst测试集和深度自编码器问题上验证了效率。

Conclusion: 提出的MINRES-based牛顿型算法能有效处理非凸优化问题，避开鞍点并收敛到二阶临界点，在适当条件下实现快速收敛，具有实际应用价值。

Abstract: We propose a MINRES-based Newton-type algorithm for solving unconstrained nonconvex optimization problems. Our approach uses the minimal residual method (MINRES), a well-known solver for indefinite symmetric linear systems, to compute descent directions that leverage second-order and non-positive curvature (NPC) information. Comprehensive asymptotic convergence properties are derived under standard assumptions. In particular, under the Kurdyka-Łojasiewicz inequality and a mild NPC-detectability condition, we prove that our algorithm can avoid strict saddle points and converge to second-order critical points. This is primarily achieved by integrating proper regularization techniques and forward linesearch mechanisms along NPC directions. Furthermore, fast local superlinear convergence to potentially non-isolated minima is established, when the local Polyak-Łojasiewicz condition is satisfied. Numerical experiments on the CUTEst test collection and on a deep auto-encoder problem illustrate the efficiency of the proposed method.

</details>


### [334] [Optimization problems for elliptic PDEs](https://arxiv.org/abs/2601.01591)
*Giuseppe Buttazzo,Juan Casado-Díaz,Faustino Maestre*

Main category: math.OC

TL;DR: 该论文研究由椭圆偏微分方程控制的最优控制问题，其中状态变量是PDE解，控制变量可以是PDE系数、势函数或右端项，成本泛函为涉及状态和控制变量的积分形式。


<details>
  <summary>Details</summary>
Motivation: 研究椭圆偏微分方程控制的最优控制问题，这类问题在工程、物理和数学中具有广泛应用，如结构优化、热传导控制等，需要系统分析不同控制变量类型下的最优控制理论框架。

Method: 采用椭圆偏微分方程作为控制方程，状态变量为PDE解，控制变量可以是PDE系数、势函数或右端项，成本泛函采用积分形式同时考虑状态和控制变量，通过变分法和优化理论求解最优控制问题。

Result: 论文建立了椭圆PDE最优控制问题的数学框架，分析了不同控制变量类型下的最优性条件，为这类问题的理论分析和数值计算提供了基础。

Conclusion: 椭圆偏微分方程最优控制问题具有重要的理论和应用价值，建立的数学框架为后续研究提供了基础，不同控制变量类型需要不同的分析方法和数值技术。

Abstract: In this paper we consider some optimal control problems governed by elliptic partial differential equations. The solution is the state variable, while the control variable is, depending on the case, the coefficient of the PDE, the potential, the right-hand side. The cost functional is of integral type and involves both the state and control variables.

</details>


### [335] [A Three-Tier Time-Scale Architecture for Controlling Complex Nonlinear Systems](https://arxiv.org/abs/2601.01621)
*Vyacheslav Kungurtsev*

Main category: math.OC

TL;DR: 提出三层计算架构用于非线性复杂系统的实时控制，解决现有单层和双层方法在系统知识、计算复杂度、模型保真度和不确定性方面的不足


<details>
  <summary>Details</summary>
Motivation: 现有单层和双层时间尺度方法对于一类重要的非线性复杂系统（如时间依赖PDE）控制问题存在根本性不足，包括缺乏先验系统知识、计算复杂度高、模型保真度要求和不确定性等问题

Method: 提出三层计算架构：离线层、中尺度层和实时层，每层有明确角色和特定的信息流，形成系统级范式

Result: 建立了一个实用的系统级范式，能够实现复杂非线性控制问题的实时操作

Conclusion: 三层计算架构为解决非线性复杂系统的实时控制问题提供了有效的解决方案，克服了现有方法的局限性

Abstract: This letter proposes a three-tier computational architecture for the real-time control of nonlinear complex systems, such as time-dependent PDEs. There is an important class of such problems for which existing single- and two-time-scale approaches are fundamentally insufficient due to lack of a priori system knowledge, computational complexity, model fidelity requirements, and uncertainty. The proposed architecture consists of an offline, meso-scale, and real-time layer of computation, with distinct roles for each layer and specific information flow between them. The result is a practical systems-level paradigm that enables real-time operation of complex nonlinear control problems.

</details>


### [336] [Stochastic Maximum Principles and Linear-Quadratic Optimal Control Problems for Fractional Backward Stochastic Evolution Equations in Hilbert Spaces](https://arxiv.org/abs/2601.01631)
*Javad A. Asadzade,Nazim I. Mahmudov*

Main category: math.OC

TL;DR: 本文为希尔伯特空间中分数阶后向随机演化方程(FBSEE)系统的最优控制建立了完整框架，包括随机最大值原理和线性二次最优控制问题的显式解。


<details>
  <summary>Details</summary>
Motivation: 将分数阶微积分与随机控制理论结合，为具有记忆和长程依赖性的无限维系统控制提供严格理论基础。

Method: 1. 引入尖峰变分，推导变分方程的精确估计；2. 构建适应分数阶动力学的伴随过程；3. 建立随机最大值原理作为最优性的必要条件；4. 将该原理应用于线性二次最优控制问题，获得显式解。

Result: 1. 建立了分数阶后向随机演化方程的随机最大值原理；2. 获得了线性二次最优控制问题的闭式解；3. 最优控制由耦合的分数阶前向-后向随机方程系统描述。

Conclusion: 该工作成功将分数阶微积分与随机控制理论结合，为具有记忆和长程依赖性的无限维系统控制提供了严格的数学框架和实用工具。

Abstract: This paper develops a comprehensive framework for optimal control of systems governed by fractional backward stochastic evolution equations (FBSEEs) in Hilbert spaces. We first establish a stochastic maximum principle (SMP) as a necessary condition for optimality. This is achieved by introducing spike variations, deriving precise estimates for the associated variational equations, and constructing an adjoint process tailored to the fractional dynamics. Subsequently, we apply this general principle to solve the linear-quadratic (LQ) optimal control problem explicitly. The resulting optimal control is characterized in closed form via the adjoint process and is shown to be governed by a system of coupled fractional forward-backward stochastic equations. Our work bridges fractional calculus with stochastic control theory, providing a rigorous foundation for controlling infinite-dimensional systems with memory and long-range dependencies.

</details>


### [337] [Boundary control systems on a one-dimension spatial domain](https://arxiv.org/abs/2601.01634)
*Bouchra Elghazi,Birgit Jacob,Hans Zwart*

Main category: math.OC

TL;DR: 研究一维边界控制观测系统的适定性，给出充要条件，并证明适定性+完全控制观测可推出精确可控可观性，以欧拉-伯努利梁模型为例说明。


<details>
  <summary>Details</summary>
Motivation: 研究一维空间域上边界控制与观测系统的适定性问题，为这类系统的理论分析提供基础框架。

Method: 推导边界控制观测系统适定性的充要条件，建立适定性与完全控制观测到精确可控可观性的理论关系。

Result: 获得了适定性的充要条件，证明了适定性加完全控制观测可推出精确可控性和精确可观性。

Conclusion: 为一维边界控制观测系统建立了完整的适定性理论框架，并以欧拉-伯努利梁模型验证了理论结果的有效性。

Abstract: The aim of this paper is to investigate the well-posedness of a class of boundary control and observation systems on a one dimensional spatial domain. We derive a necessary and sufficient condition characterizing the well-posedness of these systems. Furthermore, we show that the well-posedness and full control and observation implies exact controllability and exact observability. The theoretical results are illustrated using Euler-Bernoulli beam models.

</details>


### [338] [Adaptive Thrust Regulation in Solid-fuel Ramjet with Variable Geometry Inlet](https://arxiv.org/abs/2601.01683)
*Parham Oveissi,Ryan DeBoskey,Venkateswaran Narayanaswamy,Ankit Goel*

Main category: math.OC

TL;DR: 提出动态模态自适应控制(DMAC)方法，用于固体燃料冲压发动机(SFRJ)的推力调节，无需系统解析模型，通过可变几何进气道实现推力控制。


<details>
  <summary>Details</summary>
Motivation: 固体燃料冲压发动机推力调节具有挑战性，传统方法需要精确的解析模型。本文旨在开发一种无需解析模型的数据驱动自适应控制方法，实现SFRJ的推力精确调节。

Method: 1) 建立SFRJ准静态一维模型计算推力；2) 采用动态模态自适应控制(DMAC)框架：利用动态模态分解近似局部系统行为，基于识别模型设计跟踪控制器；3) 以可变几何进气道作为控制输入。

Result: 仿真结果表明，DMAC能在多种指令曲线和操作条件下实现精确的推力调节，无需SFRJ的解析模型，验证了该方法的可靠性和有效性。

Conclusion: DMAC为具有可变几何进气道的SFRJ提供了一种可靠有效的无模型推力调节方法，展示了数据驱动自适应控制在复杂推进系统控制中的潜力。

Abstract: This paper presents the application of a novel data-driven adaptive control technique, dynamic mode adaptive control (DMAC), to regulate thrust in a solid-fuel ramjet (SFRJ). A quasi-static one-dimensional model of SFRJ with a variable geometry inlet is developed to compute thrust. An adaptive tracking controller is then designed using the DMAC framework, which leverages dynamic mode decomposition to approximate the local system behavior, followed by a tracking controller designed around the identified model. Simulation results demonstrate that DMAC achieves accurate thrust regulation across a range of commanded profiles and operating conditions, without requiring an analytical model of the SFRJ. These findings indicate that DMAC provides a reliable and effective approach for model-free thrust regulation in an SFRJ with variable-geometry inlets as the control input.

</details>


### [339] [Asymptotic Convergence and Stability of Adaptive Gradient Methods in Smooth Non-convex Optimization](https://arxiv.org/abs/2601.01853)
*Ruinan Jin,Xiaoyu Wang*

Main category: math.OC

TL;DR: 首次对AdaGrad-Norm在非凸优化中的渐近收敛性进行了严格分析，证明了其稳定性和收敛性，并将结果扩展到RMSProp。


<details>
  <summary>Details</summary>
Motivation: 尽管自适应梯度方法如AdaGrad在深度学习中广泛应用，但在非凸场景下的渐近收敛性仍缺乏深入理解，需要建立严格的理论基础。

Method: 采用新颖的停时分割技术，建立稳定性结果：在温和的强制性假设下，目标函数值在期望上有界，迭代点几乎必然有界。基于这些稳定性结果，证明AdaGrad-Norm的收敛性。

Result: 证明了AdaGrad-Norm在光滑非凸优化中具有几乎必然收敛和均方收敛。进一步将分析扩展到RMSProp，表明在适当的超参数选择下，RMSProp也具有稳定性和渐近收敛性。

Conclusion: 首次为AdaGrad-Norm在非凸优化中的渐近收敛性提供了严格理论保证，所开发的技术对分析其他自适应随机优化算法具有独立价值。

Abstract: Adaptive gradient methods, such as AdaGrad, have become fundamental tools in deep learning. Despite their widespread use, the asymptotic convergence of AdaGrad remains poorly understood in non-convex scenarios. In this work, we present the first rigorous asymptotic convergence analysis of AdaGrad-Norm for smooth non-convex optimization. Using a novel stopping-time partitioning technique, we establish a key stability result: the objective function values remain bounded in expectation, and the iterates are bounded almost surely under a mild coercivity assumption. Building on these stability results, we prove that AdaGrad-Norm achieves both almost sure and mean-square convergence. Furthermore, we extend our analysis to RMSProp and show that, with appropriate hyperparameter choices, it also enjoys stability and asymptotic convergence. The techniques developed herein may be of independent interest for analyzing other adaptive stochastic optimization algorithms.

</details>


### [340] [A Perturbed DCA for Computing d-Stationary Points of Nonsmooth DC Programs](https://arxiv.org/abs/2601.02084)
*Zhangcheng Feng,Yancheng Yuan*

Main category: math.OC

TL;DR: 提出了一种高效的扰动DC算法(pDCA)，用于计算一类重要非光滑DC问题的d-稳定点，相比现有算法计算成本更低，只需解决单个子问题。


<details>
  <summary>Details</summary>
Motivation: 现有的DC算法如Pang等人提出的方法需要解决多个子问题来完成一步更新，计算成本较高。需要开发更高效的算法来计算非光滑DC问题的d-稳定点。

Method: 提出扰动DC算法(pDCA)，该算法只需解决单个子问题即可完成一步更新，计算成本与广泛使用的DCA相当。在实用假设下证明算法生成的序列的每个聚点几乎必然为d-稳定点。

Result: pDCA在多个重要非光滑DC程序示例上的数值实验结果表明，该算法能高效计算d-稳定点。

Conclusion: pDCA是一种计算非光滑DC问题d-稳定点的高效算法，相比现有方法计算成本更低，且具有理论保证。

Abstract: This paper introduces an efficient perturbed difference-of-convex algorithm (pDCA) for computing d-stationary points of an important class of structured nonsmooth difference-of-convex problems. Compared to the principal algorithms introduced in [J.-S. Pang, M. Razaviyayn, and A. Alvarado, Math. Oper. Res. 42(1):95--118 (2017)], which may require solving several subproblems for a one-step update, pDCA only requires solving a single subproblem. Therefore, the computational cost of pDCA for one-step update is comparable to the widely used difference-of-convex algorithm (DCA) introduced in [D. T. Pham and H. A. Le Thi, Acta Math. Vietnam. 22(1):289--355 (1997)] for computing a critical point. Importantly, under practical assumptions, we prove that every accumulation point of the sequence generated by pDCA is a d-stationary point almost surely. Numerical experiment results on several important examples of nonsmooth DC programs demonstrate the efficiency of pDCA for computing d-stationary points.

</details>


### [341] [Complexity of quadratic penalty methods with adaptive accuracy under a PL condition for the constraints](https://arxiv.org/abs/2601.02134)
*Florentin Goyens,Geovani N. Grapiglia*

Main category: math.OC

TL;DR: 本文研究了带等式约束的非凸优化问题的二次罚函数法，在约束违反满足PL条件时，推导了更尖锐的最坏情况复杂度界，并提出了自适应停止准则。


<details>
  <summary>Details</summary>
Motivation: 对于带等式约束的非凸优化问题，二次罚函数法是常用方法，但现有理论分析在复杂度界方面不够精确。特别是在约束违反满足Polyak-Łojasiewicz条件时，需要更精确的复杂度分析来指导算法设计。

Method: 采用二次罚函数法，假设约束违反在可行集附近满足PL条件。针对一阶和二阶可微性假设，分别设计相应的一阶和二阶内层求解器。提出自适应、可行性感知的子问题停止准则，在远离可行性时放宽平稳性容忍度。

Result: 当目标和约束二阶连续可微时，QPM配合一阶内层求解器最多需要O(ε₀⁻¹ε₁⁻²)次一阶oracle调用找到(ε₀,ε₁)-近似KKT点。当三阶连续可微时，配合二阶内层求解器最多需要O(ε₀⁻¹/²ε₁⁻³/²)次二阶oracle调用。自适应停止准则在保持理论保证的同时显著减少计算量。

Conclusion: 二次罚函数法在约束违反满足PL条件时具有优越的复杂度界，提出的自适应停止准则能有效平衡可行性和平稳性要求，为实际计算带来显著效率提升。

Abstract: We study the quadratic penalty method (QPM) for smooth nonconvex optimization problems with equality constraints. Assuming the constraint violation satisfies the PL condition near the feasible set, we derive sharper worst-case complexity bounds for obtaining approximate first-order KKT points. When the objective and constraints are twice continuously differentiable, we show that QPM equipped with a suitable first-order inner solver requires at most $O(\varepsilon_{0}^{-1}\varepsilon_{1}^{-2})$ first-order oracle calls to find an $(\varepsilon_{0},\varepsilon_{1})$-approximate KKT point -- that is, a point that is $\varepsilon_{0}$-approximately feasible and $\varepsilon_{1}$-approximately stationary. Furthermore, when the objective and constraints are three times continuously differentiable, we show that QPM with a suitable second-order inner solver requires at most $O\left(\varepsilon_{0}^{-1/2}\varepsilon_{1}^{-3/2}\right)$ second-order oracle calls to find an $(\varepsilon_{0},\varepsilon_{1})$-approximate KKT point. We also introduce an adaptive, feasibility-aware stopping criterion for the subproblems, which relaxes the stationarity tolerance when far from feasibility. This rule preserves all theoretical guarantees while substantially reducing computational effort in practice.

</details>


### [342] [Risk-Averse Markov Decision Processes: Applications to Electricity Grid and Reservoir Management](https://arxiv.org/abs/2601.02207)
*Arash Khojaste,Jonathan Pearce,Daniela Pucci de Farias,Geoffrey Pritchard,Golbon Zakeri*

Main category: math.OC

TL;DR: 该论文开发了风险规避模型，用于在可再生能源发电不确定性下支持电网规划与运行，结合CVaR金融风险对冲和MDP框架，并提出高效精确的求解方法。


<details>
  <summary>Details</summary>
Motivation: 可再生能源发电的不确定性给电网规划与运行带来挑战，需要开发风险规避模型来帮助系统运营商在不确定性环境下做出决策，同时考虑财务风险对冲。

Method: 将条件风险价值(CVaR)金融风险对冲方法整合到马尔可夫决策过程(MDP)框架中，提出高效精确的求解方法，并引入面向电力可靠性的风险度量。

Result: 提出了新的计算高效的风险规避电网规划与运行模型，能够有效处理可再生能源不确定性，为系统运营商提供风险规避决策支持。

Conclusion: 该研究为电网在可再生能源不确定性下的风险规避规划与运行提供了有效的建模框架和求解方法，结合了金融风险管理和电力可靠性需求。

Abstract: This paper develops risk-averse models to support system operators in planning and operating the electricity grid under uncertainty from renewable power generation. We incorporate financial risk hedging using conditional value at risk (CVaR) within a Markov Decision Process (MDP) framework and propose efficient, exact solution methods for these models. In addition, we introduce a power reliability-oriented risk measure and present new, computationally efficient models for risk-averse grid planning and operations.

</details>


### [343] [Extended real number arithmetics via Dedekind cuts](https://arxiv.org/abs/2601.02229)
*Andreas H Hamel*

Main category: math.OC

TL;DR: 使用戴德金分割引入扩展实数，通过集合加法规则建立算术定律，利用分割的下部和上部分别定义inf-加法和sup-加法，形成共线性空间和完备格结构，支持伪差运算处理无穷表达式。


<details>
  <summary>Details</summary>
Motivation: 主要动机有两个：一是统一处理正常和异常的扩展实值函数；二是集合值函数通常可以用包含异常函数的标量函数族来表示。

Method: 使用戴德金分割引入扩展实数，通过一个简单的集合加法规则建立算术定律。关键思想是利用分割的下部和上部分别定义两种不同的加法（inf-加法和sup-加法），形成共线性空间和完备格结构，从而定义伪差运算。

Result: 建立了扩展实数的算术系统，能够处理如$(+\infty) - (+\infty)$和$(-\infty) - (-\infty)$等表达式，形成了共线性空间和完备格结构。

Conclusion: 戴德金分割方法为扩展实数提供了统一的算术框架，支持处理无穷表达式，为函数分析和集合值函数表示提供了理论基础。

Abstract: It is shown how Dedekind cuts can be used to introduce the extended real numbers along with sound arithmetic laws via one simple rule for the addition of sets. The crucial idea is that the use of the lower and the upper part of the cuts, respectively, leads to two different additions which are known in the literature as inf-addition and sup-addition. Moreover, the two resulting structures are conlinear spaces which at the same time are complete lattices with respect to the natural order. This admits the definition of pseudo-differences on the extended reals which also provide formulas for expressions like $(+\infty) - (+\infty)$, $(-\infty) - (-\infty)$. There are two major motivations: one is that proper and improper extended real-valued functions can be treated in a unified manner, the other that set-valued functions can often be represented by families of scalar functions which may include improper ones.

</details>


### [344] [Solving Matrix Games with Even Fewer Matrix-Vector Products](https://arxiv.org/abs/2601.02347)
*Ishani Karmarkar,Liam O'Carroll,Aaron Sidford*

Main category: math.OC

TL;DR: 本文提出了在两种约束条件下计算双人零和博弈ε-近似纳什均衡的改进算法，将复杂度从之前的$\tilde{O}(ε^{-8/9})$和$\tilde{O}(ε^{-7/9})$提升到$\tilde{O}(ε^{-2/3})$。


<details>
  <summary>Details</summary>
Motivation: 研究双人零和博弈中ε-近似纳什均衡的计算问题，特别是在策略空间受约束的情况下。现有算法复杂度较高，需要改进以匹配理论下界。

Method: 针对两种特定约束情况：ℓ₁-ℓ₁博弈（双方策略都在概率单纯形中）和ℓ₂-ℓ₁博弈（一方在单位欧几里得球中，另一方在概率单纯形中），设计了新的算法。这些算法通过优化矩阵向量乘法（matvecs）的复杂度来实现改进。

Result: 将ℓ₁-ℓ₁博弈的复杂度从$\tilde{O}(ε^{-8/9})$提升到$\tilde{O}(ε^{-2/3})$，将ℓ₂-ℓ₁博弈的复杂度从$\tilde{O}(ε^{-7/9})$提升到$\tilde{O}(ε^{-2/3})$。特别地，ℓ₂-ℓ₁博弈的结果（对应硬间隔支持向量机）与[KS '25]的下界匹配（忽略对数因子）。

Conclusion: 本文显著改进了约束零和博弈中近似纳什均衡的计算复杂度，达到了接近理论最优的$\tilde{O}(ε^{-2/3})$复杂度，特别在硬间隔SVM问题上匹配了下界。

Abstract: We study the problem of computing an $ε$-approximate Nash equilibrium of a two-player, bilinear, zero-sum game with a bounded payoff matrix $A \in \mathbb{R}^{m \times n}$, when the players' strategies are constrained to lie in simple sets. We provide algorithms which solve this problem in $\tilde{O}(ε^{-2/3})$ matrix-vector multiplies (matvecs) in two well-studied cases: $\ell_1$-$\ell_1$ games, where the players' strategies are both in the probability simplex, and $\ell_2$-$\ell_1$ games, where the players' strategies are in the unit Euclidean ball and probability simplex respectively. These results improve upon the previous state-of-the-art complexities of $\tilde{O}(ε^{-8/9})$ for $\ell_1$-$\ell_1$ and of $\tilde{O}(ε^{-7/9})$ for $\ell_2$-$\ell_1$ due to [KOS '25]. In particular, our result for $\ell_2$-$\ell_1$, which corresponds to hard-margin support vector machines (SVMs), matches the lower bound of [KS '25] up to polylogarithmic factors.

</details>
